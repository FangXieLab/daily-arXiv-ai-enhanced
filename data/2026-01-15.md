<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 2]
- [math.ST](#math.ST) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings](https://arxiv.org/abs/2601.09019)
*Nawaf Bou-Rabee,Siddharth Mitra,Andre Wibisono*

Main category: stat.ML

TL;DR: 该论文提出了一种将哈密顿蒙特卡洛（HMC）算法的Wasserstein收敛保证提升到KL散度和Rényi散度保证的框架，用于量化相对密度不匹配。


<details>
  <summary>Details</summary>
Motivation: 尽管HMC算法在高维设置中被广泛使用，但其在量化相对密度不匹配的散度（如KL散度和Rényi散度）中的收敛性质研究不足。这些散度自然地控制着Metropolis调整马尔可夫链的接受概率和预热启动要求。

Method: 基于单次耦合方法，建立了未调整哈密顿蒙特卡洛（uHMC）转移核的正则化性质。该正则化允许将Wasserstein-2混合时间和渐近偏差界限提升到KL散度，并将类似的Orlicz-Wasserstein界限提升到Rényi散度。

Result: 该框架提供了相对密度不匹配的定量控制，阐明了离散化偏差在强散度中的作用，并为未调整采样和生成Metropolis调整马尔可夫链的预热启动提供了原则性保证。

Conclusion: 该研究通过建立uHMC的正则化性质，成功将Wasserstein收敛保证提升到KL和Rényi散度，为HMC算法在相对密度度量下的收敛分析提供了新的理论框架。

Abstract: Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains.

</details>


### [2] [Horseshoe Mixtures-of-Experts (HS-MoE)](https://arxiv.org/abs/2601.09043)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: HS-MoE模型结合马蹄先验与输入依赖门控，实现专家选择的数据自适应稀疏性，提出粒子学习算法进行序列推断，并与大语言模型中的MoE层建立联系


<details>
  <summary>Details</summary>
Motivation: 为混合专家架构提供贝叶斯稀疏专家选择框架，将马蹄先验的自适应全局-局部收缩与输入依赖门控相结合，实现数据自适应的专家使用稀疏性

Method: 提出粒子学习算法进行序列推断，滤波器随时间向前传播，仅跟踪充分统计量；结合马蹄先验的全局-局部收缩特性与输入依赖门控机制

Result: 建立了HS-MoE模型框架，实现了数据自适应的专家选择稀疏性，为序列推断提供了高效的粒子学习算法

Conclusion: HS-MoE为混合专家架构提供了有效的贝叶斯稀疏选择方法，其粒子学习算法支持序列推断，并与现代大语言模型中的MoE层在极端稀疏约束下具有相关性

Abstract: Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool).

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [3] [Global polynomial-time estimation in statistical nonlinear inverse problems via generalized stability](https://arxiv.org/abs/2601.09007)
*Sven Wang*

Main category: math.ST

TL;DR: 提出针对非线性统计逆问题的可计算估计器，通过弱化PDE约束实现条件凸优化，在椭圆PDE逆问题中达到最优统计收敛率且具有多项式时间计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 非线性统计逆问题在统计分析和计算上都面临重大挑战：基于似然的估计器通常导致非凸且可能多峰的优化景观，而MCMC方法可能混合速度极慢。需要开发既具有统计最优性又可在多项式时间内计算的估计方法。

Method: 提出两类计算可行的估计器：插件估计器和PDE惩罚M估计器。核心思想是用弱强制松弛替代精确的PDE约束，产生条件凸优化问题，避免评估前向映射G(f)且不需要PDE求解器。对于椭圆PDE逆问题（如达西流模型和稳态薛定谔模型），证明这些估计器具有最优统计收敛率。

Result: 对于达西流模型等椭圆PDE逆问题，证明所提估计器达到当前已知的最佳统计收敛率，同时可在多项式时间内全局计算。在达西模型中，从N个噪声样本估计f获得了显式的亚二次o(N²)算术运行时间界。还推导了达西问题的自适应收敛率。

Conclusion: 提出的估计器为广泛非线性逆问题提供了可证明多项式时间的统计算法设计蓝图。这些估计器还为多项式时间贝叶斯计算提供了原则性的热启动初始化方法，解决了非线性逆问题中统计最优性与计算可行性之间的权衡问题。

Abstract: Non-linear statistical inverse problems pose major challenges both for statistical analysis and computation. Likelihood-based estimators typically lead to non-convex and possibly multimodal optimization landscapes, and Markov chain Monte Carlo (MCMC) methods may mix exponentially slowly. We propose a class of computationally tractable estimators--plug-in and PDE-penalized M-estimators--for inverse problems defined through operator equations of the form $L_f u = g$, where $f$ is the unknown parameter and $u$ is the observed solution. The key idea is to replace the exact PDE constraint by a weakly enforced relaxation, yielding conditionally convex and, in many PDE examples, nested quadratic optimization problems that avoid evaluating the forward map $G(f)$ and do not require PDE solvers. For prototypical non-linear inverse problems arising from elliptic PDEs, including the Darcy flow model $L_f u = \nabla\!\cdot(f\nabla u)$ and a steady-state Schrödinger model, we prove that these estimators attain the best currently known statistical convergence rates while being globally computable in polynomial time. In the Darcy model, we obtain an explicit sub-quadratic $o(N^2)$ arithmetic runtime bound for estimating $f$ from $N$ noisy samples. Our analysis is based on new generalized stability estimates, extending classical stability beyond the range of the forward operator, combined with tools from nonparametric M-estimation. We also derive adaptive rates for the Darcy problem, providing a blueprint for designing provably polynomial-time statistical algorithms for a broad class of non-linear inverse problems. Our estimators also provide principled warm-start initializations for polynomial-time Bayesian computation.

</details>


### [4] [Stochastic representation of Sarmanov copulas](https://arxiv.org/abs/2601.09016)
*Christopher Blier-Wong*

Main category: math.ST

TL;DR: 该论文提出了Sarmanov copulas的随机表示方法，解决了高维应用中参数约束验证困难的问题，并扩展了该类copula的适用范围。


<details>
  <summary>Details</summary>
Motivation: Sarmanov copulas虽然具有简单易处理的特点，但在高维应用中，确保copula有效的参数约束条件复杂且难以验证，这限制了其实际应用。

Method: 1. 为二元Sarmanov copulas开发随机表示，证明每个可容许的Sarmanov copula都可以表示为由潜在伯努利对索引的独立单变量分布的混合；2. 引入高维伯努利混合构造，创建新的多元Sarmanov copulas类；3. 推导Spearman's rho和Kendall's tau的全局边界；4. 证明二元Sarmanov copulas的幂次版本通过块极大顺序统计量具有类似的随机表示。

Result: 1. 将copula有效性验证问题转化为伯努利概率质量函数的非负性验证问题；2. 恢复了包括Farlie-Gumbel-Morgenstern、Huang-Kotz和Bairamov-Kotz-Bekçi在内的经典copula族；3. 建立了具有易于验证参数约束和可扩展模拟算法的新多元Sarmanov copulas类。

Conclusion: 提出的随机表示方法解决了Sarmanov copulas在高维应用中的验证难题，扩展了其适用范围，并为实际应用提供了更简便的构造和模拟方法。

Abstract: Sarmanov copulas offer a simple and tractable way to build multivariate distributions by perturbing the independence copula. They admit closed-form expressions for densities and many functionals of interest, making them attractive for practical applications. However, the complex conditions on the dependence parameters to ensure that Sarmanov copulas are valid limit their application in high dimensions. Verifying the $d$-increasing property typically requires satisfying a combinatorial set of inequalities that makes direct construction difficult. To circumvent this issue, we develop a stochastic representation for bivariate Sarmanov copulas. We prove that every admissible Sarmanov can be realized as a mixture of independent univariate distributions indexed by a latent Bernoulli pair. The stochastic representation replaces the problem of verifying copula validity with the problem of ensuring nonnegativity of a Bernoulli probability mass function. The representation also recovers classical copula families, including Farlie--Gumbel--Morgenstern, Huang--Kotz, and Bairamov--Kotz--Bekçi as special cases. We further derive sharp global bounds for Spearman's rho and Kendall's tau. We then introduce a Bernoulli-mixing construction in higher dimensions, leading to a new class of multivariate Sarmanov copulas with easily verifiable parameter constraints and scalable simulation algorithms. Finally, we show that powered versions of bivariate Sarmanov copulas admit a similar stochastic representation through block-maximal order statistics.

</details>


### [5] [Statistical Guarantees for Data-driven Posterior Tempering](https://arxiv.org/abs/2601.09122)
*Ruchira Ray,Marco Avella Medina,Cynthia Rush*

Main category: math.ST

TL;DR: 论文研究了后验温度调节（power posterior）的渐近性质，特别是α选择对后验矩一致性和后验均值渐近正态性的影响，发现了临界阈值α∝1/√n。


<details>
  <summary>Details</summary>
Motivation: 后验温度调节通过将似然函数提升到分数幂α来减少似然对后验的影响，具有模型误设鲁棒性和渐近正态性等优点，但实际应用中如何选择α参数以及相应的统计保证仍是未解决的问题。

Method: 采用交叉验证方法选择α参数，分析其渐近行为模式（趋于零或混合分布形式），建立新的拉普拉斯近似作为关键技术工具，推导后验矩一致性和后验均值渐近正态性的充分条件。

Result: 发现了临界阈值α∝1/√n，在此阈值处后验均值的渐近正态性会失效；建立了后验矩一致性和后验均值渐近正态性的充分条件；允许α以任意方式依赖于数据。

Conclusion: 后验温度调节的渐近性质取决于α的选择，存在临界阈值影响渐近正态性，研究结果为实际应用中选择α参数提供了理论指导。

Abstract: Posterior tempering reduces the influence of the likelihood in the calculation of the posterior by raising the likelihood to a fractional power $α$. The resulting power posterior - also known as an $α$-posterior or fractional posterior - has been shown to exhibit appealing properties, including robustness to model misspecification and asymptotic normality (Bernstein-von Mises theorem). However, practical recommendations for selecting the tempering parameter and statistical guarantees for the resulting power posterior remain open questions. Cross-validation-based approaches to tuning this parameter suggest interesting asymptotic regimes for the selected $α$, which can either vanish or behave like a mixture distribution with a point mass at infinity and the remaining mass converging to zero. We formalize the asymptotic properties of the power posterior in these regimes. In particular, we provide sufficient conditions for (i) consistency of the power posterior moments and (ii) asymptotic normality of the power posterior mean. Our analysis required us to establish a new Laplace approximation that is interesting in its own right and is the key technical tool for showing a critical threshold $α\asymp 1/\sqrt{n}$ where the asymptotic normality of the posterior mean breaks. Our results allow for the power to depend on the data in an arbitrary way.

</details>
