<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 2]
- [math.ST](#math.ST) [Total: 3]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Tail-Sensitive KL and Rényi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings](https://arxiv.org/abs/2601.09019)
*Nawaf Bou-Rabee,Siddharth Mitra,Andre Wibisono*

Main category: stat.ML

TL;DR: 该论文提出了一个框架，将未调整哈密顿蒙特卡洛（uHMC）的Wasserstein收敛保证升级到KL散度和Rényi散度等尾部敏感散度的保证，为相对密度失配提供定量控制。


<details>
  <summary>Details</summary>
Motivation: 哈密顿蒙特卡洛（HMC）算法在高维设置中广泛使用，但其在KL散度和Rényi散度等相对密度失配度量下的收敛性质研究不足。这些散度自然控制着Metropolis调整马尔可夫链的接受概率和预热启动要求。

Method: 基于单次耦合方法，建立了uHMC转移核的正则化性质。该正则化允许将Wasserstein-2混合时间和渐近偏差界限提升到KL散度，并将类似的Orlicz-Wasserstein界限提升到Rényi散度。

Result: 开发了一个框架，将uHMC的Wasserstein收敛保证升级到KL和Rényi散度保证。提供了相对密度失配的定量控制，阐明了离散化偏差在强散度中的作用，并为未调整采样和Metropolis调整马尔可夫链的预热启动生成提供了原则性保证。

Conclusion: 该工作为理解HMC算法在相对密度失配度量下的收敛性质提供了理论框架，填补了现有研究的空白，并为实际应用中的采样质量和预热启动要求提供了理论指导。

Abstract: Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and Rényi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and Rényi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to Rényi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains.

</details>


### [2] [Horseshoe Mixtures-of-Experts (HS-MoE)](https://arxiv.org/abs/2601.09043)
*Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: 提出基于马蹄先验的混合专家模型（HS-MoE），结合自适应全局-局部收缩与输入依赖门控，实现专家使用的数据自适应稀疏性；开发粒子学习算法进行序列推断，仅跟踪充分统计量；探讨其与大型语言模型中稀疏专家层的关系。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构中需要稀疏的专家选择机制，特别是在大型语言模型中需要从大量专家池中为每个token激活少量专家。现有方法缺乏贝叶斯框架下的自适应稀疏性控制。

Method: 结合马蹄先验的自适应全局-局部收缩特性与输入依赖门控机制，构建HS-MoE模型；开发粒子学习算法进行序列推断，通过前向传播滤波器并仅跟踪充分统计量来高效处理时序数据。

Result: HS-MoE模型实现了数据自适应的专家使用稀疏性，粒子学习算法能够在序列推断中高效运行，仅需维护充分统计量；该方法与大型语言模型中的稀疏专家层机制有直接关联。

Conclusion: HS-MoE为混合专家架构中的稀疏专家选择提供了贝叶斯框架，结合马蹄先验的自适应稀疏控制与粒子学习的高效序列推断，适用于大型语言模型等需要极端稀疏约束的场景。

Abstract: Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool).

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [3] [Global polynomial-time estimation in statistical nonlinear inverse problems via generalized stability](https://arxiv.org/abs/2601.09007)
*Sven Wang*

Main category: math.ST

TL;DR: 提出针对非线性统计反问题的计算高效估计器，通过弱化PDE约束实现条件凸优化，在椭圆PDE反问题中达到最优统计收敛率且具有多项式时间计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 非线性统计反问题面临统计分析和计算的双重挑战：似然估计导致非凸多模态优化，MCMC方法混合缓慢。需要开发既具有统计最优性又计算可行的估计方法。

Method: 提出插件式和PDE惩罚M估计器，将精确PDE约束替换为弱化松弛，形成条件凸优化问题。避免计算前向映射和求解PDE，产生嵌套二次优化。基于广义稳定性估计和非参数M估计理论进行分析。

Result: 对于椭圆PDE反问题（达西流模型和稳态薛定谔模型），证明估计器达到当前已知最优统计收敛率，且可在多项式时间内全局计算。达西模型中获得了显式亚二次算术运行时间界限。

Conclusion: 该方法为广泛非线性反问题提供了可证明多项式时间的统计算法蓝图，同时为贝叶斯计算提供了原则性的预热初始化策略，解决了统计最优性与计算可行性之间的权衡问题。

Abstract: Non-linear statistical inverse problems pose major challenges both for statistical analysis and computation. Likelihood-based estimators typically lead to non-convex and possibly multimodal optimization landscapes, and Markov chain Monte Carlo (MCMC) methods may mix exponentially slowly. We propose a class of computationally tractable estimators--plug-in and PDE-penalized M-estimators--for inverse problems defined through operator equations of the form $L_f u = g$, where $f$ is the unknown parameter and $u$ is the observed solution. The key idea is to replace the exact PDE constraint by a weakly enforced relaxation, yielding conditionally convex and, in many PDE examples, nested quadratic optimization problems that avoid evaluating the forward map $G(f)$ and do not require PDE solvers. For prototypical non-linear inverse problems arising from elliptic PDEs, including the Darcy flow model $L_f u = \nabla\!\cdot(f\nabla u)$ and a steady-state Schrödinger model, we prove that these estimators attain the best currently known statistical convergence rates while being globally computable in polynomial time. In the Darcy model, we obtain an explicit sub-quadratic $o(N^2)$ arithmetic runtime bound for estimating $f$ from $N$ noisy samples. Our analysis is based on new generalized stability estimates, extending classical stability beyond the range of the forward operator, combined with tools from nonparametric M-estimation. We also derive adaptive rates for the Darcy problem, providing a blueprint for designing provably polynomial-time statistical algorithms for a broad class of non-linear inverse problems. Our estimators also provide principled warm-start initializations for polynomial-time Bayesian computation.

</details>


### [4] [Stochastic representation of Sarmanov copulas](https://arxiv.org/abs/2601.09016)
*Christopher Blier-Wong*

Main category: math.ST

TL;DR: 本文为Sarmanov copula提出了随机表示方法，通过伯努利混合构造解决了高维参数验证难题，并推广到多变量情形。


<details>
  <summary>Details</summary>
Motivation: Sarmanov copula虽然具有闭式密度和函数表达式等优点，但其参数在高维时需要满足复杂的组合不等式条件，限制了实际应用。需要解决参数验证困难的问题。

Method: 1. 为二元Sarmanov copula建立随机表示：证明每个可容许的Sarmanov copula都可以表示为由潜在伯努利对索引的独立单变量分布的混合。2. 将copula有效性验证问题转化为伯努利概率质量函数的非负性验证。3. 推广到高维：引入伯努利混合构造，创建新的多变量Sarmanov copula类。4. 研究二元Sarmanov copula的幂次版本，通过块极大顺序统计量建立类似随机表示。

Result: 1. 获得了Spearman's rho和Kendall's tau的尖锐全局边界。2. 恢复了经典copula族（Farlie-Gumbel-Morgenstern、Huang-Kotz、Bairamov-Kotz-Bekçi）作为特例。3. 提出了具有易验证参数约束和可扩展模拟算法的新多变量Sarmanov copula类。4. 证明了二元Sarmanov copula的幂次版本具有类似的随机表示。

Conclusion: 通过随机表示方法，成功解决了Sarmanov copula在高维应用中的参数验证难题，建立了更易处理的多变量构造框架，为实际应用提供了理论支持和计算工具。

Abstract: Sarmanov copulas offer a simple and tractable way to build multivariate distributions by perturbing the independence copula. They admit closed-form expressions for densities and many functionals of interest, making them attractive for practical applications. However, the complex conditions on the dependence parameters to ensure that Sarmanov copulas are valid limit their application in high dimensions. Verifying the $d$-increasing property typically requires satisfying a combinatorial set of inequalities that makes direct construction difficult. To circumvent this issue, we develop a stochastic representation for bivariate Sarmanov copulas. We prove that every admissible Sarmanov can be realized as a mixture of independent univariate distributions indexed by a latent Bernoulli pair. The stochastic representation replaces the problem of verifying copula validity with the problem of ensuring nonnegativity of a Bernoulli probability mass function. The representation also recovers classical copula families, including Farlie--Gumbel--Morgenstern, Huang--Kotz, and Bairamov--Kotz--Bekçi as special cases. We further derive sharp global bounds for Spearman's rho and Kendall's tau. We then introduce a Bernoulli-mixing construction in higher dimensions, leading to a new class of multivariate Sarmanov copulas with easily verifiable parameter constraints and scalable simulation algorithms. Finally, we show that powered versions of bivariate Sarmanov copulas admit a similar stochastic representation through block-maximal order statistics.

</details>


### [5] [Statistical Guarantees for Data-driven Posterior Tempering](https://arxiv.org/abs/2601.09122)
*Ruchira Ray,Marco Avella Medina,Cynthia Rush*

Main category: math.ST

TL;DR: 论文研究了后验温度调节（通过将似然函数提升到分数幂α）的渐近性质，分析了交叉验证选择α时的两种渐近机制，并建立了新的拉普拉斯近似方法。


<details>
  <summary>Details</summary>
Motivation: 后验温度调节通过降低似然函数在计算后验分布中的影响，展现出对模型误设的鲁棒性和渐近正态性等优点。然而，如何选择温度参数α以及所得功率后验的统计保证仍是开放问题。

Method: 通过交叉验证方法选择温度参数α，分析其渐近机制；建立新的拉普拉斯近似方法；推导功率后验矩的一致性和后验均值渐近正态性的充分条件。

Result: 发现了α选择的两种渐近机制：要么消失，要么表现为混合分布；确定了临界阈值α≍1/√n，超过该阈值后验均值的渐近正态性被破坏；建立了允许α以任意方式依赖于数据的一般理论框架。

Conclusion: 论文为功率后验提供了严格的渐近理论保证，包括一致性和渐近正态性条件，并建立了关键的拉普拉斯近似方法，为实际应用中温度参数的选择提供了理论基础。

Abstract: Posterior tempering reduces the influence of the likelihood in the calculation of the posterior by raising the likelihood to a fractional power $α$. The resulting power posterior - also known as an $α$-posterior or fractional posterior - has been shown to exhibit appealing properties, including robustness to model misspecification and asymptotic normality (Bernstein-von Mises theorem). However, practical recommendations for selecting the tempering parameter and statistical guarantees for the resulting power posterior remain open questions. Cross-validation-based approaches to tuning this parameter suggest interesting asymptotic regimes for the selected $α$, which can either vanish or behave like a mixture distribution with a point mass at infinity and the remaining mass converging to zero. We formalize the asymptotic properties of the power posterior in these regimes. In particular, we provide sufficient conditions for (i) consistency of the power posterior moments and (ii) asymptotic normality of the power posterior mean. Our analysis required us to establish a new Laplace approximation that is interesting in its own right and is the key technical tool for showing a critical threshold $α\asymp 1/\sqrt{n}$ where the asymptotic normality of the posterior mean breaks. Our results allow for the power to depend on the data in an arbitrary way.

</details>
