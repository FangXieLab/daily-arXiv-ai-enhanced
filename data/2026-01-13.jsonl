{"id": "2601.10100", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.10100", "abs": "https://arxiv.org/abs/2601.10100", "authors": ["Guo Liu"], "title": "Admissibility Breakdown in High-Dimensional Sparse Regression with L1 Regularization", "comment": "19 pages", "summary": "The choice of the tuning parameter in the Lasso is central to its statistical performance in high-dimensional linear regression. Classical consistency theory identifies the rate of the Lasso tuning parameter, and numerous studies have established non-asymptotic guarantees. Nevertheless, the question of optimal tuning within a non-asymptotic framework has not yet been fully resolved. We establish tuning criteria above which the Lasso becomes inadmissible under mean squared prediction error. More specifically, we establish thresholds showing that certain classical tuning choices yield Lasso estimators strictly dominated by a simple Lasso-Ridge refinement. We also address how the structure of the design matrix and the noise vector influences the inadmissibility phenomenon."}
{"id": "2601.10133", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.10133", "abs": "https://arxiv.org/abs/2601.10133", "authors": ["Ruowei Li", "Zhigang Yao"], "title": "Curvature-driven manifold fitting under unbounded isotropic noise", "comment": "40 pages, 9 figures", "summary": "Manifold fitting aims to reconstruct a low-dimensional manifold from high-dimensional data, whose framework is established by Fefferman et al. \\cite{fefferman2020reconstruction,fefferman2021reconstruction}. This paper studies the recovery of a compact $C^3$ submanifold $\\mathcal{M} \\subset \\mathbb{R}^D$ with dimension $d<D$ and positive reach $τ$ from observations $Y = X + ξ$, where $X$ is uniformly distributed on $\\mathcal{M}$ and $ξ\\sim \\mathcal{N}(0, σ^2 I_D)$ denotes isotropic Gaussian noise. To project any points $z$ in a tubular neighborhood $Γ$ of $\\mathcal{M}$ onto $\\mathcal{M}$, we construct a sample-based estimator $F:Γ\\to\\mathbb{R}^D$ by a normalized local kernel with the theoretically derived bandwidth $r = c_Dσ$. Under a sample size of $O(σ^{-3d-5})$, we establish with high probability the uniform asymptotic expansion \\[ F(z) = π(z) + \\frac{d}{2} H_{π(z)} σ^2 + O(σ^3), \\qquad z \\in Γ, \\] where $π(z)$ is the projection of $z$ onto $\\mathcal{M}$ and $H_{π(z)}$ is the mean curvature vector of $\\mathcal{M}$ at $π(z)$. The resulting manifold $F(Γ)$ has reach bounded below by $c τ$ for $c>0$ and achieves a state-of-the-art Hausdorff distance of $O(σ^2)$ to $\\mathcal{M}$. Numerical experiments confirm the quadratic decay of the reconstruction error and demonstrate the computational efficiency of the estimator $F$. Our work provides a curvature-driven framework for denoising and reconstructing manifolds with second-order accuracy."}
{"id": "2601.10326", "categories": ["math.ST", "math.AP", "math.NA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.10326", "abs": "https://arxiv.org/abs/2601.10326", "authors": ["Aurélien Castre", "Richard Nickl"], "title": "On gradient stability in nonlinear PDE models and inference in interacting particle systems", "comment": "36 pages", "summary": "We consider general parameter to solution maps $θ\\mapsto \\mathcal G(θ)$ of non-linear partial differential equations and describe an approach based on a Banach space version of the implicit function theorem to verify the gradient stability condition of Nickl&Wang (JEMS 2024) for the underlying non-linear inverse problem, providing also injectivity estimates and corresponding statistical identifiability results. We illustrate our methods in two examples involving a non-linear reaction diffusion system as well as a McKean--Vlasov interacting particle model, both with periodic boundary conditions. We apply our results to prove the polynomial time convergence of a Langevin-type algorithm sampling the posterior measure of the interaction potential arising from a discrete aggregate measurement of the interacting particle system."}
{"id": "2601.10626", "categories": ["math.ST", "cs.CR", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.10626", "abs": "https://arxiv.org/abs/2601.10626", "authors": ["Getoar Sopa", "Marco Avella Medina", "Cynthia Rush"], "title": "Differentially Private Inference for Longitudinal Linear Regression", "comment": "68 pages, 3 figures", "summary": "Differential Privacy (DP) provides a rigorous framework for releasing statistics while protecting individual information present in a dataset. Although substantial progress has been made on differentially private linear regression, existing methods almost exclusively address the item-level DP setting, where each user contributes a single observation. Many scientific and economic applications instead involve longitudinal or panel data, in which each user contributes multiple dependent observations. In these settings, item-level DP offers inadequate protection, and user-level DP - shielding an individual's entire trajectory - is the appropriate privacy notion. We develop a comprehensive framework for estimation and inference in longitudinal linear regression under user-level DP. We propose a user-level private regression estimator based on aggregating local regressions, and we establish finite-sample guarantees and asymptotic normality under short-range dependence. For inference, we develop a privatized, bias-corrected covariance estimator that is automatically heteroskedasticity- and autocorrelation-consistent. These results provide the first unified framework for practical user-level DP estimation and inference in longitudinal linear regression under dependence, with strong theoretical guarantees and promising empirical performance."}
