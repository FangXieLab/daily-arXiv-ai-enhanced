{"id": "2601.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15305", "abs": "https://arxiv.org/abs/2601.15305", "authors": ["Alfred Shen", "Aaron Shen"], "title": "Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models", "comment": "15 pages, 1 figure, attention mechanism, sparse attention, gating, long-context", "summary": "The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.", "AI": {"tldr": "Gated Sparse Attention (GSA) \u7ed3\u5408\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u7684\u4f18\u52bf\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u8d28\u91cf\u63d0\u5347\u7684\u53cc\u91cd\u76ee\u6807\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u8ba1\u7b97\u8d1f\u62c5\u8fc7\u91cd\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u53d8\u4f53\u5404\u81ea\u89e3\u51b3\u4e0d\u540c\u95ee\u9898\u4f46\u76f8\u4e92\u72ec\u7acb\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51faGated Sparse Attention (GSA)\uff0c\u5305\u542b\uff1a1) \u5e26sigmoid\u6fc0\u6d3b\u7684\u95e8\u63a7\u95ea\u7535\u7d22\u5f15\u5668\uff0c\u751f\u6210\u6709\u754c\u53ef\u89e3\u91ca\u9009\u62e9\u5206\u6570\uff1b2) \u57fa\u4e8e\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u9002\u5e94\u7a00\u758f\u63a7\u5236\u5668\uff0c\u8c03\u8282\u6ce8\u610f\u529btoken\u6570\u91cf\uff1b3) \u503c\u548c\u8f93\u51fa\u9636\u6bb5\u7684\u53cc\u91cd\u95e8\u63a7\u673a\u5236\u3002", "result": "\u57281.7B\u53c2\u6570\u6a21\u578b\u3001400B token\u8bad\u7ec3\u4e2d\uff0cGSA\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b012-16\u500d\u52a0\u901f\uff0c\u56f0\u60d1\u5ea6\u4ece6.03\u964d\u81f35.70\uff0cRULER\u5206\u6570\u51e0\u4e4e\u7ffb\u500d\uff0c\u9996token\u6ce8\u610f\u529b\u5360\u6bd4\u4ece47%\u964d\u81f34%\u4ee5\u4e0b\uff0c\u8bad\u7ec3\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u5347\uff08\u635f\u5931\u5cf0\u503c\u51cf\u5c1198%\uff09\u3002", "conclusion": "GSA\u6210\u529f\u7ed3\u5408\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6548\u7387\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u7684\u8d28\u91cf\u4f18\u52bf\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5efa\u7acb\u4e86\u7406\u8bba\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2601.15307", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15307", "abs": "https://arxiv.org/abs/2601.15307", "authors": ["Guo-Biao Zhang", "Ding-Yuan Liu", "Da-Yi Wu", "Tian Lan", "Heyan Huang", "Zhijing Wu", "Xian-Ling Mao"], "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey", "comment": null, "summary": "The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.", "AI": {"tldr": "DeepSurvey-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u751f\u6210\u5f0f\u5b66\u672f\u7efc\u8ff0\u8d28\u91cf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\uff08\u4fe1\u606f\u4ef7\u503c\u3001\u5b66\u672f\u4ea4\u6d41\u4ef7\u503c\u3001\u7814\u7a76\u6307\u5bfc\u4ef7\u503c\uff09\u6765\u8bc4\u4f30\u7efc\u8ff0\u7684\u6df1\u5c42\u5b66\u672f\u4ef7\u503c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u8868\u9762\u8d28\u91cf\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u751f\u6210\u5f0f\u5b66\u672f\u7efc\u8ff0\u7684\u57fa\u51c6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u57fa\u4e8e\u5f15\u7528\u91cf\u7b49\u6709\u7f3a\u9677\u6807\u51c6\u9009\u62e9\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\u96c6\u4e0d\u53ef\u9760\uff1b2\uff09\u8bc4\u4f30\u6307\u6807\u4ec5\u5173\u6ce8\u8868\u9762\u8d28\u91cf\uff08\u5982\u903b\u8f91\u8fde\u8d2f\u6027\uff09\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6df1\u5c42\u5b66\u672f\u4ef7\u503c\uff08\u5982\u6838\u5fc3\u7814\u7a76\u76ee\u6807\u3001\u5bf9\u4e0d\u540c\u7814\u7a76\u7684\u6279\u5224\u6027\u5206\u6790\uff09\u3002", "method": "\u63d0\u51faDeepSurvey-Bench\u57fa\u51c6\uff0c\u5305\u542b\uff1a1\uff09\u5168\u9762\u7684\u5b66\u672f\u4ef7\u503c\u8bc4\u4f30\u6807\u51c6\uff0c\u6db5\u76d6\u4fe1\u606f\u4ef7\u503c\u3001\u5b66\u672f\u4ea4\u6d41\u4ef7\u503c\u548c\u7814\u7a76\u6307\u5bfc\u4ef7\u503c\u4e09\u4e2a\u7ef4\u5ea6\uff1b2\uff09\u6784\u5efa\u5e26\u6709\u5b66\u672f\u4ef7\u503c\u6807\u6ce8\u7684\u53ef\u9760\u6570\u636e\u96c6\uff1b3\uff09\u57fa\u4e8e\u6b64\u6807\u51c6\u8bc4\u4f30\u751f\u6210\u5f0f\u7efc\u8ff0\u7684\u6df1\u5c42\u5b66\u672f\u4ef7\u503c\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepSurvey-Bench\u57fa\u51c6\u5728\u8bc4\u4f30\u751f\u6210\u5f0f\u7efc\u8ff0\u7684\u5b66\u672f\u4ef7\u503c\u65b9\u9762\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8868\u73b0\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "DeepSurvey-Bench\u901a\u8fc7\u5168\u9762\u7684\u5b66\u672f\u4ef7\u503c\u8bc4\u4f30\u6807\u51c6\u548c\u53ef\u9760\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u751f\u6210\u5f0f\u5b66\u672f\u7efc\u8ff0\u7684\u6df1\u5c42\u5b66\u672f\u4ef7\u503c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.15311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15311", "abs": "https://arxiv.org/abs/2601.15311", "authors": ["Mustafa Arslan"], "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to \"Vector Haze\", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.", "AI": {"tldr": "Aeon\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u8ba4\u77e5\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5185\u5b58\u7ba1\u7406\u89e3\u51b3LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8bb0\u5fc6\u8fde\u7eed\u6027\u96be\u9898\uff0c\u5b9e\u73b0\u4e9a\u6beb\u79d2\u7ea7\u68c0\u7d22\u5ef6\u8fdf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u548c\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\uff0c\u73b0\u6709\u57fa\u4e8e\u5411\u91cf\u6570\u636e\u5e93\u7684\"\u6241\u5e73RAG\"\u67b6\u6784\u65e0\u6cd5\u6355\u6349\u957f\u65f6\u4ea4\u4e92\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u5e8f\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\"\u5411\u91cf\u8ff7\u96fe\"\u95ee\u9898\u3002", "method": "\u63d0\u51faAeon\u795e\u7ecf\u7b26\u53f7\u8ba4\u77e5\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5c06\u5185\u5b58\u7ed3\u6784\u5316\u4e3a\u8bb0\u5fc6\u5bab\u6bbf\uff08\u57fa\u4e8eAtlas\u7684SIMD\u52a0\u901f\u9875\u9762\u805a\u7c7b\u5411\u91cf\u7d22\u5f15\uff09\u548c\u8f68\u8ff9\uff08\u795e\u7ecf\u7b26\u53f7\u60c5\u8282\u56fe\uff09\uff0c\u5f15\u5165\u8bed\u4e49\u65c1\u8def\u7f13\u51b2\u533a\u5b9e\u73b0\u9884\u6d4b\u6027\u7f13\u5b58\u3002", "result": "\u5728\u5bf9\u8bdd\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0<1ms\u68c0\u7d22\u5ef6\u8fdf\uff0c\u901a\u8fc7\u96f6\u62f7\u8d1dC++/Python\u6865\u786e\u4fdd\u72b6\u6001\u4e00\u81f4\u6027\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u6301\u4e45\u5316\u7ed3\u6784\u5316\u5185\u5b58\u652f\u6301\u3002", "conclusion": "Aeon\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u5185\u5b58\u4e3a\u64cd\u4f5c\u7cfb\u7edf\u7ba1\u7406\u7684\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86LLM\u5728\u957f\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u8bb0\u5fc6\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u6846\u67b6\u3002"}}
{"id": "2601.15316", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15316", "abs": "https://arxiv.org/abs/2601.15316", "authors": ["Wei Ai", "Yilong Tan", "Yuntao Shou", "Tao Meng", "Haowen Chen", "Zhixiong He", "Keqin Li"], "title": "The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection", "comment": null, "summary": "In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.", "code_url": "https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection", "code_stars": 15, "code_last_update": "2025-10-28", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u5173\u4e8e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u9886\u57df\u5e94\u7528\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u8ffd\u8e2a\u4e86\u4ece\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u5230\u7aef\u5230\u7aef\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u9886\u57df\u7ecf\u5386\u4e86\u4ece\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u5230\u7edf\u4e00\u7aef\u5230\u7aef\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\u7684\u8303\u5f0f\u8f6c\u53d8\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u6765\u8ffd\u8e2a\u8fd9\u4e00\u8f6c\u53d8\u5e76\u6574\u5408\u6700\u65b0\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u5168\u9762\u56de\u987eLVLMs\u5728\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff1a1) \u63d0\u4f9b\u5386\u53f2\u89c6\u89d2\uff0c\u8ffd\u8e2a\u4ece\u4f20\u7edf\u591a\u6a21\u6001\u68c0\u6d4b\u6d41\u7a0b\u5230\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u8303\u5f0f\u7684\u6f14\u53d8\uff1b2) \u5efa\u7acb\u7ed3\u6784\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u6027\u80fd\u57fa\u51c6\uff1b3) \u5206\u6790\u5269\u4f59\u6280\u672f\u6311\u6218\uff1b4) \u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u8bb0\u5f55\u548c\u5206\u6790LVLMs\u5728\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u53d8\u9769\u6027\u4f5c\u7528\u7684\u5168\u9762\u7efc\u8ff0\u3002\u8bba\u6587\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u6280\u672f\u6311\u6218\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u4e0b\u4e00\u9636\u6bb5\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\u3002", "conclusion": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u683c\u5c40\uff0c\u901a\u8fc7\u5f3a\u5927\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u5b9e\u73b0\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u8054\u5408\u5efa\u6a21\u3002\u672c\u6587\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2601.15401", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.15401", "abs": "https://arxiv.org/abs/2601.15401", "authors": ["Sajjad Akherati", "Xinmiao Zhang"], "title": "Multi-Input Ciphertext Multiplication for Homomorphic Encryption", "comment": "13 Pages, 6 Figures, 7 Tables, Journal paper", "summary": "Homomorphic encryption (HE) enables arithmetic operations to be performed directly on encrypted data. It is essential for privacy-preserving applications such as machine learning, medical diagnosis, and financial data analysis. In popular HE schemes, ciphertext multiplication is only defined for two inputs. However, the multiplication of multiple inputs is needed in many HE applications. In our previous work, a three-input ciphertext multiplication method for the CKKS HE scheme was developed. This paper first reformulates the three-input ciphertext multiplication to enable the combination of computations in order to further reduce the complexity. The second contribution is extending the multiplication to multiple inputs without compromising the noise overhead. Additional evaluation keys are introduced to achieve relinearization of polynomial multiplication results. To minimize the complexity of the large number of rescaling units in the multiplier, a theoretical analysis is developed to relocate the rescaling, and a multi-level rescaling approach is proposed to implement combined rescaling with complexity similar to that of a single rescaling unit. Guidelines and examples are provided on the input partition to enable the combination of more rescaling. Additionally, efficient hardware architectures are designed to implement our proposed multipliers. The improved three-input ciphertext multiplier reduces the logic area and latency by 15% and 50%, respectively, compared to the best prior design. For multipliers with more inputs, ranging from 4 to 12, the architectural analysis reveals 32% savings in area and 45% shorter latency, on average, compared to prior work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9CKKS\u540c\u6001\u52a0\u5bc6\u65b9\u6848\u7684\u591a\u8f93\u5165\u5bc6\u6587\u4e58\u6cd5\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u4e09\u8f93\u5165\u4e58\u6cd5\u3001\u6269\u5c55\u81f3\u591a\u8f93\u5165\u3001\u91cd\u65b0\u5b9a\u4f4d\u7f29\u653e\u64cd\u4f5c\u548c\u8bbe\u8ba1\u9ad8\u6548\u786c\u4ef6\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u786c\u4ef6\u5f00\u9500\u3002", "motivation": "\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u5141\u8bb8\u76f4\u63a5\u5728\u52a0\u5bc6\u6570\u636e\u4e0a\u8fdb\u884c\u7b97\u672f\u8fd0\u7b97\uff0c\u5bf9\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709HE\u65b9\u6848\u4e2d\uff0c\u5bc6\u6587\u4e58\u6cd5\u901a\u5e38\u53ea\u652f\u6301\u4e24\u4e2a\u8f93\u5165\uff0c\u4f46\u8bb8\u591a\u5e94\u7528\u9700\u8981\u591a\u4e2a\u8f93\u5165\u7684\u4e58\u6cd5\u3002\u867d\u7136\u4e4b\u524d\u5de5\u4f5c\u5f00\u53d1\u4e86\u4e09\u8f93\u5165\u5bc6\u6587\u4e58\u6cd5\uff0c\u4f46\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u6269\u5c55\u5230\u66f4\u591a\u8f93\u5165\u3002", "method": "1. \u91cd\u6784\u4e09\u8f93\u5165\u5bc6\u6587\u4e58\u6cd5\u4ee5\u652f\u6301\u8ba1\u7b97\u7ec4\u5408\uff1b2. \u6269\u5c55\u81f3\u591a\u8f93\u5165\u4e58\u6cd5\u800c\u4e0d\u589e\u52a0\u566a\u58f0\u5f00\u9500\uff1b3. \u5f15\u5165\u989d\u5916\u8bc4\u4f30\u5bc6\u94a5\u5b9e\u73b0\u591a\u9879\u5f0f\u4e58\u6cd5\u7ed3\u679c\u7684\u91cd\u7ebf\u6027\u5316\uff1b4. \u901a\u8fc7\u7406\u8bba\u5206\u6790\u91cd\u65b0\u5b9a\u4f4d\u7f29\u653e\u64cd\u4f5c\uff0c\u63d0\u51fa\u591a\u7ea7\u7f29\u653e\u65b9\u6cd5\uff1b5. \u63d0\u4f9b\u8f93\u5165\u5206\u533a\u6307\u5357\u4ee5\u7ec4\u5408\u66f4\u591a\u7f29\u653e\u64cd\u4f5c\uff1b6. \u8bbe\u8ba1\u9ad8\u6548\u786c\u4ef6\u67b6\u6784\u5b9e\u73b0\u6240\u63d0\u51fa\u7684\u4e58\u6cd5\u5668\u3002", "result": "\u6539\u8fdb\u7684\u4e09\u8f93\u5165\u5bc6\u6587\u4e58\u6cd5\u5668\u76f8\u6bd4\u5148\u524d\u6700\u4f73\u8bbe\u8ba1\u51cf\u5c11\u4e8615%\u7684\u903b\u8f91\u9762\u79ef\u548c50%\u7684\u5ef6\u8fdf\u3002\u5bf9\u4e8e4\u523012\u8f93\u5165\u7684\u4e58\u6cd5\u5668\uff0c\u67b6\u6784\u5206\u6790\u663e\u793a\u5e73\u5747\u8282\u770132%\u7684\u9762\u79ef\u548c\u7f29\u77ed45%\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u8f93\u5165\u5bc6\u6587\u4e58\u6cd5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u91cd\u6784\u3001\u7f29\u653e\u4f18\u5316\u548c\u9ad8\u6548\u786c\u4ef6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86CKKS\u540c\u6001\u52a0\u5bc6\u65b9\u6848\u4e2d\u591a\u8f93\u5165\u4e58\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u57fa\u7840\u3002"}}
{"id": "2601.15397", "categories": ["cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.15397", "abs": "https://arxiv.org/abs/2601.15397", "authors": ["Peidong Wang"], "title": "Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)", "comment": null, "summary": "The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the \"lost-in-the-middle\" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from \"over-correction\", introducing hallucinations of entities that were never spoken.\n  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.", "AI": {"tldr": "LOGIC\u6846\u67b6\u901a\u8fc7\u5728\u89e3\u7801\u5c42\u76f4\u63a5\u64cd\u4f5c\uff0c\u89e3\u51b3\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u65b0\u5174\u5b9e\u4f53\u65f6\u7684\u5c40\u9650\u6027\uff0c\u76f8\u6bd4\u63d0\u793a\u65b9\u6cd5\u5177\u6709\u6052\u5b9a\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u9886\u57df\u7279\u5b9a\u5b9e\u4f53\uff08\u5982\u8054\u7cfb\u4eba\u59d3\u540d\u3001\u64ad\u653e\u5217\u8868\u3001\u6280\u672f\u672f\u8bed\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u56e0\u4e3a\u9759\u6001\u8bad\u7ec3\u77e5\u8bc6\u65e0\u6cd5\u9002\u5e94\u5feb\u901f\u53d8\u5316\u7684\u65b0\u5b9e\u4f53\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u63d0\u793a\u65b9\u6cd5\uff09\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u800c\u751f\u6210\u5f0f\u9519\u8bef\u6821\u6b63\u65b9\u6cd5\u5219\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u6821\u6b63\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "LOGIC\uff08Logit-Space Integration for Contextual Biasing\uff09\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u89e3\u7801\u5c42\u8fdb\u884c\u64cd\u4f5c\u3002\u4e0e\u63d0\u793a\u65b9\u6cd5\u4e0d\u540c\uff0cLOGIC\u5c06\u4e0a\u4e0b\u6587\u6ce8\u5165\u4e0e\u8f93\u5165\u5904\u7406\u89e3\u8026\uff0c\u786e\u4fdd\u76f8\u5bf9\u4e8e\u63d0\u793a\u957f\u5ea6\u7684\u6052\u5b9a\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u572811\u79cd\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u4f7f\u7528Phi-4-MM\u6a21\u578b\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLOGIC\u5b9e\u73b0\u4e86\u5e73\u57479%\u7684\u76f8\u5bf9\u5b9e\u4f53\u8bcd\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u540c\u65f6\u8bef\u62a5\u7387\u4ec5\u589e\u52a00.30%\u3002", "conclusion": "LOGIC\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u5728\u89e3\u7801\u5c42\u96c6\u6210\u4e0a\u4e0b\u6587\u504f\u7f6e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u65b0\u5174\u5b9e\u4f53\u65f6\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.15476", "categories": ["cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.15476", "abs": "https://arxiv.org/abs/2601.15476", "authors": ["Alex Dantart"], "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases", "comment": null, "summary": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u51cf\u5c11\u5e7b\u89c9\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u6cd5\u5f8b\u5de5\u4f5c\uff0c\u6bd4\u8f83\u4e09\u79cdAI\u8303\u5f0f\uff0c\u5f15\u5165\u53ef\u9760\u6027\u6307\u6807\uff0c\u8bc4\u4f3012\u4e2aLLM\u572875\u4e2a\u6cd5\u5f8b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u7ea7RAG\u7cfb\u7edf\u53ef\u5c06\u9519\u8bef\u7387\u964d\u81f30.2%\u4ee5\u4e0b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u786e\u4fddAI\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u7684\u6cd5\u5f8b\u5206\u6790\uff0c\u6ee1\u8db3\u4e13\u4e1a\u6cd5\u5f8b\u5de5\u4f5c\u7684\u4e25\u8c28\u6027\u8981\u6c42\u3002", "method": "\u533a\u5206\u4e09\u79cdAI\u8303\u5f0f\uff1a\u72ec\u7acb\u751f\u6210\u6a21\u578b\u3001\u57fa\u7840\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u3001\u9ad8\u7ea7\u7aef\u5230\u7aef\u4f18\u5316RAG\u7cfb\u7edf\uff1b\u5f15\u5165\u865a\u5047\u5f15\u7528\u7387(FCR)\u548c\u865a\u6784\u4e8b\u5b9e\u7387(FFR)\u4e24\u4e2a\u53ef\u9760\u6027\u6307\u6807\uff1b\u901a\u8fc7\u4e13\u5bb6\u53cc\u76f2\u8bc4\u5ba1\u8bc4\u4f3012\u4e2aLLM\u572875\u4e2a\u6cd5\u5f8b\u4efb\u52a1\u4e0a\u751f\u6210\u76842,700\u4e2a\u53f8\u6cd5\u98ce\u683c\u7b54\u6848\u3002", "result": "\u72ec\u7acb\u751f\u6210\u6a21\u578b\u4e0d\u9002\u5408\u4e13\u4e1a\u4f7f\u7528(FCR\u8d85\u8fc730%)\uff1b\u57fa\u7840RAG\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u9519\u8bef\u4f46\u4ecd\u5b58\u5728\u660e\u663e\u9519\u8bef\uff1b\u9ad8\u7ea7RAG\u7cfb\u7edf(\u4f7f\u7528\u5d4c\u5165\u5fae\u8c03\u3001\u91cd\u65b0\u6392\u5e8f\u548c\u81ea\u6821\u6b63\u7b49\u6280\u672f)\u5c06\u865a\u6784\u4e8b\u5b9e\u7387\u964d\u81f3\u53ef\u5ffd\u7565\u6c34\u5e73(\u4f4e\u4e8e0.2%)\u3002", "conclusion": "\u53ef\u4fe1\u8d56\u7684\u6cd5\u5f8bAI\u9700\u8981\u57fa\u4e8e\u68c0\u7d22\u7684\u67b6\u6784\uff0c\u5f3a\u8c03\u9a8c\u8bc1\u548c\u53ef\u8ffd\u6eaf\u6027\uff1b\u7814\u7a76\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\u7684\u8bc4\u4f30\u6846\u67b6\uff1b\u9ad8\u7ea7RAG\u7cfb\u7edf\u662f\u5b9e\u73b0\u6cd5\u5f8bAI\u53ef\u9760\u6027\u7684\u5173\u952e\u3002"}}
{"id": "2601.15509", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15509", "abs": "https://arxiv.org/abs/2601.15509", "authors": ["Prasanna Kumar"], "title": "The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers", "comment": null, "summary": "The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faTransformer\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4e2d\u867d\u7136\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u5bfc\u81f4\u4e86\u60c5\u611f\u6781\u5316\u548c\u4e2d\u7acb\u6027\u7f3a\u5931\u7684\u95ee\u9898", "motivation": "\u867d\u7136\u8fc1\u79fb\u5b66\u4e60\u548cTransformer\u6a21\u578b\u5728\u590d\u6742\u8ba1\u7b97\u95ee\u9898\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u5728\u5e94\u7528AI\u5206\u6790\u7279\u522b\u662f\u60c5\u611f\u5206\u6790\u9886\u57df\uff0c\u8fd9\u79cd\u57fa\u4e8eTransformer\u7684\u51c6\u786e\u6027\u63d0\u5347\u5e26\u6765\u4e86\u8d1f\u9762\u6548\u5e94\u3002\u7814\u7a76\u53d1\u73b0\uff0cTransformer\u6a21\u578b\u5bf9\u67d0\u4e00\u7c7b\u60c5\u611f\u51c6\u786e\u6027\u7684\u63d0\u5347\u5f80\u5f80\u4ee5\u53e6\u4e00\u7c7b\u60c5\u611f\u7684\u6781\u5316\u548c\u4e2d\u7acb\u6027\u5931\u8d25\u4e3a\u4ee3\u4ef7\uff0c\u8fd9\u5728\u4f9d\u8d56\u60c5\u611f\u5206\u6790\u8ba1\u7b97\u8f93\u51fa\u7684\u5de5\u4e1a\u7ea7\u5e94\u7528\u4e2d\u6784\u6210\u4e86\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdf\u53d1\u73b0\uff0cTransformer\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7279\u5b9a\u7684\u504f\u5dee\u6a21\u5f0f\u3002\u867d\u7136\u6ca1\u6709\u8be6\u7ec6\u63cf\u8ff0\u5177\u4f53\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u4f46\u7814\u7a76\u5173\u6ce8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u60c5\u611f\u7c7b\u522b\uff08\u5982\u6b63\u9762\u3001\u8d1f\u9762\u3001\u4e2d\u7acb\uff09\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u89c2\u5bdf\u5230Transformer\u6a21\u578b\u5728\u63d0\u5347\u67d0\u4e00\u7c7b\u60c5\u611f\u5206\u6790\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u4f1a\u5bfc\u81f4\u53e6\u4e00\u7c7b\u60c5\u611f\u7684\u6781\u5316\uff08polarization\uff09\u4ee5\u53ca\u4e2d\u7acb\u6027\u5206\u7c7b\u7684\u5931\u8d25\u3002\u8fd9\u8868\u660e\u6a21\u578b\u51c6\u786e\u6027\u7684\u63d0\u5347\u5e76\u975e\u5747\u5300\u5206\u5e03\uff0c\u800c\u662f\u4ee5\u727a\u7272\u67d0\u4e9b\u60c5\u611f\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u51c6\u786e\u6027\u63d0\u5347\u4f34\u968f\u7740\u4e2d\u7acb\u6027\u7f3a\u5931\u548c\u60c5\u611f\u6781\u5316\u7684\u95ee\u9898\uff0c\u8fd9\u5bf9\u4f9d\u8d56\u60c5\u611f\u5206\u6790\u8f93\u51fa\u7684\u5e94\u7528NLP\u9886\u57df\u6784\u6210\u4e86\u4e25\u91cd\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u51c6\u786e\u6027\u63d0\u5347\u4e0e\u4fdd\u6301\u60c5\u611f\u5206\u7c7b\u7684\u5747\u8861\u6027\u548c\u4e2d\u7acb\u6027\u3002"}}
{"id": "2601.15754", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15754", "abs": "https://arxiv.org/abs/2601.15754", "authors": ["Ajvad Haneef K", "Karan Kuwar Singh", "Madhu Kumar S D"], "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting", "comment": null, "summary": "High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.", "AI": {"tldr": "CAFE-GB\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u53ef\u6269\u5c55\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u805a\u5408\u68af\u5ea6\u63d0\u5347\u7279\u5f81\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1195%\u4ee5\u4e0a\u7279\u5f81\u7ef4\u5ea6\u3002", "motivation": "\u9ad8\u7ef4\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u5b58\u5728\u7279\u5f81\u5197\u4f59\u3001\u4e0d\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5f71\u54cd\u673a\u5668\u5b66\u4e60\u68c0\u6d4b\u7cfb\u7edf\u7684\u6548\u679c\u548c\u53ef\u89e3\u91ca\u6027\u3002\u73b0\u6709\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u5f02\u6784\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCAFE-GB\u6846\u67b6\uff1a1) \u5c06\u8bad\u7ec3\u6570\u636e\u5212\u5206\u4e3a\u91cd\u53e0\u5757\uff1b2) \u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u4f30\u8ba1\u5c40\u90e8\u7279\u5f81\u91cd\u8981\u6027\uff1b3) \u805a\u5408\u5c40\u90e8\u4f30\u8ba1\u5f97\u5230\u9c81\u68d2\u7684\u5168\u5c40\u7279\u5f81\u6392\u540d\uff1b4) \u901a\u8fc7\u7cfb\u7edf\u5316\u7684k\u9009\u62e9\u548c\u7a33\u5b9a\u6027\u5206\u6790\u8fdb\u884c\u7279\u5f81\u9884\u7b97\u9009\u62e9\uff0c\u5e73\u8861\u68c0\u6d4b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728BODMAS\u548cCIC-AndMal2020\u4e24\u4e2a\u5927\u89c4\u6a21\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAFE-GB\u9009\u62e9\u7684\u7279\u5f81\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001F1\u5206\u6570\u3001MCC\u3001ROC-AUC\u548cPR-AUC\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u4e0e\u5168\u7279\u5f81\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c1195%\u4ee5\u4e0a\u7279\u5f81\u7ef4\u5ea6\u3002\u914d\u5bf9Wilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u786e\u8ba4\u6027\u80fd\u4e0b\u964d\u65e0\u7edf\u8ba1\u663e\u8457\u6027\u3002\u5206\u6790\u663e\u793a\u4f4e\u7279\u5f81\u95f4\u5197\u4f59\uff0c\u901a\u8fc7SHAP\u89e3\u91ca\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u5206\u6790\u8868\u660e\u51cf\u5c11\u4e86\u4e0b\u6e38\u5206\u7c7b\u5f00\u9500\u3002", "conclusion": "CAFE-GB\u4e3a\u5927\u89c4\u6a21\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.16199", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.16199", "abs": "https://arxiv.org/abs/2601.16199", "authors": ["Prach Chantasantitam", "Adam Ilyas Caulfield", "Vasisht Duddu", "Lachlan J. Gunn", "N. Asokan"], "title": "PAL*M: Property Attestation for Large Generative Models", "comment": null, "summary": "Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.", "AI": {"tldr": "PAL*M\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u578b\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u5c5e\u6027\u8bc1\u660e\u6846\u67b6\uff0c\u652f\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u5c5e\u6027\u9a8c\u8bc1\uff0c\u5229\u7528\u673a\u5bc6\u865a\u62df\u673a\u548c\u5b89\u5168\u611f\u77e5GPU\uff0c\u901a\u8fc7\u589e\u91cf\u591a\u91cd\u96c6\u54c8\u5e0c\u9ad8\u6548\u8ddf\u8e2a\u6570\u636e\u96c6\u5b8c\u6574\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5c5e\u6027\u8bc1\u660e\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u751f\u6210\u6a21\u578b\u6216\u5927\u578b\u6570\u636e\u96c6\uff0c\u800c\u76d1\u7ba1\u673a\u6784\u3001\u5ba2\u6237\u7b49\u9a8c\u8bc1\u8005\u9700\u8981\u5bf9\u6a21\u578b/\u6570\u636e\u96c6\u7684\u5c5e\u6027\u8fdb\u884c\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u7b26\u5408\u6cd5\u89c4\u548c\u653f\u7b56\u8981\u6c42\u3002", "method": "PAL*M\u5b9a\u4e49\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u5c5e\u6027\uff0c\u5229\u7528\u673a\u5bc6\u865a\u62df\u673a\u548c\u5b89\u5168\u611f\u77e5GPU\u8986\u76d6CPU-GPU\u64cd\u4f5c\uff0c\u63d0\u51fa\u4f7f\u7528\u589e\u91cf\u591a\u91cd\u96c6\u54c8\u5e0c\u5bf9\u5185\u5b58\u6620\u5c04\u6570\u636e\u96c6\u8fdb\u884c\u9ad8\u6548\u5b8c\u6574\u6027\u8ddf\u8e2a\u3002", "result": "\u5728Intel TDX\u548cNVIDIA H100\u4e0a\u5b9e\u73b0\u4e86PAL*M\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u3001\u901a\u7528\u4e14\u5b89\u5168\u3002", "conclusion": "PAL*M\u4e3a\u5927\u578b\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5c5e\u6027\u8bc1\u660e\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u751f\u6210\u6a21\u578b\u548c\u5927\u578b\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c5e\u6027\u7684\u53ef\u9a8c\u8bc1\u8bc1\u660e\u3002"}}
{"id": "2601.15628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15628", "abs": "https://arxiv.org/abs/2601.15628", "authors": ["Haibo Tong", "Zeyang Yue", "Feifei Zhao", "Erliang Lin", "Lu Jia", "Ruolin Chen", "Yinqian Sun", "Qian Zhang", "Yi Zeng"], "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models", "comment": null, "summary": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.", "AI": {"tldr": "CogToM\u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u7406\u8bba\u57fa\u7840\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8000\u591a\u4e2a\u53cc\u8bed\u5b9e\u4f8b\u548c46\u79cd\u8303\u5f0f\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u6027\u80fd\u5f02\u8d28\u6027\u548c\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u7684\u6f5c\u5728\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5c40\u9650\u4e8e\u9519\u8bef\u4fe1\u5ff5\u4efb\u52a1\u7b49\u72ed\u7a84\u8303\u5f0f\uff0c\u672a\u80fd\u6355\u6349\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\u7684\u5168\u8c8c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u7814\u7a76LLMs\u662f\u5426\u771f\u6b63\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86CogToM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8000\u591a\u4e2a\u53cc\u8bed\u5b9e\u4f8b\uff0c\u6db5\u76d646\u79cd\u8ba4\u77e5\u8303\u5f0f\uff0c\u753149\u540d\u4eba\u7c7b\u6807\u6ce8\u8005\u9a8c\u8bc1\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e8622\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff08\u5305\u62ecGPT-5.1\u548cQwen3-Max\u7b49\u524d\u6cbf\u6a21\u578b\uff09\uff0c\u5e76\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u5f02\u8d28\u6027\uff0c\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e0a\u5b58\u5728\u6301\u7eed\u74f6\u9888\u3002\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u7684\u5206\u6790\u8868\u660eLLMs\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u53ef\u80fd\u5b58\u5728\u6f5c\u5728\u5dee\u5f02\u3002", "conclusion": "CogToM\u4e3a\u7814\u7a76LLMs\u4e0d\u65ad\u6f14\u5316\u7684\u8ba4\u77e5\u8fb9\u754c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u548c\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u7406\u89e3LLMs\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u53ca\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5dee\u5f02\u3002"}}
{"id": "2601.15652", "categories": ["cs.AI", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.15652", "abs": "https://arxiv.org/abs/2601.15652", "authors": ["Manish Bhatt"], "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9884\u6d4b\u7f16\u7801\u548c\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u7684\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u4fe1\u53f7\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u6548\u5e7b\u89c9\u68c0\u6d4b\uff0c\u6027\u80fd\u63a5\u8fd1\u5927\u6a21\u578b\u4f46\u6570\u636e\u9700\u6c42\u5c1175\u500d\u3001\u63a8\u7406\u5feb1000\u500d", "motivation": "\u89e3\u51b3LLM\u5e7b\u89c9\u68c0\u6d4b\u7684\u74f6\u9888\uff1a\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u5916\u90e8\u68c0\u7d22\u6216\u9700\u898170B+\u53c2\u6570\u7684\u9ed1\u76d2LLM\u6cd5\u5b98\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u6548\u7387", "method": "\u63d0\u51fa\u6df7\u5408\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4fe1\u53f7\u8bbe\u8ba1\u548c\u76d1\u7763\u5b66\u4e60\u3002\u63d0\u53d6\u57fa\u4e8e\u9884\u6d4b\u7f16\u7801\uff08\u91cf\u5316\u5bf9\u5185\u90e8\u5148\u9a8c\u7684\u60ca\u8bb6\u5ea6\uff09\u548c\u4fe1\u606f\u74f6\u9888\uff08\u6d4b\u91cf\u6270\u52a8\u4e0b\u4fe1\u53f7\u4fdd\u7559\uff09\u7684\u53ef\u89e3\u91ca\u4fe1\u53f7\uff0c\u5305\u62ec\u5b9e\u4f53\u805a\u7126\u6444\u53d6\u3001\u4e0a\u4e0b\u6587\u4f9d\u4ece\u6027\u548c\u53ef\u8bc1\u4f2a\u6027\u8bc4\u5206", "result": "\u5728HaluBench\u6570\u636e\u96c6\u4e0a\uff0c\u7406\u8bba\u6307\u5bfc\u57fa\u7ebf\u8fbe\u52300.8017 AUROC\uff0c\u57fa\u7840\u76d1\u7763\u6a21\u578b\u8fbe0.8274 AUROC\uff0c\u6539\u8fdb\u7279\u5f81\u63d0\u5347\u81f30.8669 AUROC\uff084.95%\u589e\u76ca\uff09\u3002\u4f7f\u7528200\u4e2a\u6837\u672c\uff08\u6bd4Lynx\u5c1175\u500d\uff09\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u5feb1000\u500d\uff085ms vs 5s\uff09\uff0c\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027", "conclusion": "\u9886\u57df\u77e5\u8bc6\u7f16\u7801\u7684\u4fe1\u53f7\u67b6\u6784\u76f8\u6bd4\u6269\u5c55LLM\u6cd5\u5b98\u63d0\u4f9b\u66f4\u4f18\u7684\u6570\u636e\u6548\u7387\uff0c\u8f7b\u91cf\u7ea7\uff08<1M\u53c2\u6570\uff09\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u9002\u5408\u751f\u4ea7\u90e8\u7f72\uff0c\u4e14\u53d1\u73b0\u5408\u7406\u5316\u4fe1\u53f7\u65e0\u6cd5\u533a\u5206\u5e7b\u89c9\uff08\"\u5949\u627f\"\u73b0\u8c61\uff09"}}
{"id": "2601.15778", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15778", "abs": "https://arxiv.org/abs/2601.15778", "authors": ["Jiaxin Zhang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Confidence Calibration", "comment": "37 pages, 15 figures, 12 tables", "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u667a\u80fd\u4f53\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u5f00\u53d1HTC\u6846\u67b6\u4ece\u8f68\u8ff9\u4e2d\u63d0\u53d6\u8fc7\u7a0b\u7ea7\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u6821\u51c6\u6027\u80fd\u5e76\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316", "motivation": "AI\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u7cfb\u7edf\u6f14\u8fdb\uff0c\u4f46\u5176\u5728\u5931\u8d25\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u6210\u4e3a\u9ad8\u98ce\u9669\u90e8\u7f72\u7684\u6839\u672c\u969c\u788d\u3002\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u9488\u5bf9\u9759\u6001\u5355\u8f6e\u8f93\u51fa\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u89e3\u51b3\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u72ec\u7279\u6311\u6218\uff0c\u5982\u8f68\u8ff9\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\u3001\u5916\u90e8\u5de5\u5177\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u4e0d\u900f\u660e\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u667a\u80fd\u4f53\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u5f00\u53d1Holistic Trajectory Calibration (HTC)\u8bca\u65ad\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4ece\u667a\u80fd\u4f53\u6574\u4e2a\u8f68\u8ff9\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u8fc7\u7a0b\u7ea7\u7279\u5f81\uff0c\u6db5\u76d6\u4ece\u5b8f\u89c2\u52a8\u6001\u5230\u5fae\u89c2\u7a33\u5b9a\u6027\u3002\u91c7\u7528\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5e76\u5f00\u53d1General Agent Calibrator (GAC)\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\u3002", "result": "HTC\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3001\u591a\u79cdLLM\u548c\u4e0d\u540c\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\uff0c\u5728\u6821\u51c6\u548c\u5224\u522b\u65b9\u9762\u5747\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u3002\u5728\u8de8\u57dfGAIA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cGAC\u5b9e\u73b0\u4e86\u6700\u4f73\u6821\u51c6\uff08\u6700\u4f4eECE\uff09\u3002HTC\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3001\u8de8\u57df\u53ef\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e09\u5927\u8fdb\u5c55\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u5efa\u7acb\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b0\u8303\u5f0f\uff0c\u4e3a\u8bca\u65ad\u548c\u589e\u5f3aAI\u667a\u80fd\u4f53\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\u3002HTC\u901a\u8fc7\u63ed\u793a\u5931\u8d25\u80cc\u540e\u7684\u4fe1\u53f7\u3001\u5b9e\u73b0\u8de8\u57df\u5e94\u7528\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.15931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15931", "abs": "https://arxiv.org/abs/2601.15931", "authors": ["Xiangyu Wang", "Zhixin Lv", "Yongjiao Sun", "Anrui Han", "Ye Yuan", "Hangxu Ji"], "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search", "comment": null, "summary": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.", "AI": {"tldr": "ICON\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u4e0e\u62d3\u6251\u5148\u9a8c\u89e3\u51b3\u6587\u672c\u884c\u4eba\u641c\u7d22\u4e2d\u7684\u865a\u5047\u76f8\u5173\u4e0e\u7a7a\u95f4\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e0d\u53d8\u6027\u4e0e\u73af\u5883\u72ec\u7acb\u6027", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6587\u672c\u884c\u4eba\u641c\u7d22\u65b9\u6cd5\u5728\u590d\u6742\u5f00\u653e\u573a\u666f\u4e2d\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\uff0c\u4f9d\u8d56\"\u88ab\u52a8\u89c2\u5bdf\"\u5bfc\u81f4\u591a\u65b9\u9762\u7684\u865a\u5047\u76f8\u5173\u548c\u7a7a\u95f4\u8bed\u4e49\u9519\u4f4d\uff0c\u7f3a\u4e4f\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027", "method": "\u63d0\u51faICON\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u89c4\u5219\u5f15\u5bfc\u7684\u7a7a\u95f4\u5e72\u9884\uff0c\u60e9\u7f5a\u8fb9\u754c\u6846\u566a\u58f0\u654f\u611f\u6027\uff1b2)\u53cd\u4e8b\u5b9e\u4e0a\u4e0b\u6587\u89e3\u8026\uff0c\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684\u80cc\u666f\u79fb\u690d\u5b9e\u73b0\u73af\u5883\u72ec\u7acb\u6027\uff1b3)\u663e\u8457\u6027\u9a71\u52a8\u7684\u8bed\u4e49\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u5c40\u90e8\u663e\u8457\u6027\u504f\u5dee\uff1b4)\u795e\u7ecf\u7b26\u53f7\u62d3\u6251\u5bf9\u9f50\uff0c\u786e\u4fdd\u6fc0\u6d3b\u533a\u57df\u4e0e\u4eba\u7c7b\u7ed3\u6784\u903b\u8f91\u7684\u62d3\u6251\u4e00\u81f4\u6027", "result": "ICON\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u9886\u5148\u6027\u80fd\uff0c\u5e76\u5728\u906e\u6321\u3001\u80cc\u666f\u5e72\u6270\u548c\u5b9a\u4f4d\u566a\u58f0\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u62df\u5408\u7edf\u8ba1\u5171\u73b0\u8f6c\u5411\u5b66\u4e60\u56e0\u679c\u4e0d\u53d8\u6027\uff0c\u6709\u6548\u63a8\u8fdb\u4e86\u6587\u672c\u884c\u4eba\u641c\u7d22\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2601.15949", "categories": ["cs.AI", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.15949", "abs": "https://arxiv.org/abs/2601.15949", "authors": ["Yiran Wang", "Shuoyuan Wang", "Zhaoran Wei", "Jiannan Zhao", "Zhonghua Yao", "Zejian Xie", "Songxin Zhang", "Jun Huang", "Bingyi Jing", "Hongxin Wei"], "title": "Natural Language-Driven Global Mapping of Martian Landforms", "comment": null, "summary": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.", "AI": {"tldr": "MarScope\u662f\u4e00\u4e2a\u884c\u661f\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u3001\u65e0\u9700\u6807\u7b7e\u7684\u65b9\u5f0f\u5b9e\u73b0\u706b\u661f\u5730\u8c8c\u6620\u5c04\uff0c\u5c06\u884c\u661f\u56fe\u50cf\u4e0e\u6587\u672c\u5bf9\u9f50\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u652f\u6301\u4efb\u610f\u7528\u6237\u67e5\u8be2\u5e76\u57285\u79d2\u5185\u5b8c\u6210\u5168\u7403\u68c0\u7d22\u3002", "motivation": "\u884c\u661f\u8868\u9762\u901a\u5e38\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u9ad8\u7ea7\u8bed\u4e49\u6982\u5ff5\u8fdb\u884c\u5206\u6790\uff0c\u4f46\u5927\u91cf\u8f68\u9053\u56fe\u50cf\u6863\u6848\u4ecd\u4ee5\u50cf\u7d20\u7ea7\u522b\u7ec4\u7ec7\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u9650\u5236\u4e86\u884c\u661f\u8868\u9762\u7684\u53ef\u6269\u5c55\u3001\u5f00\u653e\u5f0f\u63a2\u7d22\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u76f4\u63a5\u63a5\u53e3\u8fdb\u884c\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u5f00\u53d1MarScope\u6846\u67b6\uff0c\u5c06\u884c\u661f\u56fe\u50cf\u548c\u6587\u672c\u5bf9\u9f50\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u3002\u57fa\u4e8e\u8d85\u8fc720\u4e07\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u65e0\u6807\u7b7e\u6620\u5c04\u3002\u8be5\u6846\u67b6\u7528\u7075\u6d3b\u7684\u8bed\u4e49\u68c0\u7d22\u53d6\u4ee3\u9884\u5b9a\u4e49\u5206\u7c7b\uff0c\u652f\u6301\u4efb\u610f\u7528\u6237\u67e5\u8be2\u3002", "result": "MarScope\u80fd\u591f\u57285\u79d2\u5185\u5b8c\u6210\u6574\u4e2a\u706b\u661f\u7684\u4efb\u610f\u67e5\u8be2\uff0cF1\u5206\u6570\u9ad8\u8fbe0.978\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u9650\u4e8e\u5f62\u6001\u5206\u7c7b\uff0c\u8fd8\u80fd\u4fc3\u8fdb\u8fc7\u7a0b\u5bfc\u5411\u5206\u6790\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u884c\u661f\u5c3a\u5ea6\u5730\u8c8c\u5236\u56fe\u3002", "conclusion": "MarScope\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\uff0c\u4f7f\u81ea\u7136\u8bed\u8a00\u6210\u4e3a\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u79d1\u5b66\u53d1\u73b0\u7684\u76f4\u63a5\u63a5\u53e3\uff0c\u6539\u53d8\u4e86\u5168\u7403\u5730\u8c8c\u5236\u56fe\u65b9\u6cd5\uff0c\u4e3a\u884c\u661f\u8868\u9762\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f00\u653e\u5f0f\u6846\u67b6\u3002"}}
{"id": "2601.15953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15953", "abs": "https://arxiv.org/abs/2601.15953", "authors": ["Yongyi Wang", "Hanyu Liu", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "Decoupling Return-to-Go for Efficient Decision Transformer", "comment": null, "summary": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.", "AI": {"tldr": "\u51b3\u7b56\u53d8\u6362\u5668(DT)\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u91c7\u7528\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728RTG\u5e8f\u5217\u5197\u4f59\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u89e3\u8026DT(DDT)\uff0c\u7b80\u5316\u67b6\u6784\uff0c\u4ec5\u7528\u6700\u65b0RTG\u6307\u5bfc\u52a8\u4f5c\u9884\u6d4b\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u53d1\u73b0\u51b3\u7b56\u53d8\u6362\u5668\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u5197\u4f59\u95ee\u9898\uff1a\u5c06\u6574\u4e2aRTG\u5e8f\u5217\u8f93\u5165Transformer\u5728\u7406\u8bba\u4e0a\u662f\u591a\u4f59\u7684\uff0c\u56e0\u4e3a\u53ea\u6709\u6700\u65b0\u7684RTG\u5f71\u54cd\u52a8\u4f5c\u9884\u6d4b\u3002\u8fd9\u79cd\u5197\u4f59\u53ef\u80fd\u635f\u5bb3DT\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u51b3\u7b56\u53d8\u6362\u5668(DDT)\uff0c\u7b80\u5316\u67b6\u6784\uff1a\u4ec5\u5c06\u89c2\u6d4b\u548c\u52a8\u4f5c\u5e8f\u5217\u8f93\u5165Transformer\u5904\u7406\uff0c\u4f7f\u7528\u6700\u65b0\u7684RTG\u6765\u6307\u5bfc\u52a8\u4f5c\u9884\u6d4b\uff0c\u4ece\u800c\u6d88\u9664RTG\u5e8f\u5217\u7684\u5197\u4f59\u3002", "result": "DDT\u663e\u8457\u4f18\u4e8e\u539f\u59cbDT\uff0c\u5e76\u5728\u591a\u4e2a\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684DT\u53d8\u4f53\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u540c\u65f6\uff0c\u7b80\u5316\u67b6\u6784\u8fd8\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664RTG\u5e8f\u5217\u7684\u5197\u4f59\uff0cDDT\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6027\u80fd\u66f4\u597d\u7684\u51b3\u7b56\u53d8\u6362\u5668\u67b6\u6784\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.16027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16027", "abs": "https://arxiv.org/abs/2601.16027", "authors": ["Yiran Qiao", "Xiang Ao", "Jing Chen", "Yang Liu", "Qiwei Zhong", "Qing He"], "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment", "comment": null, "summary": "The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.", "AI": {"tldr": "CS-VAR\uff1a\u7528\u4e8e\u76f4\u64ad\u98ce\u9669\u8bc4\u4f30\u7684\u8de8\u4f1a\u8bdd\u8bc1\u636e\u611f\u77e5\u68c0\u7d22\u589e\u5f3a\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7LLM\u6307\u5bfc\u7684\u5c0f\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u98ce\u9669\u8bc6\u522b", "motivation": "\u76f4\u64ad\u7684\u5174\u8d77\u5e26\u6765\u4e86\u5927\u89c4\u6a21\u5b9e\u65f6\u4e92\u52a8\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u5e73\u53f0\u9762\u4e34\u590d\u6742\u98ce\u9669\uff08\u5982\u8bc8\u9a97\u3001\u534f\u540c\u6076\u610f\u884c\u4e3a\uff09\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u98ce\u9669\u68c0\u6d4b\u56f0\u96be\uff0c\u56e0\u4e3a\u6709\u5bb3\u884c\u4e3a\u5f80\u5f80\u9010\u6e10\u7d2f\u79ef\u5e76\u5728\u770b\u4f3c\u65e0\u5173\u7684\u76f4\u64ad\u4e2d\u91cd\u590d\u51fa\u73b0\u3002", "method": "\u63d0\u51faCS-VAR\uff08\u8de8\u4f1a\u8bdd\u8bc1\u636e\u611f\u77e5\u68c0\u7d22\u589e\u5f3a\u68c0\u6d4b\u5668\uff09\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u5feb\u901f\u4f1a\u8bdd\u7ea7\u98ce\u9669\u63a8\u65ad\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6307\u5bfc\uff0cLLM\u57fa\u4e8e\u68c0\u7d22\u7684\u8de8\u4f1a\u8bdd\u884c\u4e3a\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5c06\u5176\u5c40\u90e8\u5230\u5168\u5c40\u7684\u6d1e\u5bdf\u8f6c\u79fb\u5230\u5c0f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u79bb\u7ebf\u5b9e\u9a8c\u7ed3\u5408\u5728\u7ebf\u9a8c\u8bc1\u8868\u660e\uff0cCS-VAR\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0cCS-VAR\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5c40\u90e8\u5316\u4fe1\u53f7\uff0c\u6709\u6548\u8d4b\u80fd\u73b0\u5b9e\u4e16\u754c\u7684\u76f4\u64ad\u5185\u5bb9\u5ba1\u6838\u3002", "conclusion": "CS-VAR\u8bbe\u8ba1\u4f7f\u5c0f\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u8de8\u76f4\u64ad\u6d41\u7684\u91cd\u590d\u6a21\u5f0f\uff0c\u6267\u884c\u7ed3\u6784\u5316\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u90e8\u7f72\u7684\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u76f4\u64ad\u5e73\u53f0\u590d\u6742\u98ce\u9669\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2601.16038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16038", "abs": "https://arxiv.org/abs/2601.16038", "authors": ["Olga Bunkova", "Lorenzo Di Fruscia", "Sophia Rupprecht", "Artur M. Schweidtmann", "Marcel J. T. Reinders", "Jana M. Weber"], "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval", "comment": "Accepted at ML4Molecules 2025 (ELLIS UnConference workshop), Copenhagen, Denmark, December 2, 2025. Workshop page: https://moleculediscovery.github.io/workshop2025/", "summary": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.", "code_url": "https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval", "code_stars": 1, "code_last_update": "2026-01-23", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u5316\u5b66\u53cd\u5e94\u8def\u5f84\u68c0\u7d22\u8f6c\u5316\u4e3aText2Cypher\uff08\u81ea\u7136\u8bed\u8a00\u5230\u56fe\u67e5\u8be2\uff09\u751f\u6210\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f7f\u7528\u5bf9\u9f50\u793a\u4f8b\u7684\u5c11\u6837\u672c\u63d0\u793a\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u5408\u6210\u89c4\u5212\u4e2d\u5e38\u4ea7\u751f\u5e7b\u89c9\u6216\u8fc7\u65f6\u5efa\u8bae\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5229\u7528\u53cd\u5e94\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u5408\u6210\u8def\u5f84\u68c0\u7d22\u3002", "method": "\u5c06\u53cd\u5e94\u8def\u5f84\u68c0\u7d22\u5b9a\u4e49\u4e3aText2Cypher\u751f\u6210\u95ee\u9898\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u63d0\u793a\u4e0e\u4f7f\u7528\u9759\u6001\u3001\u968f\u673a\u548c\u5d4c\u5165\u5bf9\u9f50\u793a\u4f8b\u7684\u5c11\u6837\u672c\u63d0\u793a\uff0c\u5e76\u8bc4\u4f30\u68c0\u67e5\u8868\u9a71\u52a8\u7684\u9a8c\u8bc1/\u6821\u6b63\u5faa\u73af\u3002", "result": "\u4f7f\u7528\u5bf9\u9f50\u793a\u4f8b\u7684\u5c11\u6837\u672c\u63d0\u793a\u5728\u67e5\u8be2\u6709\u6548\u6027\u548c\u68c0\u7d22\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u68c0\u67e5\u8868\u5f0f\u81ea\u6821\u6b63\u5faa\u73af\u4e3b\u8981\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u63d0\u9ad8\u53ef\u6267\u884c\u6027\uff0c\u5f53\u5df2\u6709\u826f\u597d\u793a\u4f8b\u65f6\u589e\u76ca\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684Text2Cypher\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5408\u6210\u89c4\u5212\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16108", "abs": "https://arxiv.org/abs/2601.16108", "authors": ["Marzieh Adeli Shamsabad", "Hamed Ghodrati"], "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources", "comment": null, "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6c14\u5019\u865a\u5047\u4fe1\u606f\u4e2d\u7684\u8bef\u5bfc\u6027\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u6a21\u578b\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u65f6\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6c14\u5019\u865a\u5047\u4fe1\u606f\u5728\u6570\u5b57\u4e16\u754c\u4e2d\u65e5\u76ca\u4e25\u91cd\uff0c\u7279\u522b\u662f\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f20\u64ad\u7684\u8bef\u5bfc\u6027\u56fe\u50cf\u548c\u89c6\u9891\u3002\u8fd9\u4e9b\u865a\u5047\u5185\u5bb9\u5f80\u5f80\u5177\u6709\u8bf4\u670d\u529b\u4e14\u96be\u4ee5\u68c0\u6d4b\uff0c\u53ef\u80fd\u5ef6\u7f13\u6c14\u5019\u884c\u52a8\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u4f9d\u8d56\u8bad\u7ec3\u65f6\u7684\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5904\u7406\u8fd1\u671f\u4e8b\u4ef6\u6216\u66f4\u65b0\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u7cfb\u7edf\u80fd\u591f\u83b7\u53d6\u6700\u65b0\u4fe1\u606f\uff0c\u5305\u62ec\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u7ed3\u679c\u3001\u5728\u7ebf\u4e8b\u5b9e\u6838\u67e5\u548c\u53ef\u4fe1\u4e13\u5bb6\u5185\u5bb9\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u56fe\u50cf\u53ca\u5176\u58f0\u79f0\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6c14\u5019\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u56fe\u50cf\u53ca\u5176\u58f0\u79f0\u5185\u5bb9\u662f\u51c6\u786e\u3001\u8bef\u5bfc\u3001\u865a\u5047\u8fd8\u662f\u65e0\u6cd5\u9a8c\u8bc1\u7684\u3002", "conclusion": "\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u5bf9\u6c14\u5019\u865a\u5047\u4fe1\u606f\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u516c\u4f17\u5bf9\u79d1\u5b66\u7684\u7406\u89e3\uff0c\u9002\u5e94\u5feb\u901f\u53d8\u5316\u7684\u4fe1\u606f\u73af\u5883\u3002"}}
{"id": "2601.16134", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.16134", "abs": "https://arxiv.org/abs/2601.16134", "authors": ["Langdon Holmes", "Adam Coscia", "Scott Crossley", "Joon Suh Choi", "Wesley Morris"], "title": "LLM Prompt Evaluation for Educational Applications", "comment": null, "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bc4\u4f30LLM\u63d0\u793a\u6a21\u677f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9526\u6807\u8d5b\u5f0f\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u6559\u80b2\u5e94\u7528\u4e2dLLM\u751f\u6210\u7684\u540e\u7eed\u95ee\u9898\uff0c\u53d1\u73b0\u7ed3\u5408\u89d2\u8272\u626e\u6f14\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6a21\u5f0f\u7684\u63d0\u793a\u5728\u652f\u6301\u5143\u8ba4\u77e5\u5b66\u4e60\u7b56\u7565\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740LLM\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u57fa\u4e8e\u8bc1\u636e\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u8bc4\u4f30\u80fd\u591f\u4ea7\u751f\u4e2a\u6027\u5316\u3001\u6559\u5b66\u5bf9\u9f50\u8f93\u51fa\u7684\u63d0\u793a\u3002\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u8d85\u8d8a\u4e34\u65f6\u6027\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u8f6c\u5411\u57fa\u4e8e\u8bc1\u636e\u7684\u63d0\u793a\u5f00\u53d1\u3002", "method": "\u8bbe\u8ba1\u4e866\u4e2a\u5f3a\u8c03\u4e0d\u540c\u6559\u5b66\u7b56\u7565\u7684\u63d0\u793a\u6a21\u677f\uff0c\u91c7\u7528\u9526\u6807\u8d5b\u5f0f\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528Glicko2\u8bc4\u5206\u7cfb\u7edf\uff0c\u75318\u4f4d\u8bc4\u59d4\u4ece\u683c\u5f0f\u3001\u5bf9\u8bdd\u652f\u6301\u548c\u5b66\u4e60\u8005\u9002\u5b9c\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u95ee\u9898\u5bf9\u3002\u6570\u636e\u6765\u81ea3\u4e2a\u4e0d\u540c\u6559\u80b2\u90e8\u7f72\u4e2d\u7684120\u4e2a\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u89d2\u8272\u626e\u6f14\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6a21\u5f0f\u7684\u63d0\u793a\uff08\u652f\u6301\u5143\u8ba4\u77e5\u5b66\u4e60\u7b56\u7565\u5982\u81ea\u4e3b\u5b66\u4e60\uff09\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6210\u5bf9\u6bd4\u8f83\u4e2d\u83b7\u80dc\u6982\u7387\u4e3a81%\u5230100%\u3002\u8be5\u63d0\u793a\u5728\u652f\u6301\u6218\u7565\u9605\u8bfb\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u677f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6559\u80b2\u6280\u672f\u7814\u7a76\u8005\u5982\u4f55\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\uff0c\u4ece\u4e34\u65f6\u6027\u7684\u63d0\u793a\u5de5\u7a0b\u8f6c\u5411\u57fa\u4e8e\u8bc1\u636e\u7684\u6559\u80b2\u5e94\u7528\u63d0\u793a\u5f00\u53d1\uff0c\u4e3a\u4e2a\u6027\u5316\u3001\u6559\u5b66\u5bf9\u9f50\u7684LLM\u8f93\u51fa\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.16172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16172", "abs": "https://arxiv.org/abs/2601.16172", "authors": ["Zachary Burton"], "title": "Structured Hints for Sample-Efficient Lean Theorem Proving", "comment": "9 pages, 1 figure", "summary": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.", "AI": {"tldr": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u63a8\u7406\u65f6\u63d0\u793a\u8c03\u5ea6\uff0815\u4e2a\u5e38\u89c1\u7b56\u7565\u9aa8\u67b6\uff09\u4f7fDeepSeek-Prover-V1.5\u7684pass@16\u4ece15.2%\u63d0\u5347\u523021.7%\uff0c\u76f8\u5bf9\u63d0\u534743%", "motivation": "\u7814\u7a76\u9ad8\u5ea6\u8bad\u7ec3\u7684\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u5668\uff08\u5982DeepSeek-Prover-V1.5\uff09\u662f\u5426\u4ecd\u80fd\u4ece\u63a8\u7406\u65f6\u7684\u7b80\u5355\u7ed3\u6784\u6307\u5bfc\u4e2d\u53d7\u76ca\uff0c\u63a2\u7d22RL\u8bad\u7ec3\u6a21\u578b\u662f\u5426\u672a\u5145\u5206\u5229\u7528\u7b56\u7565\u8bed\u8a00\u4e2d\u53ef\u7528\u7684\u7ed3\u6784\u5148\u9a8c", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5e72\u9884\u65b9\u6cd5\uff1a\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u56fa\u5b9a\u7684\u63d0\u793a\u8c03\u5ea6\uff0c\u8986\u76d615\u4e2a\u5e38\u89c1\u7b56\u7565\u9aa8\u67b6\uff0c\u4e0e\u6807\u51c6\u91c7\u6837\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4f7f\u7528\u76f8\u540c\u6837\u672c\u6570\uff08k=16\uff09\u548c\u6700\u5927\u751f\u6210\u957f\u5ea6\uff081024\u4e2atoken\uff09", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u63d0\u793a\u8c03\u5ea6\u7684\u65b9\u6cd5\u8fbe\u523021.7%\u7684pass@16\uff0c\u76f8\u6bd4\u6807\u51c6\u91c7\u6837\u768415.2%\u6709\u663e\u8457\u63d0\u5347\uff0c\u76f8\u5bf9\u6539\u8fdb43%\uff0c\u8868\u660e\u7b80\u5355\u63a8\u7406\u65f6\u6307\u5bfc\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd", "conclusion": "\u5373\u4f7f\u80fd\u529b\u5f3a\u5927\u7684RL\u8bad\u7ec3\u8bc1\u660e\u5668\u4e5f\u672a\u80fd\u5145\u5206\u5229\u7528\u7b56\u7565\u8bed\u8a00\u4e2d\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u7b80\u5355\u7684\u63a8\u7406\u65f6\u6307\u5bfc\u4ecd\u7136\u662f\u5ec9\u4ef7\u4e14\u4e92\u8865\u7684\u6027\u80fd\u63d0\u5347\u65b9\u6cd5"}}
{"id": "2601.16216", "categories": ["cs.AI", "cs.GT", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.16216", "abs": "https://arxiv.org/abs/2601.16216", "authors": ["Cl\u00e9mentine Sacr\u00e9"], "title": "Scalable Board Expansion within a General Game System", "comment": "65 pages, 41 figures", "summary": "This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u901a\u7528\u6e38\u620f\u7cfb\u7edf\u7684\u52a8\u6001\u68cb\u76d8\u6269\u5c55\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u65e0\u68cb\u76d8\u6e38\u620f\u4e2d\u9759\u6001\u5927\u68cb\u76d8\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u548c\u590d\u6742\u5ea6\u95ee\u9898", "motivation": "\u4f20\u7edf\u65e0\u68cb\u76d8\u6e38\u620f\u5b9e\u73b0\u901a\u5e38\u4f9d\u8d56\u9884\u5148\u5b9a\u4e49\u7684\u8d85\u5927\u9759\u6001\u68cb\u76d8\uff0c\u5373\u4f7f\u5927\u90e8\u5206\u533a\u57df\u5728\u6e38\u620f\u4e2d\u4ece\u672a\u4f7f\u7528\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u6d6a\u8d39", "method": "\u91c7\u7528\u901a\u7528\u6e38\u620f\u7cfb\u7edf\u652f\u6301\u52a8\u6001\u68cb\u76d8\u6269\u5c55\u673a\u5236\uff0c\u5728\u6e38\u620f\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u6269\u5c55\u6e38\u620f\u68cb\u76d8", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u68cb\u76d8\u6269\u5c55\u673a\u5236\uff0c\u4f46\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u6216\u6027\u80fd\u6570\u636e", "conclusion": "\u52a8\u6001\u68cb\u76d8\u6269\u5c55\u673a\u5236\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u68cb\u76d8\u5e26\u6765\u7684\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u63d0\u9ad8\u65e0\u68cb\u76d8\u6e38\u620f\u5b9e\u73b0\u7684\u6548\u7387\u548c\u7b80\u6d01\u6027"}}
{"id": "2601.16041", "categories": ["math.ST", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.16041", "abs": "https://arxiv.org/abs/2601.16041", "authors": ["Omar Al-Ghattas"], "title": "Risk reversal for least squares estimators under nested convex constraints", "comment": "31 pages, 5 figures", "summary": "In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.\n  We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $\u0398\\subset \\mathbb{R}^d$ one observes \\[ Y = \u03b8^\\star + \u03c3Z, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $\u03b8^\\star \\in \u0398$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $\u0398$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $\u0398_S \\subset \u0398_L$ and a parameter $\u03b8^\\star \\in \u0398_S$ such that the LSE constrained to $\u0398_S$ has strictly larger risk than the LSE constrained to $\u0398_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.\n  We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $\u03b8^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $\u0398_S$ within $\u0398_L$ can reverse the risk ordering.\n  These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.", "AI": {"tldr": "\u5728\u7ea6\u675f\u968f\u673a\u4f18\u5316\u4e2d\uff0c\u5bf9\u66f4\u4e25\u683c\u7684\u53ef\u884c\u96c6\u8fdb\u884c\u6295\u5f71\u7684\u4f30\u8ba1\u5668\u53ef\u80fd\u6bd4\u5728\u66f4\u5bbd\u677e\u96c6\u4e0a\u7684\u4f30\u8ba1\u5668\u5177\u6709\u66f4\u5927\u7684\u7edf\u8ba1\u98ce\u9669\uff0c\u8fd9\u4e0e\u76f4\u89c9\u76f8\u53cd\u3002", "motivation": "\u7814\u7a76\u5728\u7ea6\u675f\u968f\u673a\u4f18\u5316\u4e2d\uff0c\u76f4\u89c9\u8ba4\u4e3a\u65bd\u52a0\u66f4\u4e25\u683c\u7684\u53ef\u884c\u96c6\u4e0d\u4f1a\u589e\u52a0\u6295\u5f71\u4f30\u8ba1\u5668\u7684\u7edf\u8ba1\u98ce\u9669\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u8fd9\u79cd\u76f4\u89c9\u5728\u5178\u578b\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u5931\u6548\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u5e8f\u5217\u6a21\u578b\u4f5c\u4e3a\u6d4b\u8bd5\u57fa\u51c6\uff0c\u6784\u9020\u5d4c\u5957\u7d27\u51f8\u96c6\u7684\u5177\u4f53\u4f8b\u5b50\uff0c\u6bd4\u8f83\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\u5728\u4e0d\u540c\u7ea6\u675f\u96c6\u4e0a\u7684\u98ce\u9669\uff0c\u5206\u6790\u4e0d\u540c\u566a\u58f0\u673a\u5236\u4e0b\u7684\u98ce\u9669\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e86\u98ce\u9669\u53cd\u8f6c\u73b0\u8c61\uff1a\u5728\u8db3\u591f\u5927\u7684\u566a\u58f0\u4e0b\uff0c\u5b58\u5728\u5d4c\u5957\u7d27\u51f8\u96c6\u548c\u53c2\u6570\uff0c\u4f7f\u5f97\u8f83\u5c0f\u7ea6\u675f\u96c6\u4e0a\u7684LSE\u98ce\u9669\u4e25\u683c\u5927\u4e8e\u8f83\u5927\u7ea6\u675f\u96c6\u4e0a\u7684LSE\u98ce\u9669\u3002\u8fd9\u79cd\u73b0\u8c61\u5728\u6781\u5c0f\u566a\u58f0\u548c\u6781\u5927\u566a\u58f0\u673a\u5236\u4e0b\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u6295\u5f71\u4f30\u8ba1\u5668\u5b58\u5728\u5148\u524d\u672a\u88ab\u8bc6\u522b\u7684\u5931\u6548\u6a21\u5f0f\uff1a\u5728\u8db3\u591f\u5608\u6742\u7684\u73af\u5883\u4e2d\uff0c\u6536\u7d27\u7ea6\u675f\u53ef\u80fd\u53cd\u800c\u4f1a\u964d\u4f4e\u7edf\u8ba1\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u7ea6\u675f\u4f18\u5316\u4e2d\u98ce\u9669\u6392\u5e8f\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2601.15370", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15370", "abs": "https://arxiv.org/abs/2601.15370", "authors": ["Maciej Kilian", "Oleg Mkrtchyan", "Luke Zettlemoyer", "Akshat Shrivastava", "Armen Aghajanyan"], "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity", "comment": null, "summary": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u96f6\u8ba1\u7b97\uff08null\uff09\u4e13\u5bb6\u5b9e\u73b0\u6570\u636e\u7a00\u758f\u6027\uff0c\u7ed3\u5408\u6743\u91cd\u7a00\u758f\u6027\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u9690\u5f0f\u6a21\u6001\u611f\u77e5\u8def\u7531", "motivation": "\u6df7\u5408\u4e13\u5bb6\u5c42\u901a\u8fc7\u6743\u91cd\u7a00\u758f\u6027\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u6570\u636e\u7a00\u758f\u6027\uff08\u6bcf\u4e2a\u4e13\u5bb6\u4ec5\u5904\u7406\u90e8\u5206token\uff09\u63d0\u4f9b\u4e86\u8865\u5145\u7ef4\u5ea6\u3002\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u76f4\u63a5\u5b9e\u73b0\u6570\u636e\u7a00\u758f\u6027\uff0c\u4f46\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u8fdd\u53cd\u56e0\u679c\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u3002\u9700\u8981\u5728\u4e0d\u8fdd\u53cd\u56e0\u679c\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6570\u636e\u7a00\u758f\u6027\u3002", "method": "\u5728\u56e0\u679ctoken\u9009\u62e9MoE\u4e2d\u5f15\u5165\u96f6\u8ba1\u7b97\uff08null\uff09\u4e13\u5bb6\u5230\u8def\u7531\u6c60\u4e2d\u3002\u5f53token\u8def\u7531\u5230null\u4e13\u5bb6\u65f6\uff0c\u8fd9\u4e9b\u69fd\u4f4d\u4e0d\u6d88\u8017\u8ba1\u7b97\u3002\u901a\u8fc7\u6807\u51c6\u7684\u8d1f\u8f7d\u5747\u8861\u76ee\u6807\u8bad\u7ec3\u6a21\u578b\u5747\u5300\u4f7f\u7528\u6240\u6709\u4e13\u5bb6\uff08\u771f\u5b9e\u548cnull\uff09\uff0c\u4ece\u800c\u5728\u671f\u671b\u4e0a\u5b9e\u73b0\u6570\u636e\u7a00\u758f\u6027\u800c\u4e0d\u8fdd\u53cd\u56e0\u679c\u6027\u3002", "result": "\u5728\u5339\u914d\u7684\u9884\u671fFLOPs\u4e0b\uff0c\u7ec4\u5408\u6743\u91cd\u548c\u6570\u636e\u7a00\u758f\u6027\u6bd4\u4ec5\u4f7f\u7528\u6743\u91cd\u7a00\u758f\u6027\u4ea7\u751f\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u8fb9\u754c\uff0c\u5728\u8bad\u7ec3\u635f\u5931\u548c\u4e0b\u6e38\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\u3002\u6a21\u578b\u5b66\u4e60\u5230\u9690\u5f0f\u7684\u6a21\u6001\u611f\u77e5\u5206\u914d\uff0c\u5c06\u89c6\u89c9token\u6bd4\u6587\u672ctoken\u66f4\u79ef\u6781\u5730\u8def\u7531\u5230null\u4e13\u5bb6\uff0c\u65e0\u9700\u663e\u5f0f\u6a21\u6001\u8def\u7531\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165null\u4e13\u5bb6\u5b9e\u73b0\u6570\u636e\u7a00\u758f\u6027\uff0c\u7ed3\u5408\u6743\u91cd\u7a00\u758f\u6027\uff0c\u53ef\u4ee5\u5728\u4e0d\u8fdd\u53cd\u56e0\u679c\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6df7\u5408\u4e13\u5bb6\u5c42\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fd9\u79cd\u6570\u636e\u5f02\u6784\u6027\u663e\u8457\u7684\u5e94\u7528\u4e2d\u6548\u679c\u660e\u663e\u3002"}}
{"id": "2601.16058", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.16058", "abs": "https://arxiv.org/abs/2601.16058", "authors": ["Claudia Kirch", "Hedvika Rano\u0161ov\u00e1", "Martin Wendler"], "title": "Fully Functional Weighted Testing for Abrupt and Gradual Location Changes in Functional Time Series", "comment": null, "summary": "Change point tests for abrupt changes in the mean of functional data, i.e., random elements in infinite-dimensional Hilbert spaces, are either based on dimension reduction techniques, e.g., based on principal components, or directly based on a functional CUSUM (cumulative sum) statistic. The former have often been criticized as not being fully functional and losing too much information. On the other hand, unlike the latter, they take the covariance structure of the data into account by weighting the CUSUM statistics obtained after dimension reduction with the inverse covariance matrix. In this paper, as a middle ground between these two approaches, we propose an alternative statistic that includes the covariance structure with an offset parameter to produce a scale-invariant test procedure and to increase power when the change is not aligned with the first components. We obtain the asymptotic distribution under the null hypothesis for this new test statistic, allowing for time dependence of the data. Furthermore, we introduce versions of all three test statistics for gradual change situations, which have not been previously considered for functional data, and derive their limit distribution. Further results shed light on the asymptotic power behavior for all test statistics under various ground truths for the alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51fd\u6570\u6570\u636e\u5747\u503c\u7a81\u53d8\u70b9\u68c0\u9a8c\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8e\u4e3b\u6210\u5206\u964d\u7ef4\u7684\u52a0\u6743CUSUM\u65b9\u6cd5\u548c\u5b8c\u5168\u51fd\u6570CUSUM\u65b9\u6cd5\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u901a\u8fc7\u5f15\u5165\u504f\u79fb\u53c2\u6570\u8003\u8651\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u63d0\u9ad8\u68c0\u9a8c\u529f\u6548\u3002", "motivation": "\u73b0\u6709\u51fd\u6570\u6570\u636e\u5747\u503c\u7a81\u53d8\u70b9\u68c0\u9a8c\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4e3b\u6210\u5206\u964d\u7ef4\u7684\u65b9\u6cd5\u867d\u7136\u8003\u8651\u4e86\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u4f46\u53ef\u80fd\u635f\u5931\u4fe1\u606f\u4e14\u975e\u5b8c\u5168\u51fd\u6570\u65b9\u6cd5\uff1b\u5b8c\u5168\u51fd\u6570CUSUM\u65b9\u6cd5\u867d\u7136\u5b8c\u5168\u51fd\u6570\u4f46\u672a\u8003\u8651\u534f\u65b9\u5dee\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u6298\u4e2d\u65b9\u6cd5\uff0c\u65e2\u80fd\u8003\u8651\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u53c8\u80fd\u4fdd\u6301\u51fd\u6570\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5728\u51fd\u6570CUSUM\u7edf\u8ba1\u91cf\u57fa\u7840\u4e0a\u5f15\u5165\u504f\u79fb\u53c2\u6570\uff0c\u4f7f\u5176\u5177\u6709\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u5e76\u63d0\u9ad8\u5f53\u53d8\u5316\u4e0d\u4e0e\u4e3b\u6210\u5206\u5bf9\u9f50\u65f6\u7684\u68c0\u9a8c\u529f\u6548\u3002\u63a8\u5bfc\u4e86\u8be5\u7edf\u8ba1\u91cf\u5728\u96f6\u5047\u8bbe\u4e0b\u7684\u6e10\u8fd1\u5206\u5e03\uff0c\u5141\u8bb8\u6570\u636e\u5b58\u5728\u65f6\u95f4\u4f9d\u8d56\u6027\u3002\u540c\u65f6\u4e3a\u4e09\u79cd\u7edf\u8ba1\u91cf\uff08\u4e3b\u6210\u5206\u52a0\u6743CUSUM\u3001\u51fd\u6570CUSUM\u3001\u65b0\u65b9\u6cd5\uff09\u5f15\u5165\u4e86\u6e10\u8fdb\u53d8\u5316\u7684\u7248\u672c\u3002", "result": "\u83b7\u5f97\u4e86\u65b0\u68c0\u9a8c\u7edf\u8ba1\u91cf\u5728\u96f6\u5047\u8bbe\u4e0b\u7684\u6e10\u8fd1\u5206\u5e03\uff0c\u5e76\u63a8\u5bfc\u4e86\u6240\u6709\u4e09\u79cd\u7edf\u8ba1\u91cf\u5728\u6e10\u8fdb\u53d8\u5316\u60c5\u51b5\u4e0b\u7684\u6781\u9650\u5206\u5e03\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u5404\u79cd\u5907\u62e9\u5047\u8bbe\u4e0b\u6240\u6709\u68c0\u9a8c\u7edf\u8ba1\u91cf\u7684\u6e10\u8fd1\u529f\u6548\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u68c0\u9a8c\u65b9\u6cd5\u5728\u8003\u8651\u534f\u65b9\u5dee\u7ed3\u6784\u548c\u4fdd\u6301\u51fd\u6570\u7279\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u901a\u8fc7\u504f\u79fb\u53c2\u6570\u63d0\u9ad8\u4e86\u68c0\u9a8c\u529f\u6548\uff0c\u7279\u522b\u662f\u5728\u53d8\u5316\u4e0d\u4e0e\u4e3b\u6210\u5206\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u3002\u8be5\u65b9\u6cd5\u4e3a\u51fd\u6570\u6570\u636e\u5747\u503c\u7a81\u53d8\u70b9\u68c0\u9a8c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u5e76\u6269\u5c55\u5230\u4e86\u6e10\u8fdb\u53d8\u5316\u573a\u666f\u3002"}}
{"id": "2601.15408", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "Karen S\u00e1nchez", "Carlos Hinojosa", "Denis Parra", "\u00c1lvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "code_url": "https://github.com/PabloMessina/CURE", "code_stars": 0, "code_last_update": "2026-01-20", "AI": {"tldr": "CURE\u662f\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u9519\u8bef\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6837\u672c\u91c7\u6837\u7b56\u7565\u6765\u63d0\u5347\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u5b58\u5728\u89c6\u89c9\u5b9a\u4f4d\u4e0d\u51c6\u786e\u548c\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u53ef\u9760\u4e14\u7f3a\u4e4f\u5145\u5206\u7684\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002", "method": "CURE\u91c7\u7528\u9519\u8bef\u611f\u77e5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5bf9\u591a\u6a21\u6001\u6307\u4ee4\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5305\u62ec\u77ed\u8bed\u5b9a\u4f4d\u3001\u57fa\u4e8e\u5b9a\u4f4d\u7684\u62a5\u544a\u751f\u6210\u548c\u89e3\u5256\u5b66\u5b9a\u4f4d\u62a5\u544a\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\uff0c\u5e76\u57fa\u4e8e\u6a21\u578b\u6027\u80fd\u52a8\u6001\u8c03\u6574\u6837\u672c\u91c7\u6837\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u56f0\u96be\u6837\u672c\u4ee5\u6539\u5584\u7a7a\u95f4\u548c\u6587\u672c\u5bf9\u9f50\u3002", "result": "CURE\u5c06\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u5347\u4e86+0.37 IoU\uff0c\u62a5\u544a\u8d28\u91cf\u63d0\u9ad8\u4e86+0.188 CXRFEScore\uff0c\u5e7b\u89c9\u51cf\u5c11\u4e8618.6%\uff0c\u663e\u8457\u6539\u5584\u4e86\u7a7a\u95f4\u548c\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "CURE\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u5347\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u62a5\u544a\u53ef\u9760\u6027\uff0c\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15500", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.15500", "abs": "https://arxiv.org/abs/2601.15500", "authors": ["Saptarshi Roy", "Alessandro Rinaldo", "Purnamrita Sarkar"], "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization", "comment": "32 pages, 7 figures", "summary": "In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of\n  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6574\u6d41\u6d41\uff08RF\uff09\u5982\u4f55\u81ea\u52a8\u9002\u5e94\u76ee\u6807\u5206\u5e03\u7684\u5185\u5728\u4f4e\u7ef4\u6027\u4ee5\u52a0\u901f\u91c7\u6837\uff0c\u8bc1\u660e\u4e86RF\u91c7\u6837\u5668\u5177\u6709O(k/\u03b5)\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5efa\u7acb\u4e86DDPM\u4e0e\u968f\u673aRF\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9002\u5e94\u4f4e\u7ef4\u76ee\u6807\u5206\u5e03\u7684\u968f\u673aRF\u91c7\u6837\u5668\u3002", "motivation": "\u8fd1\u5e74\u6765\u6574\u6d41\u6d41\uff08RF\uff09\u56e0\u5176\u751f\u6210\u6548\u7387\u548c\u6700\u5148\u8fdb\u6027\u80fd\u800c\u5e7f\u53d7\u6b22\u8fce\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76RF\u5982\u4f55\u81ea\u52a8\u9002\u5e94\u76ee\u6807\u5206\u5e03\u652f\u6301\u7684\u5185\u5728\u4f4e\u7ef4\u6027\u6765\u52a0\u901f\u91c7\u6837\uff0c\u5e76\u63a2\u7d22RF\u4e0eDDPM\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002", "method": "1. \u5206\u6790RF\u91c7\u6837\u5668\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65f6\u95f4\u79bb\u6563\u5316\u65b9\u6848\u4e0b\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\uff1b2. \u5efa\u7acbDDPM\u4e0e\u968f\u673aRF\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u968f\u673a\u5b9a\u4f4d\u5efa\u7acb\u7406\u8bba\u8054\u7cfb\uff1b3. \u8bbe\u8ba1\u9002\u5e94\u76ee\u6807\u5206\u5e03\u4f4e\u7ef4\u6027\u7684\u968f\u673aRF\u91c7\u6837\u5668\uff0c\u91c7\u7528\u7279\u5b9a\u7684\u65f6\u95f4\u8c03\u5ea6\u65b9\u6848\u3002", "result": "1. \u8bc1\u660e\u5728\u7cbe\u786e\u6f02\u79fb\u4f30\u8ba1\u4e0b\uff0cRF\u91c7\u6837\u5668\u5177\u6709O(k/\u03b5)\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\uff08k\u4e3a\u76ee\u6807\u5206\u5e03\u5185\u5728\u7ef4\u5ea6\uff0c\u03b5\u4e3a\u603b\u53d8\u5dee\u8ddd\u79bb\u7cbe\u5ea6\uff09\uff1b2. \u5efa\u7acb\u4e86DDPM\u4e0e\u968f\u673aRF\u7684\u7b49\u4ef7\u5173\u7cfb\uff1b3. \u8bbe\u8ba1\u4e86\u5728\u6f02\u79fb\u4f30\u8ba1\u7cbe\u5ea6\u8981\u6c42\u66f4\u5bbd\u677e\u6761\u4ef6\u4e0b\u4ecd\u80fd\u9002\u5e94\u76ee\u6807\u5206\u5e03\u4f4e\u7ef4\u6027\u7684\u968f\u673aRF\u91c7\u6837\u5668\uff1b4. \u5728\u5408\u6210\u6570\u636e\u548c\u6587\u751f\u56fe\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b0\u91c7\u6837\u5668\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RF\u91c7\u6837\u5668\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u76ee\u6807\u5206\u5e03\u7684\u5185\u5728\u4f4e\u7ef4\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u91c7\u6837\u52a0\u901f\u3002\u901a\u8fc7\u5efa\u7acbDDPM\u4e0e\u968f\u673aRF\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u8bbe\u8ba1\u4e86\u66f4\u7075\u6d3b\u7684\u968f\u673aRF\u91c7\u6837\u5668\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u5bf9\u6f02\u79fb\u4f30\u8ba1\u7cbe\u5ea6\u7684\u8981\u6c42\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u5b9e\u7528\u91c7\u6837\u65b9\u6cd5\u3002"}}
{"id": "2601.15416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "DuFal\u662f\u4e00\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u9525\u675fCT\u91cd\u5efa\u7684\u53cc\u9891\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u9ad8\u9891\u589e\u5f3a\u7684\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u6765\u6062\u590d\u4f20\u7edfCNN\u65b9\u6cd5\u96be\u4ee5\u91cd\u5efa\u7684\u9ad8\u9891\u89e3\u5256\u7ec6\u8282\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u9525\u675fCT\u91cd\u5efa\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u6709\u9650X\u5c04\u7ebf\u6295\u5f71\u4e0b\u9ad8\u9891\u89e3\u5256\u7ec6\u8282\u7684\u6b20\u91c7\u6837\u95ee\u9898\u3002\u4f20\u7edfCNN\u65b9\u6cd5\u503e\u5411\u4e8e\u5b66\u4e60\u4f4e\u9891\u4fe1\u606f\uff0c\u96be\u4ee5\u6062\u590d\u7cbe\u7ec6\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDuFal\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\u6574\u5408\u9891\u57df\u548c\u7a7a\u57df\u5904\u7406\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1\uff09\u9ad8\u5c40\u90e8\u56e0\u5b50\u5316\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff0c\u5305\u542b\u5168\u5c40\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c\u5c40\u90e8\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u4e24\u4e2a\u4e92\u8865\u5206\u652f\uff1b2\uff09\u8c31\u901a\u9053\u56e0\u5b50\u5316\u65b9\u6848\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff1b3\uff09\u4ea4\u53c9\u6ce8\u610f\u529b\u9891\u7387\u878d\u5408\u6a21\u5757\u6709\u6548\u6574\u5408\u7a7a\u95f4\u548c\u9891\u7387\u7279\u5f81\uff1b4\uff09\u7279\u5f81\u89e3\u7801\u5668\u751f\u6210\u6295\u5f71\u8868\u793a\uff1b5\uff09\u5f3a\u5ea6\u573a\u89e3\u7801\u7ba1\u9053\u91cd\u5efa\u6700\u7ec8CT\u4f53\u79ef\u3002", "result": "\u5728LUNA16\u548cToothFairy\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDuFal\u5728\u4fdd\u6301\u9ad8\u9891\u89e3\u5256\u7279\u5f81\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u7a00\u758f\u89c6\u56fe\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DuFal\u901a\u8fc7\u53cc\u9891\u611f\u77e5\u5b66\u4e60\u548c\u521b\u65b0\u7684\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\u7684\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u7cbe\u7ec6\u7ed3\u6784\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15417", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15417", "abs": "https://arxiv.org/abs/2601.15417", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "William Daspit", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis", "Giannis Daras"], "title": "Ambient Dataloops: Generative Models for Dataset Refinement", "comment": "27 pages, 9 figures, 11 tables", "summary": "We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.", "AI": {"tldr": "Ambient Dataloops\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u5f0f\u6570\u636e\u96c6\u7cbe\u70bc\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u7684\u534f\u540c\u8fdb\u5316\u8fc7\u7a0b\uff0c\u9010\u6b65\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\uff0c\u907f\u514d\u81ea\u6d88\u8017\u5faa\u73af\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u96c6\u5305\u542b\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\u7684\u6837\u672c\uff0c\u76f4\u63a5\u5728\u8fd9\u6837\u5f02\u8d28\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u5f80\u5f80\u5f97\u5230\u6b21\u4f18\u6a21\u578b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u540c\u65f6\u907f\u514d\u81ea\u6d88\u8017\u5faa\u73af\u7684\u7834\u574f\u6027\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u6570\u636e\u96c6-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u8fc7\u7a0b\uff1a\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u5c06\u5408\u6210\u6539\u8fdb\u7684\u6837\u672c\u89c6\u4e3a\u566a\u58f0\u6837\u672c\uff08\u4f46\u566a\u58f0\u6c34\u5e73\u7565\u4f4e\u4e8e\u524d\u4e00\u6b21\u8fed\u4ee3\uff09\uff0c\u4f7f\u7528Ambient Diffusion\u6280\u672f\u8fdb\u884c\u5728\u566a\u58f0\u4e0b\u7684\u5b66\u4e60\uff0c\u4f7f\u6570\u636e\u96c6\u9010\u6b65\u63d0\u5347\u8d28\u91cf\uff0c\u6a21\u578b\u76f8\u5e94\u6539\u8fdb\u3002", "result": "Ambient Dataloops\u5728\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u4ece\u5934\u86cb\u767d\u8d28\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6570\u636e\u5faa\u73af\u8fc7\u7a0b\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u5f0f\u6570\u636e\u96c6\u7cbe\u70bc\u548cAmbient Diffusion\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28\u6570\u636e\u96c6\u8bad\u7ec3\u95ee\u9898\uff0c\u907f\u514d\u4e86\u81ea\u6d88\u8017\u5faa\u73af\uff0c\u5728\u591a\u4e2a\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2601.15473", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15473", "abs": "https://arxiv.org/abs/2601.15473", "authors": ["Fahd Seddik", "Abdulrahman Elbedewy", "Gaser Sami", "Mohamed Abdelmoniem", "Yahia Zakaria"], "title": "Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra", "comment": "5 pages, 3 figures, 2 listings", "summary": "Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.", "code_url": "https://github.com/FahdSeddik/panther", "code_stars": 2, "code_last_update": "2026-01-21", "AI": {"tldr": "Panther\u662f\u4e00\u4e2aPyTorch\u517c\u5bb9\u7684RandNLA\u5e93\uff0c\u901a\u8fc7\u9ad8\u6548\u7b97\u6cd5\u538b\u7f29\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8fbe75%\u7684\u5185\u5b58\u8282\u7701", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u53d7GPU\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\uff0cRandNLA\u6280\u672f\u867d\u7136\u6709\u6548\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u751f\u4ea7\u7ea7\u5e93\uff0c\u963b\u788d\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e7f\u6cdb\u91c7\u7528", "method": "\u5f00\u53d1Panther\u5e93\uff0c\u6574\u5408\u6210\u719f\u7684RandNLA\u7b97\u6cd5\uff0c\u63d0\u4f9bsketched\u7ebf\u6027\u5c42\u30012D\u5377\u79ef\u3001\u591a\u5934\u6ce8\u610f\u529b\u7b49\u6807\u51c6\u7ec4\u4ef6\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7C++/CUDA\u540e\u7aef(pawX)\u5b9e\u73b0CPU/GPU\u4f18\u5316", "result": "\u4ec5\u9700\u51e0\u884c\u4ee3\u7801\u66ff\u6362PyTorch\u7ebf\u6027\u5c42\uff0c\u5373\u53ef\u5728BERT\u4e0a\u5b9e\u73b0\u9ad8\u8fbe75%\u7684\u5185\u5b58\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u635f\u5931\u6027\u80fd", "conclusion": "Panther\u8bc1\u660e\u4e86RandNLA\u6280\u672f\u7684\u6709\u6548\u6027\u5e76\u7b80\u5316\u4e86\u5176\u91c7\u7528\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u751f\u4ea7\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.15482", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15482", "abs": "https://arxiv.org/abs/2601.15482", "authors": ["Huayu Li", "ZhengXiao He", "Siyuan Tian", "Jinghao Wen", "Ao Li"], "title": "Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding", "comment": null, "summary": "Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.", "code_url": "https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Samplin", "AI": {"tldr": "MFS\u5c06LLM\u89e3\u7801\u91cd\u6784\u4e3a\u5bfb\u627e\u6700\u4f18\u968f\u673a\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u5229\u7528\u9785\u7406\u8bba\u8bbe\u8ba1\u539f\u5219\u6027\u7b97\u6cd5\uff0c\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\u5728LLMs\u4e2d\u662f\u77ed\u89c6\u7684\uff0c\u65e0\u6cd5\u627e\u5230\u5168\u5c40\u6700\u4f18\u63a8\u7406\u8def\u5f84\u3002\u73b0\u6709\u7684\u524d\u77bb\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u673a\u5236\u8fdb\u884c\u8def\u5f84\u8bc4\u4f30\u548c\u641c\u7d22\u7a7a\u95f4\u526a\u679d\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u9785\u524d\u77bb\u91c7\u6837(MFS)\u6846\u67b6\uff0c\u5c06LLM\u89e3\u7801\u5efa\u6a21\u4e3a\u8bc6\u522b\u6700\u4f18\u968f\u673a\u8fc7\u7a0b\u7684\u95ee\u9898\u3002\u5229\u7528\u9785\u7406\u8bba\uff1a1) \u57fa\u4e8eDoob\u5206\u89e3\u5b9a\u7406\u63a8\u5bfc\u6b65\u9aa4\u8bc4\u4f30\uff0c\u6d4b\u91cf\u8def\u5f84\u7684\u53ef\u9884\u6d4b\u4f18\u52bf\uff1b2) \u4f7f\u7528\u53ef\u9009\u505c\u6b62\u7406\u8bba\u8fdb\u884c\u539f\u5219\u6027\u526a\u679d\uff1b3) \u57fa\u4e8e\u9785\u6536\u655b\u5b9a\u7406\u8bbe\u8ba1\u81ea\u9002\u5e94\u505c\u6b62\u89c4\u5219\u3002", "result": "\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMFS\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MFS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6846\u67b6\uff0c\u7528\u6982\u7387\u8bba\u539f\u7406\u66ff\u4ee3\u542f\u53d1\u5f0f\u673a\u5236\uff0c\u4e3aLLM\u89e3\u7801\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u7684\u63a8\u7406\u8def\u5f84\u641c\u7d22\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.15538", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15538", "abs": "https://arxiv.org/abs/2601.15538", "authors": ["Himanshu Mishra", "Kanwal Mehreen"], "title": "QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs", "comment": null, "summary": "Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.", "AI": {"tldr": "\u91cf\u5316\u611f\u77e5\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff1a\u9488\u5bf9\u4f4e\u6bd4\u7279\u91cf\u5316\u4f1a\u707e\u96be\u6027\u6062\u590d\u5df2\u9057\u5fd8\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8elogits\u7a7a\u95f4\u94f0\u94fe\u635f\u5931\u7684\u91cf\u5316\u611f\u77e5\u9057\u5fd8\u65b9\u6cd5", "motivation": "\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65e8\u5728\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u77e5\u8bc6\uff08\u5982\u7248\u6743\u6216\u9690\u79c1\u6570\u636e\uff09\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u5b9e\u8df5\u4e2d\u6a21\u578b\u5e38\u88ab\u91cf\u5316\uff08\u59824\u4f4d\uff09\u90e8\u7f72\uff0c\u7814\u7a76\u53d1\u73b0\u91cf\u5316\u4f1a\u707e\u96be\u6027\u5730\u6062\u590d\u5df2\u9057\u5fd8\u7684\u4fe1\u606f\u3002", "method": "\u9996\u5148\u901a\u8fc7\u8ba1\u7b97\u6743\u91cd\u53d8\u5316\u7edf\u8ba1\u548c\u91cf\u5316\u6876\u91cd\u53e0\u6765\u5206\u6790\u4f4e\u6bd4\u7279\u91cf\u5316\u7834\u574f\u9057\u5fd8\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u5178\u578b\u9057\u5fd8\u66f4\u65b0\u592a\u5c0f\u65e0\u6cd5\u8de8\u8d8a\u91cf\u5316\u9608\u503c\u3002\u57fa\u4e8e\u6b64\u63d0\u51falogits\u7a7a\u95f4\u94f0\u94fe\u635f\u5931\uff1a\u5bf9\u6bcf\u4e2a\u9057\u5fd8\u6837\u672c\uff0c\u5f3a\u5236\u672a\u5b66\u4e60\u6a21\u578b\u7684\u8f93\u51falogits\u4e0e\u539f\u6a21\u578b\u81f3\u5c11\u76f8\u5dee\u4e00\u4e2a\u8fb9\u754c\u503c\uff08\u91cf\u5316\u6b65\u957f\u7684\u4e00\u534a\uff09\uff0c\u786e\u4fdd\u9057\u5fd8\u6837\u672c\u5728\u91cf\u5316\u540e\u4ecd\u53ef\u533a\u5206\u3002", "result": "\u5728\u8bed\u8a00\u548c\u5206\u7c7b\u4efb\u52a1\uff08\u5305\u62ecTwitter\u865a\u5047\u4fe1\u606f\u6570\u636e\u96c6\uff09\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u57284\u4f4d\u91cf\u5316\u4e0b\u80fd\u4fdd\u6301\u9057\u5fd8\u6548\u679c\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u51e0\u4e4e\u5b8c\u5168\u6062\u590d\u5df2\u9057\u5fd8\u77e5\u8bc6\u3002", "conclusion": "\u91cf\u5316\u4f1a\u7834\u574f\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u6548\u679c\uff0c\u63d0\u51fa\u7684\u91cf\u5316\u611f\u77e5\u9057\u5fd8\u65b9\u6cd5\u901a\u8fc7logits\u7a7a\u95f4\u94f0\u94fe\u635f\u5931\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u91cf\u5316\u90e8\u7f72\u540e\u4ecd\u80fd\u4fdd\u6301\u9057\u5fd8\u6548\u679c\u3002"}}
{"id": "2601.15540", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.15540", "abs": "https://arxiv.org/abs/2601.15540", "authors": ["Dongchen Huang"], "title": "PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction", "comment": null, "summary": "Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($\u03c0$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.", "AI": {"tldr": "\u63d0\u51faPrism\u67b6\u6784\uff0c\u57fa\u4e8eMCR\u00b2\u539f\u7406\u7684\u767d\u76d2\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u5b9e\u73b0\u65e0\u76d1\u7763\u529f\u80fd\u89e3\u8026\uff0c\u5728TinyStories\u4e0a\u9a8c\u8bc1\u4e86\u9891\u8c31\u5206\u79bb\u7279\u6027", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fTransformer\uff09\u5e38\u88ab\u89c6\u4e3a\"\u9ed1\u7bb1\"\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u5efa\u7acb\u5177\u6709\u660e\u786e\u51e0\u4f55\u548c\u7269\u7406\u7ea6\u675f\u7684\u767d\u76d2\u67b6\u6784", "method": "\u63d0\u51faPrism\u67b6\u6784\uff0c\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u4e3a\u4fe1\u53f7-\u566a\u58f0\u6d41\u5f62\u4e0a\u7684\u68af\u5ea6\u4e0a\u5347\u8fc7\u7a0b\uff0c\u5f15\u5165\u8fc7\u5b8c\u5907\u5b57\u5178\u6269\u5c55\u8868\u793a\u76f8\u7a7a\u95f4\uff0c\u4f7f\u7528\u65e0\u7406\u9891\u7387\u5206\u79bb\uff08\u03c0-RoPE\uff09\u5f3a\u5236\u4fe1\u53f7\u4e0e\u566a\u58f0\u5b50\u7a7a\u95f4\u7684\u4e0d\u76f8\u5e72\u6027", "result": "\u5728TinyStories\u6d4b\u8bd5\u4e2d\uff0cPrism\u7684\u6ce8\u610f\u529b\u5934\u81ea\u53d1\u5730\u4e13\u95e8\u5316\u4e3a\u9891\u8c31\u4e0d\u540c\u7684\u673a\u5236\uff1a\u4f4e\u9891\u5934\u6355\u83b7\u957f\u7a0b\u56e0\u679c\u4f9d\u8d56\uff08\u4fe1\u53f7\uff09\uff0c\u9ad8\u9891\u5934\u5904\u7406\u5c40\u90e8\u53e5\u6cd5\u7ea6\u675f\uff08\u566a\u58f0\uff09", "conclusion": "\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u5e76\u975e\u6743\u8861\u5173\u7cfb\uff0c\u53ef\u4ee5\u901a\u8fc7\u6709\u539f\u5219\u7684\u51e0\u4f55\u6784\u9020\u7edf\u4e00\u8d77\u6765\uff0c\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u8db3\u4ee5\u5355\u72ec\u8bf1\u5bfc\u65e0\u76d1\u7763\u529f\u80fd\u89e3\u8026"}}
{"id": "2601.15544", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15544", "abs": "https://arxiv.org/abs/2601.15544", "authors": ["Himanshu Mishra"], "title": "RDumb++: Drift-Aware Continual Test-Time Adaptation", "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.", "AI": {"tldr": "RDumb++\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff08\u57fa\u4e8e\u71b5\u548cKL\u6563\u5ea6\uff09\u548c\u81ea\u9002\u5e94\u91cd\u7f6e\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u57df\u5206\u5e03\u53d8\u5316\u4e2d\u9632\u6b62\u9884\u6d4b\u5d29\u6e83\uff0c\u5728CCC\u57fa\u51c6\u4e0a\u76f8\u6bd4RDumb\u83b7\u5f97\u7ea63%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff08\u5982Tent\u3001EATA\u7b49\uff09\u5728\u77ed\u671f\u5206\u5e03\u53d8\u5316\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5feb\u901f\u53d8\u5316\u6216\u6781\u957f\u65f6\u57df\u7684\u6d4b\u8bd5\u5206\u5e03\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728CCC\u57fa\u51c6\uff08\u5305\u542b750\u4e07\u4e2a\u6837\u672c\u7684\u8fde\u7eed\u53d8\u5316\u6d41\uff09\u4e2d\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u6709\u5bb3\u9002\u5e94\u5e76\u9632\u6b62\u9884\u6d4b\u5d29\u6e83\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faRDumb++\u65b9\u6cd5\uff0c\u4f5c\u4e3aRDumb\u7684\u6269\u5c55\uff0c\u5f15\u5165\u4e24\u79cd\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff1a1\uff09\u57fa\u4e8e\u71b5\u7684\u6f02\u79fb\u8bc4\u5206\uff0c2\uff09\u57fa\u4e8eKL\u6563\u5ea6\u7684\u6f02\u79fb\u8bc4\u5206\u3002\u7ed3\u5408\u81ea\u9002\u5e94\u91cd\u7f6e\u7b56\u7565\uff0c\u5f53\u68c0\u6d4b\u5230\u7d2f\u79ef\u9002\u5e94\u53d8\u5f97\u6709\u5bb3\u65f6\uff0c\u6a21\u578b\u80fd\u591f\u91cd\u7f6e\u4ee5\u5728\u9884\u6d4b\u5d29\u6e83\u53d1\u751f\u524d\u6062\u590d\u3002", "result": "\u5728CCC-medium\u57fa\u51c6\u7684\u4e09\u4e2a\u901f\u5ea6\u548c\u4e09\u4e2a\u79cd\u5b50\uff08\u51719\u6b21\u8fd0\u884c\uff0c\u6bcf\u6b21\u5305\u542b100\u4e07\u4e2a\u6837\u672c\uff09\u4e0a\uff0cRDumb++\u59cb\u7ec8\u4f18\u4e8eRDumb\uff0c\u83b7\u5f97\u7ea63%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6574\u4e2a\u6d41\u4e2d\u4fdd\u6301\u7a33\u5b9a\u7684\u9002\u5e94\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6f02\u79fb\u611f\u77e5\u91cd\u7f6e\u5bf9\u4e8e\u9632\u6b62\u5d29\u6e83\u548c\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u671fCTTA\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "RDumb++\u901a\u8fc7\u6f02\u79fb\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u91cd\u7f6e\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u5728\u957f\u65f6\u57df\u5206\u5e03\u53d8\u5316\u4e2d\u7684\u6311\u6218\uff0c\u9632\u6b62\u4e86\u9884\u6d4b\u5d29\u6e83\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u957f\u671f\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2601.15547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15547", "abs": "https://arxiv.org/abs/2601.15547", "authors": ["Jingren Hou", "Hong Wang", "Pengyu Xu", "Chang Gao", "Huafeng Liu", "Liping Jing"], "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling", "comment": null, "summary": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4ece\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u795e\u7ecf\u7b97\u5b50\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u8bad\u7ec3\u548c\u7269\u7406\u611f\u77e5\u6f5c\u5728\u4f20\u64ad\u5668\u89e3\u51b3\u76d1\u7763\u7f3a\u53e3\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728PDE\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u8bef\u5dee\u964d\u4f4e\u3002", "motivation": "\u73b0\u5b9e\u79d1\u5b66\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u4e0d\u5b8c\u6574\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u800c\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u5047\u8bbe\u5b8c\u5168\u89c2\u6d4b\u7684\u7a7a\u95f4\u8f93\u5165\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u89e3\u51b3\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51faLatent Autoregressive Neural Operator (LARNO)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u63a9\u7801\u9884\u6d4b\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u5730\u63a9\u7801\u89c2\u6d4b\u533a\u57df\u521b\u5efa\u4eba\u5de5\u76d1\u7763\uff1b2) \u7269\u7406\u611f\u77e5\u6f5c\u5728\u4f20\u64ad\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u8fb9\u754c\u4f18\u5148\u7684\u81ea\u56de\u5f52\u751f\u6210\u91cd\u5efa\u89e3\u3002", "result": "\u5728\u4e13\u95e8\u8bbe\u8ba1\u7684POBench-PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLARNO\u5728\u8865\u4e01\u5f0f\u7f3a\u5931\u7387\u4f4e\u4e8e50%\u7684\u6240\u6709\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8618-69%\u7684\u76f8\u5bf9L2\u8bef\u5dee\u964d\u4f4e\uff0c\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u6c14\u5019\u9884\u6d4b\u3002\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9ad8\u8fbe75%\u7f3a\u5931\u7387\u7684\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u4ece\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u795e\u7ecf\u7b97\u5b50\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5f25\u5408\u4e86\u7406\u60f3\u5316\u7814\u7a76\u8bbe\u7f6e\u4e0e\u73b0\u5b9e\u4e16\u754c\u79d1\u5b66\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.15549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15549", "abs": "https://arxiv.org/abs/2601.15549", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations", "comment": null, "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.", "AI": {"tldr": "VIOLA\u6846\u67b6\u901a\u8fc7\u5bc6\u5ea6-\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u91c7\u6837\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u673a\u5236\uff0c\u5728\u6700\u5c0f\u6807\u6ce8\u6210\u672c\u4e0b\u5b9e\u73b0\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u9886\u57df\u9002\u5e94", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u9700\u8981\u9002\u5e94\u65b0\u7684\u89c6\u9891\u9886\u57df\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u7279\u522b\u662f\u5728\u5de5\u4e1a\u6216\u624b\u672f\u7b49\u4e13\u4e1a\u73af\u5883\u4e2d\uff0c\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\u3002\u867d\u7136\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u514d\u8bad\u7ec3\u9002\u5e94\u8def\u5f84\uff0c\u4f46\u6807\u51c6\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5728\u4e13\u4e1a\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51faVIOLA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u5bc6\u5ea6-\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u91c7\u6837\uff0c\u901a\u8fc7\u5bc6\u5ea6\u4f30\u8ba1\u540c\u65f6\u8003\u8651\u591a\u6837\u6027\u3001\u4ee3\u8868\u6027\u548c\u4fe1\u606f\u91cf\uff0c\u6700\u5927\u5316\u6709\u9650\u6807\u6ce8\u9884\u7b97\u7684\u6548\u7387\uff1b2) \u6784\u5efa\u6df7\u5408\u6c60\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u611f\u77e5\u68c0\u7d22\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63d0\u793a\uff0c\u663e\u5f0f\u5efa\u6a21\u6807\u7b7e\u53ef\u9760\u6027\uff0c\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u548c\u7f6e\u4fe1\u5ea6\u7684\u590d\u5408\u5206\u6570\u68c0\u7d22\u6f14\u793a\uff0c\u4f7fMLLM\u80fd\u591f\u81ea\u9002\u5e94\u533a\u5206\u5df2\u9a8c\u8bc1\u7684\u771f\u5b9e\u6807\u7b7e\u548c\u566a\u58f0\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u4e5d\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u56db\u4e2aMLLM\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u6807\u6ce8\u6210\u672c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u9886\u57df\u9002\u5e94\u3002", "conclusion": "VIOLA\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u6700\u5c0f\u4e13\u5bb6\u76d1\u7763\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\uff0c\u89e3\u51b3\u4e86MLLM\u5728\u4e13\u4e1a\u89c6\u9891\u9886\u57df\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f4e\u8d44\u6e90\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15595", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15595", "abs": "https://arxiv.org/abs/2601.15595", "authors": ["Xinjie Zhou", "Zhihui Yang", "Lechao Cheng", "Sai Wu", "Gang Chen"], "title": "Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning", "comment": null, "summary": "Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6DFSU\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u53cd\u6f14\u5408\u6210\u4f2aPII\uff0c\u6784\u5efatoken\u7ea7\u9690\u79c1\u63a9\u7801\uff0c\u5728LoRA\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5bf9\u6bd4\u63a9\u7801\u635f\u5931\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bb0\u5fc6\u654f\u611f\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f(PII)\u7684\u98ce\u9669\uff0c\u73b0\u6709\u673a\u5668\u9057\u5fd8\u6280\u672f\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u8bbf\u95ee\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u8bad\u7ec3\u6570\u636e\u901a\u5e38\u662f\u4e13\u6709\u6216\u4e0d\u53ef\u8bbf\u95ee\u7684\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u6570\u636e\u81ea\u7531\u9009\u62e9\u6027\u9057\u5fd8(DFSU)\u6846\u67b6\uff1a1) \u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u53cd\u6f14\u5408\u6210\u4f2aPII\u6837\u672c\uff1b2) \u4e3a\u5408\u6210\u6837\u672c\u6784\u5efatoken\u7ea7\u9690\u79c1\u63a9\u7801\uff1b3) \u5728\u4f4e\u79e9\u9002\u5e94(LoRA)\u5b50\u7a7a\u95f4\u4e2d\u4f7f\u7528\u5bf9\u6bd4\u63a9\u7801\u635f\u5931\u8fdb\u884ctoken\u7ea7\u9009\u62e9\u6027\u9057\u5fd8\u3002", "result": "\u5728AI4Privacy PII-Masking\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Pythia\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u79fb\u9664\u76ee\u6807PII\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "conclusion": "DFSU\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u4eceLLM\u4e2d\u79fb\u9664\u654f\u611fPII\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15625", "abs": "https://arxiv.org/abs/2601.15625", "authors": ["Zhiwei Zhang", "Fei Zhao", "Rui Wang", "Zezhong Wang", "Bin Liang", "Jiakang Wang", "Yao Hu", "Shaosheng Cao", "Kam-Fai Wong"], "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors", "comment": "8 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.", "AI": {"tldr": "Fission-GRPO\uff1a\u901a\u8fc7\u5c06\u6267\u884c\u9519\u8bef\u8f6c\u5316\u4e3a\u7ea0\u6b63\u6027\u76d1\u7763\u7684RL\u6846\u67b6\uff0c\u63d0\u5347LLM\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u9519\u8bef\u6062\u590d\u80fd\u529b", "motivation": "\u5f53\u524dLLM\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u5b58\u5728\u8106\u5f31\u6027\uff1a\u9047\u5230\u5de5\u5177\u8c03\u7528\u9519\u8bef\u65f6\uff0c\u5c0f\u6a21\u578b\u5f80\u5f80\u9677\u5165\u91cd\u590d\u65e0\u6548\u8c03\u7528\uff0c\u65e0\u6cd5\u89e3\u91ca\u9519\u8bef\u53cd\u9988\u5e76\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u6807\u51c6RL\u5c06\u9519\u8bef\u89c6\u4e3a\u7a00\u758f\u8d1f\u5956\u52b1\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6062\u590d\u6307\u5bfc\uff1b\u9884\u6536\u96c6\u7684\u5408\u6210\u9519\u8bef\u7ea0\u6b63\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u5728\u7ebf\u9519\u8bef\u6a21\u5f0f\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faFission-GRPO\u6846\u67b6\uff0c\u6838\u5fc3\u673a\u5236\u662f\u5c06\u6bcf\u4e2a\u5931\u8d25\u8f68\u8ff9\"\u88c2\u53d8\"\u4e3a\u65b0\u7684\u8bad\u7ec3\u5b9e\u4f8b\uff1a\u901a\u8fc7\u5fae\u8c03\u7684Error Simulator\u63d0\u4f9b\u8bca\u65ad\u53cd\u9988\uff0c\u7136\u540e\u5728\u7b56\u7565\u4e0a\u91cd\u65b0\u91c7\u6837\u6062\u590d\u8f68\u8ff9\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u4ece\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u5177\u4f53\u9519\u8bef\u4e2d\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u4ece\u9759\u6001\u7684\u9884\u6536\u96c6\u9519\u8bef\u6848\u4f8b\u4e2d\u5b66\u4e60\u3002", "result": "\u5728BFCL v4 Multi-Turn\u57fa\u51c6\u4e0a\uff0cFission-GRPO\u5c06Qwen3-8B\u7684\u9519\u8bef\u6062\u590d\u7387\u63d0\u5347\u4e865.7%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4ece42.75%\u63d0\u9ad8\u523046.75%\uff08\u76f8\u5bf9GRPO\u63d0\u53474%\uff09\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u3002", "conclusion": "Fission-GRPO\u901a\u8fc7\u5c06\u6267\u884c\u9519\u8bef\u8f6c\u5316\u4e3a\u7ea0\u6b63\u6027\u76d1\u7763\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u9519\u8bef\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15655", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15655", "abs": "https://arxiv.org/abs/2601.15655", "authors": ["Zhenghui Guo", "Yuanbin Man", "Junyuan Sheng", "Bowen Lin", "Ahmed Ahmed", "Bo Jiang", "Boyuan Zhang", "Miao Yin", "Sian Jin", "Omprakash Gnawal", "Chengming Zhang"], "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams", "comment": null, "summary": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.", "AI": {"tldr": "Event-VStream\u662f\u4e00\u4e2a\u4e8b\u4ef6\u611f\u77e5\u7684\u89c6\u9891\u6d41\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u8bed\u4e49\u8fde\u8d2f\u7684\u4e8b\u4ef6\u8fb9\u754c\u6765\u89e6\u53d1\u8bed\u8a00\u751f\u6210\uff0c\u51cf\u5c11\u5197\u4f59\u5904\u7406\u5e76\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6\uff0c\u5728\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6d41\u7406\u89e3\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u56fa\u5b9a\u95f4\u9694\u89e3\u7801\u5bfc\u81f4\u91cd\u590d\u8f93\u51fa\uff1b2\uff09\u7f13\u5b58\u526a\u679d\u4e22\u5f03\u5173\u952e\u65f6\u5e8f\u4fe1\u606f\u3002\u8fd9\u5bfc\u81f4\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u6d41\u65f6\u5b58\u5728\u5197\u4f59\u5e27\u5904\u7406\u548c\u5feb\u901f\u9057\u5fd8\u8fc7\u53bb\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEvent-VStream\u6846\u67b6\uff0c\u5c06\u8fde\u7eed\u89c6\u9891\u8868\u793a\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u8fde\u8d2f\u4e8b\u4ef6\u5e8f\u5217\u3002\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u8fd0\u52a8\u3001\u8bed\u4e49\u548c\u9884\u6d4b\u7ebf\u7d22\u6765\u68c0\u6d4b\u6709\u610f\u4e49\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u4ec5\u5728\u4e8b\u4ef6\u8fb9\u754c\u89e6\u53d1\u8bed\u8a00\u751f\u6210\u3002\u6bcf\u4e2a\u4e8b\u4ef6\u5d4c\u5165\u88ab\u6574\u5408\u5230\u6301\u4e45\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u652f\u6301\u957f\u671f\u63a8\u7406\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728OVOBench-Realtime\u548c\u957f\u683c\u5f0fEgo4D\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a1\uff09\u76f8\u6bd4VideoLLM-Online-8B\u57fa\u7ebf\u63d0\u534710.4\u5206\uff1b2\uff09\u4ec5\u4f7f\u7528\u901a\u7528LLaMA-3-8B\u6587\u672c\u9aa8\u5e72\uff0c\u6027\u80fd\u63a5\u8fd1Flash-VStream-7B\uff1b3\uff09\u57282\u5c0f\u65f6Ego4D\u6d41\u4e0a\u4fdd\u6301\u7ea670%\u7684GPT-5\u80dc\u7387\u3002", "conclusion": "Event-VStream\u901a\u8fc7\u4e8b\u4ef6\u611f\u77e5\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u6d41\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u5904\u7406\u548c\u8bb0\u5fc6\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u957f\u671f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15664", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15664", "abs": "https://arxiv.org/abs/2601.15664", "authors": ["Hongyang Wei", "Hongbo Liu", "Zidong Wang", "Yi Peng", "Baixin Xu", "Size Wu", "Xuying Zhang", "Xianglong He", "Zexiang Liu", "Peiyu Wang", "Xuchen Song", "Yangguang Li", "Yang Liu", "Yahui Zhou"], "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling", "comment": null, "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.", "AI": {"tldr": "Skywork UniPic 3.0\uff1a\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6\uff0c\u96c6\u6210\u5355\u56fe\u7f16\u8f91\u4e0e\u591a\u56fe\u5408\u6210\uff0c\u652f\u63011-6\u5f20\u4efb\u610f\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4eba-\u7269\u4ea4\u4e92\u5408\u6210\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534712.5\u500d\u3002", "motivation": "\u793e\u533a\u5bf9\u591a\u56fe\u50cf\u5408\u6210\u4efb\u52a1\uff08\u7279\u522b\u662fNano-Banana\u548cSeedream 4.0\uff09\u5174\u8da3\u6d53\u539a\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u672a\u516c\u5f00\u9ad8\u8d28\u91cf\u878d\u5408\u7684\u5177\u4f53\u65b9\u6cd5\u7ec6\u8282\u3002\u76f8\u6bd4\u5355\u56fe\u7f16\u8f91\uff0c\u591a\u56fe\u5408\u6210\u5728\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u65b9\u9762\u9762\u4e34\u66f4\u5927\u6311\u6218\uff0c\u800c\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u662f\u793e\u533a\u6700\u5173\u6ce8\u7684\u9700\u6c42\u7c7b\u522b\u3002", "method": "1. \u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6Skywork UniPic 3.0\uff0c\u652f\u63011-6\u5f20\u4efb\u610f\u5206\u8fa8\u7387\u8f93\u5165\u548c\u4efb\u610f\u8f93\u51fa\u5206\u8fa8\u7387\uff08\u603b\u50cf\u7d20\u9884\u7b971024x1024\uff09\uff1b2. \u8bbe\u8ba1\u5168\u9762\u7684\u6570\u636e\u6536\u96c6\u3001\u8fc7\u6ee4\u548c\u5408\u6210\u6d41\u7a0b\uff0c\u4ec5\u752870\u4e07\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\uff1b3. \u521b\u65b0\u8bad\u7ec3\u8303\u5f0f\uff1a\u5c06\u591a\u56fe\u5408\u6210\u5efa\u6a21\u4e3a\u5e8f\u5217\u95ee\u9898\uff0c\u5c06\u6761\u4ef6\u751f\u6210\u8f6c\u5316\u4e3a\u7edf\u4e00\u5e8f\u5217\u5408\u6210\uff1b4. \u63a8\u7406\u52a0\u901f\uff1a\u5728\u8bad\u7ec3\u540e\u9636\u6bb5\u96c6\u6210\u8f68\u8ff9\u6620\u5c04\u548c\u5206\u5e03\u5339\u914d\uff0c\u5b9e\u73b0\u4ec5\u97008\u6b65\u7684\u9ad8\u4fdd\u771f\u91c7\u6837\u3002", "result": "\u5728\u5355\u56fe\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u591a\u56fe\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aNano-Banana\u548cSeedream 4.0\uff1b\u63a8\u7406\u901f\u5ea6\u76f8\u6bd4\u6807\u51c6\u5408\u6210\u91c7\u6837\u63d0\u534712.5\u500d\uff0c\u4ec5\u97008\u6b65\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "conclusion": "Skywork UniPic 3.0\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u548c\u8bad\u7ec3\u8303\u5f0f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u56fe\u50cf\u5408\u6210\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4eba-\u7269\u4ea4\u4e92\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u5728\u590d\u6742\u591a\u6a21\u6001\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.15678", "categories": ["cs.CR", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15678", "abs": "https://arxiv.org/abs/2601.15678", "authors": ["Mengyu Yao", "Ziqi Zhang", "Ning Luo", "Shaofei Li", "Yifeng Cai", "Xiangqun Chen", "Yao Guo", "Ding Li"], "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.", "AI": {"tldr": "RAGCRAWLER\uff1a\u4e00\u79cd\u9488\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u6761\u4ef6\u8fb9\u9645\u589e\u76ca\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u9690\u79c1\u6570\u636e\u63d0\u53d6", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u9690\u79c1\u573a\u666f\u4e0b\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67e5\u8be2\u9010\u6b65\u63d0\u53d6\u5e95\u5c42\u8bed\u6599\u5e93\u4e2d\u7684\u654f\u611f\u5185\u5bb9\u3002\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u7f3a\u4e4f\u957f\u671f\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5b9e\u9645RAG\u653b\u51fb\u7684\u6311\u6218\u3002", "method": "\u5c06RAG\u63d0\u53d6\u653b\u51fb\u5efa\u6a21\u4e3a\u81ea\u9002\u5e94\u968f\u673a\u8986\u76d6\u95ee\u9898\uff0c\u63d0\u51faRAGCRAWLER\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5df2\u63ed\u793a\u4fe1\u606f\uff1b2\uff09\u5229\u7528\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6761\u4ef6\u8fb9\u9645\u589e\u76ca\uff1b3\uff09\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u89c4\u5212\u9488\u5bf9\u672a\u68c0\u7d22\u533a\u57df\u7684\u67e5\u8be2\uff1b4\uff09\u901a\u8fc7\u6982\u7387\u52a8\u4f5c\u6700\u5927\u5316CMG\u5b9e\u73b0\u957f\u671f\u89c4\u5212\u3002", "result": "\u5728\u591a\u79cdRAG\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAGCRAWLER\u5728\u56fa\u5b9a\u67e5\u8be2\u9884\u7b97\u4e0b\u8fbe\u523084.4%\u7684\u8bed\u6599\u5e93\u8986\u76d6\u7387\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u5e73\u5747\u63d0\u534720.7%\u3002\u65b9\u6cd5\u4fdd\u6301\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5185\u5bb9\u91cd\u5efa\u51c6\u786e\u7387\uff0c\u653b\u51fb\u6210\u672c\u4f4e\uff0c\u4e14\u5bf9\u91c7\u7528\u67e5\u8be2\u91cd\u5199\u548c\u591a\u67e5\u8be2\u68c0\u7d22\u7b56\u7565\u7684\u5148\u8fdbRAG\u7cfb\u7edf\u4fdd\u6301\u6709\u6548\u3002", "conclusion": "RAGCRAWLER\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u5b58\u5728\u7684\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u5bf9\u66f4\u5f3a\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u7684\u8feb\u5207\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u968f\u673a\u8986\u76d6\u95ee\u9898\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u5bf9RAG\u7cfb\u7edf\u9690\u79c1\u6570\u636e\u7684\u6709\u6548\u63d0\u53d6\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u673a\u5236\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.15698", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15698", "abs": "https://arxiv.org/abs/2601.15698", "authors": ["Mingyu Yu", "Lana Liu", "Zhehao Zhao", "Wei Wang", "Sujuan Qin"], "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs", "comment": null, "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.", "AI": {"tldr": "BVS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u8d8a\u72f1\u6846\u67b6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u63a2\u6d4b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b89\u5168\u8fb9\u754c\uff0c\u901a\u8fc7\"\u91cd\u5efa-\u751f\u6210\"\u7b56\u7565\u5b9e\u73b0\u9ad8\u8fbe98.21%\u7684\u8d8a\u72f1\u6210\u529f\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u548c\u89c6\u89c9\u5b89\u5168\u7684\u4ea4\u53c9\u9886\u57df\u3002\u73b0\u6709\u65b9\u6848\u867d\u7136\u63a2\u7d22\u4e86MLLMs\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4f46\u5bf9\u89c6\u89c9\u5b89\u5168\u8fb9\u754c\u7684\u7814\u7a76\u4ecd\u4e0d\u5145\u5206\u3002", "method": "BVS\u91c7\u7528\"\u91cd\u5efa-\u751f\u6210\"\u7b56\u7565\uff0c\u5229\u7528\u4e2d\u6027\u5316\u89c6\u89c9\u62fc\u63a5\u548c\u5f52\u7eb3\u91cd\u7ec4\u6280\u672f\uff0c\u5c06\u6076\u610f\u610f\u56fe\u4ece\u539f\u59cb\u8f93\u5165\u4e2d\u89e3\u8026\uff0c\u4ece\u800c\u8bf1\u5bfcMLLMs\u751f\u6210\u6709\u5bb3\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBVS\u5bf9GPT-5\uff082026\u5e741\u670812\u65e5\u53d1\u5e03\uff09\u5b9e\u73b0\u4e8698.21%\u7684\u663e\u8457\u8d8a\u72f1\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u9700\u8981\u52a0\u5f3a\u89c6\u89c9\u5b89\u5168\u8fb9\u754c\u7684\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2601.15714", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15714", "abs": "https://arxiv.org/abs/2601.15714", "authors": ["Ryoma Sato"], "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs", "comment": null, "summary": "We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u96f6\u8bef\u5dee\u89c6\u754c\uff08ZEH\uff09\u4f5c\u4e3a\u8bc4\u4f30LLM\u53ef\u4fe1\u5ea6\u7684\u6307\u6807\uff0c\u4ee3\u8868\u6a21\u578b\u80fd\u591f\u65e0\u9519\u8bef\u89e3\u51b3\u7684\u6700\u5927\u8303\u56f4\u3002\u901a\u8fc7\u8bc4\u4f30GPT-5.2\u548cQwen2.5\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u4ecd\u4f1a\u51fa\u9519\uff0c\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u5177\u6709\u91cd\u8981\u8b66\u793a\u610f\u4e49\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u4ecd\u53ef\u80fd\u51fa\u9519\u3002\u4e3a\u4e86\u8bc4\u4f30LLM\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u6a21\u578b\u65e0\u9519\u8bef\u89e3\u51b3\u80fd\u529b\u8303\u56f4\u7684\u6307\u6807\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51fa\u96f6\u8bef\u5dee\u89c6\u754c\uff08ZEH\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u9010\u6e10\u589e\u52a0\u590d\u6742\u5ea6\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6765\u786e\u5b9a\u5176\u65e0\u9519\u8bef\u89e3\u51b3\u7684\u6700\u5927\u8303\u56f4\u3002\u4f7f\u7528\u6811\u7ed3\u6784\u548c\u5728\u7ebfsoftmax\u6280\u672f\u4f18\u5316\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u8fbe\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0GPT-5.2\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u5b58\u5728\u9519\u8bef\uff0c\u5982\u65e0\u6cd5\u8ba1\u7b97\u77ed\u5b57\u7b26\u4e3211000\u7684\u5947\u5076\u6027\uff0c\u4e5f\u65e0\u6cd5\u5224\u65ad\u62ec\u53f7\u5e8f\u5217((((()))))\u662f\u5426\u5e73\u8861\u3002\u867d\u7136ZEH\u4e0e\u51c6\u786e\u7387\u76f8\u5173\uff0c\u4f46\u8be6\u7ec6\u884c\u4e3a\u5b58\u5728\u5dee\u5f02\u3002ZEH\u4e3a\u7b97\u6cd5\u80fd\u529b\u7684\u6d8c\u73b0\u63d0\u4f9b\u4e86\u7ebf\u7d22\u3002", "conclusion": "ZEH\u662f\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684LLM\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u5148\u8fdb\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u9886\u57df\u7684LLM\u5e94\u7528\u5177\u6709\u91cd\u8981\u8b66\u793a\u610f\u4e49\u3002\u901a\u8fc7\u4f18\u5316\u6280\u672f\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f7fZEH\u8bc4\u4f30\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2601.15731", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15731", "abs": "https://arxiv.org/abs/2601.15731", "authors": ["Linyong Zou", "Liang Zhang", "Xiongfei Wang", "Jia-Hong Gao", "Yi Sun", "Shurong Sheng", "Kuntao Xiao", "Wanli Yang", "Pengfei Teng", "Guoming Luan", "Zhao Lv", "Zikang Xu"], "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging", "comment": null, "summary": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.", "AI": {"tldr": "FAIR-ESI\uff1a\u4e00\u79cd\u901a\u8fc7\u591a\u89c6\u56fe\u81ea\u9002\u5e94\u7279\u5f81\u91cd\u8981\u6027\u7cbe\u70bc\u6765\u63d0\u5347\u8111\u7535\u751f\u7406\u6e90\u6210\u50cf\u7cbe\u5ea6\u7684\u65b0\u578b\u6846\u67b6", "motivation": "\u8111\u7535\u751f\u7406\u6e90\u6210\u50cf\uff08ESI\uff09\u662f\u8bca\u65ad\u8111\u90e8\u75be\u75c5\u7684\u91cd\u8981\u6280\u672f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u9009\u62e9\u548c\u7cbe\u70bc\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5f71\u54cd\u6210\u50cf\u7cbe\u5ea6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u7cbe\u70bc\u4e0d\u540c\u89c6\u56fe\u7279\u5f81\u91cd\u8981\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFAIR-ESI\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8eFFT\u7684\u9891\u8c31\u7279\u5f81\u7cbe\u70bc\uff1b2\uff09\u52a0\u6743\u65f6\u57df\u7279\u5f81\u7cbe\u70bc\uff1b3\uff09\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u5206\u5757\u7279\u5f81\u7cbe\u70bc\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u89c6\u56fe\u81ea\u9002\u5e94\u7279\u5f81\u91cd\u8981\u6027\u7cbe\u70bc\u6765\u63d0\u5347ESI\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u914d\u7f6e\u7684\u6a21\u62df\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8111\u90e8\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "FAIR-ESI\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u7cbe\u70bc\u663e\u8457\u63d0\u5347\u4e86ESI\u7cbe\u5ea6\uff0c\u4e3a\u8111\u90e8\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u4e3a\u7406\u89e3\u8111\u529f\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2601.15871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15871", "abs": "https://arxiv.org/abs/2601.15871", "authors": ["Jidong Jin"], "title": "Why Inference in Large Models Becomes Decomposable After Training", "comment": "10 pages, 6 figures", "summary": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540e\u8bad\u7ec3\u7edf\u8ba1\u51c6\u5219\u548c\u7ed3\u6784\u9000\u706b\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u672a\u652f\u6301\u7684\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\uff0c\u63ed\u793a\u5927\u578bAI\u6a21\u578b\u4e2d\u7a33\u5b9a\u7684\u72ec\u7acb\u5b50\u7ed3\u6784\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u5e76\u884c\u63a8\u7406\u800c\u4e0d\u6539\u53d8\u6a21\u578b\u529f\u80fd\u6216\u63a5\u53e3\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u63a8\u7406\u901a\u5e38\u5728\u5bc6\u96c6\u53c2\u6570\u77e9\u9635\u4e0a\u8fdb\u884c\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u548c\u7cfb\u7edf\u590d\u6742\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u4e0d\u53ef\u6301\u7eed\u5730\u589e\u957f\u3002\u8fd9\u79cd\u9650\u5236\u5e76\u975e\u6e90\u4e8e\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\uff0c\u800c\u662f\u7531\u4e8e\u5c06\u540e\u8bad\u7ec3\u63a8\u7406\u7cfb\u7edf\u89c6\u4e3a\u6574\u4f53\u7b97\u5b50\u800c\u5ffd\u7565\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u5185\u90e8\u7ed3\u6784\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u66f4\u65b0\u4e8b\u4ef6\u9ad8\u5ea6\u5c40\u90e8\u5316\u548c\u9009\u62e9\u6027\uff0c\u8bb8\u591a\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u5728\u8bad\u7ec3\u540e\u4e0e\u5176\u521d\u59cb\u5316\u5206\u5e03\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u63d0\u51fa\u540e\u8bad\u7ec3\u7edf\u8ba1\u51c6\u5219\u548c\u7ed3\u6784\u9000\u706b\u7a0b\u5e8f\uff0c\u79fb\u9664\u672a\u652f\u6301\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u63ed\u793a\u7a33\u5b9a\u7684\u72ec\u7acb\u5b50\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u540e\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u63a8\u7406\u7cfb\u7edf\u7ed3\u6784\u89c6\u56fe\uff0c\u80fd\u591f\u5b9e\u73b0\u7ed3\u6784\u5316\u5e76\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u9700\u8981\u4fee\u6539\u6a21\u578b\u529f\u80fd\u6216\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u89e3\u63a8\u7406\u7cfb\u7edf\uff0c\u964d\u4f4e\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u63a8\u7406\u7684\u6210\u672c\u548c\u590d\u6742\u5ea6\u3002", "conclusion": "\u5927\u578bAI\u6a21\u578b\u7684\u63a8\u7406\u7cfb\u7edf\u672c\u8d28\u4e0a\u662f\u7ed3\u6784\u975e\u5747\u5300\u4e14\u53ef\u5206\u89e3\u7684\u3002\u901a\u8fc7\u8bc6\u522b\u548c\u5229\u7528\u8bad\u7ec3\u540e\u5f62\u6210\u7684\u7a33\u5b9a\u5b50\u7ed3\u6784\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\u3002"}}
{"id": "2601.15888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15888", "abs": "https://arxiv.org/abs/2601.15888", "authors": ["Shiqi Huang", "Yipei Wang", "Natasha Thorley", "Alexander Ng", "Shaheer Saeed", "Mark Emberton", "Shonit Punwani", "Veeru Kasivisvanathan", "Dean Barratt", "Daniel Alexander", "Yipeng Hu"], "title": "Understanding the Transfer Limits of Vision Foundation Models", "comment": "accepted in ISBI 2026", "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u4e0b\u6e38\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u524d\u5217\u817aMRI\u4efb\u52a1\u9a8c\u8bc1\u4e86\u4efb\u52a1\u5bf9\u9f50\u5bf9\u8fc1\u79fb\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5927\u91cf\u8ba1\u7b97\u6295\u5165\u540e\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5747\u3002\u7814\u7a76\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\uff08\u5982\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u6216\u5bf9\u6bd4\u5b66\u4e60\uff09\u4e0e\u4e0b\u6e38\u89c6\u89c9\u6210\u50cf\u4efb\u52a1\uff08\u5982\u5206\u5272\u3001\u5206\u7c7b\u3001\u56fe\u50cf\u5408\u6210\uff09\u7684\u5177\u4f53\u9700\u6c42\u4e0d\u5339\u914d\u6240\u81f4\u3002\u4e3a\u5728\u4e34\u5e8a\u9886\u57df\u5177\u4f53\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u7814\u7a76\u9009\u62e9\u524d\u5217\u817a\u591a\u53c2\u6570MRI\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u4e2a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a\u57fa\u4e8eMAE\u7684\u91cd\u5efa\u6a21\u578bProFound\u548c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6a21\u578bProViCNet\u3002\u5728\u4e94\u4e2a\u524d\u5217\u817a\u591a\u53c2\u6570MRI\u4efb\u52a1\u4e0a\uff0c\u901a\u8fc7\u6700\u5927\u5747\u503c\u5dee\u5f02\u7b49\u7b80\u5355\u53d1\u6563\u5ea6\u91cf\uff0c\u6d4b\u91cf\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u5206\u6790\u8fd9\u79cd\u5bf9\u9f50\u5982\u4f55\u5f71\u54cd\u8fc1\u79fb\u6027\u80fd\uff08\u4ece\u9884\u8bad\u7ec3\u5230\u5fae\u8c03\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u66f4\u597d\u7684\u5bf9\u9f50\uff08\u901a\u8fc7\u5fae\u8c03\u524d\u540e\u76f8\u540c\u7279\u5f81\u7684MMD\u5ea6\u91cf\uff09\u4e0e\u66f4\u5927\u7684\u6027\u80fd\u6539\u8fdb\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u76f8\u5173\u3002\u8fd9\u8868\u660e\u4efb\u52a1\u5bf9\u9f50\u662f\u5f71\u54cd\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fc1\u79fb\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8bbe\u8ba1\u9884\u8bad\u7ec3\u76ee\u6807\u65f6\u5e94\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u9002\u7528\u6027\uff0c\u4efb\u52a1\u5bf9\u9f50\u662f\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u529f\u8fc1\u79fb\u7684\u5173\u952e\u3002\u7b80\u5355\u7684\u53d1\u6563\u5ea6\u91cf\u5982MMD\u53ef\u7528\u4e8e\u8bc4\u4f30\u8fd9\u79cd\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u4ece\u800c\u6307\u5bfc\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u3002"}}
{"id": "2601.15894", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15894", "abs": "https://arxiv.org/abs/2601.15894", "authors": ["Simon W. Penninga", "Ruud J. G. van Sloun"], "title": "Iterative Amortized Hierarchical VAE", "comment": null, "summary": "In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.", "AI": {"tldr": "IA-HVAE\u7ed3\u5408\u4e86\u644a\u9500\u63a8\u7406\u548c\u8fed\u4ee3\u7ec6\u5316\uff0c\u901a\u8fc7\u53d8\u6362\u57df\u4e2d\u7684\u7ebf\u6027\u53ef\u5206\u79bb\u89e3\u7801\u5668\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\uff0c\u76f8\u6bd4\u4f20\u7edfHVAE\u5728\u8fed\u4ee3\u63a8\u7406\u4e0a\u83b7\u5f9735\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u5206\u5c42\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08HVAE\uff09\u5728\u63a8\u7406\u65f6\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u5b8c\u5168\u644a\u9500\u63a8\u7406\u901f\u5ea6\u5feb\u4f46\u7cbe\u5ea6\u6709\u9650\uff0c\u5b8c\u5168\u8fed\u4ee3\u63a8\u7406\u7cbe\u5ea6\u9ad8\u4f46\u8ba1\u7b97\u6210\u672c\u5927\u3002\u9700\u8981\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u5e94\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u644a\u9500\u5206\u5c42\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08IA-HVAE\uff09\uff0c\u91c7\u7528\u6df7\u5408\u63a8\u7406\u65b9\u6848\uff1a1\uff09\u521d\u59cb\u644a\u9500\u731c\u6d4b\uff1b2\uff09\u4f7f\u7528\u89e3\u7801\u5668\u68af\u5ea6\u8fdb\u884c\u8fed\u4ee3\u7ec6\u5316\u3002\u5173\u952e\u521b\u65b0\u662f\u5728\u53d8\u6362\u57df\uff08\u5982\u5085\u91cc\u53f6\u7a7a\u95f4\uff09\u4e2d\u521b\u5efa\u7ebf\u6027\u53ef\u5206\u79bb\u7684\u89e3\u7801\u5668\uff0c\u4f7f\u5f97\u5373\u4f7f\u6a21\u578b\u6df1\u5ea6\u5f88\u9ad8\u4e5f\u80fd\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "result": "1\uff09\u67b6\u6784\u6539\u8fdb\u5e26\u676535\u500d\u7684\u8fed\u4ee3\u63a8\u7406\u52a0\u901f\uff1b2\uff09\u6df7\u5408\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5b8c\u5168\u644a\u9500\u65b9\u6cd5\uff0c\u5728\u901f\u5ea6\u4e0a\u4f18\u4e8e\u5b8c\u5168\u8fed\u4ee3\u65b9\u6cd5\uff1b3\uff09\u5728\u53bb\u6a21\u7cca\u548c\u53bb\u566a\u7b49\u9006\u95ee\u9898\u4e2d\uff0cIA-HVAE\u76f8\u6bd4\u666e\u901aHVAE\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "IA-HVAE\u901a\u8fc7\u7ed3\u5408\u644a\u9500\u63a8\u7406\u7684\u6548\u7387\u548c\u8fed\u4ee3\u7ec6\u5316\u7684\u7cbe\u5ea6\uff0c\u4e3a\u5206\u5c42\u53d8\u5206\u81ea\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u63a8\u7406\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u5904\u7406\u7684\u9ad8\u6df1\u5ea6\u6a21\u578b\u548c\u9006\u95ee\u9898\u5e94\u7528\u3002"}}
{"id": "2601.16007", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16007", "abs": "https://arxiv.org/abs/2601.16007", "authors": ["Chak-Wing Mak", "Guanyu Zhu", "Boyi Zhang", "Hongji Li", "Xiaowei Chi", "Kevin Zhang", "Yichen Wu", "Yangfan He", "Chun-Kai Fan", "Wentao Lu", "Kuangzhi Ge", "Xinyu Fang", "Hongyang He", "Kuan Lu", "Tianxiang Xu", "Li Zhang", "Yongxin Ni", "Youhua Li", "Shanghang Zhang"], "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models", "comment": null, "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.", "AI": {"tldr": "PhysicsMind\uff1a\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7269\u7406\u7406\u89e3\u80fd\u529b\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\uff0c\u6d4b\u8bd5\u8d28\u5fc3\u3001\u6760\u6746\u5e73\u8861\u548c\u725b\u987f\u7b2c\u4e00\u5b9a\u5f8b\u4e09\u4e2a\u7269\u7406\u539f\u7406\u7684\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u6570\u5b66\u3001\u5e38\u8bc6\u548c\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u57fa\u7840\u7269\u7406\u539f\u7406\u7684\u7406\u89e3\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u57fa\u51c6\u8981\u4e48\u4f9d\u8d56\u5408\u6210\u7684\u89c6\u89c9\u95ee\u7b54\u6a21\u677f\uff0c\u8981\u4e48\u5173\u6ce8\u4e0e\u7269\u7406\u5b9a\u5f8b\u4e00\u81f4\u6027\u65e0\u5173\u7684\u611f\u77e5\u89c6\u9891\u8d28\u91cf\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u89c4\u5f8b\u4e00\u81f4\u6027\u63a8\u7406\u548c\u751f\u6210\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51faPhysicsMind\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u548c\u4eff\u771f\u4e24\u79cd\u73af\u5883\uff0c\u8bc4\u4f30\u4e09\u4e2a\u7ecf\u5178\u7269\u7406\u539f\u7406\uff1a\u8d28\u5fc3\u3001\u6760\u6746\u5e73\u8861\u548c\u725b\u987f\u7b2c\u4e00\u5b9a\u5f8b\u3002\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a1) VQA\u4efb\u52a1\uff1a\u6d4b\u8bd5\u6a21\u578b\u80fd\u5426\u4ece\u56fe\u50cf\u6216\u77ed\u89c6\u9891\u4e2d\u63a8\u7406\u5e76\u786e\u5b9a\u7269\u7406\u91cf\u548c\u6570\u503c\uff1b2) \u89c6\u9891\u751f\u6210\u4efb\u52a1\uff1a\u8bc4\u4f30\u9884\u6d4b\u7684\u8fd0\u52a8\u8f68\u8ff9\u662f\u5426\u4e0e\u5730\u9762\u771f\u5b9e\u503c\u9075\u5faa\u76f8\u540c\u7684\u8d28\u5fc3\u3001\u626d\u77e9\u548c\u60ef\u6027\u7ea6\u675f\u3002", "result": "\u5bf9\u4e00\u7cfb\u5217\u6700\u65b0\u6a21\u578b\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u53d1\u73b0\uff0c\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u5916\u89c2\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7ecf\u5e38\u8fdd\u53cd\u57fa\u672c\u529b\u5b66\u539f\u7406\u3002\u8fd9\u4e9b\u5dee\u8ddd\u8868\u660e\u5f53\u524d\u7684\u6269\u5c55\u548c\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u7269\u7406\u7406\u89e3\u3002", "conclusion": "PhysicsMind\u4f5c\u4e3a\u4e00\u4e2a\u805a\u7126\u7269\u7406\u611f\u77e5\u591a\u6a21\u6001\u6a21\u578b\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u7269\u7406\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002\u6570\u636e\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2601.16083", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16083", "abs": "https://arxiv.org/abs/2601.16083", "authors": ["Matthew Shorvon", "Frederik Mallmann-Trenn", "David S. Watson"], "title": "Probably Approximately Correct Maximum A Posteriori Inference", "comment": "7 pages main text, 16 total, 3 figures", "summary": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8eMAP\u63a8\u7406\u7684PAC\u7b97\u6cd5\uff0c\u5728\u53ef\u53d8\u548c\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6700\u4f18\u89e3\uff0c\u4f7f\u7528\u6982\u7387\u7535\u8def\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u5316\u7b56\u7565\u589e\u5f3a\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5", "motivation": "MAP\u4f30\u8ba1\u662f\u6982\u7387\u63a8\u7406\u4e2d\u7684\u57fa\u672c\u4efb\u52a1\uff0c\u4f46\u901a\u5e38\u96be\u4ee5\u8ba1\u7b97\uff0c\u5373\u4f7f\u5728\u8bb8\u591a\u5e38\u89c1\u7684\u7ed3\u6784\u7ea6\u675f\u548c\u8fd1\u4f3c\u65b9\u6848\u4e0b\u4ecd\u7136\u56f0\u96be\u3002\u9700\u8981\u5f00\u53d1\u5728\u8ba1\u7b97\u9884\u7b97\u9650\u5236\u4e0b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684MAP\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u5f15\u5165PAC-MAP\u7b97\u6cd5\uff0c\u4f7f\u7528\u4fe1\u606f\u8bba\u5ea6\u91cf\u8868\u5f81\u53ef\u5904\u7406\u6027\u6761\u4ef6\uff0c\u901a\u8fc7\u6982\u7387\u7535\u8def\u9ad8\u6548\u5b9e\u73b0\uff0c\u5f00\u53d1\u968f\u673a\u5316\u7b56\u7565\u4f5c\u4e3a\u72ec\u7acbMAP\u63a8\u7406\u6280\u672f\u6216\u6539\u8fdb\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cPAC-MAP\u7b97\u6cd5\u80fd\u591f\u5728\u8ba1\u7b97\u9884\u7b97\u9650\u5236\u4e0b\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6700\u4f18\u89e3\uff0c\u968f\u673a\u5316\u7b56\u7565\u80fd\u591f\u589e\u5f3a\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u63d0\u51fa\u7684PAC-MAP\u6846\u67b6\u4e3aMAP\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u5ea6\u91cf\u548c\u6982\u7387\u7535\u8def\u5b9e\u73b0\uff0c\u80fd\u591f\u5904\u7406\u4f20\u7edf\u4e0a\u96be\u4ee5\u8ba1\u7b97\u7684MAP\u4f30\u8ba1\u95ee\u9898"}}
{"id": "2601.16140", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.16140", "abs": "https://arxiv.org/abs/2601.16140", "authors": ["Sylvestre-Alvise Rebuffi", "Tuan Tran", "Valeriu Lacatusu", "Pierre Fernandez", "Tom\u00e1\u0161 Sou\u010dek", "Nikola Jovanovi\u0107", "Tom Sander", "Hady Elsahar", "Alexandre Mourachko"], "title": "Learning to Watermark in the Latent Space of Generative Models", "comment": "Code and models are available at https://github.com/facebookresearch/distseal", "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.", "AI": {"tldr": "DistSeal\uff1a\u4e00\u79cd\u5728\u6269\u6563\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u6c34\u5370\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u5b9e\u73b0\u6a21\u578b\u5185\u6c34\u5370\uff0c\u76f8\u6bd4\u50cf\u7d20\u7a7a\u95f4\u65b9\u6cd5\u63d0\u901f20\u500d", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5\u901a\u5e38\u5728\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u53ef\u80fd\u5f15\u5165\u89c6\u89c9\u4f2a\u5f71\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u4e0d\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u7684\u6c34\u5370\u65b9\u6848", "method": "\u63d0\u51faDistSeal\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u540e\u5904\u7406\u6c34\u5370\u6a21\u578b\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6f5c\u5728\u6c34\u5370\u5668\u84b8\u998f\u5230\u751f\u6210\u6a21\u578b\u672c\u8eab\u6216\u6f5c\u5728\u89e3\u7801\u5668\u4e2d\uff0c\u5b9e\u73b0\u6a21\u578b\u5185\u6c34\u5370", "result": "\u6f5c\u5728\u6c34\u5370\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u76f8\u4f3c\u7684\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u76f8\u6bd4\u50cf\u7d20\u7a7a\u95f4\u57fa\u7ebf\u63d0\u901f\u9ad8\u8fbe20\u500d\uff1b\u84b8\u998f\u6f5c\u5728\u6c34\u5370\u5668\u4f18\u4e8e\u84b8\u998f\u50cf\u7d20\u7a7a\u95f4\u6c34\u5370\u5668", "conclusion": "DistSeal\u4e3aAI\u751f\u6210\u56fe\u50cf\u6c34\u5370\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6c34\u5370\u548c\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861"}}
{"id": "2601.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16175", "abs": "https://arxiv.org/abs/2601.16175", "authors": ["Mert Yuksekgonul", "Daniel Koceja", "Xinhao Li", "Federico Bianchi", "Jed McCaleb", "Xiaolong Wang", "Jan Kautz", "Yejin Choi", "James Zou", "Carlos Guestrin", "Yu Sun"], "title": "Learning to Discover at Test Time", "comment": "Code: https://github.com/test-time-training/discover", "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "AI": {"tldr": "TTT-Discover\uff1a\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u7279\u5b9a\u79d1\u5b66\u95ee\u9898\u53d1\u73b0\u6700\u4f18\u89e3\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff08\u5982AlphaEvolve\uff09\u4f7f\u7528\u51bb\u7ed3\u7684LLM\u8fdb\u884c\u641c\u7d22\uff0c\u65e0\u6cd5\u9488\u5bf9\u7279\u5b9a\u6d4b\u8bd5\u95ee\u9898\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9LLM\u5728\u6d4b\u8bd5\u65f6\u80fd\u591f\u57fa\u4e8e\u7279\u5b9a\u95ee\u9898\u7684\u7ecf\u9a8c\u7ee7\u7eed\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u4e3a\u5355\u4e2a\u95ee\u9898\u627e\u5230\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u800c\u975e\u5e73\u5747\u8868\u73b0\u3002", "method": "TTT-Discover\uff08\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u53d1\u73b0\uff09\uff1a\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7fLLM\u80fd\u591f\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u6301\u7eed\u8bad\u7ec3\u3002\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u5b66\u4e60\u76ee\u6807\u548c\u641c\u7d22\u5b50\u7a0b\u5e8f\uff0c\u4f18\u5148\u8003\u8651\u6700\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f7f\u7528\u5f00\u6e90\u6a21\u578bOpenAI gpt-oss-120b\uff0c\u901a\u8fc7Tinker API\u8fdb\u884c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\uff1a1) \u6570\u5b66\uff1aErd\u0151s\u6700\u5c0f\u91cd\u53e0\u95ee\u9898\u548c\u81ea\u76f8\u5173\u4e0d\u7b49\u5f0f\uff1b2) GPU\u5185\u6838\u5de5\u7a0b\uff1aGPUMode\u5185\u6838\u7ade\u8d5b\uff08\u6bd4\u73b0\u6709\u6280\u672f\u5feb2\u500d\uff09\uff1b3) \u7b97\u6cd5\u8bbe\u8ba1\uff1a\u8fc7\u53bb\u7684AtCoder\u7b97\u6cd5\u7ade\u8d5b\uff1b4) \u751f\u7269\u5b66\uff1a\u5355\u7ec6\u80de\u5206\u6790\u4e2d\u7684\u53bb\u566a\u95ee\u9898\u3002\u6240\u6709\u7ed3\u679c\u5747\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\uff0c\u6210\u672c\u4ec5\u4e3a\u6bcf\u4e2a\u95ee\u9898\u51e0\u767e\u7f8e\u5143\u3002", "conclusion": "TTT-Discover\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7fLLM\u80fd\u591f\u9488\u5bf9\u7279\u5b9a\u79d1\u5b66\u95ee\u9898\u8fdb\u884c\u6301\u7eed\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\uff0c\u6210\u672c\u53ef\u63a7\uff0c\u4e14\u7ed3\u679c\u53ef\u590d\u73b0\uff0c\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684AI\u9a71\u52a8\u8303\u5f0f\u3002"}}
{"id": "2601.16205", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16205", "abs": "https://arxiv.org/abs/2601.16205", "authors": ["Patrick Altmeyer", "Aleksander Buszydlik", "Arie van Deursen", "Cynthia C. S. Liem"], "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations", "comment": "This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore", "summary": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\"\u53cd\u4e8b\u5b9e\u8bad\u7ec3\"\u7684\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6765\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u6a21\u578b\u5728\u8bad\u7ec3\u9636\u6bb5\u5c31\u5b66\u4e60\u751f\u6210\u5408\u7406\u4e14\u53ef\u64cd\u4f5c\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u591a\u4e3a\u540e\u5904\u7406\u6280\u672f\uff0c\u867d\u7136\u80fd\u751f\u6210\u5408\u7406\u4e14\u53ef\u64cd\u4f5c\u7684\u53cd\u4e8b\u5b9e\uff0c\u4f46\u65e0\u6cd5\u786e\u4fdd\u6a21\u578b\u672c\u8eab\u5177\u5907\u826f\u597d\u7684\u89e3\u91ca\u80fd\u529b\u3002\u7814\u7a76\u8005\u5e0c\u671b\u6a21\u578b\u5728\u8bad\u7ec3\u9636\u6bb5\u5c31\u76f4\u63a5\u5b66\u4e60\u751f\u6210\u7b26\u5408\u8981\u6c42\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u800c\u4e0d\u662f\u4e8b\u540e\u8865\u6551\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6765\u6700\u5c0f\u5316\u5b66\u5f97\u8868\u793a\u4e0e\u5408\u7406\u3001\u53ef\u64cd\u4f5c\u89e3\u91ca\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u5185\u5728\u7b26\u5408\u8981\u6c42\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8bad\u7ec3\u51fa\u65e2\u80fd\u63d0\u4f9b\u7406\u60f3\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u53c8\u5177\u6709\u66f4\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002\u5b9e\u8bc1\u548c\u7406\u8bba\u5206\u6790\u5747\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u8bad\u7ec3\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u6574\u5408\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8981\u6c42\uff0c\u4f7f\u6a21\u578b\u5177\u5907\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u540e\u5904\u7406\u89e3\u91ca\u65b9\u6cd5\u3002"}}
{"id": "2601.16210", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16210", "abs": "https://arxiv.org/abs/2601.16210", "authors": ["Onkar Susladkar", "Tushar Prakash", "Adheesh Juvekar", "Kiet A. Nguyen", "Dong-Hwan Jang", "Inderjit S Dhillon", "Ismini Lourentzou"], "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation", "comment": null, "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.", "AI": {"tldr": "PyraTok\u662f\u4e00\u79cd\u8bed\u8a00\u5bf9\u9f50\u7684\u91d1\u5b57\u5854\u5f0f\u89c6\u9891\u6807\u8bb0\u5668\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6587\u672c\u5f15\u5bfc\u91cf\u5316\u548c\u5171\u4eab\u5927\u578b\u4e8c\u8fdb\u5236\u7801\u672c\uff0c\u5728\u591a\u4e2a\u65f6\u7a7a\u5206\u8fa8\u7387\u4e0a\u5b66\u4e60\u8bed\u4e49\u7ed3\u6784\u5316\u7684\u79bb\u6563\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u91cd\u5efa\u3001\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u548c\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u89c6\u9891VAE\u901a\u5e38\u5b66\u4e60\u5355\u5c3a\u5ea6\u3001\u6709\u9650\u8bcd\u6c47\u91cf\u7684\u89c6\u89c9\u7801\u672c\uff0c\u4e14\u8bed\u8a00\u76d1\u7763\u8f83\u6d45\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b66\u4e60\u591a\u5206\u8fa8\u7387\u8bed\u4e49\u7ed3\u6784\u5316\u79bb\u6563\u6f5c\u5728\u8868\u793a\u7684\u89c6\u9891\u6807\u8bb0\u5668\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891VAE\uff0c\u63d0\u51fa\u8bed\u8a00\u5bf9\u9f50\u91d1\u5b57\u5854\u91cf\u5316(LaPQ)\u6a21\u5757\uff0c\u4f7f\u7528\u5171\u4eab\u5927\u578b\u4e8c\u8fdb\u5236\u7801\u672c\u5728\u591a\u4e2a\u6df1\u5ea6\u79bb\u6563\u5316\u7f16\u7801\u5668\u7279\u5f81\uff0c\u751f\u6210\u7d27\u51d1\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u89c6\u9891\u6807\u8bb0\u5e8f\u5217\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u5c3a\u5ea6\u6587\u672c\u5f15\u5bfc\u91cf\u5316\u548c\u6807\u8bb0\u5c42\u6b21\u7ed3\u6784\u7684\u5168\u5c40\u81ea\u56de\u5f52\u76ee\u6807\uff0c\u7d27\u5bc6\u8026\u5408\u89c6\u89c9\u6807\u8bb0\u4e0e\u8bed\u8a00\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPyraTok\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u91cd\u5efa\u8d28\u91cf\uff0c\u6301\u7eed\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u89c6\u9891\u5206\u5272\u3001\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u521b\u9020\u4e86\u65b0\u7684\u96f6\u6837\u672c\u6027\u80fd\u8bb0\u5f55\uff0c\u80fd\u591f\u7a33\u5065\u6269\u5c55\u52304K/8K\u5206\u8fa8\u7387\u3002", "conclusion": "PyraTok\u901a\u8fc7\u8bed\u8a00\u5bf9\u9f50\u7684\u91d1\u5b57\u5854\u5f0f\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u6807\u8bb0\u5668\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u5206\u8fa8\u7387\u4e0a\u5b66\u4e60\u8bed\u4e49\u7ed3\u6784\u5316\u7684\u79bb\u6563\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u548c\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u3002"}}
{"id": "2601.16211", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16211", "abs": "https://arxiv.org/abs/2601.16211", "authors": ["Geo Ahn", "Inwoong Lee", "Taeoh Kim", "Minho Shim", "Dongyoon Wee", "Jinwoo Choi"], "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition", "comment": "The code is available at https://github.com/KHU-VLL/RCORE", "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRCORE\u6846\u67b6\u89e3\u51b3\u96f6\u6837\u672c\u7ec4\u5408\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u5bf9\u8c61\u9a71\u52a8\u52a8\u8bcd\u6377\u5f84\u95ee\u9898\uff0c\u901a\u8fc7\u7ec4\u5408\u611f\u77e5\u589e\u5f3a\u548c\u65f6\u95f4\u987a\u5e8f\u6b63\u5219\u5316\u6765\u5f3a\u5236\u65f6\u57df\u57fa\u7840\u52a8\u8bcd\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u672a\u89c1\u7ec4\u5408\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u7ec4\u5408\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u672a\u89c1\u52a8\u8bcd-\u5bf9\u8c61\u7ec4\u5408\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u5bf9\u8c61\u9a71\u52a8\u52a8\u8bcd\u6377\u5f84\u3002\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u5171\u73b0\u7edf\u8ba1\u800c\u5ffd\u89c6\u89c6\u89c9\u8bc1\u636e\uff0c\u5bfc\u81f4\u65e0\u6cd5\u83b7\u5f97\u7ec4\u5408\u8bc6\u522b\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faRCORE\u6846\u67b6\uff1a1) \u7ec4\u5408\u611f\u77e5\u589e\u5f3a\uff0c\u5728\u4e0d\u7834\u574f\u8fd0\u52a8\u7ebf\u7d22\u7684\u60c5\u51b5\u4e0b\u591a\u6837\u5316\u52a8\u8bcd-\u5bf9\u8c61\u7ec4\u5408\uff1b2) \u65f6\u95f4\u987a\u5e8f\u6b63\u5219\u5316\u635f\u5931\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u7ed3\u6784\u6765\u60e9\u7f5a\u6377\u5f84\u884c\u4e3a\uff0c\u5f3a\u5236\u65f6\u57df\u57fa\u7840\u52a8\u8bcd\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5(Sth-com\u548c\u65b0\u6784\u5efa\u7684EK100-com)\u4e0a\uff0cRCORE\u663e\u8457\u63d0\u5347\u672a\u89c1\u7ec4\u5408\u51c6\u786e\u7387\uff0c\u51cf\u5c11\u5bf9\u5171\u73b0\u504f\u5dee\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u6b63\u7ec4\u5408\u5dee\u8ddd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5bf9\u8c61\u9a71\u52a8\u6377\u5f84\u662f\u96f6\u6837\u672c\u7ec4\u5408\u52a8\u4f5c\u8bc6\u522b\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5bf9\u4e8e\u7a33\u5065\u7684\u7ec4\u5408\u89c6\u9891\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002RCORE\u901a\u8fc7\u5f3a\u5236\u65f6\u57df\u57fa\u7840\u52a8\u8bcd\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\u3002"}}
