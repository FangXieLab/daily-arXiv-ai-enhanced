<div id=toc></div>

# Table of Contents

- [math.ST](#math.ST) [Total: 3]


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1] [Admissibility Breakdown in High-Dimensional Sparse Regression with L1 Regularization](https://arxiv.org/abs/2601.10100)
*Guo Liu*

Main category: math.ST

TL;DR: 论文研究了Lasso回归中调优参数的选择问题，发现在非渐近框架下，某些经典调优参数选择会使Lasso估计量在均方预测误差意义下不可接受，并提出Lasso-Ridge改进方法。


<details>
  <summary>Details</summary>
Motivation: Lasso调优参数的选择对其在高维线性回归中的统计性能至关重要。虽然经典一致性理论确定了Lasso调优参数的速率，许多研究也建立了非渐近保证，但在非渐近框架下最优调优的问题尚未完全解决。

Method: 建立调优准则，证明当调优参数超过特定阈值时，Lasso在均方预测误差意义下变得不可接受。具体建立了阈值，显示某些经典调优选择产生的Lasso估计量严格被简单的Lasso-Ridge改进方法所支配。同时研究了设计矩阵和噪声向量的结构如何影响不可接受现象。

Result: 确定了Lasso变得不可接受的调优参数阈值，证明某些经典调优选择产生的Lasso估计量严格劣于Lasso-Ridge改进方法。分析了设计矩阵和噪声向量结构对不可接受现象的影响。

Conclusion: 在非渐近框架下，某些经典Lasso调优参数选择并非最优，会导致估计量不可接受。Lasso-Ridge改进方法能够提供更好的统计性能，设计矩阵和噪声向量的结构特征会影响Lasso的不可接受性。

Abstract: The choice of the tuning parameter in the Lasso is central to its statistical performance in high-dimensional linear regression. Classical consistency theory identifies the rate of the Lasso tuning parameter, and numerous studies have established non-asymptotic guarantees. Nevertheless, the question of optimal tuning within a non-asymptotic framework has not yet been fully resolved. We establish tuning criteria above which the Lasso becomes inadmissible under mean squared prediction error. More specifically, we establish thresholds showing that certain classical tuning choices yield Lasso estimators strictly dominated by a simple Lasso-Ridge refinement. We also address how the structure of the design matrix and the noise vector influences the inadmissibility phenomenon.

</details>


### [2] [On gradient stability in nonlinear PDE models and inference in interacting particle systems](https://arxiv.org/abs/2601.10326)
*Aurélien Castre,Richard Nickl*

Main category: math.ST

TL;DR: 基于Banach空间隐函数定理验证非线性PDE参数-解映射的梯度稳定性条件，应用于反应扩散系统和McKean-Vlasov粒子模型，并用于证明Langevin算法对后验分布的采样收敛性


<details>
  <summary>Details</summary>
Motivation: 针对非线性偏微分方程的参数-解映射，需要验证Nickl&Wang (JEMS 2024)提出的梯度稳定性条件，以建立非线性反问题的统计可识别性理论框架

Method: 采用Banach空间版本的隐函数定理来验证梯度稳定性条件，同时提供单射性估计；将方法应用于具有周期边界条件的非线性反应扩散系统和McKean-Vlasov相互作用粒子模型

Result: 成功验证了梯度稳定性条件，获得了统计可识别性结果，并证明了Langevin型算法在采样相互作用势能后验分布时的多项式时间收敛性

Conclusion: 基于Banach空间隐函数定理的方法能够有效验证非线性PDE反问题的梯度稳定性条件，为统计推断提供了理论基础，并成功应用于具体模型和算法收敛性分析

Abstract: We consider general parameter to solution maps $θ\mapsto \mathcal G(θ)$ of non-linear partial differential equations and describe an approach based on a Banach space version of the implicit function theorem to verify the gradient stability condition of Nickl&Wang (JEMS 2024) for the underlying non-linear inverse problem, providing also injectivity estimates and corresponding statistical identifiability results. We illustrate our methods in two examples involving a non-linear reaction diffusion system as well as a McKean--Vlasov interacting particle model, both with periodic boundary conditions. We apply our results to prove the polynomial time convergence of a Langevin-type algorithm sampling the posterior measure of the interaction potential arising from a discrete aggregate measurement of the interacting particle system.

</details>


### [3] [Differentially Private Inference for Longitudinal Linear Regression](https://arxiv.org/abs/2601.10626)
*Getoar Sopa,Marco Avella Medina,Cynthia Rush*

Main category: math.ST

TL;DR: 该论文提出了首个针对纵向线性回归的用户级差分隐私统一框架，包含估计、推断和协方差估计方法，适用于面板数据中用户贡献多个相关观测的场景。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私线性回归方法主要针对项目级隐私（每个用户贡献单个观测），但许多科学和经济应用涉及纵向或面板数据，其中每个用户贡献多个相关观测。在这些场景中，项目级隐私保护不足，需要用户级隐私来保护个体的整个轨迹。

Method: 提出基于聚合局部回归的用户级隐私回归估计器；开发私有化、偏差校正的协方差估计器，自动保持异方差和自相关一致性；在短程依赖条件下建立有限样本保证和渐近正态性。

Result: 建立了首个针对纵向线性回归的用户级差分隐私统一框架，提供了实用的估计和推断方法，具有强大的理论保证和良好的实证性能。

Conclusion: 该研究填补了纵向数据用户级差分隐私回归的空白，为面板数据中的隐私保护估计和推断提供了完整的理论框架和实用工具，适用于依赖条件下的科学和经济应用。

Abstract: Differential Privacy (DP) provides a rigorous framework for releasing statistics while protecting individual information present in a dataset. Although substantial progress has been made on differentially private linear regression, existing methods almost exclusively address the item-level DP setting, where each user contributes a single observation. Many scientific and economic applications instead involve longitudinal or panel data, in which each user contributes multiple dependent observations. In these settings, item-level DP offers inadequate protection, and user-level DP - shielding an individual's entire trajectory - is the appropriate privacy notion. We develop a comprehensive framework for estimation and inference in longitudinal linear regression under user-level DP. We propose a user-level private regression estimator based on aggregating local regressions, and we establish finite-sample guarantees and asymptotic normality under short-range dependence. For inference, we develop a privatized, bias-corrected covariance estimator that is automatically heteroskedasticity- and autocorrelation-consistent. These results provide the first unified framework for practical user-level DP estimation and inference in longitudinal linear regression under dependence, with strong theoretical guarantees and promising empirical performance.

</details>
