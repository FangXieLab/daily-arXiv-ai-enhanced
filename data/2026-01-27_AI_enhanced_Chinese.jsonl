{"id": "2601.17644", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17644", "abs": "https://arxiv.org/abs/2601.17644", "authors": ["Ali Al-Lawati", "Suhang Wang"], "title": "A Systemic Evaluation of Multimodal RAG Privacy", "comment": null, "summary": "The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.", "AI": {"tldr": "\u5bf9\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08mRAG\uff09\u7ba1\u9053\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u901a\u8fc7\u6807\u51c6\u6a21\u578b\u63d0\u793a\u6cc4\u9732\u79c1\u6709\u56fe\u50cf\u53ca\u5176\u5143\u6570\u636e\uff08\u5982\u6807\u9898\uff09\u7684\u98ce\u9669", "motivation": "\u968f\u7740mRAG\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u95ee\u7b54\uff09\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u867d\u7136\u5b83\u80fd\u591f\u8fde\u63a5\u79c1\u6709\u6570\u636e\u96c6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u91cd\u8981\u7684\u9690\u79c1\u6311\u6218\uff0c\u5b58\u5728\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6cc4\u9732\u79c1\u6709\u4fe1\u606f\u7684\u98ce\u9669", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790mRAG\u7ba1\u9053\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5b9e\u65bd\u6848\u4f8b\u7814\u7a76\uff0c\u5c1d\u8bd5\u63a8\u65ad\u89c6\u89c9\u8d44\u4ea7\uff08\u5982\u56fe\u50cf\uff09\u662f\u5426\u5305\u542b\u5728mRAG\u4e2d\uff0c\u5982\u679c\u5b58\u5728\u5219\u6cc4\u9732\u5176\u76f8\u5173\u5143\u6570\u636e\uff08\u5982\u6807\u9898\uff09", "result": "\u7814\u7a76\u53d1\u73b0mRAG\u7ba1\u9053\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u98ce\u9669\uff0c\u901a\u8fc7\u6807\u51c6\u6a21\u578b\u63d0\u793a\u53ef\u80fd\u6cc4\u9732\u79c1\u6709\u89c6\u89c9\u8d44\u4ea7\u53ca\u5176\u5143\u6570\u636e", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u5bf9mRAG\u9690\u79c1\u7684\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2601.17661", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17661", "abs": "https://arxiv.org/abs/2601.17661", "authors": ["Ahmed Oun", "Rishabh Das", "Clay Hess", "Aakriti Barat", "Savas Kaya"], "title": "A PUF-Based Security Framework for Fault and Intrusion Detection", "comment": null, "summary": "Industrial Control Systems (ICS) rely on sensor feedback to keep safety-critical processes within operational limits. This research presents a hardware-root-of-trust that embeds a Physically Unclonable Function (PUF) at the measurement layer to authenticate sensor readings. The architecture combines voltage fingerprinting with a temporal authentication that integrates with standard industrial control system architecture. The research prototypes the PUF integration on a hardware-in-the-loop (HIL) water tank testbed using a Simulink-based PUF emulator. The system maintains 99.97% accuracy over a 5.18-hour period of normal operation and flags all injected anomalies, including spike faults, hard-over faults, and hardware trojan scenarios that push the system over to an unsafe operational state. The proposed architecture provides a process-aware, vendor-agnostic approach that can integrate with legacy plants to detect sensor signal degradation or sophisticated supply chain attacks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786c\u4ef6\u4fe1\u4efb\u6839\u7684\u4f20\u611f\u5668\u8ba4\u8bc1\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u6d4b\u91cf\u5c42\u5d4c\u5165\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\u6765\u9a8c\u8bc1\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u5668\u8bfb\u6570\uff0c\u80fd\u591f\u68c0\u6d4b\u5404\u79cd\u6ce8\u5165\u5f02\u5e38\u548c\u786c\u4ef6\u6728\u9a6c\u653b\u51fb\u3002", "motivation": "\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u4f9d\u8d56\u4f20\u611f\u5668\u53cd\u9988\u6765\u786e\u4fdd\u5b89\u5168\u5173\u952e\u8fc7\u7a0b\u5728\u64cd\u4f5c\u9650\u503c\u5185\u8fd0\u884c\uff0c\u4f46\u4f20\u611f\u5668\u8bfb\u6570\u53ef\u80fd\u53d7\u5230\u4fe1\u53f7\u9000\u5316\u6216\u4f9b\u5e94\u94fe\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u8ba4\u8bc1\u673a\u5236\u6765\u4fdd\u969c\u7cfb\u7edf\u5b89\u5168\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u4fe1\u4efb\u6839\u67b6\u6784\uff0c\u5728\u6d4b\u91cf\u5c42\u5d4c\u5165\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\uff0c\u7ed3\u5408\u7535\u538b\u6307\u7eb9\u548c\u65f6\u95f4\u8ba4\u8bc1\u6280\u672f\uff0c\u4e0e\u6807\u51c6\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u67b6\u6784\u96c6\u6210\u3002\u901a\u8fc7\u57fa\u4e8eSimulink\u7684PUF\u4eff\u771f\u5668\u5728\u786c\u4ef6\u5728\u73af\u6c34\u7bb1\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u539f\u578b\u5b9e\u73b0\u3002", "result": "\u7cfb\u7edf\u57285.18\u5c0f\u65f6\u6b63\u5e38\u64cd\u4f5c\u671f\u95f4\u4fdd\u630199.97%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u6807\u8bb0\u6240\u6709\u6ce8\u5165\u7684\u5f02\u5e38\uff0c\u5305\u62ec\u5c16\u5cf0\u6545\u969c\u3001\u786c\u8fc7\u6545\u969c\u4ee5\u53ca\u5c06\u7cfb\u7edf\u63a8\u5411\u4e0d\u5b89\u5168\u64cd\u4f5c\u72b6\u6001\u7684\u786c\u4ef6\u6728\u9a6c\u573a\u666f\u3002", "conclusion": "\u8be5\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fc7\u7a0b\u611f\u77e5\u3001\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u4e0e\u9057\u7559\u5de5\u5382\u96c6\u6210\uff0c\u68c0\u6d4b\u4f20\u611f\u5668\u4fe1\u53f7\u9000\u5316\u6216\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u653b\u51fb\uff0c\u4e3a\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u589e\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2601.18145", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.18145", "abs": "https://arxiv.org/abs/2601.18145", "authors": ["Heguang Lin", "Binhao Chen", "Mengze Li", "Daniel Pimentel-Alarc\u00f3n", "Matthew L. Malloy"], "title": "Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes", "comment": "15 pages, 1 figure", "summary": "Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u8bc1\u7b97\u6cd5\uff0c\u7528\u4e8e\u5224\u65ad\u4e24\u4e2a\u89c2\u5bdf\u5230\u7684\u591a\u9879\u5206\u5e03\u7ed3\u679c\u7684\u6700\u5c0f\u4f53\u79ef\u7f6e\u4fe1\u96c6(MVCs)\u662f\u5426\u76f8\u4ea4\uff0c\u89e3\u51b3\u4e86A/B\u6d4b\u8bd5\u4e2d\u7684\u6838\u5fc3\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u6700\u5c0f\u4f53\u79ef\u7f6e\u4fe1\u96c6(MVCs)\u5728\u591a\u9879\u5206\u5e03\u53c2\u6570\u63a8\u65ad\u4e2d\u662f\u6700\u4f18\u7684\uff0c\u4f46\u5176\u51e0\u4f55\u5f62\u72b6\u590d\u6742\u4e14\u96be\u4ee5\u8ba1\u7b97\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u89e3\u51b3\u4e00\u4e2a\u6838\u5fc3\u51b3\u7b56\u95ee\u9898\uff1a\u7ed9\u5b9a\u4e24\u4e2a\u89c2\u5bdf\u7ed3\u679c\uff0c\u80fd\u5426\u8ba4\u8bc1\u5b83\u4eec\u7684MVCs\u662f\u5426\u76f8\u4ea4\uff1f\u8fd9\u76f4\u63a5\u5173\u7cfb\u5230A/B\u6d4b\u8bd5\u4e2d\u7684\u51b3\u7b56\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u4f3c\u7136\u6392\u5e8f\u5728\u5bf9\u6570\u51e0\u7387\u5750\u6807\u4e2d\u4ea7\u751f\u534a\u7a7a\u95f4\u7ea6\u675f\uff0c\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u7684\u81ea\u9002\u5e94\u51e0\u4f55\u5212\u5206\uff0c\u8ba1\u7b97\u6bcf\u4e2a\u5355\u5143\u4e0ap\u503c\u7684\u53ef\u8ba1\u7b97\u4e0a\u4e0b\u754c\u3002\u5bf9\u4e8e\u4e09\u7c7b\u522b\u60c5\u51b5\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u4e14\u53ef\u8bc1\u660e\u53ef\u9760\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u8ba4\u8bc1\u4ea4\u96c6\u3001\u8ba4\u8bc1\u4e0d\u76f8\u4ea4\uff0c\u6216\u5728\u51b3\u7b56\u5904\u4e8e\u9884\u8bbe\u8fb9\u754c\u5185\u65f6\u8fd4\u56de\u4e0d\u786e\u5b9a\u7ed3\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u7c7b\u522b\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u8ba4\u8bc1\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6269\u5c55\u5230\u66f4\u9ad8\u7ef4\u5ea6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1MVCs\u5177\u6709\u4e0d\u89c4\u5219\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4f46\u5bf9\u4e8eA/B\u6d4b\u8bd5\u4e2d\u7684\u6838\u5fc3\u4efb\u52a1\u4ecd\u53ef\u8bbe\u8ba1\u53ef\u9760\u7684\u8ba4\u8bc1\u51b3\u7b56\u7a0b\u5e8f\u3002", "conclusion": "\u5c3d\u7ba1\u6700\u5c0f\u4f53\u79ef\u7f6e\u4fe1\u96c6\u7684\u51e0\u4f55\u5f62\u72b6\u590d\u6742\uff0c\u4f46\u901a\u8fc7\u5229\u7528\u4f3c\u7136\u6392\u5e8f\u5728\u5bf9\u6570\u51e0\u7387\u5750\u6807\u4e2d\u7684\u534a\u7a7a\u95f4\u7ea6\u675f\u7279\u6027\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u53ef\u9760\u7684\u8ba4\u8bc1\u51b3\u7b56\u7a0b\u5e8f\u6765\u89e3\u51b3MVCs\u4ea4\u96c6\u95ee\u9898\uff0c\u8fd9\u5bf9\u4e8eA/B\u6d4b\u8bd5\u7b49\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17785", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.17785", "abs": "https://arxiv.org/abs/2601.17785", "authors": ["Tushar Jain"], "title": "Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain", "comment": null, "summary": "The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u79cd\u540e\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u7b97\u6cd5\u7684\u533a\u5757\u94fe\u539f\u578b\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u683c\u7684CRYSTALS-Dilithium\u3001Falcon\u548cHawk\u7b97\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u52a0\u5bc6\u8d27\u5e01\u548c\u533a\u5757\u94fe\u5e73\u53f0\u4f9d\u8d56\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u5b66\uff0c\u800cShor\u7b97\u6cd5\u4f7f\u5176\u6613\u53d7\u91cf\u5b50\u653b\u51fb\u5a01\u80c1\u3002\u4e3a\u786e\u4fdd\u533a\u5757\u94fe\u7684\u957f\u671f\u5b89\u5168\u6027\uff0c\u9700\u8981\u7814\u7a76\u540e\u91cf\u5b50\u6570\u5b57\u7b7e\u540d\u5728\u5b9e\u9645\u533a\u5757\u94fe\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u79cd\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u7b97\u6cd5\u7684\u533a\u5757\u94fe\u539f\u578b\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u683c\u7684\u7b7e\u540d\u65b9\u6848\uff08CRYSTALS-Dilithium\u3001Falcon\u3001Hawk\uff09\uff0c\u8bc4\u4f30\u4e86\u5bc6\u94a5\u751f\u6210\u3001\u7b7e\u540d\u3001\u9a8c\u8bc1\u65f6\u95f4\u3001\u5bc6\u94a5\u5927\u5c0f\u548c\u7b7e\u540d\u5927\u5c0f\u7b49\u6027\u80fd\u6307\u6807\u3002", "result": "\u63d0\u4f9b\u4e86\u91cf\u5b50\u5b89\u5168\u7b7e\u540d\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u8be6\u7ec6\u6027\u80fd\u6bd4\u8f83\uff0c\u5305\u62ec\u5bf9CRYSTALS-Dilithium\u3001Falcon\u3001Hawk\u4ee5\u53caHAETAE\u7b49\u65b9\u6848\u7684\u5206\u6790\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u540e\u91cf\u5b50\u7b7e\u540d\u7b97\u6cd5\u5728\u533a\u5757\u94fe\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\u6570\u636e\u3002", "conclusion": "\u540e\u91cf\u5b50\u6570\u5b57\u7b7e\u540d\u5bf9\u4e8e\u533a\u5757\u94fe\u7684\u957f\u671f\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u4e8e\u683c\u7684\u7b7e\u540d\u65b9\u6848\u5728\u533a\u5757\u94fe\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8003\u8651\u6027\u80fd\u6743\u8861\u548c\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002"}}
{"id": "2601.17905", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17905", "abs": "https://arxiv.org/abs/2601.17905", "authors": ["Jack Foster", "Kirill Paramonov", "Mete Ozay", "Umberto Michieli"], "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning", "comment": null, "summary": "Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.", "AI": {"tldr": "Gen1S\uff1a\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5d4c\u5165\u7a7a\u95f4\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\u5e76\u5b66\u4e60\u6b8b\u5dee\u5206\u5e03\u6765\u63d0\u5347\u65b0\u7c7b\u8bc6\u522b\u80fd\u529b", "motivation": "\u89e3\u51b3\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6bcf\u4e2a\u65b0\u7c7b\u53ea\u6709\u4e00\u4e2a\u6837\u672c\u4e14\u4e0d\u5141\u8bb8\u540e\u7eed\u8bad\u7ec3\u6216\u6a21\u578b\u4fee\u6539\u7684\u4e25\u683c\u6761\u4ef6\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8bc6\u522b\u65b0\u7c7b", "method": "\u63d0\u51faGen1S\u65b9\u6cd5\uff1a1\uff09\u5c06\u539f\u59cb\u5d4c\u5165\u7a7a\u95f4\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\uff08\u51cf\u53bb\u7c7b\u522b\u539f\u578b\uff09\uff1b2\uff09\u4f7f\u7528VAE\u6216\u6269\u6563\u6a21\u578b\u5b66\u4e60\u57fa\u7c7b\u6b8b\u5dee\u7684\u591a\u6a21\u6001\u5206\u5e03\uff1b3\uff09\u5229\u7528\u8be5\u7ed3\u6784\u5148\u9a8c\u63d0\u5347\u65b0\u7c7b\u8bc6\u522b", "result": "Gen1S\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u67b6\u6784\u4e0a\u4e00\u81f4\u5730\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u7c7b\u8bc6\u522b\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5c06\u5d4c\u5165\u7a7a\u95f4\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4e25\u683c\u6761\u4ef6\u4e0b\u7684\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u4e3aFSCIL\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "\u6210\u529f\u6761\u4ef6\u5316\uff08success conditioning\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b56\u7565\u6539\u8fdb\u6280\u672f\uff0c\u901a\u8fc7\u6536\u96c6\u8f68\u8ff9\u3001\u8bc6\u522b\u6210\u529f\u8f68\u8ff9\u5e76\u6a21\u4eff\u5176\u884c\u52a8\u6765\u66f4\u65b0\u7b56\u7565\u3002\u672c\u6587\u8bc1\u660e\u8be5\u65b9\u6cd5\u7cbe\u786e\u89e3\u51b3\u4e86\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u5728\u81ea\u52a8\u786e\u5b9a\u7684\u03c7\u00b2\u6563\u5ea6\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u7b56\u7565\u6539\u8fdb\u3002", "motivation": "\u6210\u529f\u6761\u4ef6\u5316\u6280\u672f\u5728\u4e0d\u540c\u9886\u57df\u6709\u591a\u79cd\u540d\u79f0\uff08\u62d2\u7edd\u91c7\u6837+SFT\u3001\u76ee\u6807\u6761\u4ef6RL\u3001\u51b3\u7b56\u53d8\u6362\u5668\uff09\uff0c\u4f46\u5176\u89e3\u51b3\u7684\u4f18\u5316\u95ee\u9898\u672c\u8d28\u4e00\u76f4\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8be5\u65b9\u6cd5\u7684\u6570\u5b66\u57fa\u7840\uff0c\u8bc1\u660e\u5176\u7cbe\u786e\u5bf9\u5e94\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6210\u529f\u6761\u4ef6\u5316\u7cbe\u786e\u89e3\u51b3\u4e86\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff1a\u5728\u03c7\u00b2\u6563\u5ea6\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u7b56\u7565\u6539\u8fdb\uff0c\u7ea6\u675f\u534a\u5f84\u7531\u6570\u636e\u81ea\u52a8\u786e\u5b9a\u3002\u5efa\u7acb\u4e86\u76f8\u5bf9\u7b56\u7565\u6539\u8fdb\u3001\u7b56\u7565\u53d8\u5316\u5e45\u5ea6\u548c\u884c\u52a8\u5f71\u54cd\uff08action-influence\uff09\u4e4b\u95f4\u7684\u6052\u7b49\u5f0f\u3002", "result": "\u6210\u529f\u6761\u4ef6\u5316\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u4fdd\u5b88\u6539\u8fdb\u7b97\u5b50\uff0c\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u6216\u5f15\u53d1\u5371\u9669\u7684\u5206\u5e03\u504f\u79fb\u3002\u5f53\u5931\u8d25\u65f6\uff0c\u5b83\u4f1a\u901a\u8fc7\u51e0\u4e4e\u4e0d\u6539\u53d8\u7b56\u7565\u6765\u53ef\u89c2\u5bdf\u5730\u5931\u8d25\u3002\u7406\u8bba\u5e94\u7528\u4e8e\u56de\u62a5\u9608\u503c\u5316\u5b9e\u8df5\uff0c\u663e\u793a\u5176\u80fd\u653e\u5927\u6539\u8fdb\u4f46\u53ef\u80fd\u504f\u79bb\u771f\u5b9e\u76ee\u6807\u3002", "conclusion": "\u6210\u529f\u6761\u4ef6\u5316\u6280\u672f\u5177\u6709\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\uff0c\u7cbe\u786e\u5bf9\u5e94\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u5b89\u5168\u53ef\u9760\u7684\u7b56\u7565\u6539\u8fdb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u5931\u8d25\u65f6\u5177\u6709\u53ef\u89c2\u5bdf\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2601.18252", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18252", "abs": "https://arxiv.org/abs/2601.18252", "authors": ["Chao Wang", "Xuanying Li", "Cheng Dai", "Jinglei Feng", "Yuxiang Luo", "Yuqi Ouyang", "Hao Qin"], "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing", "comment": null, "summary": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.", "AI": {"tldr": "Co-PLNet\uff1a\u4e00\u79cd\u70b9\u7ebf\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u7f16\u7801\u548c\u4ea4\u53c9\u5f15\u5bfc\u89e3\u7801\u5b9e\u73b0\u7ebf\u6846\u89e3\u6790\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7ebf\u6846\u89e3\u6790\u65b9\u6cd5\u5206\u522b\u9884\u6d4b\u7ebf\u6bb5\u548c\u8fde\u63a5\u70b9\uff0c\u7136\u540e\u8fdb\u884c\u540e\u5904\u7406\u534f\u8c03\uff0c\u5bfc\u81f4\u4e0d\u5339\u914d\u548c\u9c81\u68d2\u6027\u964d\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u534f\u540c\u6846\u67b6\u6765\u540c\u65f6\u5904\u7406\u70b9\u548c\u7ebf\uff0c\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faCo-PLNet\u70b9\u7ebf\u534f\u540c\u6846\u67b6\uff1a1\uff09\u70b9\u7ebf\u63d0\u793a\u7f16\u7801\u5668\uff08PLP-Encoder\uff09\u5c06\u65e9\u671f\u68c0\u6d4b\u8f6c\u6362\u4e3a\u7a7a\u95f4\u63d0\u793a\uff0c\u7f16\u7801\u51e0\u4f55\u5c5e\u6027\u4e3a\u7d27\u51d1\u7684\u7a7a\u95f4\u5bf9\u9f50\u56fe\uff1b2\uff09\u4ea4\u53c9\u5f15\u5bfc\u7ebf\u6bb5\u89e3\u7801\u5668\uff08CGL-Decoder\uff09\u4f7f\u7528\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u57fa\u4e8e\u4e92\u8865\u63d0\u793a\u7ec6\u5316\u9884\u6d4b\uff0c\u5f3a\u5236\u70b9\u7ebf\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728Wireframe\u548cYorkUrban\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u5b9e\u65f6\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7ed3\u6784\u5316\u51e0\u4f55\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Co-PLNet\u901a\u8fc7\u70b9\u7ebf\u534f\u540c\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u7ebf\u6846\u89e3\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u7684\u7ebf\u6bb5\u548c\u8fde\u63a5\u70b9\u68c0\u6d4b\uff0c\u4e3aSLAM\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7ed3\u6784\u5316\u51e0\u4f55\u8868\u793a\u3002"}}
{"id": "2601.17907", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17907", "abs": "https://arxiv.org/abs/2601.17907", "authors": ["Numan Halit Guldemir", "Oluwafemi Olukoya", "Jes\u00fas Mart\u00ednez-del-Rinc\u00f3n"], "title": "FARM: Few-shot Adaptive Malware Family Classification under Concept Drift", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.", "AI": {"tldr": "FARM\u6846\u67b6\u901a\u8fc7\u4e09\u5143\u7ec4\u81ea\u7f16\u7801\u5668\u3001DBSCAN\u805a\u7c7b\u548c\u539f\u578b\u5206\u7c7b\uff0c\u5b9e\u73b0\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u4e0e\u5c11\u6837\u672c\u81ea\u9002\u5e94\u5b66\u4e60", "motivation": "\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u6a21\u578b\u9762\u4e34\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5305\u62ec\u534f\u53d8\u91cf\u6f02\u79fb\u548c\u6807\u7b7e\u6f02\u79fb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u52a8\u6001\u5a01\u80c1\u73af\u5883\u4e2d\u6709\u6548\u68c0\u6d4b\u548c\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u76d1\u7763\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "1. \u4f7f\u7528\u4e09\u5143\u7ec4\u81ea\u7f16\u7801\u5668\u5c06\u6837\u672c\u6295\u5f71\u5230\u5224\u522b\u6027\u6f5c\u5728\u7a7a\u95f4\uff1b2. \u901a\u8fc7DBSCAN\u805a\u7c7b\u548c\u52a8\u6001\u9608\u503c\u8fdb\u884c\u65e0\u76d1\u7763\u6f02\u79fb\u68c0\u6d4b\uff1b3. \u91c7\u7528\u57fa\u4e8e\u539f\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u8fdb\u884c\u5feb\u901f\u9002\u5e94\uff1b4. \u5f53\u79ef\u7d2f\u8db3\u591f\u6f02\u79fb\u6837\u672c\u65f6\u652f\u6301\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u4ee5\u66f4\u65b0\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728BenchMFC\u6570\u636e\u96c6\u4e0a\uff0cFARM\u5728\u534f\u53d8\u91cf\u6f02\u79fb\u4e0b\u5c06\u5206\u7c7b\u6027\u80fd\u63d0\u53475.6%\uff0c\u4ec5\u4f7f\u7528\u5c11\u6837\u672c\u9002\u5e94\u5728\u672a\u89c1\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u4e0a\u8fbe\u5230\u5e73\u5747F1\u5206\u65700.85\uff0c\u91cd\u65b0\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u81f30.94\u3002", "conclusion": "FARM\u6846\u67b6\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5c55\u73b0\u51fa\u5bf9\u52a8\u6001\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u73af\u5883\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18626", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18626", "abs": "https://arxiv.org/abs/2601.18626", "authors": ["Yingxiao Huo", "Satya Prakash Dash", "Radu Stoican", "Samuel Kaski", "Mingfei Sun"], "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning", "comment": null, "summary": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79e9-1\u8fd1\u4f3c\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u9006Fisher\u4fe1\u606f\u77e9\u9635\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u7136\u68af\u5ea6\u8ba1\u7b97\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4f18\u4e8e\u6807\u51c6actor-critic\u548c\u4fe1\u4efb\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u7136\u68af\u5ea6\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u5feb\u901f\u6536\u655b\u7279\u6027\uff0c\u4f46\u8ba1\u7b97\u81ea\u7136\u68af\u5ea6\u9700\u8981\u6bcf\u6b21\u8fed\u4ee3\u90fd\u6c42\u9006Fisher\u4fe1\u606f\u77e9\u9635(FIM)\uff0c\u8fd9\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u79e9-1\u8fd1\u4f3c\u6765\u8fd1\u4f3c\u5b8c\u6574\u9006FIM\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u907f\u514d\u4e86\u76f4\u63a5\u8ba1\u7b97\u9006FIM\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u81ea\u7136\u68af\u5ea6\u7684\u4f18\u52bf\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u79e9-1\u8fd1\u4f3c\u9006FIM\u6bd4\u7b56\u7565\u68af\u5ea6\u6536\u655b\u66f4\u5feb\uff0c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5177\u6709\u4e0e\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u76f8\u540c\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6actor-critic\u548c\u4fe1\u4efb\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u79e9-1\u8fd1\u4f3c\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u81ea\u7136\u68af\u5ea6\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u7136\u68af\u5ea6\u8ba1\u7b97\uff0c\u5728\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.17909", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17909", "abs": "https://arxiv.org/abs/2601.17909", "authors": ["Adriana Watson"], "title": "From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy", "comment": "8 pages, 3 figures", "summary": "Differential privacy has become the gold standard for privacy-preserving machine learning systems. Unfortunately, subsequent work has primarily fixated on the privacy-utility tradeoff, leaving the subject of fairness constraints undervalued and under-researched. This paper provides a systematic treatment connecting three threads: (1) Dalenius's impossibility results for semantic privacy, (2) Dwork's differential privacy as an achievable alternative, and (3) emerging impossibility results from the addition of a fairness requirement. Through concrete examples and technical analysis, the three-way Pareto frontier between privacy, utility, and fairness is demonstrated to showcase the fundamental limits on what can be simultaneously achieved. In this work, these limits are characterized, the impact on minority groups is demonstrated, and practical guidance for navigating these tradeoffs are provided. This forms a unified framework synthesizing scattered results to help practitioners and policymakers make informed decisions when deploying private fair learning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u4e09\u65b9\u6743\u8861\uff0c\u63ed\u793a\u4e86\u540c\u65f6\u5b9e\u73b0\u8fd9\u4e09\u8005\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u90e8\u7f72\u9690\u79c1\u4fdd\u62a4\u516c\u5e73\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u5dee\u5206\u9690\u79c1\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u800c\u5ffd\u89c6\u4e86\u516c\u5e73\u6027\u7ea6\u675f\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8fde\u63a5Dalenius\u8bed\u4e49\u9690\u79c1\u4e0d\u53ef\u80fd\u6027\u3001Dwork\u5dee\u5206\u9690\u79c1\u53ef\u5b9e\u73b0\u6027\u4ee5\u53ca\u52a0\u5165\u516c\u5e73\u6027\u8981\u6c42\u540e\u7684\u65b0\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u4e3a\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5177\u4f53\u793a\u4f8b\u548c\u6280\u672f\u5206\u6790\uff0c\u5c55\u793a\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u4e09\u65b9\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u8868\u5f81\u8fd9\u4e9b\u57fa\u672c\u9650\u5236\uff0c\u5c55\u793a\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u5bfc\u822a\u8fd9\u4e9b\u6743\u8861\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "result": "\u63ed\u793a\u4e86\u9690\u79c1\u3001\u6548\u7528\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\u9650\u5236\uff0c\u8fd9\u4e9b\u9650\u5236\u65e0\u6cd5\u540c\u65f6\u88ab\u6ee1\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u5c11\u6570\u7fa4\u4f53\u6709\u663e\u8457\u5f71\u54cd\u3002\u6784\u5efa\u4e86\u7edf\u4e00\u6846\u67b6\u6765\u7efc\u5408\u5206\u6563\u7684\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u9690\u79c1\u4fdd\u62a4\u516c\u5e73\u5b66\u4e60\u7cfb\u7edf\u9762\u4e34\u6839\u672c\u6027\u7684\u4e09\u65b9\u6743\u8861\u9650\u5236\uff0c\u9700\u8981\u5b9e\u8df5\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5728\u90e8\u7f72\u65f6\u505a\u51fa\u660e\u667a\u6743\u8861\u51b3\u7b56\u3002\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u5bfc\u822a\u8fd9\u4e9b\u6743\u8861\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5bfc\u3002"}}
{"id": "2601.17911", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17911", "abs": "https://arxiv.org/abs/2601.17911", "authors": ["Thomas Heverin"], "title": "Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models", "comment": "15 pages, 3 figures, 1 table", "summary": "Prompt injection evaluations typically treat refusal as a stable, binary indicator of safety. This study challenges that paradigm by modeling refusal as a local decision boundary and examining its stability under structured perturbations. We evaluated two models, GPT-4.1 and GPT-4o, using 3,274 perturbation runs derived from refusal-inducing prompt injection attempts. Each base prompt was subjected to 25 perturbations across five structured families, with outcomes manually coded as Refusal, Partial Compliance, or Full Compliance.\n  Using chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric, we demonstrate that while both models refuse >94% of attempts, refusal instability is persistent and non-uniform. Approximately one-third of initial refusal-inducing prompts exhibited at least one \"refusal escape,\" a transition to compliance under perturbation. We find that artifact type is a stronger predictor of refusal failure than perturbation style. Textual artifacts, such as ransomware notes, exhibited significantly higher instability, with flip rates exceeding 20%. Conversely, executable malware artifacts showed zero refusal escapes in both models. While GPT-4o demonstrated tighter refusal enforcement and lower RBE than GPT-4.1, it did not eliminate artifact-dependent risks. These findings suggest that single-prompt evaluations systematically overestimate safety robustness. We conclude that refusal behavior is a probabilistic, artifact-dependent boundary phenomenon rather than a stable binary property, requiring a shift in how LLM safety is measured and audited.", "AI": {"tldr": "\u7814\u7a76\u6311\u6218\u4e86\u5c06\u62d2\u7edd\u89c6\u4e3a\u7a33\u5b9a\u4e8c\u5143\u5b89\u5168\u6307\u6807\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5efa\u6a21\u62d2\u7edd\u4e3a\u5c40\u90e8\u51b3\u7b56\u8fb9\u754c\u5e76\u68c0\u9a8c\u5176\u5728\u7ed3\u6784\u5316\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u62d2\u7edd\u884c\u4e3a\u662f\u6982\u7387\u6027\u3001\u4f9d\u8d56\u7279\u5b9a\u5185\u5bb9\u7684\u8fb9\u754c\u73b0\u8c61\u800c\u975e\u7a33\u5b9a\u4e8c\u5143\u5c5e\u6027\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u6ce8\u5165\u8bc4\u4f30\u901a\u5e38\u5c06\u62d2\u7edd\u89c6\u4e3a\u7a33\u5b9a\u3001\u4e8c\u5143\u7684\u5b89\u5168\u6307\u6807\uff0c\u4f46\u672c\u7814\u7a76\u8d28\u7591\u8fd9\u4e00\u8303\u5f0f\uff0c\u8ba4\u4e3a\u62d2\u7edd\u884c\u4e3a\u53ef\u80fd\u662f\u4e0d\u7a33\u5b9a\u7684\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5176\u4f5c\u4e3a\u51b3\u7b56\u8fb9\u754c\u7684\u7279\u6027\u3002", "method": "\u8bc4\u4f30GPT-4.1\u548cGPT-4o\u4e24\u4e2a\u6a21\u578b\uff0c\u4f7f\u75283,274\u6b21\u6270\u52a8\u8fd0\u884c\uff08\u6e90\u81ea\u62d2\u7edd\u8bf1\u5bfc\u7684\u63d0\u793a\u6ce8\u5165\u5c1d\u8bd5\uff09\u3002\u6bcf\u4e2a\u57fa\u7840\u63d0\u793a\u53d7\u523025\u6b21\u8de8\u4e94\u4e2a\u7ed3\u6784\u5316\u5bb6\u65cf\u7684\u6270\u52a8\uff0c\u7ed3\u679c\u624b\u52a8\u7f16\u7801\u4e3a\u62d2\u7edd\u3001\u90e8\u5206\u5408\u89c4\u6216\u5b8c\u5168\u5408\u89c4\u3002\u4f7f\u7528\u5361\u65b9\u68c0\u9a8c\u3001\u903b\u8f91\u56de\u5f52\u3001\u6df7\u5408\u6548\u5e94\u6a21\u578b\u548c\u65b0\u578b\u62d2\u7edd\u8fb9\u754c\u71b5(RBE)\u6307\u6807\u8fdb\u884c\u5206\u6790\u3002", "result": "\u867d\u7136\u4e24\u4e2a\u6a21\u578b\u62d2\u7edd\u7387\u5747\u8d85\u8fc794%\uff0c\u4f46\u62d2\u7edd\u4e0d\u7a33\u5b9a\u6027\u6301\u7eed\u5b58\u5728\u4e14\u4e0d\u5747\u5300\u3002\u7ea6\u4e09\u5206\u4e4b\u4e00\u521d\u59cb\u62d2\u7edd\u8bf1\u5bfc\u63d0\u793a\u8868\u73b0\u51fa\u81f3\u5c11\u4e00\u6b21\"\u62d2\u7edd\u9003\u9038\"\uff08\u6270\u52a8\u4e0b\u8f6c\u5411\u5408\u89c4\uff09\u3002\u6587\u672c\u7c7b\u5185\u5bb9\uff08\u5982\u52d2\u7d22\u8f6f\u4ef6\u8bf4\u660e\uff09\u4e0d\u7a33\u5b9a\u6027\u663e\u8457\u66f4\u9ad8\uff0c\u7ffb\u8f6c\u7387\u8d85\u8fc720%\uff1b\u800c\u53ef\u6267\u884c\u6076\u610f\u8f6f\u4ef6\u5185\u5bb9\u5728\u4e24\u4e2a\u6a21\u578b\u4e2d\u5747\u663e\u793a\u96f6\u62d2\u7edd\u9003\u9038\u3002GPT-4o\u8868\u73b0\u51fa\u66f4\u4e25\u683c\u7684\u62d2\u7edd\u6267\u884c\u548c\u66f4\u4f4e\u7684RBE\uff0c\u4f46\u672a\u6d88\u9664\u5185\u5bb9\u4f9d\u8d56\u98ce\u9669\u3002", "conclusion": "\u62d2\u7edd\u884c\u4e3a\u662f\u6982\u7387\u6027\u3001\u4f9d\u8d56\u7279\u5b9a\u5185\u5bb9\u7684\u8fb9\u754c\u73b0\u8c61\u800c\u975e\u7a33\u5b9a\u4e8c\u5143\u5c5e\u6027\uff0c\u5355\u63d0\u793a\u8bc4\u4f30\u7cfb\u7edf\u6027\u9ad8\u4f30\u5b89\u5168\u9c81\u68d2\u6027\uff0c\u9700\u8981\u6539\u53d8LLM\u5b89\u5168\u6d4b\u91cf\u548c\u5ba1\u8ba1\u65b9\u5f0f\u3002"}}
{"id": "2601.17967", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17967", "abs": "https://arxiv.org/abs/2601.17967", "authors": ["Alon Hillel-Tuch"], "title": "Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer", "comment": "3 pages, extended abstract", "summary": "Data at the physical layer transmits via media such as copper cable, fiber optic, or wireless. Physical attack vectors exist that challenge data confidentiality and availability. Protocols and encryption standards help obfuscate but often cannot keep the data type and destination secure, with limited insight into confidentiality and integrity. We will investigate the feasibility of developing an awareness and integrity protocol to help mitigate physical side-channel attacks that lead to eavesdropping of data communication and denial-of-service.\n  Keywords: data confidentiality, siphoning, eavesdropping, person-in-the-middle, denial-of-service, physical layer attacks, nation-states", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u7269\u7406\u5c42\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\u4ee5\u7f13\u89e3\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u5bfc\u81f4\u6570\u636e\u7a83\u542c\u548c\u62d2\u7edd\u670d\u52a1", "motivation": "\u7269\u7406\u5c42\u4f20\u8f93\u4ecb\u8d28\uff08\u94dc\u7f06\u3001\u5149\u7ea4\u3001\u65e0\u7ebf\uff09\u5b58\u5728\u653b\u51fb\u5411\u91cf\uff0c\u5a01\u80c1\u6570\u636e\u673a\u5bc6\u6027\u548c\u53ef\u7528\u6027\u3002\u73b0\u6709\u534f\u8bae\u548c\u52a0\u5bc6\u6807\u51c6\u96be\u4ee5\u4fdd\u62a4\u6570\u636e\u7c7b\u578b\u548c\u76ee\u7684\u5730\u5b89\u5168\uff0c\u5bf9\u673a\u5bc6\u6027\u548c\u5b8c\u6574\u6027\u7684\u6d1e\u5bdf\u6709\u9650\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e00\u79cd\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\uff0c\u4e13\u95e8\u9488\u5bf9\u7269\u7406\u4fa7\u4fe1\u9053\u653b\u51fb\u8fdb\u884c\u7f13\u89e3\uff0c\u5305\u62ec\u7a83\u542c\u548c\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u3002", "result": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8be5\u534f\u8bae\u7684\u53ef\u884c\u6027\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u6216\u6027\u80fd\u6570\u636e\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u7269\u7406\u5c42\u611f\u77e5\u4e0e\u5b8c\u6574\u6027\u534f\u8bae\u6765\u5e94\u5bf9\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u4ee5\u589e\u5f3a\u6570\u636e\u901a\u4fe1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.17039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17039", "abs": "https://arxiv.org/abs/2601.17039", "authors": ["Junhyuk Heo", "Beomkyu Choi", "Hyunjin Shin", "Darongsae Kwon"], "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation", "comment": null, "summary": "Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.", "AI": {"tldr": "\u63d0\u51faMANGO\u6570\u636e\u96c6\uff1a\u5305\u542b42,703\u4e2a\u6807\u6ce8\u56fe\u50cf-\u63a9\u7801\u5bf9\u7684\u5168\u7403\u7ea2\u6811\u6797\u6570\u636e\u96c6\uff0c\u8986\u76d6124\u4e2a\u56fd\u5bb6\uff0c\u57fa\u4e8e2020\u5e74Sentinel-2\u5f71\u50cf\u6784\u5efa\uff0c\u91c7\u7528\u76ee\u6807\u68c0\u6d4b\u9a71\u52a8\u65b9\u6cd5\u9009\u62e9\u6700\u4f73\u5355\u65e5\u671f\u89c2\u6d4b", "motivation": "\u73b0\u6709\u7ea2\u6811\u6797\u76d1\u6d4b\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4ec5\u63d0\u4f9b\u5e74\u5ea6\u5730\u56fe\u4ea7\u54c1\u800c\u975e\u5355\u65e5\u671f\u56fe\u50cf-\u63a9\u7801\u5bf9\u3001\u5c40\u9650\u4e8e\u7279\u5b9a\u533a\u57df\u800c\u975e\u5168\u7403\u8986\u76d6\u3001\u6216\u672a\u516c\u5f00\u53ef\u7528\uff0c\u8fd9\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7ea2\u6811\u6797\u68c0\u6d4b\u4e2d\u7684\u8fdb\u5c55", "method": "\u68c0\u7d222020\u5e74\u7ea2\u6811\u6797\u533a\u57df\u6240\u6709\u53ef\u7528Sentinel-2\u5f71\u50cf\uff0c\u91c7\u7528\u76ee\u6807\u68c0\u6d4b\u9a71\u52a8\u65b9\u6cd5\u9009\u62e9\u4e0e\u5e74\u5ea6\u7ea2\u6811\u6797\u63a9\u7801\u5bf9\u9f50\u7684\u6700\u4f73\u5355\u65e5\u671f\u89c2\u6d4b\uff0c\u5229\u7528\u50cf\u7d20\u7ea7\u5750\u6807\u53c2\u8003\u786e\u4fdd\u81ea\u9002\u5e94\u548c\u4ee3\u8868\u6027\u7684\u56fe\u50cf-\u63a9\u7801\u914d\u5bf9", "result": "\u6784\u5efa\u4e86MANGO\u6570\u636e\u96c6\uff0c\u5305\u542b42,703\u4e2a\u6807\u6ce8\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u8986\u76d6124\u4e2a\u56fd\u5bb6\uff0c\u5e76\u5728\u591a\u79cd\u8bed\u4e49\u5206\u5272\u67b6\u6784\u4e0b\u5efa\u7acb\u4e86\u57fa\u4e8e\u56fd\u5bb6\u4e0d\u76f8\u4ea4\u5212\u5206\u7684\u57fa\u51c6", "conclusion": "MANGO\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u73b0\u6709\u7ea2\u6811\u6797\u76d1\u6d4b\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u5168\u7403\u7ea2\u6811\u6797\u76d1\u6d4b\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7ea2\u6811\u6797\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528"}}
{"id": "2601.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17040", "abs": "https://arxiv.org/abs/2601.17040", "authors": ["H Neji", "J Nogueras-Iso", "J Lacasta", "M\u00c1 Latre", "FJ Garc\u00eda-Marco"], "title": "FP-THD: Full page transcription of historical documents", "comment": "Figure 1: FP-THD architecture Overview: Layout Analysis and Masked Auto-encoder with Vision Trans- former", "summary": "The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u8f6c\u5f5515-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u5e03\u5c40\u5206\u6790\u6a21\u578b\u63d0\u53d6\u6587\u672c\u884c\uff0c\u518d\u4f7f\u7528OCR\u6a21\u578b\u8fdb\u884c\u8bc6\u522b\uff0c\u4fdd\u7559\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\u4ee5\u7ef4\u6301\u5386\u53f2\u6587\u672c\u7684\u539f\u59cb\u98ce\u683c\u548c\u610f\u4e49\u3002", "motivation": "15-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u7684\u8f6c\u5f55\u9762\u4e34\u7279\u6b8a\u6311\u6218\uff0c\u9700\u8981\u4fdd\u7559\u5177\u6709\u7279\u5b9a\u542b\u4e49\u7684\u5b57\u7b26\u548c\u7279\u6b8a\u7b26\u53f7\uff0c\u4ee5\u786e\u4fdd\u5386\u53f2\u6587\u672c\u4fdd\u6301\u5176\u539f\u59cb\u98ce\u683c\u548c\u91cd\u8981\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e9b\u7279\u6b8a\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6d41\u7a0b\uff0c\u6269\u5c55\u73b0\u6709\u7684\u6587\u672c\u884c\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e03\u5c40\u5206\u6790\u6a21\u578b\u3002\u9996\u5148\u4f7f\u7528\u5e03\u5c40\u5206\u6790\u6a21\u578b\u5206\u6790\u5386\u53f2\u6587\u672c\u56fe\u50cf\u4ee5\u63d0\u53d6\u6587\u672c\u884c\uff0c\u7136\u540e\u901a\u8fc7OCR\u6a21\u578b\u5904\u7406\u8fd9\u4e9b\u6587\u672c\u884c\uff0c\u751f\u6210\u5b8c\u5168\u6570\u5b57\u5316\u7684\u9875\u9762\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u6709\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\u3002", "result": "\u8be5\u6d41\u7a0b\u4fc3\u8fdb\u4e86\u9875\u9762\u5904\u7406\u5e76\u4ea7\u751f\u4e86\u9ad8\u6548\u7ed3\u679c\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\uff0c\u5305\u62ec\u624b\u5199\u4f53\u3001\u5370\u5237\u4f53\u548c\u591a\u8bed\u8a00\u6587\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u7a0b\u6210\u529f\u89e3\u51b3\u4e86\u5386\u53f2\u6587\u732e\u8f6c\u5f55\u4e2d\u4fdd\u7559\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u5e03\u5c40\u5206\u6790\u548cOCR\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u740615-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u6863\uff0c\u4fdd\u6301\u5176\u539f\u59cb\u98ce\u683c\u548c\u610f\u4e49\u3002"}}
{"id": "2601.15801", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.15801", "abs": "https://arxiv.org/abs/2601.15801", "authors": ["Fengheng Chu", "Jiahao Chen", "Yuhong Wang", "Jun Wang", "Zhihui Fu", "Shouling Ji", "Songze Li"], "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.", "AI": {"tldr": "GOSV\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u8bc6\u522bLLM\u4e2d\u7684\u5b89\u5168\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\u4e24\u79cd\u7a7a\u95f4\u5206\u79bb\u7684\u5b89\u5168\u5411\u91cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684\u767d\u76d2\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u3001\u8d2a\u5a6a\u7684\u5c5e\u6027\u5f52\u56e0\uff0c\u5047\u8bbe\u7ec4\u4ef6\u8d21\u732e\u72ec\u7acb\uff0c\u5ffd\u89c6\u4e86LLM\u4e2d\u4e0d\u540c\u7ec4\u4ef6\uff08\u5982\u6ce8\u610f\u529b\u5934\uff09\u4e4b\u95f4\u7684\u534f\u540c\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u6784\u6210\u5b89\u5168\u673a\u5236\u3002\u8fd9\u5bfc\u81f4\u5bf9\u5b89\u5168\u7ec4\u4ef6\u7406\u89e3\u6709\u9650\uff0c\u5b89\u5168\u62a4\u680f\u8106\u5f31\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\u3002", "method": "\u63d0\u51faGOSV\uff08\u5168\u5c40\u4f18\u5316\u5b89\u5168\u5411\u91cf\u63d0\u53d6\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u540c\u65f6\u8003\u8651\u6240\u6709\u6ce8\u610f\u529b\u5934\u6765\u8bc6\u522b\u5b89\u5168\u5173\u952e\u6ce8\u610f\u529b\u5934\u3002\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u6fc0\u6d3b\u91cd\u8865\u4e01\u7b56\u7565\uff1a\u6709\u5bb3\u8865\u4e01\u548c\u96f6\u6d88\u878d\u3002\u8fd9\u4e9b\u7b56\u7565\u8bc6\u522b\u51fa\u7a7a\u95f4\u4e0a\u4e0d\u540c\u7684\u4e24\u7ec4\u5b89\u5168\u5411\u91cf\uff1a\u6076\u610f\u6ce8\u5165\u5411\u91cf\u548c\u5b89\u5168\u6291\u5236\u5411\u91cf\u3002", "result": "\u53d1\u73b0\u5bf9\u9f50\u7684LLM\u4e3a\u5b89\u5168\u76ee\u7684\u7ef4\u6301\u7740\u5206\u79bb\u7684\u529f\u80fd\u901a\u8def\u3002\u7cfb\u7edf\u5206\u6790\u8868\u660e\uff0c\u5f53\u7ea630%\u7684\u603b\u5934\u88ab\u91cd\u8865\u4e01\u65f6\uff0c\u6240\u6709\u6a21\u578b\u90fd\u4f1a\u51fa\u73b0\u5b8c\u5168\u7684\u5b89\u5168\u5d29\u6e83\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u767d\u76d2\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u91cd\u8865\u4e01\u5229\u7528\u8bc6\u522b\u7684\u5b89\u5168\u5411\u91cf\u3002\u8be5\u653b\u51fb\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u767d\u76d2\u653b\u51fb\u3002", "conclusion": "GOSV\u6846\u67b6\u5728LLM\u5b89\u5168\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u8bc1\u636e\uff0c\u901a\u8fc7\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u63ed\u793a\u4e86LLM\u5b89\u5168\u673a\u5236\u4e2d\u7ec4\u4ef6\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u66f4\u6709\u6548\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u52a0\u6df1\u4e86\u5bf9LLM\u5b89\u5168\u7ec4\u4ef6\u7684\u7406\u89e3\u3002"}}
{"id": "2601.17042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17042", "abs": "https://arxiv.org/abs/2601.17042", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Bin Yan"], "title": "Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective", "comment": "8 pages with 6 figures", "summary": "Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between \"membership matrix\" and \"subspace matrix U\" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the \"membership matrix\" and \"subspaces U\" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u6210\u5458\u77e9\u9635\u4e0e\u5b50\u7a7a\u95f4\u77e9\u9635\u7684MCR2\u9a71\u52a8\u767d\u76d2Transformer\uff0c\u901a\u8fc7\u68af\u5ea6\u5c55\u5f00\u5f97\u5230\u53ef\u89e3\u91ca\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50DMSA\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7387\u548c\u6027\u80fd", "motivation": "\u73b0\u6709MCR2\u9a71\u52a8\u7684\u767d\u76d2Transformer\u8bbe\u8ba1\u4e2d\uff0c\u6210\u5458\u77e9\u9635\u4e0e\u5b50\u7a7a\u95f4\u77e9\u9635U\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u5728\u9519\u8bef\u7684token\u6295\u5f71\u4e0b\u4ea7\u751f\u5197\u4f59\u7f16\u7801\uff0c\u9700\u8981\u89e3\u8026\u8fd9\u79cd\u529f\u80fd\u5173\u7cfb\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027", "method": "\u89e3\u8026MCR2\u76ee\u6807\u4e2d\u7684\u6210\u5458\u77e9\u9635\u4e0e\u5b50\u7a7a\u95f4U\u7684\u529f\u80fd\u5173\u7cfb\uff0c\u76f4\u63a5\u4ece\u8f93\u5165\u5b66\u4e60\u6210\u5458\u77e9\u9635\uff0c\u7136\u540e\u4ece\u5168\u7a7a\u95f4S\u63a8\u5bfc\u7a00\u758f\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u7684\u68af\u5ea6\u5c55\u5f00\u5f97\u5230\u53ef\u89e3\u91ca\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50DMSA", "result": "\u5728Token Statistics Transformer\u4e2d\u66ff\u6362\u6ce8\u610f\u529b\u6a21\u5757\u4e3aDMSA\uff08\u79f0\u4e3aDMST\uff09\uff0c\u5728ImageNet-1K\u4e0a\u6bd4ToST\u63d0\u53471.08%-1.45%\u7684top-1\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u7f16\u7801\u964d\u7ef4\u7387\uff0c\u76f8\u6bd4\u4f20\u7edfTransformer\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u89e3\u8026\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86MCR2\u4e2d\u7684\u5197\u4f59\u7f16\u7801\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u5c55\u5f00\u5f97\u5230\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50DMSA\u5728\u4fdd\u6301\u767d\u76d2\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5efa\u6a21\u7684\u6027\u80fd\u548c\u6548\u7387"}}
{"id": "2601.17360", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17360", "abs": "https://arxiv.org/abs/2601.17360", "authors": ["Jiankai Jin", "Xiangzheng Zhang", "Zhao Liu", "Deyue Zhang", "Quanchen Zou"], "title": "Robust Privacy: Inference-Time Privacy through Certified Robustness", "comment": null, "summary": "Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\u03c3=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u6982\u5ff5\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u7684\u4e0d\u53d8\u6027\u6765\u63d0\u4f9b\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a\uff08APE\uff09\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u53ef\u80fd\u4ea7\u751f\u4e2a\u6027\u5316\u7684\u8f93\u51fa\uff0c\u4f7f\u653b\u51fb\u8005\u80fd\u591f\u5728\u63a8\u7406\u65f6\u63a8\u65ad\u654f\u611f\u8f93\u5165\u5c5e\u6027\u3002\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u4fdd\u62a4\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u9690\u79c1\u6982\u5ff5\u6765\u9632\u6b62\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u7684\u654f\u611f\u5c5e\u6027\u63a8\u65ad\u3002", "method": "\u5f15\u5165\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u6982\u5ff5\uff0c\u7c7b\u4f3c\u4e8e\u8ba4\u8bc1\u9c81\u68d2\u6027\uff1a\u5982\u679c\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165x\u7684\u534a\u5f84R\u90bb\u57df\u5185\uff08\u5982\u2113\u2082\u8303\u6570\u4e0b\uff09\u5177\u6709\u53ef\u8bc1\u660e\u7684\u4e0d\u53d8\u6027\uff0c\u5219x\u4eab\u6709R-\u9c81\u68d2\u9690\u79c1\u3002\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a\uff08APE\uff09\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\u3002\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u5b9e\u73b0\u9884\u6d4b\u4e0d\u53d8\u6027\uff0c\u5e76\u5728\u53d7\u63a7\u63a8\u8350\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5728\u4f9d\u8d56\u654f\u611f\u5c5e\u6027\u7684\u63a8\u8350\u4efb\u52a1\u4e2d\uff0cRP\u6269\u5c55\u4e86\u4e0e\u6b63\u63a8\u8350\u517c\u5bb9\u7684\u654f\u611f\u5c5e\u6027\u503c\u96c6\u5408\uff0c\u6269\u5927\u4e86\u63a8\u65ad\u533a\u95f4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5c0f\u566a\u58f0\u6c34\u5e73\uff08\u03c3=0.1\uff09\u4e0b\uff0cRP\u5c06\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\uff08MIA\uff09\u7684\u6210\u529f\u7387\u4ece73%\u964d\u81f34%\uff0c\u540c\u65f6\u90e8\u5206\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002RP\u4e5f\u53ef\u5728\u4e0d\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u90e8\u5206\u7f13\u89e3MIA\uff08\u5982\u653b\u51fb\u6210\u529f\u7387\u964d\u81f344%\uff09\u3002", "conclusion": "\u9c81\u68d2\u9690\u79c1\uff08RP\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u7684\u4e0d\u53d8\u6027\u6765\u9632\u6b62\u654f\u611f\u5c5e\u6027\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u53ef\u63a5\u53d7\u6c34\u5e73\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17088", "abs": "https://arxiv.org/abs/2601.17088", "authors": ["Rui-Yang Ju", "Jen-Shiun Chiang"], "title": "GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars", "comment": "IEEE VR 2026 Poster", "summary": "Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.", "code_url": "https://ruiyangju.github.io/GlassesGB", "AI": {"tldr": "GlassesGB\uff1a\u4e00\u4e2a\u5c062D\u751f\u6210\u5f0f\u773c\u955c\u5b9a\u5236\u4e0e3D\u5934\u90e8\u5316\u8eab\u6e32\u67d3\u76f8\u7ed3\u5408\u7684\u53ef\u5b9a\u5236\u773c\u955c\u751f\u6210\u6846\u67b6", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u6234\u7cfb\u7edf\u5927\u591a\u57fa\u4e8e\u9884\u5b9a\u4e49\u6a21\u677f\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7528\u6237\u9a71\u52a8\u5b9a\u5236\u3002\u867d\u7136GlassesGAN\u652f\u6301\u4e2a\u6027\u53162D\u773c\u955c\u8bbe\u8ba1\uff0c\u4f46\u4ec5\u9650\u4e8e2D\u56fe\u50cf\u751f\u6210\u3002\u9700\u8981\u5c062D\u751f\u6210\u5f0f\u5b9a\u5236\u80fd\u529b\u6269\u5c55\u52303D\u5934\u90e8\u5316\u8eab\uff0c\u4ee5\u652f\u6301VR\u5e94\u7528\u4e2d\u7684\u4e2a\u6027\u5316\u773c\u955c\u8bbe\u8ba1\u3002", "method": "\u6574\u54083D\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\uff08\u7528\u4e8e\u5934\u90e8\u91cd\u5efa\uff09\u4e0e2D\u751f\u6210\u6280\u672f\uff0c\u63d0\u51faGlassesGB\u6846\u67b6\u3002\u8be5\u6846\u67b6\u6709\u6548\u6865\u63a52D\u751f\u6210\u5f0f\u5b9a\u5236\u4e0e3D\u5934\u90e8\u5316\u8eab\u6e32\u67d3\uff0c\u652f\u63013D\u5934\u90e8\u5316\u8eab\u7684\u53ef\u5b9a\u5236\u773c\u955c\u751f\u6210\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u652f\u63013D\u5934\u90e8\u5316\u8eab\u53ef\u5b9a\u5236\u773c\u955c\u751f\u6210\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86VR\u5e94\u7528\u4e2d\u4e2a\u6027\u5316\u773c\u955c\u8bbe\u8ba1\u7684\u6311\u6218\u3002\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "conclusion": "GlassesGB\u901a\u8fc7\u6574\u54083D\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u548c2D\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4ece2D\u751f\u6210\u5f0f\u5b9a\u5236\u52303D\u5934\u90e8\u5316\u8eab\u6e32\u67d3\u7684\u6709\u6548\u6865\u63a5\uff0c\u4e3aVR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u5b9a\u5236\u7684\u773c\u955c\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17063", "abs": "https://arxiv.org/abs/2601.17063", "authors": ["Byeongju Kim", "Jungwan Lee", "Donghyeon Han", "Hoi-Jun Yoo", "Sangyeob Kim"], "title": "FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices", "comment": null, "summary": "Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.", "AI": {"tldr": "FlashMoE\uff1a\u4e00\u4e2a\u5c06\u975e\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6709\u9650RAM\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684MoE\u63a8\u7406\uff0c\u901a\u8fc7ML\u7f13\u5b58\u7b56\u7565\u63d0\u5347\u4e13\u5bb6\u91cd\u7528\u7387\uff0c\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad82.6\u500d\u52a0\u901f\u3002", "motivation": "MoE\u6a21\u578b\u867d\u7136\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u4f46\u73b0\u6709DRAM\u5378\u8f7d\u65b9\u6848\uff08\u5982Fiddler\u3001DAOP\uff09\u4e0d\u9002\u5408\u5185\u5b58\u53d7\u9650\u7684\u7aef\u4fa7\u8bbe\u5907\u73af\u5883\u3002\u968f\u7740MoE\u6a21\u578b\u589e\u957f\u5230\u6570\u767eGB\uff0cRAM\u5378\u8f7d\u65b9\u6848\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlashMoE\u7cfb\u7edf\uff0c\u5c06\u975e\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\uff0c\u652f\u6301\u6709\u9650RAM\u4e0b\u7684\u9ad8\u6548MoE\u63a8\u7406\u3002\u7cfb\u7edf\u5305\u542b\u8f7b\u91cf\u7ea7ML\u7f13\u5b58\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u7ed3\u5408\u6700\u8fd1\u4f7f\u7528\u548c\u9891\u7387\u4fe1\u53f7\u4ee5\u6700\u5927\u5316\u4e13\u5bb6\u91cd\u7528\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8I/O\u3002\u540c\u65f6\u6784\u5efa\u7528\u6237\u7ea7\u684c\u9762\u5e73\u53f0\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u5728\u771f\u5b9e\u786c\u4ef6\u8bbe\u7f6e\u4e0a\uff0cFlashMoE\u76f8\u6bd4LRU\u548cLFU\u7b49\u5e38\u89c1\u5378\u8f7d\u7b56\u7565\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u5347\u6700\u9ad8\u8fbe51%\u3002\u76f8\u6bd4\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0\u6700\u9ad82.6\u500d\u52a0\u901f\u3002", "conclusion": "FlashMoE\u901a\u8fc7SSD\u5378\u8f7d\u548c\u667a\u80fd\u7f13\u5b58\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5927\u6a21\u578bMoE\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6311\u6218\uff0c\u4e3a\u7aef\u4fa7\u8bbe\u5907\u90e8\u7f72\u5927\u578bMoE\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "\u5229\u7528\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u8bb0\u5f55\u5f00\u53d1\u751f\u6210\u5f0f\u6a21\u62df\u5668\uff0c\u53ef\u57fa\u4e8e\u60a3\u8005\u5386\u53f2\u751f\u6210\u9ad8\u4fdd\u771f\u672a\u6765\u4e34\u5e8a\u8f68\u8ff9\uff0c\u5728\u8d85\u8fc72\u4ebf\u6761\u8bb0\u5f55\u4e0a\u9884\u8bad\u7ec3\uff0c\u51c6\u786e\u9884\u6d4b\u4e8b\u4ef6\u53d1\u751f\u7387\u548c\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u6a21\u62df\u5728\u4e34\u5e8a\u533b\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u590d\u6742\u7684\u751f\u7269\u548c\u793e\u4f1a\u6587\u5316\u5f71\u54cd\uff0c\u6a21\u62df\u60a3\u8005\u8f68\u8ff9\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u8bb0\u5f55\u7ecf\u9a8c\u6027\u5730\u5efa\u6a21\u60a3\u8005\u65f6\u95f4\u7ebf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u751f\u6210\u5f0f\u6a21\u62df\u5668\u6a21\u578b\uff0c\u4ee5\u60a3\u8005\u5386\u53f2\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u7ec6\u7c92\u5ea6\u3001\u771f\u5b9e\u7684\u672a\u6765\u8f68\u8ff9\u3002\u6a21\u578b\u5728\u8d85\u8fc72\u4ebf\u6761\u4e34\u5e8a\u8bb0\u5f55\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u672a\u6765\u65f6\u95f4\u7ebf\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u65f6\u95f4\u7ebf\u4e0e\u771f\u5b9e\u60a3\u8005\u672a\u6765\u6570\u636e\u9ad8\u5ea6\u5339\u914d\uff0c\u5305\u62ec\u4e8b\u4ef6\u53d1\u751f\u7387\u3001\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u7ed3\u679c\u548c\u65f6\u95f4\u52a8\u6001\u3002\u51c6\u786e\u4f30\u8ba1\u672a\u6765\u4e8b\u4ef6\u6982\u7387\uff0c\u89c2\u5bdf\u503c\u4e0e\u9884\u671f\u503c\u6bd4\u7387\u5728\u4e0d\u540c\u7ed3\u679c\u548c\u65f6\u95f4\u8303\u56f4\u5185\u5747\u63a5\u8fd11.0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u672a\u5f00\u53d1\u4ef7\u503c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4e34\u5e8a\u62a4\u7406\u8ba1\u7b97\u673a\u6a21\u62df\u6846\u67b6\uff0c\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc6\u522b\u51fa\u5e2e\u52a9\u3001\u9971\u548c\u548c\u5d29\u6e83\u4e09\u79cd\u673a\u5236\uff0c\u5e76\u63a8\u5bfc\u51fa\u9884\u7b97\u534f\u540c\u7684\u7cbe\u786e\u6761\u4ef6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7406\u8bba\u4e0a\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u5448\u73b0\u5e2e\u52a9\u3001\u9971\u548c\u751a\u81f3\u5d29\u6e83\u4e09\u79cd\u4e0d\u540c\u673a\u5236\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u80fd\u591f\u7edf\u4e00\u89e3\u91ca\u8fd9\u4e9b\u73b0\u8c61\u5e76\u6307\u5bfc\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u4e14\u53ef\u6821\u51c6\u7684\u7406\u8bba\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ea6\u675f\uff1a\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u6709\u635f\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3001\u76f8\u4f3c\u667a\u80fd\u4f53\u95f4\u7684\u5171\u4eab\u6545\u969c\u3002\u901a\u8fc7\u5b9a\u4e49\u667a\u80fd\u4f53\u8ba1\u7b97\u6027\u80fd\u7f29\u653e\u6307\u6570\u03b2\u3001\u6d88\u606f\u957f\u5ea6\u4fdd\u771f\u5ea6\u66f2\u7ebf\u03b3(m)\u3001\u6709\u6548\u5171\u4eab\u8bef\u5dee\u76f8\u5173\u6027\u03c1\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3W\u7b49\u53c2\u6570\uff0c\u5206\u6790\u6df1\u5ea6b\u53c9\u6811\u7ed3\u6784\u4e2d\u7684\u591a\u6570\u805a\u5408\u4efb\u52a1\u3002", "result": "\u8bc1\u660e\u4e86\u6df1\u5ea6b\u53c9\u6811\u5728\u76f8\u5173\u8f93\u5165\u548c\u6709\u635f\u901a\u4fe1\u4e0b\u5b58\u5728\u5c16\u9510\u7684\u76f8\u53d8\uff1a\u5355\u4e2a\u6807\u91cf\u03b1\u03c1\uff08\u7ed3\u5408\u03b3(m)\u3001\u03c1\u548c\u6247\u5165b\uff09\u51b3\u5b9a\u5f31\u4fe1\u53f7\u662f\u88ab\u653e\u5927\u5230\u975e\u5e73\u51e1\u56fa\u5b9a\u70b9\u8fd8\u662f\u88ab\u6d17\u724c\u5230\u968f\u673a\u6c34\u5e73\u3002\u5728\u653e\u5927\u673a\u5236\u4e2d\uff0c\u63a8\u5bfc\u51fa\u7ec4\u7ec7\u6307\u6570s\uff0c\u5e76\u8bc1\u660e\u5f53s>\u03b2\u65f6\u51fa\u73b0\u9884\u7b97\u534f\u540c\uff08\u5373\u7cfb\u7edf\u5728\u76f8\u540c\u603b\u9884\u7b97\u4e0b\u4f18\u4e8e\u6700\u4f73\u5355\u4e2a\u667a\u80fd\u4f53\uff09\uff0c\u5f97\u5230\u4e86\u5c01\u95ed\u5f62\u5f0f\u7684\u8ba1\u7b97\u5206\u914d\u89c4\u5219\u548c\u660e\u786e\u7684\u9884\u7b97\u9608\u503c\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u6df7\u5408\u6df1\u5ea6\u8868\u5f81\u9971\u548c\u73b0\u8c61\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u589e\u957f\u548c\u9971\u548c\u9636\u6bb5\u90fd\u4fdd\u6301\u51c6\u786e\u7684\u4fdd\u5b88\u88c1\u526a\u9884\u6d4b\u5668\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u6210\u529f\u9884\u6d4b\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u89e3\u91ca\u4e86\u8fd1\u671f\u5927\u89c4\u6a21\u5339\u914d\u9884\u7b97\u7814\u7a76\u4e2d\u89c2\u5bdf\u5230\u7684LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u7f29\u653e\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6307\u5bfc\u539f\u5219\u548c\u53ef\u8ba1\u7b97\u7684\u6743\u8861\u3002"}}
{"id": "2601.17470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17470", "abs": "https://arxiv.org/abs/2601.17470", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Yu-Jou Hsiao", "Jing-Hui Jung", "Yu-Lun Liu", "Chih-Chung Hsu"], "title": "PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors", "comment": "Project Page: https://ming053l.github.io/PhaSR", "summary": "Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.", "code_url": "https://github.com/ming053l/PhaSR", "code_stars": 0, "code_last_update": "2026-01-27", "AI": {"tldr": "PhaSR\u901a\u8fc7\u7269\u7406\u5bf9\u9f50\u7684\u53cc\u7ea7\u5148\u9a8c\u89e3\u51b3\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9634\u5f71\u53bb\u9664\u95ee\u9898\uff0c\u5305\u62ec\u7269\u7406\u5bf9\u9f50\u5f52\u4e00\u5316\u548c\u51e0\u4f55\u8bed\u4e49\u77eb\u6b63\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5355\u5149\u6e90\u5230\u591a\u6e90\u73af\u5883\u5149\u7167\u4e0b\u5747\u8868\u73b0\u9c81\u68d2\u3002", "motivation": "\u5728\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u9634\u5f71\u53bb\u9664\u9700\u8981\u5c06\u5149\u7167\u4e0e\u5185\u5728\u53cd\u5c04\u7387\u89e3\u8026\uff0c\u5f53\u7269\u7406\u5148\u9a8c\u672a\u6b63\u786e\u5bf9\u9f50\u65f6\u8fd9\u4e00\u6311\u6218\u5c24\u4e3a\u7a81\u51fa\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u6e90\u73af\u5883\u5149\u7167\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPhaSR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7269\u7406\u5bf9\u9f50\u5f52\u4e00\u5316(PAN)\uff1a\u901a\u8fc7\u7070\u4e16\u754c\u5f52\u4e00\u5316\u3001\u5bf9\u6570\u57dfRetinex\u5206\u89e3\u548c\u52a8\u6001\u8303\u56f4\u91cd\u7ec4\u8fdb\u884c\u95ed\u5f0f\u5149\u7167\u6821\u6b63\uff0c\u6291\u5236\u8272\u504f\uff1b2) \u51e0\u4f55\u8bed\u4e49\u77eb\u6b63\u6ce8\u610f\u529b(GSRA)\uff1a\u5c06\u5dee\u5206\u6ce8\u610f\u529b\u6269\u5c55\u5230\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u534f\u8c03\u6df1\u5ea6\u5bfc\u51fa\u7684\u51e0\u4f55\u4fe1\u606f\u4e0eDINO-v2\u8bed\u4e49\u5d4c\u5165\uff0c\u89e3\u51b3\u4e0d\u540c\u5149\u7167\u4e0b\u7684\u6a21\u6001\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u73af\u5883\u5149\u7167\u573a\u666f\uff0c\u5728\u591a\u6e90\u5149\u7167\u4e0b\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u65f6\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "PhaSR\u901a\u8fc7\u7269\u7406\u5bf9\u9f50\u7684\u53cc\u7ea7\u5148\u9a8c\u5b9e\u73b0\u4e86\u4ece\u5355\u5149\u6e90\u9634\u5f71\u5230\u591a\u6e90\u73af\u5883\u5149\u7167\u7684\u9c81\u68d2\u9634\u5f71\u53bb\u9664\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u51b2\u7a81\u548c\u8272\u504f\u95ee\u9898\uff0c\u4e3a\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u9634\u5f71\u53bb\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17260", "abs": "https://arxiv.org/abs/2601.17260", "authors": ["Marco Pollanen"], "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment", "comment": "10 Pages, 5 Figures", "summary": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $\u03b2$) yields progressively \"better\" behavior. We instead treat $\u03b2$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $\u03b2\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $\u03b2$ induces capability losses that persist even after $\u03b2$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $\u03b2$ landscape rather than reliance on margins or aggregate benchmarks.", "AI": {"tldr": "DPO\u7684\u03b2\u53c2\u6570\u4e0d\u5e94\u88ab\u89c6\u4e3a\u7b80\u5355\u7684\u5bf9\u9f50\u538b\u529b\u63a7\u5236\uff0c\u800c\u662f\u9700\u8981\u5bc6\u96c6\u626b\u63cf\u7684\u63a7\u5236\u53c2\u6570\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u5bf9\u03b2\u53d8\u5316\u5448\u73b0\u4e0d\u540c\u54cd\u5e94\u6a21\u5f0f\uff0c\u504f\u597d\u8fb9\u754c\u53ef\u80fd\u4e0e\u63a8\u7406\u80fd\u529b\u8d1f\u76f8\u5173\uff0c\u4e14\u5b58\u5728\u8bad\u7ec3\u8def\u5f84\u4f9d\u8d56\u7684\u6ede\u540e\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u5c06DPO\u4e2d\u7684\u03b2\u53c2\u6570\u89c6\u4e3a\u5bf9\u9f50\u538b\u529b\u7684\u7b80\u5355\u63a7\u5236\uff0c\u8ba4\u4e3a\u589e\u52a0\u03b2\u4f1a\u6301\u7eed\u6539\u5584\u6a21\u578b\u884c\u4e3a\u3002\u672c\u7814\u7a76\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u03b2\u53c2\u6570\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u7684\u54cd\u5e94\u5dee\u5f02\uff0c\u5e76\u8bc4\u4f30\u504f\u597d\u8fb9\u754c\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u5bf9\u4e09\u4e2a7B\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\uff08Mistral\u3001Llama\u3001Qwen\uff09\u5728\u56fa\u5b9aDPO\u914d\u65b9\u4e0b\u5bc6\u96c6\u626b\u63cf\u03b2\u53c2\u6570\u3002\u4f7f\u7528\u903b\u8f91\u63a2\u9488\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\uff0c\u5206\u6790\u80fd\u529b\u4e0e\u504f\u597d\u8fb9\u754c\u7684\u5173\u7cfb\uff08Pearson\u76f8\u5173\u7cfb\u6570\uff09\uff0c\u5e76\u7814\u7a76\u8bad\u7ec3\u8def\u5f84\u4f9d\u8d56\uff08\u6ede\u540e\u6548\u5e94\uff09\u3002", "result": "1. Mistral\u80fd\u529b\u5448\u5c16\u9510\u975e\u5355\u8c03\u6027\uff1a\u903b\u8f91\u63a2\u9488\u8fb9\u754c\u4ec5\u5728\u03b2\u224810\u207b\u00b2\u9644\u8fd1\u7a84\u5e26\u5185\u4e3a\u6b63\uff0c\u4e14\u8fb9\u754c\u70b9\u5bf9\u968f\u673a\u79cd\u5b50\u654f\u611f\u30022. \u4e0d\u540c\u67b6\u6784\u54cd\u5e94\u6a21\u5f0f\u4e0d\u540c\uff1aMistral\u5448\u73b0\u5c16\u9510\u91cd\u7ec4\uff0cLlama\u9009\u62e9\u6027\u53d8\u5316\uff0cQwen\u5e73\u6ed1\u6743\u8861\u30023. DPO\u504f\u597d\u8fb9\u754c\u4e0e\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u8d1f\u76f8\u5173\uff08Llama\u903b\u8f91\u4efb\u52a1r=-0.91\uff09\u30024. \u9ad8\u03b2\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u80fd\u529b\u635f\u5931\uff0c\u5373\u4f7f\u964d\u4f4e\u03b2\u4e5f\u65e0\u6cd5\u6062\u590d\uff08\u6ede\u540e\u6548\u5e94\uff09\u3002", "conclusion": "DPO\u7684\u03b2\u53c2\u6570\u9700\u8981\u5728\u6574\u4e2a\u53c2\u6570\u7a7a\u95f4\u8fdb\u884c\u80fd\u529b\u89e3\u6790\u8bc4\u4f30\uff0c\u800c\u975e\u4f9d\u8d56\u504f\u597d\u8fb9\u754c\u6216\u805a\u5408\u57fa\u51c6\u3002\u4e0d\u540c\u67b6\u6784\u5bf9\u03b2\u53d8\u5316\u5448\u73b0\u672c\u8d28\u4e0d\u540c\u7684\u54cd\u5e94\u6a21\u5f0f\uff0c\u504f\u597d\u8fb9\u754c\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u9009\u62e9\u3002\u8bad\u7ec3\u5386\u53f2\u5bf9\u6700\u7ec8\u80fd\u529b\u6709\u6301\u4e45\u5f71\u54cd\uff0c\u5f3a\u8c03\u9700\u8981\u7cfb\u7edf\u63a2\u7d22\u03b2\u666f\u89c2\u3002"}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "code_url": "https://github.com/timechess/TheoremForge", "code_stars": 4, "code_last_update": "2026-01-27", "AI": {"tldr": "TheoremForge\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u5f62\u5f0f\u6570\u5b66\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u89e3\u8026\u63d0\u53d6\u7b56\u7565\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u6062\u590d\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\uff0c\u663e\u8457\u964d\u4f4e\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u751f\u6210\u6210\u672c\u3002", "motivation": "\u5f62\u5f0f\u6570\u5b66\u4e2d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u9ad8\u6210\u672c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\uff0c\u52a0\u5267\u4e86\u5f00\u6e90\u8bed\u6599\u5e93\u7684\u7a00\u7f3a\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u5f62\u5f0f\u5316\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e94\u4e2a\u5b50\u4efb\u52a1\uff1a\u9648\u8ff0\u5f62\u5f0f\u5316\u3001\u8bc1\u660e\u751f\u6210\u3001\u524d\u63d0\u9009\u62e9\u3001\u8bc1\u660e\u4fee\u6b63\u548c\u8bc1\u660e\u8349\u56fe\uff1b\u91c7\u7528\u89e3\u8026\u63d0\u53d6\u7b56\u7565\u4ece\u5168\u5c40\u5931\u8d25\u8f68\u8ff9\u4e2d\u6062\u590d\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u57282000\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTheoremForge\u5b9e\u73b0\u4e8612.6%\u7684\u9a8c\u8bc1\u7387\uff08\u4f18\u4e8e8.6%\u7684\u57fa\u7ebf\uff09\uff0c\u6bcf\u4e2a\u6210\u529f\u8f68\u8ff9\u7684\u5e73\u5747\u6210\u672c\u4ec5\u4e3a0.481\u7f8e\u5143\uff1b\u89e3\u8026\u7b56\u7565\u4f7f\u8bc1\u660e\u751f\u6210\u7684\u6570\u636e\u4ea7\u91cf\u63d0\u9ad8\u4e861.6\u500d\u3002", "conclusion": "TheoremForge\u4e3a\u6784\u5efa\u6570\u636e\u98de\u8f6e\u4ee5\u8bad\u7ec3\u672a\u6765\u4e13\u5bb6\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f62\u5f0f\u6570\u5b66\u6570\u636e\u5408\u6210\u7684\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2601.17302", "categories": ["math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.17302", "abs": "https://arxiv.org/abs/2601.17302", "authors": ["John H. J. Einmahl", "Yi He"], "title": "Tighter confidence intervals for quantiles of heterogeneous data", "comment": null, "summary": "It is well known that the asymptotic variance of sample quantiles can be reduced under heterogeneity relative to the i.i.d. setting. However, asymptotically correct confidence intervals for quantiles are not yet available. We propose a novel, consistent estimator of the reduced asymptotic variance arising when quantiles are computed from groups of observations, leading to asymptotically correct confidence intervals. Simulation studies show that our confidence intervals are substantially shorter than those in the i.i.d. case and attain nearly correct coverage across a wide range of heterogeneous settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u5dee\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f02\u8d28\u6027\u6570\u636e\u4e2d\u6784\u5efa\u5206\u4f4d\u6570\u7684\u6e10\u8fd1\u6b63\u786e\u7f6e\u4fe1\u533a\u95f4\uff0c\u76f8\u6bd4i.i.d.\u60c5\u51b5\u80fd\u663e\u8457\u7f29\u77ed\u533a\u95f4\u957f\u5ea6", "motivation": "\u5728\u5f02\u8d28\u6027\u6570\u636e\u4e2d\uff0c\u6837\u672c\u5206\u4f4d\u6570\u7684\u6e10\u8fd1\u65b9\u5dee\u76f8\u6bd4i.i.d.\u8bbe\u7f6e\u53ef\u4ee5\u964d\u4f4e\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6e10\u8fd1\u6b63\u786e\u7684\u7f6e\u4fe1\u533a\u95f4\u6784\u5efa\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u4e14\u4e00\u81f4\u7684\u65b9\u5dee\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u4f30\u8ba1\u4ece\u89c2\u6d4b\u7ec4\u8ba1\u7b97\u5206\u4f4d\u6570\u65f6\u4ea7\u751f\u7684\u964d\u4f4e\u7684\u6e10\u8fd1\u65b9\u5dee\uff0c\u4ece\u800c\u6784\u5efa\u6e10\u8fd1\u6b63\u786e\u7684\u7f6e\u4fe1\u533a\u95f4", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7f6e\u4fe1\u533a\u95f4\u76f8\u6bd4i.i.d.\u60c5\u51b5\u663e\u8457\u7f29\u77ed\uff0c\u5e76\u5728\u5e7f\u6cdb\u7684\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u63a5\u8fd1\u6b63\u786e\u7684\u8986\u76d6\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u8d28\u6027\u6570\u636e\u4e2d\u5206\u4f4d\u6570\u7f6e\u4fe1\u533a\u95f4\u6784\u5efa\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u7edf\u8ba1\u63a8\u65ad\u5de5\u5177"}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "Lattice\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u673a\u5236\uff0c\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u521b\u5efa\u81ea\u9002\u5e94\u9632\u62a4\u680f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9759\u6001\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u9632\u62a4\u680f\u4f7f\u7528\u9759\u6001\u89c4\u5219\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u5a01\u80c1\u548c\u90e8\u7f72\u73af\u5883\u53d8\u5316\uff0c\u9700\u8981\u80fd\u591f\u81ea\u6211\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u7684\u81ea\u9002\u5e94\u9632\u62a4\u6846\u67b6", "method": "Lattice\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u6784\u5efa\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u548c\u4f18\u5316\u4ece\u6807\u6ce8\u793a\u4f8b\u521b\u5efa\u521d\u59cb\u9632\u62a4\u680f\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u6574\u5408\u6765\u81ea\u4e3b\u9002\u5e94\u5df2\u90e8\u7f72\u7684\u9632\u62a4\u680f", "result": "\u5728ProsocialDialog\u6570\u636e\u96c6\u4e0a\uff0cLattice\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fbe\u523091% F1\u5206\u6570\uff0c\u6bd4\u5173\u952e\u8bcd\u57fa\u7ebf\u9ad843\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4LlamaGuard\u9ad825\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4NeMo\u9ad84\u4e2a\u767e\u5206\u70b9\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u5728\u8de8\u57df\u6570\u636e\u4e0a\u5b9e\u73b07\u4e2a\u767e\u5206\u70b9F1\u63d0\u5347", "conclusion": "Lattice\u6846\u67b6\u8bc1\u660e\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53ef\u4ee5\u81ea\u6784\u5efa\u6709\u6548\u7684\u9632\u62a4\u680f\uff0c\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u3001\u53ef\u6301\u7eed\u6539\u8fdb\u7684\u9632\u62a4\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17309", "abs": "https://arxiv.org/abs/2601.17309", "authors": ["Anagha Sabu", "Vidhya S", "Narayanan C Krishnan"], "title": "PAR: Plausibility-aware Amortized Recourse Generation", "comment": null, "summary": "Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.", "AI": {"tldr": "PAR\uff1a\u57fa\u4e8e\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u63a8\u7406\u7684\u6982\u7387\u6027\u7b97\u6cd5\u8865\u6551\u65b9\u6cd5\uff0c\u901a\u8fc7\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u9ad8\u6548\u751f\u6210\u9ad8\u4f3c\u7136\u5ea6\u7684\u53ef\u884c\u8865\u6551\u65b9\u6848", "motivation": "\u73b0\u6709\u7b97\u6cd5\u8865\u6551\u65b9\u6cd5\u5728\u751f\u6210\u73b0\u5b9e\u53ef\u884c\u4e14\u9ad8\u4f3c\u7136\u5ea6\u7684\u8865\u6551\u65b9\u6848\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u6709\u6548\u6027\u3001\u76f8\u4f3c\u6027\u3001\u7a00\u758f\u6027\u548c\u9ad8\u4f3c\u7136\u5ea6\u8981\u6c42\u7684\u7cfb\u7edf\u65b9\u6cd5", "method": "\u5c06\u8865\u6551\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u7a0b\u5e8fPAR\uff0c\u4f7f\u7528\u53ef\u7cbe\u786e\u8ba1\u7b97\u4f3c\u7136\u7684\u53ef\u5904\u7406\u6982\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u5927\u5316\u63a5\u53d7\u7c7b\u5206\u5e03\u4f3c\u7136\u3001\u6700\u5c0f\u5316\u62d2\u7edd\u7c7b\u5206\u5e03\u4f3c\u7136\u4ee5\u53ca\u7f16\u7801\u5176\u4ed6\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u90bb\u57df\u7684\u8c03\u8282\u673a\u5236\u5b9e\u73b0\u4e2a\u6027\u5316\u8865\u6551\u751f\u6210", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b97\u6cd5\u8865\u6551\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86PAR\u7684\u9ad8\u6548\u6027\uff0c\u751f\u6210\u7684\u8865\u6551\u65b9\u6848\u5728\u6709\u6548\u6027\u3001\u4e0e\u4e8b\u5b9e\u7684\u76f8\u4f3c\u6027\u3001\u7a00\u758f\u6027\u548c\u9ad8\u4f3c\u7136\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "PAR\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u548c\u644a\u9500\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u7684\u7b97\u6cd5\u8865\u6551\u65b9\u6848\uff0c\u4e3a\u7b97\u6cd5\u8865\u6551\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17657", "abs": "https://arxiv.org/abs/2601.17657", "authors": ["Taewan Cho", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation", "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip", "code_url": "https://github.com/taewan2002/space-clip", "code_stars": 0, "code_last_update": "2025-06-23", "AI": {"tldr": "SPACE-CLIP\u662f\u4e00\u79cd\u76f4\u63a5\u4ece\u51bb\u7ed3\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u53d6\u51e0\u4f55\u77e5\u8bc6\u7684\u53cc\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\uff0c\u65e0\u9700\u6587\u672c\u7f16\u7801\u5668\u548c\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd", "motivation": "CLIP\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u56fa\u6709\u5730\u96be\u4ee5\u611f\u77e5\u51e0\u4f55\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u67e5\u8be2CLIP\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u95f4\u63a5\u4e14\u4f4e\u6548\u3002\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u9501CLIP\u4e2d\u7684\u51e0\u4f55\u77e5\u8bc6\u3002", "method": "\u63d0\u51faSPACE-CLIP\u67b6\u6784\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u89e3\u7801\u5668\uff1a1) \u8bed\u4e49\u8def\u5f84\u89e3\u91ca\u9ad8\u7ea7\u7279\u5f81\uff0c\u4f7f\u7528\u7279\u5f81\u7ea7\u7ebf\u6027\u8c03\u5236(FiLM)\u6839\u636e\u5168\u5c40\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u8282\uff1b2) \u7ed3\u6784\u8def\u5f84\u4ece\u65e9\u671f\u5c42\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u3002\u4e24\u6761\u4e92\u8865\u8def\u5f84\u901a\u8fc7\u5206\u5c42\u878d\u5408\u5b9e\u73b0\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u7cbe\u786e\u51e0\u4f55\u7684\u9c81\u68d2\u5408\u6210\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSPACE-CLIP\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684CLIP\u57fa\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u53cc\u8def\u5f84\u534f\u540c\u878d\u5408\u5bf9\u6210\u529f\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "SPACE-CLIP\u4e3a\u91cd\u65b0\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u3001\u9ad8\u6548\u4e14\u67b6\u6784\u4f18\u96c5\u7684\u84dd\u56fe\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u6df1\u5ea6\u4f30\u8ba1\u5668\uff0c\u66f4\u662f\u4e0b\u4e00\u4ee3\u5177\u8eabAI\u7cfb\u7edf\uff08\u5982\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff09\u4e2d\u6613\u4e8e\u96c6\u6210\u7684\u7a7a\u95f4\u611f\u77e5\u6a21\u5757\u3002"}}
{"id": "2601.17329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17329", "abs": "https://arxiv.org/abs/2601.17329", "authors": ["Tiejin Chen", "Xiaoou Liu", "Vishnu Nandam", "Kuan-Ru Liou", "Hua Wei"], "title": "Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment", "comment": "Accetped to Findings of EACL", "summary": "Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \\emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \\emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.", "AI": {"tldr": "CFA\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\uff0c\u5c06\u7b54\u6848\u7ea7\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u504f\u597d\u5bf9\u9f50\uff0c\u63d0\u5347RLHF\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u53ea\u5173\u6ce8\u504f\u597d\u6743\u91cd\uff0c\u5ffd\u7565\u4e86\u88ab\u6bd4\u8f83\u7b54\u6848\u672c\u8eab\u7684\u53ef\u9760\u6027\uff0c\u800c\u4eba\u7c7b\u53cd\u9988\u6807\u7b7e\u901a\u5e38\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027", "method": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u6784\u5efa\u5177\u6709\u53ef\u63a7\u8986\u76d6\u7387\u7684\u9884\u6d4b\u96c6\u6765\u91cf\u5316\u7b54\u6848\u7ea7\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u53ef\u9760\u6027\u805a\u5408\u4e3aDPO\u548cPPO\u98ce\u683c\u8bad\u7ec3\u7684\u6743\u91cd", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFA\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u8bc1\u660e\u5efa\u6a21\u7b54\u6848\u4fa7\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u8865\u5145\u504f\u597d\u7ea7\u52a0\u6743", "conclusion": "CFA\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u7684\u7edf\u8ba1\u4fdd\u8bc1\u5c06\u7b54\u6848\u53ef\u9760\u6027\u7eb3\u5165\u504f\u597d\u52a0\u6743\uff0c\u4e3a\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18371", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.18371", "abs": "https://arxiv.org/abs/2601.18371", "authors": ["Chengxin Yan", "Dachuan Chen", "Jia Li"], "title": "Nonparametric inference for spot volatility in pure-jump semimartingales", "comment": null, "summary": "We provide a comprehensive analysis of spot volatility inference in pure-jump semimartingales under two asymptotic settings: fixed-$k$, where each local window uses a fixed number of observations, and large-$k$, where this number grows with sampling frequency. For both active- and possibly inactive-jump settings, we derive generally nonstandard, typically non-Gaussian limit distributions and establish valid inference, including when the jump-activity index is consistently estimated. Simulations show that fixed-$k$ asymptotics offer markedly better finite-sample accuracy, underscoring their practical advantage for nonparametric spot volatility inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u7eaf\u8df3\u8dc3\u534a\u9785\u4e2d\u73b0\u8d27\u6ce2\u52a8\u7387\u63a8\u65ad\u7684\u4e24\u79cd\u6e10\u8fd1\u6846\u67b6\uff1a\u56fa\u5b9ak\uff08\u6bcf\u4e2a\u5c40\u90e8\u7a97\u53e3\u4f7f\u7528\u56fa\u5b9a\u89c2\u6d4b\u6570\uff09\u548c\u5927k\uff08\u89c2\u6d4b\u6570\u968f\u91c7\u6837\u9891\u7387\u589e\u957f\uff09\uff0c\u53d1\u73b0\u5728\u4e24\u79cd\u8df3\u8dc3\u6d3b\u52a8\u6027\u8bbe\u7f6e\u4e0b\uff0c\u56fa\u5b9ak\u6e10\u8fd1\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u7cbe\u5ea6\u4e0a\u663e\u8457\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u7eaf\u8df3\u8dc3\u534a\u9785\u4e2d\u73b0\u8d27\u6ce2\u52a8\u7387\u63a8\u65ad\u7684\u6e10\u8fd1\u7406\u8bba\uff0c\u6bd4\u8f83\u56fa\u5b9ak\u548c\u5927k\u4e24\u79cd\u6e10\u8fd1\u6846\u67b6\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u6e10\u8fd1\u8bbe\u7f6e\uff1a\u56fa\u5b9ak\u6e10\u8fd1\uff08\u6bcf\u4e2a\u5c40\u90e8\u7a97\u53e3\u4f7f\u7528\u56fa\u5b9a\u6570\u91cf\u7684\u89c2\u6d4b\u503c\uff09\u548c\u5927k\u6e10\u8fd1\uff08\u5c40\u90e8\u7a97\u53e3\u89c2\u6d4b\u6570\u968f\u91c7\u6837\u9891\u7387\u589e\u957f\uff09\u3002\u5728\u6d3b\u8dc3\u8df3\u8dc3\u548c\u53ef\u80fd\u975e\u6d3b\u8dc3\u8df3\u8dc3\u4e24\u79cd\u8bbe\u7f6e\u4e0b\uff0c\u63a8\u5bfc\u975e\u6807\u51c6\u3001\u901a\u5e38\u975e\u9ad8\u65af\u7684\u6781\u9650\u5206\u5e03\uff0c\u5e76\u5efa\u7acb\u6709\u6548\u7684\u63a8\u65ad\u65b9\u6cd5\uff0c\u5305\u62ec\u5f53\u8df3\u8dc3\u6d3b\u52a8\u6307\u6570\u88ab\u4e00\u81f4\u4f30\u8ba1\u65f6\u7684\u60c5\u51b5\u3002", "result": "\u5bf9\u4e8e\u4e24\u79cd\u8df3\u8dc3\u6d3b\u52a8\u6027\u8bbe\u7f6e\uff0c\u90fd\u63a8\u5bfc\u51fa\u4e86\u901a\u5e38\u975e\u6807\u51c6\u3001\u975e\u9ad8\u65af\u7684\u6781\u9650\u5206\u5e03\u3002\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u56fa\u5b9ak\u6e10\u8fd1\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u5927k\u6e10\u8fd1\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u56fa\u5b9ak\u6e10\u8fd1\u6846\u67b6\u4e3a\u7eaf\u8df3\u8dc3\u534a\u9785\u4e2d\u7684\u975e\u53c2\u6570\u73b0\u8d27\u6ce2\u52a8\u7387\u63a8\u65ad\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u6709\u9650\u6837\u672c\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2601.17666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17666", "abs": "https://arxiv.org/abs/2601.17666", "authors": ["Xinyue Pan", "Yuhao Chen", "Fengqing Zhu"], "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting", "comment": "Accepted by CAI2026", "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.", "AI": {"tldr": "\u63d0\u51faPrompt Grafting\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5e03\u5c40\u5f15\u5bfc\u89e3\u51b3\u591a\u98df\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u98df\u7269\u5206\u79bb\u4e0e\u6df7\u5408", "motivation": "\u73b0\u5b9e\u4e16\u754c\u9910\u98df\u56fe\u50cf\u901a\u5e38\u5305\u542b\u591a\u79cd\u98df\u7269\uff0c\u4f46\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u591a\u98df\u7269\u751f\u6210\u65f6\u5b58\u5728\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff08\u5982\u7c73\u996d\u548c\u6c64\u878d\u5408\uff09\uff0c\u8fd9\u5f71\u54cd\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u81b3\u98df\u8bc4\u4f30\u548c\u98df\u8c31\u53ef\u89c6\u5316\u7b49\u5e94\u7528", "method": "\u63d0\u51faPrompt Grafting\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u4e2d\u7684\u663e\u5f0f\u7a7a\u95f4\u7ebf\u7d22\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u9690\u5f0f\u5e03\u5c40\u5f15\u5bfc\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u5e03\u5c40\u63d0\u793a\u5efa\u7acb\u4e0d\u540c\u533a\u57df\uff0c\u5f85\u5e03\u5c40\u7a33\u5b9a\u540e\u5c06\u76ee\u6807\u63d0\u793a\u5ac1\u63a5\u3002\u901a\u8fc7\u7f16\u8f91\u5e03\u5c40\u6392\u5217\u5b9e\u73b0\u98df\u7269\u7ea0\u7f20\u63a7\u5236", "result": "\u5728\u4e24\u4e2a\u98df\u7269\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u7269\u4f53\u7684\u5b58\u5728\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u63a7\u5206\u79bb\u7684\u5b9a\u6027\u8bc1\u636e", "conclusion": "Prompt Grafting\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u98df\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u98df\u7269\u5206\u79bb\u4e0e\u6df7\u5408\uff0c\u4e3a\u56fe\u50cf\u5f0f\u81b3\u98df\u8bc4\u4f30\u548c\u98df\u8c31\u53ef\u89c6\u5316\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591a\u98df\u7269\u6570\u636e\u589e\u5f3a\u65b9\u6848"}}
{"id": "2601.17376", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17376", "abs": "https://arxiv.org/abs/2601.17376", "authors": ["Ruijin Hua", "Zichuan Liu", "Kun Zhang", "Yiyuan Yang"], "title": "Diversified Scaling Inference in Time Series Foundation Models", "comment": "23 pages, 16 figures, 9 tables", "summary": "The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u6f5c\u529b\uff0c\u53d1\u73b0\u6807\u51c6\u91c7\u6837\u63a8\u7406\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\u6765\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f46\u63a8\u7406\u65f6\u8ba1\u7b97\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1aTSFMs\u5728\u6807\u51c6\u91c7\u6837\u63a8\u7406\u4e0b\u7684\u884c\u4e3a\u7279\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u63a7\u5236\u91c7\u6837\u591a\u6837\u6027\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u3002", "method": "\u9996\u5148\u5206\u6790TSFMs\u5728\u6807\u51c6\u91c7\u6837\u4e0b\u7684\u7279\u6027\uff0c\u53d1\u73b0\u5176\u56e0\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u800c\u96be\u4ee5\u9075\u5faa\u7f29\u653e\u5b9a\u5f8b\u3002\u7136\u540e\u901a\u8fc7\u5b9a\u5236\u7684\u65f6\u95f4\u5e8f\u5217\u6270\u52a8\u5b9e\u73b0\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\uff0c\u6269\u5c55\u751f\u6210\u5206\u5e03\u7684\u652f\u6491\u96c6\u3002\u7406\u8bba\u5206\u6790\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u63a8\u5bfc\u591a\u6837\u5316\u91c7\u6837\u4f18\u4e8e\u6807\u51c6\u91c7\u6837\u7684\u4e34\u754c\u6837\u672c\u9608\u503c\u3002", "result": "\u8de8\u591a\u79cdTSFMs\u548c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5f53\u7684\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\u80fd\u5728\u4e0d\u66f4\u65b0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u63d0\u51faRobustMSE\u6307\u6807\u6765\u91cf\u5316\u56fa\u5b9a\u9884\u7b97\u4e0bTSFMs\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u63a8\u7406\u8bbe\u8ba1\u662fTSFM\u4f18\u5316\u7684\u5173\u952e\u8ba1\u7b97\u6548\u7387\u7ef4\u5ea6\u3002\u7814\u7a76\u9610\u660e\u4e86\u8fd9\u4e9b\u56e0\u7d20\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4f7f\u5f97\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3TSFMs\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5e76\u884c\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u5927\u89c4\u6a21\u63a8\u7406\u65f6\u95f4\u5e8f\u5217\u5b9e\u73b0\u53ef\u9760\u6027\u80fd\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "EntWorld\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4f01\u4e1a\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,756\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u516d\u4e2a\u4f01\u4e1a\u9886\u57df\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6d88\u8d39\u7ea7\u573a\u666f\uff08\u5982\u7535\u5546\u3001\u65c5\u884c\u9884\u8ba2\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\u548c\u4e25\u8c28\u6027\u3002\u4f01\u4e1a\u7cfb\u7edf\u5177\u6709\u9ad8\u5bc6\u5ea6\u7528\u6237\u754c\u9762\u3001\u4e25\u683c\u4e1a\u52a1\u903b\u8f91\u7ea6\u675f\u548c\u7cbe\u786e\u72b6\u6001\u4e00\u81f4\u6027\u8981\u6c42\u7b49\u72ec\u7279\u6311\u6218\uff0c\u5f53\u524d\u901a\u7528\u667a\u80fd\u4f53\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u5f0f\u7684\u4efb\u52a1\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u5e95\u5c42\u6570\u636e\u5e93\u6a21\u5f0f\u9006\u5411\u5de5\u7a0b\u4e1a\u52a1\u903b\u8f91\uff0c\u5408\u6210\u771f\u5b9e\u7684\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u3002\u63d0\u51fa\u57fa\u4e8eSQL\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u4e25\u683c\u7684\u72b6\u6001\u8f6c\u6362\u9a8c\u8bc1\u66ff\u4ee3\u6a21\u7cca\u7684\u89c6\u89c9\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u5728EntWorld\u4e0a\u7684\u6210\u529f\u7387\u4ec5\u4e3a47.61%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u9886\u57df\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "EntWorld\u4f5c\u4e3a\u4e00\u4e2a\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u4e0b\u4e00\u4ee3\u4f01\u4e1a\u7ea7\u6570\u5b57\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.17740", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17740", "abs": "https://arxiv.org/abs/2601.17740", "authors": ["Cong Cao", "Ren Li", "Corentin Dumery", "Hao Li"], "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields", "comment": null, "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9690\u5f0f\u8868\u793a\u7684\u7f1d\u7eab\u56fe\u6848\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u8868\u793a\u9762\u677f\u8fb9\u754c\uff0c\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u6807\u8bc6\u7f1d\u7aef\u70b9\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u53ef\u5fae\u5206\u7f51\u683c\u5316\uff0c\u652f\u6301\u590d\u6742\u7ed3\u6784\u7684\u7f1d\u7eab\u56fe\u6848\u751f\u6210\u4e0e\u4f30\u8ba1\u3002", "motivation": "\u7f1d\u7eab\u56fe\u6848\u5b9a\u4e49\u4e86\u670d\u88c5\u7684\u7ed3\u6784\u57fa\u7840\uff0c\u5bf9\u65f6\u5c1a\u8bbe\u8ba1\u3001\u5236\u4f5c\u548c\u7269\u7406\u6a21\u62df\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u81ea\u52a8\u56fe\u6848\u751f\u6210\u6709\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u9762\u677f\u51e0\u4f55\u5f62\u72b6\u548c\u7f1d\u7ebf\u6392\u5217\u7684\u5e7f\u6cdb\u53d8\u5f02\u6027\uff0c\u51c6\u786e\u5efa\u6a21\u7f1d\u7eab\u56fe\u6848\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u57fa\u4e8e\u9690\u5f0f\u8868\u793a\u7684\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u8868\u793a\u9762\u677f\u8fb9\u754c\uff0c\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u6807\u8bc6\u7f1d\u7aef\u70b9\uff1b2) \u5c06\u8fd9\u4e9b\u573a\u7f16\u7801\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u53ef\u5fae\u5206\u7f51\u683c\u5316\uff1b3) \u6f5c\u5728\u6d41\u5339\u914d\u6a21\u578b\u5b66\u4e60\u9762\u677f\u7ec4\u5408\u5206\u5e03\uff1b4) \u7f1d\u7ebf\u9884\u6d4b\u6a21\u5757\u4ece\u63d0\u53d6\u7684\u8fb9\u7f18\u6bb5\u6062\u590d\u7f1d\u7ebf\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u548c\u751f\u6210\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u7f1d\u7eab\u56fe\u6848\u3002\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u4ece\u56fe\u50cf\u4f30\u8ba1\u7f1d\u7eab\u56fe\u6848\u7684\u51c6\u786e\u6027\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u652f\u6301\u56fe\u6848\u8865\u5168\u548c\u91cd\u65b0\u9002\u914d\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u5b57\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u901a\u8fc7\u9690\u5f0f\u8868\u793a\u548c\u6f5c\u5728\u6d41\u5339\u914d\u5b9e\u73b0\u4e86\u590d\u6742\u7f1d\u7eab\u56fe\u6848\u7684\u51c6\u786e\u5efa\u6a21\u4e0e\u751f\u6210\uff0c\u652f\u6301\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.17441", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17441", "abs": "https://arxiv.org/abs/2601.17441", "authors": ["Ondrej Bohdal", "Taha Ceritli", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Data-driven Clustering and Merging of Adapters for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.", "AI": {"tldr": "\u63d0\u51faD2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4efb\u52a1\u793a\u4f8b\u8fdb\u884c\u9002\u914d\u5668\u805a\u7c7b\uff0c\u5408\u5e76\u540c\u7c07\u9002\u914d\u5668\u521b\u5efa\u591a\u4efb\u52a1\u9002\u914d\u5668\uff0c\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u5b58\u50a8\u9650\u5236\u4e0b\u7684\u9002\u914d\u5668\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b58\u50a8\u6240\u6709\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u4e0d\u73b0\u5b9e\uff0c\u4f46\u8bbe\u5907\u6709\u8db3\u591f\u5bb9\u91cf\u5b58\u50a8\u6709\u9650\u6570\u91cf\u7684\u9002\u914d\u5668\u53c2\u6570\u3002\u73b0\u6709\u6587\u732e\u672a\u63a2\u7d22\u5982\u4f55\u9009\u62e9\u5177\u6709\u826f\u597d\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u7684\u4ee3\u8868\u6027\u9002\u914d\u5668\u3002", "method": "\u63d0\u51faD2C\u9002\u914d\u5668\u805a\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u793a\u4f8b\uff08\u5982\u6bcf\u4e2a\u4efb\u52a110\u4e2a\uff09\uff0c\u91c7\u7528\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u7ec6\u5316\u7c07\u5206\u914d\uff0c\u5408\u5e76\u6bcf\u4e2a\u7c07\u5185\u7684\u9002\u914d\u5668\u521b\u5efa\u591a\u4efb\u52a1\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8003\u8651\u5b58\u50a8\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "D2C\u65b9\u6cd5\u901a\u8fc7\u9002\u914d\u5668\u805a\u7c7b\u548c\u5408\u5e76\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u4efb\u52a1\u9002\u914d\u5668\u90e8\u7f72\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u9002\u914d\u5668\u9009\u62e9\u7684\u6311\u6218\u3002"}}
{"id": "2601.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17467", "abs": "https://arxiv.org/abs/2601.17467", "authors": ["Jianxiong Zhang", "Bing Guo", "Yuming Jiang", "Haobo Wang", "Bo An", "Xuefeng Du"], "title": "Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping", "comment": null, "summary": "Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.", "AI": {"tldr": "ARS\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u7b54\u6848\u4e00\u81f4\u6027\u8868\u5f81\u6765\u68c0\u6d4b\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5e7b\u89c9\uff0c\u901a\u8fc7\u6f5c\u5728\u5e72\u9884\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\u5e76\u5b66\u4e60\u8868\u5f81\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u4f46\u7b54\u6848\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u5f97\u5e7b\u89c9\u68c0\u6d4b\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u8f68\u8ff9\u6587\u672c\u6216\u539f\u59cb\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u68c0\u6d4b\u5b58\u5728\u8106\u5f31\u6027\uff1a\u8f68\u8ff9\u5f62\u5f0f\u591a\u53d8\uff0c\u68c0\u6d4b\u5668\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u8868\u9762\u6a21\u5f0f\u800c\u975e\u7b54\u6848\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u7b54\u6848\u4e00\u81f4\u6027\u8868\u5f81\u5851\u5f62\uff08ARS\uff09\u65b9\u6cd5\uff1a\u901a\u8fc7\u5c0f\u89c4\u6a21\u6f5c\u5728\u5e72\u9884\uff08\u6270\u52a8\u8f68\u8ff9\u8fb9\u754c\u5d4c\u5165\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u6839\u636e\u6270\u52a8\u540e\u7b54\u6848\u662f\u5426\u4e0e\u539f\u7b54\u6848\u4e00\u81f4\u8fdb\u884c\u6807\u6ce8\uff0c\u5b66\u4e60\u5c06\u7b54\u6848\u4e00\u81f4\u7684\u9690\u85cf\u72b6\u6001\u805a\u5408\u3001\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u72b6\u6001\u5206\u79bb\u7684\u8868\u5f81\uff0c\u4ece\u800c\u66b4\u9732\u6307\u793a\u5e7b\u89c9\u98ce\u9669\u7684\u6f5c\u5728\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eARS\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002\u5851\u5f62\u540e\u7684\u5d4c\u5165\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u5668\u5373\u63d2\u5373\u7528\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "ARS\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u7b54\u6848\u7a33\u5b9a\u6027\u6765\u5b66\u4e60\u68c0\u6d4b\u53cb\u597d\u7684\u8f68\u8ff9\u6761\u4ef6\u8868\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2601.17789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17789", "abs": "https://arxiv.org/abs/2601.17789", "authors": ["Yiming Su", "Kunzhao Xu", "Yanjie Gao", "Fan Yang", "Cheng Li", "Mao Yang", "Tianyin Xu"], "title": "Neuro-Symbolic Verification on Instruction Following of LLMs", "comment": null, "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.", "AI": {"tldr": "NSVIF\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u8f93\u51fa\u662f\u5426\u9075\u5faa\u6307\u4ee4\uff0c\u5c06\u6307\u4ee4\u9075\u5faa\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u53cd\u9988\u3002", "motivation": "LLM\u5e76\u4e0d\u603b\u662f\u9075\u5faa\u6307\u4ee4\uff0c\u4e14\u8fdd\u89c4\u884c\u4e3a\u96be\u4ee5\u89c2\u5bdf\u6216\u68c0\u67e5\u3002\u5728\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\uff0c\u6b64\u7c7b\u8fdd\u89c4\u4f1a\u6cbf\u63a8\u7406\u94fe\u4f20\u64ad\u653e\u5927\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u548c\u7cfb\u7edf\u4e8b\u6545\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u3001\u901a\u7528\u7684\u9a8c\u8bc1\u5668\u6765\u786e\u4fddLLM\u8f93\u51fa\u9075\u5faa\u6307\u4ee4\u3002", "method": "NSVIF\u5c06\u6307\u4ee4\u9075\u5faa\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u5c06\u7528\u6237\u6307\u4ee4\u5efa\u6a21\u4e3a\u7ea6\u675f\u3002\u6846\u67b6\u540c\u65f6\u5efa\u6a21\u903b\u8f91\u7ea6\u675f\u548c\u8bed\u4e49\u7ea6\u675f\uff0c\u901a\u8fc7\u7edf\u4e00\u6c42\u89e3\u5668\u534f\u8c03\u903b\u8f91\u63a8\u7406\u548c\u8bed\u4e49\u5206\u6790\u6765\u89e3\u51b3\u7ea6\u675f\u3002\u5f00\u53d1\u4e86VIFBENCH\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u9a8c\u8bc1\u5668\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNSVIF\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u3002NSVIF\u7684\u53cd\u9988\u8fd8\u80fd\u5728\u4e0d\u8fdb\u884c\u540e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5e2e\u52a9\u63d0\u9ad8LLM\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "NSVIF\u662f\u4e00\u4e2a\u6709\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u80fd\u591f\u53ef\u9760\u5730\u9a8c\u8bc1LLM\u8f93\u51fa\u662f\u5426\u9075\u5faa\u6307\u4ee4\uff0c\u4e3a\u89e3\u51b3LLM\u6307\u4ee4\u9075\u5faa\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u53cd\u9988\u5e2e\u52a9\u6539\u8fdbLLM\u6027\u80fd\u3002"}}
{"id": "2601.17495", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17495", "abs": "https://arxiv.org/abs/2601.17495", "authors": ["Ruiyu Zhang", "Lin Nie", "Wai-Fung Lam", "Qihao Wang", "Xin Zhao"], "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems", "comment": "15 pages, 1 figure", "summary": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u6765\u6539\u5584\u5d4c\u5165\u7a7a\u95f4\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\uff0c\u63d0\u5347\u76f8\u4f3c\u6027\u68c0\u7d22\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u5b9e\u90e8\u7f72\u7cfb\u7edf\u4e2d\uff0c\u57fa\u4e8e\u5d4c\u5165\u7684\u6700\u8fd1\u90bb\u68c0\u7d22\u7ecf\u5e38\u5931\u8d25\uff0c\u95ee\u9898\u4e0d\u5728\u4e8e\u8bed\u8a00\u6a21\u578b\u672c\u8eab\uff0c\u800c\u5728\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u6700\u8fd1\u90bb\u5bf9\u5e94\u9519\u8bef\u6848\u4f8b\u3002\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u9884\u8bad\u7ec3\u5d4c\u5165\uff0c\u4f46\u6807\u7b7e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\u4e14\u91cd\u65b0\u8bad\u7ec3\u7f16\u7801\u5668\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u4e0b\u6e38\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\u3002\u539f\u59cb\u5d4c\u5165\u901a\u5e38\u4e0e\u6700\u8fd1\u90bb\u68c0\u7d22\u6240\u9700\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u5bf9\u9f50\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPEARL\uff08\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff09\uff0c\u5229\u7528\u6709\u9650\u76d1\u7763\u901a\u8fc7\u8f6f\u5bf9\u9f50\u5c06\u5d4c\u5165\u5411\u7c7b\u539f\u578b\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u91cd\u5851\u5c40\u90e8\u90bb\u57df\u51e0\u4f55\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u7ef4\u5ea6\u4e0d\u53d8\uff0c\u907f\u514d\u6fc0\u8fdb\u6295\u5f71\u6216\u574d\u7f29\u3002\u65e8\u5728\u586b\u8865\u7eaf\u65e0\u76d1\u7763\u540e\u5904\u7406\uff08\u589e\u76ca\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\uff09\u548c\u5b8c\u5168\u76d1\u7763\u6295\u5f71\uff08\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff09\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "result": "\u5728\u4ece\u6781\u7aef\u6807\u7b7e\u7a00\u7f3a\u5230\u8f83\u9ad8\u6807\u7b7e\u8bbe\u7f6e\u7684\u63a7\u5236\u6807\u7b7e\u673a\u5236\u4e0b\u8bc4\u4f30PEARL\u3002\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0cPEARL\u663e\u8457\u6539\u5584\u5c40\u90e8\u90bb\u57df\u8d28\u91cf\uff0c\u76f8\u6bd4\u539f\u59cb\u5d4c\u5165\u83b7\u5f9725.7%\u7684\u63d0\u5347\uff0c\u76f8\u6bd4\u5f3a\u65e0\u76d1\u7763\u540e\u5904\u7406\u83b7\u5f97\u8d85\u8fc721.1%\u7684\u63d0\u5347\uff0c\u5728\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7cfb\u7edf\u6700\u8106\u5f31\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PEARL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u6539\u5584\u5d4c\u5165\u7a7a\u95f4\u7684\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u7684\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u548c\u5206\u7c7b\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.17915", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17915", "abs": "https://arxiv.org/abs/2601.17915", "authors": ["Saurabh Jha", "Rohan Arora", "Bhavya", "Noah Zheutlin", "Paulina Toro Isaza", "Laura Shwartz", "Yu Deng", "Daby Sow", "Ruchi Mahindru", "Ruchir Puri"], "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation", "comment": null, "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.\n  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.", "AI": {"tldr": "EoG\u6846\u67b6\u901a\u8fc7\u5c06\u8c03\u67e5\u4efb\u52a1\u5efa\u6a21\u4e3a\u4f9d\u8d56\u56fe\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\uff0c\u5206\u79bbLLM\u7684\u5c40\u90e8\u8bc1\u636e\u6316\u6398\u4e0e\u786e\u5b9a\u6027\u63a7\u5236\u5668\u7684\u56fe\u904d\u5386\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86ReAct\u667a\u80fd\u4f53\u5728\u5f00\u653e\u5f0f\u8c03\u67e5\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5904\u7406\u5f00\u653e\u5f0f\u8c03\u67e5\u4efb\u52a1\u65f6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff1a\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u5bfc\u81f4\u5173\u952e\u8bc1\u636e\u53ef\u80fd\u88ab\u8fc7\u65e9\u4e22\u5f03\uff1bReAct\u98ce\u683c\u7684\u667a\u80fd\u4f53\u5bf9\u63a2\u7d22\u987a\u5e8f\u654f\u611f\u4e14\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff1b\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u5ff5\u7c3f\u8bb0\u548c\u4fee\u6b63\u673a\u5236\uff1b\u8bed\u4e49\u63a8\u7406\u4e0e\u63a7\u5236\u804c\u8d23\u7ea0\u7f20\u5bfc\u81f4\u6267\u884c\u9519\u8bef\u5f71\u54cd\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u63d0\u51faEoG\uff08Explanations over Graphs\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u8c03\u67e5\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4f9d\u8d56\u56fe\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\uff1b2\uff09\u5206\u79bbLLM\u4e0e\u63a7\u5236\u5668\u7684\u804c\u8d23\u2014\u2014LLM\u8d1f\u8d23\u6709\u754c\u7684\u5c40\u90e8\u8bc1\u636e\u6316\u6398\u548c\u6807\u6ce8\uff08\u539f\u56e0vs\u75c7\u72b6\uff09\uff0c\u786e\u5b9a\u6027\u63a7\u5236\u5668\u7ba1\u7406\u56fe\u904d\u5386\u3001\u72b6\u6001\u7ef4\u62a4\u548c\u4fe1\u5ff5\u4f20\u64ad\uff1b3\uff09\u8ba1\u7b97\u6700\u5c0f\u89e3\u91ca\u8fb9\u754c\u3002", "result": "\u5728ITBench\u8bca\u65ad\u4efb\u52a1\u4e0a\uff0cEoG\u76f8\u6bd4ReAct\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u8fd0\u884c\u95f4\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5b9e\u4f53F1\u7684Majority-at-k\u6307\u6807\u5e73\u5747\u63d0\u5347\u4e867\u500d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f00\u653e\u5f0f\u8c03\u67e5\u4efb\u52a1\u5206\u89e3\u4e3a\u4f9d\u8d56\u56fe\u4e0a\u7684\u6eaf\u56e0\u63a8\u7406\uff0c\u5e76\u5206\u79bbLLM\u7684\u5c40\u90e8\u63a8\u7406\u4e0e\u63a7\u5236\u5668\u7684\u5168\u5c40\u7ba1\u7406\uff0cEoG\u6846\u67b6\u80fd\u591f\u66f4\u53ef\u9760\u3001\u4e00\u81f4\u5730\u5904\u7406\u9700\u8981\u4ece\u6d77\u91cf\u5f02\u6784\u6570\u636e\u4e2d\u8fed\u4ee3\u6316\u6398\u8bc1\u636e\u7684\u590d\u6742\u8c03\u67e5\u4efb\u52a1\u3002"}}
{"id": "2601.17512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17512", "abs": "https://arxiv.org/abs/2601.17512", "authors": ["Yiqun Zhang", "Shenghong Cai", "Zihua Yang", "Sen Feng", "Yuzhu Ji", "Haijun Zhang"], "title": "One-Shot Federated Clustering of Non-Independent Completely Distributed Data", "comment": "This work has been accepted for publication in IEEE Internet of Things Journal", "summary": "Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGOLD\u6846\u67b6\uff0c\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u4e2dNon-IID\u6570\u636e\u5bfc\u81f4\u7684\u96c6\u7fa4\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u63d0\u5347\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u805a\u7c7b\u5728\u5904\u7406\u8fb9\u7f18\u8bbe\u5907\u65e0\u6807\u7b7e\u6570\u636e\u65f6\u9762\u4e34Non-IID\u95ee\u9898\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u5c06\u540c\u4e00\u96c6\u7fa4\u788e\u7247\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u878d\u5408Non-IID\u5ba2\u6237\u7aef\u7684\u6a21\u5f0f\u77e5\u8bc6\u3001\u5ba2\u6237\u7aef\u95f4\u96c6\u7fa4\u5206\u5e03\u5173\u7cfb\u53ca\u5176\u4e0e\u5168\u5c40\u77e5\u8bc6\u878d\u5408\u7684\u5173\u8054\u3002", "method": "\u63d0\u51faGOLD\u6846\u67b6\uff1a1\uff09\u7cbe\u7ec6\u63a2\u7d22\u5ba2\u6237\u7aef\u6f5c\u5728\u7684\u4e0d\u5b8c\u6574\u5c40\u90e8\u96c6\u7fa4\u5206\u5e03\uff1b2\uff09\u5c06\u5206\u5e03\u6458\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8fdb\u884c\u5168\u5c40\u878d\u5408\uff1b3\uff09\u5728\u5168\u5c40\u5206\u5e03\u6307\u5bfc\u4e0b\u8fdb\u884c\u5c40\u90e8\u96c6\u7fa4\u589e\u5f3a\u3002\u5f15\u5165\u66f4\u5e7f\u4e49\u7684Non-ICD\u6982\u5ff5\u63cf\u8ff0\u96c6\u7fa4\u788e\u7247\u5316\u73b0\u8c61\u3002", "result": "\u901a\u8fc7\u663e\u8457\u6027\u68c0\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u53ef\u6269\u5c55\u6027\u8bc4\u4f30\u548c\u5b9a\u6027\u7ed3\u679c\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86GOLD\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u5176\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u4e2d\u7684Non-IID\u6311\u6218\u3002", "conclusion": "GOLD\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u805a\u7c7b\u4e2dNon-IID\u6570\u636e\u5bfc\u81f4\u7684\u96c6\u7fa4\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u95ee\u9898\uff0c\u5c06\u5176\u6846\u67b6\u5316\u4e3a\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u95ee\u9898\uff0c\u56de\u987e\u4e86\u4e3b\u8981\u65b9\u6cd5\u5bb6\u65cf\uff0c\u63d0\u51fa\u4e86\u80fd\u529b\u9a71\u52a8\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u7ecf\u9a8c\u6559\u8bad\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u4e3a\u5728\u6602\u8d35\u64cd\u4f5c\u3001\u566a\u58f0\u5ef6\u8fdf\u53cd\u9988\u3001\u4e25\u683c\u53ef\u884c\u6027\u548c\u5b89\u5168\u7ea6\u675f\u4ee5\u53ca\u975e\u5e73\u7a33\u6027\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u667a\u80fd\u4f53AI\u63d0\u4f9b\u4e86\u82db\u523b\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u5206\u6790SDL\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u95ee\u9898\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u6846\u67b6\u6765\u8fde\u63a5\u5e38\u89c1\u7684SDL\u6d41\u7a0b\u4e0e\u6210\u719f\u7684AI\u539f\u7406\u3002", "method": "\u5c06SDL\u81ea\u4e3b\u6027\u6846\u67b6\u5316\u4e3a\u5177\u6709\u660e\u786e\u89c2\u6d4b\u3001\u52a8\u4f5c\u3001\u6210\u672c\u548c\u7ea6\u675f\u7684\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u95ee\u9898\u3002\u56de\u987e\u4e86\u5b9e\u73b0\u95ed\u73af\u5b9e\u9a8c\u7684\u4e3b\u8981\u65b9\u6cd5\u5bb6\u65cf\uff1a\u7528\u4e8e\u6837\u672c\u9ad8\u6548\u5b9e\u9a8c\u9009\u62e9\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u4e3b\u52a8\u5b66\u4e60\u3001\u7528\u4e8e\u957f\u65f6\u7a0b\u534f\u8bae\u4f18\u5316\u7684\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u53ca\u7528\u4e8e\u534f\u8c03\u5f02\u6784\u4eea\u5668\u548c\u8f6f\u4ef6\u7684\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u51b3\u7b56\u65f6\u57df\u3001\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u52a8\u4f5c\u53c2\u6570\u5316\u3001\u7ea6\u675f\u5904\u7406\u3001\u6545\u969c\u6062\u590d\u548c\u4eba\u7c7b\u53c2\u4e0e\u7b49\u7ef4\u5ea6\u7684\u80fd\u529b\u9a71\u52a8\u5206\u7c7b\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u8fde\u63a5SDL\u6d41\u7a0b\u4e0eAI\u539f\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6cd5\u6765\u7ec4\u7ec7SDL\u7cfb\u7edf\u3002\u5408\u6210\u4e86\u57fa\u51c6\u4efb\u52a1\u6a21\u677f\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4f18\u5148\u8003\u8651\u6210\u672c\u611f\u77e5\u6027\u80fd\u3001\u5bf9\u6f02\u79fb\u7684\u9c81\u68d2\u6027\u3001\u7ea6\u675f\u8fdd\u53cd\u884c\u4e3a\u548c\u53ef\u91cd\u590d\u6027\u3002\u4ece\u5df2\u90e8\u7f72\u7684SDL\u4e2d\u63d0\u70bc\u4e86\u5b9e\u9645\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u4ee3\u8868\u4e86AI\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u73af\u5883\u4e2d\u7684\u91cd\u8981\u5e94\u7528\u9886\u57df\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u53ef\u9a8c\u8bc1\u548c\u6eaf\u6e90\u611f\u77e5\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u5f00\u653e\u6311\u6218\u5305\u62ec\u591a\u6a21\u6001\u8868\u793a\u3001\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u63a2\u7d22\u548c\u5171\u4eab\u57fa\u51c6\u57fa\u7840\u8bbe\u65bd\u7b49\u65b9\u9762\u3002"}}
{"id": "2601.17563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17563", "abs": "https://arxiv.org/abs/2601.17563", "authors": ["Nathan Gavenski", "Matteo Leonetti", "Odinaldo Rodrigues"], "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment", "comment": "The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)", "summary": "State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u76d1\u7763\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\uff08UfO\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\u89e3\u51b3\u73b0\u6709ILfO\u65b9\u6cd5\u9700\u8981\u52a8\u4f5c\u76d1\u7763\u3001\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\u3001\u4e0d\u8003\u8651\u73af\u5883\u72b6\u6001\u5b8c\u6574\u6027\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u9700\u8981\u57fa\u4e8e\u52a8\u4f5c\u7684\u76d1\u7763\u4f18\u5316\uff1b2\uff09\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\uff1b3\uff09\u503e\u5411\u4e8e\u76f4\u63a5\u5e94\u7528\u6559\u5e08\u52a8\u4f5c\u800c\u4e0d\u5145\u5206\u8003\u8651\u5b9e\u9645\u73af\u5883\u72b6\u6001\u3002\u867d\u7136\u771f\u5b9e\u4fe1\u606f\u5b58\u5728\u4e8e\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u4e2d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u6ca1\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u63d0\u51faUfO\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u89c2\u5bdf\u5230\u7684\u72b6\u6001\u8f6c\u79fb\u4e2d\u8fd1\u4f3c\u6559\u5e08\u7684\u771f\u5b9e\u52a8\u4f5c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8c03\u6574\u667a\u80fd\u4f53\u8f68\u8ff9\u4f7f\u5176\u4e0e\u6559\u5e08\u8f68\u8ff9\u7d27\u5bc6\u5bf9\u9f50\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0cUfO\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u6559\u5e08\u548c\u6240\u6709\u5176\u4ed6ILfO\u65b9\u6cd5\uff0c\u800c\u4e14\u663e\u793a\u51fa\u6700\u5c0f\u7684\u6807\u51c6\u5dee\u3002\u6807\u51c6\u5dee\u7684\u51cf\u5c11\u8868\u660e\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UfO\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u7684\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6559\u5e08\u548c\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.17868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17868", "abs": "https://arxiv.org/abs/2601.17868", "authors": ["Zhihao He", "Tieyuan Chen", "Kangyu Wang", "Ziran Qin", "Yang Shao", "Chaofan Gan", "Shijie Li", "Zuxuan Wu", "Weiyao Lin"], "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding", "comment": null, "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.", "code_url": "https://github.com/ziHoHe/VidLaDA", "code_stars": 1, "code_last_update": "2026-01-27", "AI": {"tldr": "VidLaDA\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7MARS-Cache\u6846\u67b6\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u9891token\u6269\u6563\u89e3\u7801\u7684\u63a8\u7406\u74f6\u9888\uff0c\u5b9e\u73b012\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\u3002", "motivation": "\u6807\u51c6\u81ea\u56de\u5f52\u89c6\u9891LLM\u5b58\u5728\u56e0\u679c\u63a9\u7801\u504f\u5dee\uff0c\u963b\u788d\u5168\u5c40\u65f6\u7a7a\u5efa\u6a21\uff0c\u5bfc\u81f4\u7406\u89e3\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u83b7\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\u5e76\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u9891token\u63a8\u7406\u74f6\u9888\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVidLaDA\uff0c\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5229\u7528\u53cc\u5411\u6ce8\u610f\u529b\u6355\u83b7\u53cc\u5411\u4f9d\u8d56\u3002\u5f15\u5165MARS-Cache\u6846\u67b6\uff0c\u7ed3\u5408\u5f02\u6b65\u89c6\u89c9\u7f13\u5b58\u5237\u65b0\u548c\u5e27\u7ea7\u5206\u5757\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u951a\u70b9token\u4fdd\u6301\u5168\u5c40\u8fde\u63a5\u6027\uff0c\u6709\u6548\u526a\u679d\u5197\u4f59\u3002", "result": "VidLaDA\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u6269\u6563\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982Qwen2.5-VL\u548cLLaVA-Video\uff09\u76f8\u5f53\u3002MARS-Cache\u5b9e\u73b0\u8d85\u8fc712\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5bb3\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "VidLaDA\u901a\u8fc7\u6269\u6563\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891LLM\u7684\u56e0\u679c\u63a9\u7801\u504f\u5dee\u95ee\u9898\uff0cMARS-Cache\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89c6\u9891token\u7684\u63a8\u7406\u74f6\u9888\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.18027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18027", "abs": "https://arxiv.org/abs/2601.18027", "authors": ["Chiyuan Fu", "Lyuhao Chen", "Yunze Xiao", "Weihao Xuan", "Carlos Busso", "Mona Diab"], "title": "Sentipolis: Emotion-Aware Agents for Social Simulations", "comment": null, "summary": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.", "AI": {"tldr": "Sentipolis\u6846\u67b6\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u60c5\u611f\u72b6\u6001\u7ba1\u7406\uff0c\u901a\u8fc7PAD\u60c5\u611f\u8868\u793a\u3001\u53cc\u901f\u60c5\u611f\u52a8\u6001\u548c\u60c5\u611f-\u8bb0\u5fc6\u8026\u5408\uff0c\u63d0\u5347\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u60c5\u611f\u8fde\u7eed\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u5e38\u5c06\u60c5\u611f\u89c6\u4e3a\u77ac\u65f6\u7ebf\u7d22\uff0c\u5bfc\u81f4\u60c5\u611f\u9057\u5fd8\u548c\u957f\u671f\u8fde\u7eed\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u60c5\u611f\u72b6\u6001\u7ba1\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51faSentipolis\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u8fde\u7eed\u6109\u60a6-\u5524\u9192-\u652f\u914d\uff08PAD\uff09\u60c5\u611f\u8868\u793a\uff1b2\uff09\u53cc\u901f\u60c5\u611f\u52a8\u6001\u673a\u5236\uff1b3\uff09\u60c5\u611f-\u8bb0\u5fc6\u8026\u5408\u7cfb\u7edf\u3002", "result": "\u5728\u6570\u5343\u6b21\u4ea4\u4e92\u4e2d\uff0cSentipolis\u663e\u8457\u63d0\u5347\u60c5\u611f\u57fa\u7840\u884c\u4e3a\u3001\u6c9f\u901a\u80fd\u529b\u548c\u60c5\u611f\u8fde\u7eed\u6027\u3002\u6548\u679c\u6a21\u578b\u4f9d\u8d56\uff1a\u9ad8\u5bb9\u91cf\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u53ef\u80fd\u4e0b\u964d\uff1b\u60c5\u611f\u610f\u8bc6\u8f7b\u5fae\u964d\u4f4e\u793e\u4f1a\u89c4\u8303\u9075\u4ece\u5ea6\u3002", "conclusion": "Sentipolis\u4e3a\u793e\u4ea4\u6a21\u62df\u63d0\u4f9b\u60c5\u611f\u72b6\u6001\u7ba1\u7406\u6846\u67b6\uff0c\u652f\u6301\u7d2f\u79ef\u793e\u4f1a\u52a8\u6001\u7814\u7a76\uff08\u5982\u8054\u76df\u5f62\u6210\u3001\u5173\u7cfb\u6e10\u53d8\uff09\uff0c\u63ed\u793a\u60c5\u611f\u9a71\u52a8\u884c\u4e3a\u4e0e\u793e\u4f1a\u89c4\u8303\u9075\u4ece\u4e4b\u95f4\u7684\u4eba\u7c7b\u5f0f\u5f20\u529b\u3002"}}
{"id": "2601.18130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18130", "abs": "https://arxiv.org/abs/2601.18130", "authors": ["Jize Wang", "Han Wu", "Zhiyuan You", "Yiming Song", "Yijun Wang", "Zifei Shan", "Yining Li", "Songyang Zhang", "Xinyi Le", "Cailian Chen", "Xinping Guan", "Dacheng Tao"], "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents", "comment": null, "summary": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.", "AI": {"tldr": "RouteMoA\uff1a\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u8def\u7531\u7684\u9ad8\u6548\u6df7\u5408\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bc4\u5206\u5668\u548c\u6df7\u5408\u88c1\u5224\u673a\u5236\uff0c\u5728\u65e0\u9700\u5168\u6a21\u578b\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\u7b5b\u9009\u5019\u9009\u6a21\u578b\uff0c\u5927\u5e45\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u6df7\u5408\u667a\u80fd\u4f53\uff08MoA\uff09\u65b9\u6cd5\u91c7\u7528\u5bc6\u96c6\u62d3\u6251\u7ed3\u6784\u5bfc\u81f4\u6210\u672c\u548c\u5ef6\u8fdf\u8fc7\u9ad8\uff0c\u4e14\u4f7f\u7528LLM\u88c1\u5224\u7b5b\u9009\u54cd\u5e94\u65f6\u4ecd\u9700\u6240\u6709\u6a21\u578b\u5148\u8fdb\u884c\u63a8\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u6210\u672c\u3002\u540c\u65f6\u7f3a\u4e4f\u6a21\u578b\u9009\u62e9\u6807\u51c6\uff0c\u5728\u5927\u89c4\u6a21\u6a21\u578b\u6c60\u4e2d\u9762\u4e34\u6210\u672c\u8fc7\u9ad8\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898", "method": "1. \u8f7b\u91cf\u7ea7\u8bc4\u5206\u5668\uff1a\u57fa\u4e8e\u67e5\u8be2\u9884\u6d4b\u7c97\u7565\u6027\u80fd\uff0c\u65e0\u9700\u63a8\u7406\u5373\u53ef\u7b5b\u9009\u9ad8\u6f5c\u529b\u5019\u9009\u6a21\u578b\u5b50\u96c6\uff1b2. \u6df7\u5408\u88c1\u5224\u673a\u5236\uff1a\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u8bc4\u4f30\u548c\u4ea4\u53c9\u8bc4\u4f30\u5bf9\u73b0\u6709\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u540e\u9a8c\u6821\u6b63\uff1b3. \u6a21\u578b\u6392\u540d\u673a\u5236\uff1a\u5e73\u8861\u6027\u80fd\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u8fdb\u884c\u6a21\u578b\u9009\u62e9", "result": "RouteMoA\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u6c60\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edfMoA\uff0c\u5728\u5927\u89c4\u6a21\u6a21\u578b\u6c60\u4e2d\u5b9e\u73b0\u6210\u672c\u964d\u4f4e89.8%\u3001\u5ef6\u8fdf\u964d\u4f4e63.6%", "conclusion": "RouteMoA\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u667a\u80fd\u4f53\u6846\u67b6\u7684\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u6a21\u578b\u534f\u4f5c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.18098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18098", "abs": "https://arxiv.org/abs/2601.18098", "authors": ["Chuang Yang", "Haozhao Ma", "Xu Han", "Yuan Yuan", "Qi Wang"], "title": "Text-Pass Filter: An Efficient Scene Text Detector", "comment": null, "summary": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.", "AI": {"tldr": "TPF\u662f\u4e00\u79cd\u7528\u4e8e\u4efb\u610f\u5f62\u72b6\u6587\u672c\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u539f\u7406\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6536\u7f29-\u63a9\u7801\u6269\u5c55\u7b56\u7565\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5e76\u80fd\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\u800c\u65e0\u9700\u590d\u6742\u89e3\u7801\u6216\u540e\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u6536\u7f29-\u63a9\u7801\u6269\u5c55\u7b56\u7565\uff0c\u4f46\u6536\u7f29\u64cd\u4f5c\u4f1a\u4e22\u5931\u6587\u672c\u8fb9\u7f18\u7684\u89c6\u89c9\u7279\u5f81\u5e76\u6df7\u6dc6\u524d\u666f\u80cc\u666f\u5dee\u5f02\uff0c\u8fd9\u7ed9\u6587\u672c\u7279\u5f81\u8bc6\u522b\u5e26\u6765\u4e86\u56fa\u6709\u5c40\u9650\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\u4e14\u80fd\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\u7684\u65b9\u6cd5\u3002", "method": "TPF\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u539f\u7406\uff0c\u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u72ec\u7279\u7684\u7279\u5f81-\u6ee4\u6ce2\u5668\u5bf9\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u901a\u8fc7\u4f20\u9012\u5176\u901a\u5e26\u7279\u5f81\u5e76\u963b\u6321\u5176\u4ed6\u7279\u5f81\u6765\u63d0\u53d6\u5bf9\u5e94\u7684\u5339\u914d\u6587\u672c\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86REU\u589e\u5f3a\u540c\u4e00\u6587\u672c\u7684\u7279\u5f81\u4e00\u81f4\u6027\u5e76\u6269\u5927\u6ee4\u6ce2\u5668\u7684\u8bc6\u522b\u8303\u56f4\uff0c\u4ee5\u53caFPU\u9f13\u52b1TPF\u533a\u5206\u524d\u666f\u80cc\u666f\u5dee\u5f02\u4ee5\u63d0\u9ad8\u7279\u5f81-\u6ee4\u6ce2\u5668\u5bf9\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86REU\u548cFPU\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86TPF\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\uff0c\u65e0\u9700\u590d\u6742\u89e3\u7801\u6216\u540e\u5904\u7406\uff0c\u4e3a\u5b9e\u65f6\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "TPF\u901a\u8fc7\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u539f\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6536\u7f29-\u63a9\u7801\u6269\u5c55\u7b56\u7565\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u80fd\u591f\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\uff0c\u4e3a\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u4efb\u610f\u5f62\u72b6\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18383", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18383", "abs": "https://arxiv.org/abs/2601.18383", "authors": ["Zhenyuan Guo", "Tong Chen", "Wenlong Meng", "Chen Gong", "Xin Yu", "Chengkun Wei", "Wenzhi Chen"], "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "AI": {"tldr": "DynTS\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u5206\u6790\u63a8\u7406\u8f68\u8ff9\uff0c\u8bc6\u522b\u5173\u952e\u51b3\u7b56token\u5e76\u9009\u62e9\u6027\u4fdd\u7559\u5176KV\u7f13\u5b58\uff0c\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\u65f6\u4f1a\u4ea7\u751f\u5927\u91cf\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u6210\u4e3a\u6548\u7387\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u8f68\u8ff9\u4e2d\u53ea\u6709\u90e8\u5206\u5173\u952etoken\u5bf9\u6700\u7ec8\u7b54\u6848\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u5176\u4f59token\u8d21\u732e\u53ef\u5ffd\u7565", "method": "\u63d0\u51fa\u52a8\u6001\u601d\u7ef4token\u9009\u62e9\uff08DynTS\uff09\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6ce8\u610f\u529b\u56fe\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u4e2d\u5404token\u7684\u5f71\u54cd\uff1b2\uff09\u8bc6\u522b\u51b3\u7b56\u5173\u952etoken\uff1b3\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ec5\u4fdd\u7559\u8fd9\u4e9b\u5173\u952etoken\u7684KV\u7f13\u5b58\u72b6\u6001\uff0c\u6dd8\u6c70\u5197\u4f59\u6761\u76ee", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u7559\u5173\u952etoken\u7684KV\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4f18\u5316\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387", "conclusion": "\u63a8\u7406\u8f68\u8ff9\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59token\uff0cDynTS\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u5173\u952e\u51b3\u7b56token\u5e76\u4f18\u5316KV\u7f13\u5b58\u7ba1\u7406\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6548\u7387\u4f18\u5316\u65b9\u6848"}}
{"id": "2601.18091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18091", "abs": "https://arxiv.org/abs/2601.18091", "authors": ["Longwei Ding", "Anhao Zhao", "Fanghua Ye", "Ziyang Chen", "Xiaoyu Shen"], "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models", "comment": "18 pages, 7 figures", "summary": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.", "code_url": "https://github.com/EIT-NLP/LRM-Prunin", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u6307\u4ee4\u9075\u5faa\u578bLLM\u548c\u63a8\u7406\u589e\u5f3a\u578bLLM\u7684\u526a\u679d\u7b56\u7565\u5dee\u5f02\uff0c\u53d1\u73b0\u526a\u679d\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6a21\u578b\u8303\u5f0f\uff0c\u9700\u8981\u9488\u5bf9\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u9075\u5faa\u578bLLM\uff0c\u4f46\u63a8\u7406\u589e\u5f3a\u578bLLM\u4f1a\u751f\u6210\u957f\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff0c\u5176\u526a\u679d\u7279\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u8303\u5f0fLLM\u7684\u526a\u679d\u884c\u4e3a\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u5bf9\u7167\u7814\u7a76\u8bbe\u8ba1\uff0c\u5206\u522b\u5bf9\u6307\u4ee4\u9075\u5faa\u578b(LLM-instruct)\u548c\u63a8\u7406\u589e\u5f3a\u578b(LLM-think)\u6a21\u578b\u8fdb\u884c\u526a\u679d\u3002\u4e3a\u9694\u79bb\u526a\u679d\u6548\u5e94\uff0c\u5c06\u526a\u679d\u6821\u51c6\u548c\u526a\u679d\u540e\u6062\u590d\u6570\u636e\u4e0e\u5404\u6a21\u578b\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u5bf9\u9f50\u3002\u8bc4\u4f30\u9759\u6001\u6df1\u5ea6\u526a\u679d\u3001\u9759\u6001\u5bbd\u5ea6\u526a\u679d\u548c\u52a8\u6001\u526a\u679d\u4e09\u79cd\u7b56\u7565\uff0c\u572817\u4e2a\u6db5\u76d6\u5206\u7c7b\u3001\u751f\u6210\u548c\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u660e\u663e\u7684\u8303\u5f0f\u4f9d\u8d56\u6027\u5dee\u5f02\uff1a\u6df1\u5ea6\u526a\u679d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5bbd\u5ea6\u526a\u679d\u5728\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u66f4\u7a33\u5065\u3002\u9759\u6001\u526a\u679d\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u52a8\u6001\u526a\u679d\u5728\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u957f\u94fe\u63a8\u7406\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u63a8\u7406\u589e\u5f3a\u578bLLM\u9700\u8981\u4e13\u95e8\u8003\u8651\u5176\u7279\u6027\u7684\u526a\u679d\u7b56\u7565\uff0c\u4e0d\u80fd\u7b80\u5355\u6cbf\u7528\u6307\u4ee4\u9075\u5faa\u578bLLM\u7684\u526a\u679d\u65b9\u6cd5\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9\u4e0d\u540cLLM\u8303\u5f0f\u8bbe\u8ba1\u5b9a\u5236\u5316\u526a\u679d\u65b9\u6848\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18107", "categories": ["cs.LG", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18107", "abs": "https://arxiv.org/abs/2601.18107", "authors": ["Pedram Agand", "Mo Chen"], "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions", "comment": "11 pages, 2 figures, 2 tables", "summary": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.", "AI": {"tldr": "MoReBRAC\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5faa\u73af\u4e16\u754c\u6a21\u578b\u5408\u6210\u9ad8\u4fdd\u771f\u5ea6\u8f6c\u79fb\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\u786e\u4fdd\u5408\u6210\u6570\u636e\u53ef\u9760\u6027\uff0c\u5728D4RL\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9759\u6001\u6570\u636e\u96c6\u4e0e\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5bfc\u81f4\u9700\u8981\u9ad8\u5ea6\u4fdd\u5b88\u6027\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u6f5c\u529b\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u4ee5\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd", "method": "\u63d0\u51faMoReBRAC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u53cc\u91cd\u5faa\u73af\u4e16\u754c\u6a21\u578b\u5408\u6210\u9ad8\u4fdd\u771f\u5ea6\u8f6c\u79fb\u6570\u636e\u4ee5\u6269\u5c55\u8bad\u7ec3\u6d41\u5f62\uff1b2\uff09\u5b9e\u65bd\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\uff0c\u96c6\u6210\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6d41\u5f62\u68c0\u6d4b\u3001\u6a21\u578b\u654f\u611f\u6027\u5206\u6790\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u786e\u4fdd\u53ea\u4f7f\u7528\u5b66\u4e60\u52a8\u6001\u9ad8\u7f6e\u4fe1\u533a\u57df\u7684\u8f6c\u79fb\u6570\u636e", "result": "\u5728D4RL Gym-MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\"\u968f\u673a\"\u548c\"\u6b21\u4f18\"\u6570\u636e\u673a\u5236\u4e2d\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u4e86VAE\u4f5c\u4e3a\u51e0\u4f55\u951a\u70b9\u7684\u4f5c\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u4ece\u63a5\u8fd1\u6700\u4f18\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65f6\u7684\u5206\u5e03\u6743\u8861", "conclusion": "MoReBRAC\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6f5c\u5728\u5408\u6210\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u4fdd\u5b88\u6027\u9650\u5236\uff0c\u5728\u6311\u6218\u6027\u6570\u636e\u673a\u5236\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18330", "abs": "https://arxiv.org/abs/2601.18330", "authors": ["Muhammad Ali Shah", "Muhammad Mansoor Alam", "Saddam Hussain Khan"], "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification", "comment": "33 Pages, 8 Tables, Figures 16", "summary": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5bc6\u96c6Swin\u6df7\u5408\uff08EDSH\uff09\u6846\u67b6\u7528\u4e8e\u8111\u80bf\u7624MRI\u5206\u6790\uff0c\u901a\u8fc7\u4e24\u4e2a\u80bf\u7624\u611f\u77e5\u5b9e\u9a8c\u8bbe\u7f6e\u8054\u5408\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u6a21\u5f0f\u548c\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u5927\u578bMRI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8698.50%\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u8111\u80bf\u7624MRI\u5206\u6790\u9700\u8981\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u6a21\u5f0f\u548c\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u548c\u5168\u5c40\u5f62\u6001\u5efa\u6a21\u3002\u9488\u5bf9\u4e0d\u540c\u80bf\u7624\u7c7b\u578b\uff08\u5f25\u6f2b\u6027\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u3001\u5782\u4f53\u7624\uff09\u7684\u8bca\u65ad\u6311\u6218\u9700\u8981\u4e13\u95e8\u7684\u8bbe\u8ba1\u6765\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u80bf\u7624\u611f\u77e5\u5b9e\u9a8c\u8bbe\u7f6e\uff1a1\uff09\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\uff08BFS\uff09\u8bbe\u7f6e\uff0c\u901a\u8fc7\u72ec\u7acb\u5b9a\u5236\u7684DenseNet\u548cSwin\u5206\u652f\u5b66\u4e60\u4e92\u8865\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\uff0c\u8fdb\u884c\u7ef4\u5ea6\u5bf9\u9f50\u3001\u878d\u5408\u548c\u589e\u5f3a\uff1b2\uff09\u5206\u5c42DenseNet-Swin\u67b6\u6784\uff0c\u5177\u6709\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u53cc\u6b8b\u5dee\u8fde\u63a5\uff08DFE\u548cDR\uff09\uff0cDenseNet\u4f5c\u4e3a\u4e3b\u5e72CNN\u5b66\u4e60\u7ed3\u6784\u5316\u5c40\u90e8\u7279\u5f81\uff0cSwin_t\u5efa\u6a21\u5168\u5c40\u80bf\u7624\u5f62\u6001\u3002DenseNet\u5728\u8f93\u5165\u7ea7\u522b\u5b9a\u5236\u4ee5\u5339\u914dMRI\u7a7a\u95f4\u7279\u6027\uff0cSwin_t\u901a\u8fc7\u4efb\u52a1\u5bf9\u9f50\u7684\u8865\u4e01\u5d4c\u5165\u548c\u79fb\u4f4d\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9a\u5236\u3002", "result": "\u5728\u5927\u578bMRI\u6570\u636e\u96c6\uff0840,260\u5f20\u56fe\u50cf\uff0c\u56db\u4e2a\u80bf\u7624\u7c7b\u522b\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aEDSH\u6846\u67b6\u5728\u6d4b\u8bd5\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8698.50%\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u4e00\u81f4\u4f18\u4e8e\u72ec\u7acb\u7684CNN\u3001Vision Transformer\u548c\u6df7\u5408\u65b9\u6cd5\u3002", "conclusion": "EDSH\u6846\u67b6\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5c40\u90e8\u7eb9\u7406\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8111\u80bf\u7624MRI\u5206\u6790\u4e2d\u7684\u8bca\u65ad\u6311\u6218\uff0c\u5728\u5f25\u6f2b\u6027\u80f6\u8d28\u7624\u68c0\u6d4b\u548c\u8111\u819c\u7624/\u5782\u4f53\u7624\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18555", "abs": "https://arxiv.org/abs/2601.18555", "authors": ["Roberto Di Via", "Vito Paolo Pastore", "Francesca Odone", "Si\u00f4n Glyn-Jones", "Irina Voiculescu"], "title": "Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray", "comment": "Accepted at International Symposium on Biomedical Imaging (ISBI 2026)", "summary": "Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions", "code_url": "https://github.com/Malga-Vision/Landmarks-Hip-Conditions", "code_stars": 0, "code_last_update": "2026-01-26", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4f7f\u7528\u6807\u51c6\u70ed\u56fe\u56de\u5f52\u67b6\u6784\u5728MRI\u4e0a\u5b9e\u73b0\u4e0eX\u5c04\u7ebf\u76f8\u5f53\u7684\u9acb\u81fc\u649e\u51fb\u75c7(FAI)\u6807\u5fd7\u70b9\u68c0\u6d4b\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u652f\u6301\u5c06\u81ea\u52a8\u5316FAI\u8bc4\u4f30\u6574\u5408\u5230\u5e38\u89c4MRI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "motivation": "\u4e34\u5e8a\u7b5b\u67e5\u51b3\u7b56\u5e38\u57fa\u4e8e\u89d2\u5ea6\u6d4b\u91cf\uff0c\u7279\u522b\u662fFAI\u7b5b\u67e5\u4f20\u7edf\u4e0a\u4f9d\u8d56X\u5c04\u7ebf\u6d4b\u91cf\u89d2\u5ea6\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u649e\u51fb\u533a\u57df\u7684\u9ad8\u5ea6\u548c\u8303\u56f4\u9700\u8981MRI\u626b\u63cf\u76843D\u89c6\u56fe\u3002\u4e24\u79cd\u6a21\u6001\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\uff0c\u9700\u8981\u9a8c\u8bc1MRI\u662f\u5426\u4e5f\u80fd\u8fbe\u5230\u4e0eX\u5c04\u7ebf\u76f8\u5f53\u7684\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5339\u914d\u961f\u5217\u9a8c\u8bc1\u7814\u7a76\uff0889\u540d\u60a3\u8005\uff0c\u914d\u5bf9MRI/X\u5c04\u7ebf\uff09\uff0c\u4f7f\u7528\u6807\u51c6\u70ed\u56fe\u56de\u5f52\u67b6\u6784\u8bc4\u4f30\u8de8\u6a21\u6001\u4e34\u5e8a\u7b49\u6548\u6027\u3002\u7814\u7a76\u57283D MRI\u4f53\u79ef\u7684\u51a0\u72b6\u89c6\u56fe\u4e2d\u8fdb\u884cFAI\u8bc4\u4f30\uff0c\u901a\u8fc7\u6807\u5fd7\u70b9\u68c0\u6d4b\u65b9\u6cd5\u5b9e\u73b0\u5b9a\u4f4d\u3002", "result": "MRI\u5728cam\u578b\u649e\u51fb\u75c7\u7684\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u6027\u65b9\u9762\u8fbe\u5230\u4e0eX\u5c04\u7ebf\u7b49\u6548\u7684\u6c34\u5e73\u3002\u8be5\u65b9\u6cd5\u57283D MRI\u4f53\u79ef\u7684\u51a0\u72b6\u89c6\u56fe\u4e2d\u5c55\u793a\u4e86\u4e34\u5e8a\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u8fc7\u653e\u7f6e\u66f4\u591a\u6807\u5fd7\u70b9\u8fdb\u884c\u4f53\u79ef\u5206\u6790\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5c06\u81ea\u52a8\u5316FAI\u8bc4\u4f30\u6574\u5408\u5230\u5e38\u89c4MRI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002MRI\u5728FAI\u8bc4\u4f30\u4e2d\u5177\u6709\u4e0eX\u5c04\u7ebf\u76f8\u5f53\u7684\u4e34\u5e8a\u4ef7\u503c\uff0c\u4e3a\u672a\u6765\u8fdb\u884c\u66f4\u5168\u9762\u7684\u4f53\u79ef\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.18329", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18329", "abs": "https://arxiv.org/abs/2601.18329", "authors": ["Chuhan Feng", "Jing Li", "Jie Li", "Lu Lv", "Fengkui Gong"], "title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection", "comment": null, "summary": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u534f\u8bae\u7279\u5b9a\u65f6\u9891\u7279\u5f81\u8fdb\u884c\u81ea\u9002\u5e94\u52a0\u6743\uff0c\u7ed3\u5408\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u4e0e\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u6570\u878d\u5408\u8fdb\u884c\u8054\u5408\u63a8\u65ad\u3002", "motivation": "\u65e0\u4eba\u673a\u4fe1\u53f7\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u533a\u5206\u5df2\u77e5\u534f\u8bae\u548c\u672a\u77e5\u534f\u8bae\u4fe1\u53f7\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\u7279\u5f81\u5229\u7528\u4e0d\u8db3\uff0c\u4e14\u5bf9OOD\u6837\u672c\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\u6355\u6349\u4e0d\u591f\u3002", "method": "1. \u57fa\u4e8e\u534f\u8bae\u7279\u5b9a\u65f6\u9891\u7279\u5f81\u91cf\u5316\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u65b9\u5dee\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u65f6\u9891\u56fe\u50cf\u7279\u5f81\u7684\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\uff1b2. \u5f15\u5165\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u6355\u6349OOD\u6837\u672c\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\uff1b3. \u5c06\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u4e0e\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u6570\u878d\u5408\u8fdb\u884c\u8054\u5408\u63a8\u65ad\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4fe1\u566a\u6bd4\u548c\u591a\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5224\u522b\u80fd\u529b\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684OOD\u68c0\u6d4b\u7b97\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u672a\u77e5\u65e0\u4eba\u673a\u4fe1\u53f7\uff0c\u4e3a\u65e0\u4eba\u673a\u4fe1\u53f7\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18739", "abs": "https://arxiv.org/abs/2601.18739", "authors": ["Ignacio Antequera-S\u00e1nchez", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification", "comment": "28 pages", "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.", "AI": {"tldr": "\u63d0\u51faSeNeDiF-OOD\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u7684\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u6709\u6548\u5904\u7406\u5f00\u653e\u4e16\u754c\u4e2d\u4ece\u4f4e\u7ea7\u635f\u574f\u5230\u8bed\u4e49\u504f\u79fb\u7684\u5f02\u6784OOD\u6570\u636e\uff0c\u5728MonuMAI\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u4e16\u754cAI\u5e94\u7528\u4e2d\uff0cOOD\u68c0\u6d4b\u662f\u53ef\u9760\u90e8\u7f72\u7684\u57fa\u672c\u8981\u6c42\u3002\u7136\u800c\uff0cOOD\u6570\u636e\u7684\u5f02\u6784\u6027\uff08\u4ece\u4f4e\u7ea7\u635f\u574f\u5230\u8bed\u4e49\u504f\u79fb\uff09\u4f7f\u5f97\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u96be\u4ee5\u6709\u6548\u5904\u7406\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u590d\u6742\u6311\u6218\u3002", "method": "\u63d0\u51faSeNeDiF-OOD\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u68c0\u6d4b\u4efb\u52a1\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u7684\u4e8c\u5143\u878d\u5408\u8282\u70b9\u7ed3\u6784\uff0c\u6bcf\u4e00\u5c42\u8bbe\u8ba1\u7528\u4e8e\u6574\u5408\u4e0e\u7279\u5b9a\u8bed\u4e49\u62bd\u8c61\u7ea7\u522b\u5bf9\u9f50\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728MonuMAI\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u7684\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u9762\u5bf9\u975e\u7eaa\u5ff5\u7891\u56fe\u50cf\u3001\u672a\u77e5\u5efa\u7b51\u98ce\u683c\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u591a\u79cd\u8f93\u5165\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5c42\u6b21\u5316\u878d\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u8fc7\u6ee4\u8fd9\u4e9b\u591a\u6837\u5316\u7684OOD\u7c7b\u522b\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd\u3002", "conclusion": "SeNeDiF-OOD\u901a\u8fc7\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u7684\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784OOD\u6570\u636e\u68c0\u6d4b\u7684\u590d\u6742\u6311\u6218\uff0c\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684OOD\u68c0\u6d4b\uff0c\u4e3aAI\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18525", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18525", "abs": "https://arxiv.org/abs/2601.18525", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Emanuele Frasca", "Aurelio Uncini", "Danilo Comminiello"], "title": "Closing the Modality Gap Aligns Group-Wise Semantics", "comment": "ICLR 2026", "summary": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6a21\u6001\u95f4\u9699\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\uff08\u5982\u805a\u7c7b\uff09\u5f71\u54cd\u663e\u8457\uff0c\u800c\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff08\u5982\u68c0\u7d22\uff09\u5f71\u54cd\u6709\u9650\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u7684\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1CLIP\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5efa\u7acb\u4e86\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f46\u6a21\u6001\u95f4\u9699\uff08\u7ed3\u6784\u4e0d\u5339\u914d\uff09\u4ecd\u7136\u5b58\u5728\u3002\u867d\u7136\u6a21\u6001\u95f4\u9699\u5bf9\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5f71\u54cd\u6709\u9650\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u5176\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5728\u53cc\u6a21\u6001\u8bbe\u7f6e\u4e2d\u4e00\u81f4\u5730\u51cf\u5c11\u6a21\u6001\u95f4\u9699\uff0c\u5e76\u53ef\u7b80\u5355\u6269\u5c55\u5230n\u6a21\u6001\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u6a21\u6001\u95f4\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u53d1\u73b0\uff1a\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u5bf9\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u4ec5\u63d0\u4f9b\u8fb9\u9645\u6216\u4e0d\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u4f46\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\uff08\u5982\u805a\u7c7b\uff09\u6709\u663e\u8457\u63d0\u5347\u3002\u8fd9\u63ed\u793a\u4e86\u6a21\u6001\u95f4\u9699\u5728\u8bed\u4e49\u5206\u7ec4\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u6a21\u6001\u95f4\u9699\u7684\u5f71\u54cd\u5728\u7fa4\u4f53\u7ea7\u4efb\u52a1\u4e2d\u6bd4\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u66f4\u4e3a\u663e\u8457\uff0c\u8fd9\u4e00\u53d1\u73b0\u53ef\u80fd\u91cd\u5851\u5bf9\u6a21\u6001\u95f4\u9699\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u5176\u5728\u6539\u5584\u9700\u8981\u8bed\u4e49\u5206\u7ec4\u7684\u4efb\u52a1\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2601.18777", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18777", "abs": "https://arxiv.org/abs/2601.18777", "authors": ["Abhishek Divekar", "Anirban Majumder"], "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation", "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)", "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.", "AI": {"tldr": "\u63d0\u51faPRECISE\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u548cLLM\u5224\u65ad\u6765\u8bc4\u4f30\u641c\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42", "motivation": "\u4f20\u7edf\u641c\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u8bc4\u4f30\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u800cLLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5b58\u5728\u56fa\u6709\u504f\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u7edf\u8ba1\u6846\u67b6", "method": "\u6269\u5c55\u9884\u6d4b\u9a71\u52a8\u63a8\u7406(PPI)\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u67e5\u8be2(100\u4e2a)\u548c\u5927\u91cf\u672a\u6807\u6ce8\u793a\u4f8b(10,000\u4e2a)\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6307\u6807\u96c6\u6210\u7a7a\u95f4\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(2^|C|)\u964d\u4f4e\u5230O(2^K)", "result": "\u5728\u591a\u4e2a\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5173\u952e\u4e1a\u52a1\u6307\u6807Precision@K\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u6709\u6548\u6821\u6b63\u4e86LLM\u504f\u5dee", "conclusion": "PRECISE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u548cLLM\u5224\u65ad\uff0c\u4e3a\u9700\u8981\u5b50\u5b9e\u4f8b\u6807\u6ce8\u7684\u6307\u6807\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u9700\u6c42\u5e76\u6821\u6b63\u4e86LLM\u504f\u5dee"}}
{"id": "2601.18795", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18795", "abs": "https://arxiv.org/abs/2601.18795", "authors": ["Amrith Setlur", "Zijian Wang", "Andrew Cohen", "Paria Rashidinejad", "Sang Michael Xie"], "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes", "comment": null, "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.", "AI": {"tldr": "PrefixRL\uff1a\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b02\u500d\u8bad\u7ec3\u52a0\u901f\u548c3\u500d\u6700\u7ec8\u5956\u52b1\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u5904\u7406\u56f0\u96be\u63a8\u7406\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u5728\u7ebf\u7b56\u7565\u8f68\u8ff9\u7a00\u5c11\u3001\u7b56\u7565\u68af\u5ea6\u6d88\u5931\u5bfc\u81f4\u5b66\u4e60\u505c\u6ede\u3002\u9700\u8981\u91cd\u7528\u5148\u524d\u63a8\u7406\u6216RL\u8bad\u7ec3\u4e2d\u4ea7\u751f\u7684\u79bb\u7b56\u7565\u8f68\u8ff9\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faPrefixRL\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6210\u529f\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u7136\u540e\u8fd0\u884c\u5728\u7ebfRL\u6765\u5b8c\u6210\u5269\u4f59\u90e8\u5206\u3002\u901a\u8fc7\u8c03\u8282\u524d\u7f00\u957f\u5ea6\u6765\u8c03\u6574\u95ee\u9898\u96be\u5ea6\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\u3002\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u83b7\u53d6\u79bb\u7b56\u7565\u8f68\u8ff9\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u3002", "result": "\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\uff0cPrefixRL\u8fbe\u5230\u76f8\u540c\u8bad\u7ec3\u5956\u52b1\u7684\u901f\u5ea6\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff08\u5728\u79bb\u7b56\u7565\u6570\u636e\u4e0a\u8fdb\u884cSFT\u7136\u540eRL\uff09\u5feb2\u500d\uff0c\u5373\u4f7f\u8003\u8651\u521d\u59cb\u62d2\u7edd\u91c7\u6837\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6700\u7ec8\u5956\u52b1\u63d0\u53473\u500d\u3002\u53d1\u73b0\u540e\u6cdb\u5316\u73b0\u8c61\uff1a\u4ec5\u5728\u5e26\u524d\u7f00\u95ee\u9898\u4e0a\u8bad\u7ec3\u80fd\u6cdb\u5316\u5230\u65e0\u524d\u7f00\u7684\u5206\u5e03\u5916\u6027\u80fd\u3002\u5f53\u79bb\u7b56\u7565\u8f68\u8ff9\u6765\u81ea\u4e0d\u540c\u6a21\u578b\u65cf\u65f6\u4ecd\u6709\u6548\u3002", "conclusion": "PrefixRL\u901a\u8fc7\u91cd\u7528\u79bb\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u6709\u6548\u63d0\u5347RL\u8bad\u7ec3\u6548\u7387\uff0c\u907f\u514d\u79bb\u7b56\u7565\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u7075\u6d3b\u6027\u3002"}}
