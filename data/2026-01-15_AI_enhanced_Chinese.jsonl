{"id": "2601.09007", "categories": ["math.ST", "math.NA", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09007", "abs": "https://arxiv.org/abs/2601.09007", "authors": ["Sven Wang"], "title": "Global polynomial-time estimation in statistical nonlinear inverse problems via generalized stability", "comment": "46 pages, 2 figures", "summary": "Non-linear statistical inverse problems pose major challenges both for statistical analysis and computation. Likelihood-based estimators typically lead to non-convex and possibly multimodal optimization landscapes, and Markov chain Monte Carlo (MCMC) methods may mix exponentially slowly. We propose a class of computationally tractable estimators--plug-in and PDE-penalized M-estimators--for inverse problems defined through operator equations of the form $L_f u = g$, where $f$ is the unknown parameter and $u$ is the observed solution. The key idea is to replace the exact PDE constraint by a weakly enforced relaxation, yielding conditionally convex and, in many PDE examples, nested quadratic optimization problems that avoid evaluating the forward map $G(f)$ and do not require PDE solvers. For prototypical non-linear inverse problems arising from elliptic PDEs, including the Darcy flow model $L_f u = \\nabla\\!\\cdot(f\\nabla u)$ and a steady-state Schr\u00f6dinger model, we prove that these estimators attain the best currently known statistical convergence rates while being globally computable in polynomial time. In the Darcy model, we obtain an explicit sub-quadratic $o(N^2)$ arithmetic runtime bound for estimating $f$ from $N$ noisy samples. Our analysis is based on new generalized stability estimates, extending classical stability beyond the range of the forward operator, combined with tools from nonparametric M-estimation. We also derive adaptive rates for the Darcy problem, providing a blueprint for designing provably polynomial-time statistical algorithms for a broad class of non-linear inverse problems. Our estimators also provide principled warm-start initializations for polynomial-time Bayesian computation.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u975e\u7ebf\u6027\u7edf\u8ba1\u9006\u95ee\u9898\u7684\u53ef\u8ba1\u7b97\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5f31\u5316PDE\u7ea6\u675f\u5b9e\u73b0\u6761\u4ef6\u51f8\u4f18\u5316\uff0c\u5728\u692d\u5706PDE\u9006\u95ee\u9898\u4e2d\u8fbe\u5230\u6700\u4f18\u7edf\u8ba1\u6536\u655b\u7387\u4e14\u5177\u6709\u591a\u9879\u5f0f\u65f6\u95f4\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u975e\u7ebf\u6027\u7edf\u8ba1\u9006\u95ee\u9898\u5728\u7edf\u8ba1\u5206\u6790\u548c\u8ba1\u7b97\u4e0a\u90fd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a\u57fa\u4e8e\u4f3c\u7136\u7684\u4f30\u8ba1\u5668\u901a\u5e38\u5bfc\u81f4\u975e\u51f8\u4e14\u53ef\u80fd\u591a\u5cf0\u7684\u4f18\u5316\u666f\u89c2\uff0c\u800cMCMC\u65b9\u6cd5\u53ef\u80fd\u6df7\u5408\u901f\u5ea6\u6781\u6162\u3002\u9700\u8981\u5f00\u53d1\u65e2\u5177\u6709\u7edf\u8ba1\u6700\u4f18\u6027\u53c8\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u8ba1\u7b97\u53ef\u884c\u7684\u4f30\u8ba1\u5668\uff1a\u63d2\u4ef6\u4f30\u8ba1\u5668\u548cPDE\u60e9\u7f5aM\u4f30\u8ba1\u5668\u3002\u6838\u5fc3\u601d\u60f3\u662f\u7528\u5f31\u5f3a\u5236\u677e\u5f1b\u66ff\u4ee3\u7cbe\u786e\u7684PDE\u7ea6\u675f\uff0c\u4ea7\u751f\u6761\u4ef6\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u907f\u514d\u8bc4\u4f30\u524d\u5411\u6620\u5c04G(f)\u4e14\u4e0d\u9700\u8981PDE\u6c42\u89e3\u5668\u3002\u5bf9\u4e8e\u692d\u5706PDE\u9006\u95ee\u9898\uff08\u5982\u8fbe\u897f\u6d41\u6a21\u578b\u548c\u7a33\u6001\u859b\u5b9a\u8c14\u6a21\u578b\uff09\uff0c\u8bc1\u660e\u8fd9\u4e9b\u4f30\u8ba1\u5668\u5177\u6709\u6700\u4f18\u7edf\u8ba1\u6536\u655b\u7387\u3002", "result": "\u5bf9\u4e8e\u8fbe\u897f\u6d41\u6a21\u578b\u7b49\u692d\u5706PDE\u9006\u95ee\u9898\uff0c\u8bc1\u660e\u6240\u63d0\u4f30\u8ba1\u5668\u8fbe\u5230\u5f53\u524d\u5df2\u77e5\u7684\u6700\u4f73\u7edf\u8ba1\u6536\u655b\u7387\uff0c\u540c\u65f6\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5168\u5c40\u8ba1\u7b97\u3002\u5728\u8fbe\u897f\u6a21\u578b\u4e2d\uff0c\u4eceN\u4e2a\u566a\u58f0\u6837\u672c\u4f30\u8ba1f\u83b7\u5f97\u4e86\u663e\u5f0f\u7684\u4e9a\u4e8c\u6b21o(N\u00b2)\u7b97\u672f\u8fd0\u884c\u65f6\u95f4\u754c\u3002\u8fd8\u63a8\u5bfc\u4e86\u8fbe\u897f\u95ee\u9898\u7684\u81ea\u9002\u5e94\u6536\u655b\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u4e3a\u5e7f\u6cdb\u975e\u7ebf\u6027\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u7edf\u8ba1\u7b97\u6cd5\u8bbe\u8ba1\u84dd\u56fe\u3002\u8fd9\u4e9b\u4f30\u8ba1\u5668\u8fd8\u4e3a\u591a\u9879\u5f0f\u65f6\u95f4\u8d1d\u53f6\u65af\u8ba1\u7b97\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u70ed\u542f\u52a8\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u9006\u95ee\u9898\u4e2d\u7edf\u8ba1\u6700\u4f18\u6027\u4e0e\u8ba1\u7b97\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2601.09016", "categories": ["math.ST", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.09016", "abs": "https://arxiv.org/abs/2601.09016", "authors": ["Christopher Blier-Wong"], "title": "Stochastic representation of Sarmanov copulas", "comment": null, "summary": "Sarmanov copulas offer a simple and tractable way to build multivariate distributions by perturbing the independence copula. They admit closed-form expressions for densities and many functionals of interest, making them attractive for practical applications. However, the complex conditions on the dependence parameters to ensure that Sarmanov copulas are valid limit their application in high dimensions. Verifying the $d$-increasing property typically requires satisfying a combinatorial set of inequalities that makes direct construction difficult. To circumvent this issue, we develop a stochastic representation for bivariate Sarmanov copulas. We prove that every admissible Sarmanov can be realized as a mixture of independent univariate distributions indexed by a latent Bernoulli pair. The stochastic representation replaces the problem of verifying copula validity with the problem of ensuring nonnegativity of a Bernoulli probability mass function. The representation also recovers classical copula families, including Farlie--Gumbel--Morgenstern, Huang--Kotz, and Bairamov--Kotz--Bek\u00e7i as special cases. We further derive sharp global bounds for Spearman's rho and Kendall's tau. We then introduce a Bernoulli-mixing construction in higher dimensions, leading to a new class of multivariate Sarmanov copulas with easily verifiable parameter constraints and scalable simulation algorithms. Finally, we show that powered versions of bivariate Sarmanov copulas admit a similar stochastic representation through block-maximal order statistics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Sarmanov copulas\u7684\u968f\u673a\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5e94\u7528\u4e2d\u53c2\u6570\u7ea6\u675f\u9a8c\u8bc1\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u8be5\u7c7bcopula\u7684\u9002\u7528\u8303\u56f4\u3002", "motivation": "Sarmanov copulas\u867d\u7136\u5177\u6709\u7b80\u5355\u6613\u5904\u7406\u7684\u7279\u70b9\uff0c\u4f46\u5728\u9ad8\u7ef4\u5e94\u7528\u4e2d\uff0c\u786e\u4fddcopula\u6709\u6548\u7684\u53c2\u6570\u7ea6\u675f\u6761\u4ef6\u590d\u6742\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "1. \u4e3a\u4e8c\u5143Sarmanov copulas\u5f00\u53d1\u968f\u673a\u8868\u793a\uff0c\u8bc1\u660e\u6bcf\u4e2a\u53ef\u5bb9\u8bb8\u7684Sarmanov copula\u90fd\u53ef\u4ee5\u8868\u793a\u4e3a\u7531\u6f5c\u5728\u4f2f\u52aa\u5229\u5bf9\u7d22\u5f15\u7684\u72ec\u7acb\u5355\u53d8\u91cf\u5206\u5e03\u7684\u6df7\u5408\uff1b2. \u5f15\u5165\u9ad8\u7ef4\u4f2f\u52aa\u5229\u6df7\u5408\u6784\u9020\uff0c\u521b\u5efa\u65b0\u7684\u591a\u5143Sarmanov copulas\u7c7b\uff1b3. \u63a8\u5bfcSpearman's rho\u548cKendall's tau\u7684\u5168\u5c40\u8fb9\u754c\uff1b4. \u8bc1\u660e\u4e8c\u5143Sarmanov copulas\u7684\u5e42\u6b21\u7248\u672c\u901a\u8fc7\u5757\u6781\u5927\u987a\u5e8f\u7edf\u8ba1\u91cf\u5177\u6709\u7c7b\u4f3c\u7684\u968f\u673a\u8868\u793a\u3002", "result": "1. \u5c06copula\u6709\u6548\u6027\u9a8c\u8bc1\u95ee\u9898\u8f6c\u5316\u4e3a\u4f2f\u52aa\u5229\u6982\u7387\u8d28\u91cf\u51fd\u6570\u7684\u975e\u8d1f\u6027\u9a8c\u8bc1\u95ee\u9898\uff1b2. \u6062\u590d\u4e86\u5305\u62ecFarlie-Gumbel-Morgenstern\u3001Huang-Kotz\u548cBairamov-Kotz-Bek\u00e7i\u5728\u5185\u7684\u7ecf\u5178copula\u65cf\uff1b3. \u5efa\u7acb\u4e86\u5177\u6709\u6613\u4e8e\u9a8c\u8bc1\u53c2\u6570\u7ea6\u675f\u548c\u53ef\u6269\u5c55\u6a21\u62df\u7b97\u6cd5\u7684\u65b0\u591a\u5143Sarmanov copulas\u7c7b\u3002", "conclusion": "\u63d0\u51fa\u7684\u968f\u673a\u8868\u793a\u65b9\u6cd5\u89e3\u51b3\u4e86Sarmanov copulas\u5728\u9ad8\u7ef4\u5e94\u7528\u4e2d\u7684\u9a8c\u8bc1\u96be\u9898\uff0c\u6269\u5c55\u4e86\u5176\u9002\u7528\u8303\u56f4\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7b80\u4fbf\u7684\u6784\u9020\u548c\u6a21\u62df\u65b9\u6cd5\u3002"}}
{"id": "2601.09122", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.09122", "abs": "https://arxiv.org/abs/2601.09122", "authors": ["Ruchira Ray", "Marco Avella Medina", "Cynthia Rush"], "title": "Statistical Guarantees for Data-driven Posterior Tempering", "comment": null, "summary": "Posterior tempering reduces the influence of the likelihood in the calculation of the posterior by raising the likelihood to a fractional power $\u03b1$. The resulting power posterior - also known as an $\u03b1$-posterior or fractional posterior - has been shown to exhibit appealing properties, including robustness to model misspecification and asymptotic normality (Bernstein-von Mises theorem). However, practical recommendations for selecting the tempering parameter and statistical guarantees for the resulting power posterior remain open questions. Cross-validation-based approaches to tuning this parameter suggest interesting asymptotic regimes for the selected $\u03b1$, which can either vanish or behave like a mixture distribution with a point mass at infinity and the remaining mass converging to zero. We formalize the asymptotic properties of the power posterior in these regimes. In particular, we provide sufficient conditions for (i) consistency of the power posterior moments and (ii) asymptotic normality of the power posterior mean. Our analysis required us to establish a new Laplace approximation that is interesting in its own right and is the key technical tool for showing a critical threshold $\u03b1\\asymp 1/\\sqrt{n}$ where the asymptotic normality of the posterior mean breaks. Our results allow for the power to depend on the data in an arbitrary way.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u540e\u9a8c\u6e29\u5ea6\u8c03\u8282\uff08power posterior\uff09\u7684\u6e10\u8fd1\u6027\u8d28\uff0c\u7279\u522b\u662f\u03b1\u9009\u62e9\u5bf9\u540e\u9a8c\u77e9\u4e00\u81f4\u6027\u548c\u540e\u9a8c\u5747\u503c\u6e10\u8fd1\u6b63\u6001\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e86\u4e34\u754c\u9608\u503c\u03b1\u221d1/\u221an\u3002", "motivation": "\u540e\u9a8c\u6e29\u5ea6\u8c03\u8282\u901a\u8fc7\u5c06\u4f3c\u7136\u51fd\u6570\u63d0\u5347\u5230\u5206\u6570\u5e42\u03b1\u6765\u51cf\u5c11\u4f3c\u7136\u5bf9\u540e\u9a8c\u7684\u5f71\u54cd\uff0c\u5177\u6709\u6a21\u578b\u8bef\u8bbe\u9c81\u68d2\u6027\u548c\u6e10\u8fd1\u6b63\u6001\u6027\u7b49\u4f18\u70b9\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u9009\u62e9\u03b1\u53c2\u6570\u4ee5\u53ca\u76f8\u5e94\u7684\u7edf\u8ba1\u4fdd\u8bc1\u4ecd\u662f\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u9009\u62e9\u03b1\u53c2\u6570\uff0c\u5206\u6790\u5176\u6e10\u8fd1\u884c\u4e3a\u6a21\u5f0f\uff08\u8d8b\u4e8e\u96f6\u6216\u6df7\u5408\u5206\u5e03\u5f62\u5f0f\uff09\uff0c\u5efa\u7acb\u65b0\u7684\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u4f5c\u4e3a\u5173\u952e\u6280\u672f\u5de5\u5177\uff0c\u63a8\u5bfc\u540e\u9a8c\u77e9\u4e00\u81f4\u6027\u548c\u540e\u9a8c\u5747\u503c\u6e10\u8fd1\u6b63\u6001\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u53d1\u73b0\u4e86\u4e34\u754c\u9608\u503c\u03b1\u221d1/\u221an\uff0c\u5728\u6b64\u9608\u503c\u5904\u540e\u9a8c\u5747\u503c\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u4f1a\u5931\u6548\uff1b\u5efa\u7acb\u4e86\u540e\u9a8c\u77e9\u4e00\u81f4\u6027\u548c\u540e\u9a8c\u5747\u503c\u6e10\u8fd1\u6b63\u6001\u6027\u7684\u5145\u5206\u6761\u4ef6\uff1b\u5141\u8bb8\u03b1\u4ee5\u4efb\u610f\u65b9\u5f0f\u4f9d\u8d56\u4e8e\u6570\u636e\u3002", "conclusion": "\u540e\u9a8c\u6e29\u5ea6\u8c03\u8282\u7684\u6e10\u8fd1\u6027\u8d28\u53d6\u51b3\u4e8e\u03b1\u7684\u9009\u62e9\uff0c\u5b58\u5728\u4e34\u754c\u9608\u503c\u5f71\u54cd\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u9009\u62e9\u03b1\u53c2\u6570\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.09019", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.09019", "abs": "https://arxiv.org/abs/2601.09019", "authors": ["Nawaf Bou-Rabee", "Siddharth Mitra", "Andre Wibisono"], "title": "Tail-Sensitive KL and R\u00e9nyi Convergence of Unadjusted Hamiltonian Monte Carlo via One-Shot Couplings", "comment": "64 pages", "summary": "Hamiltonian Monte Carlo (HMC) algorithms are among the most widely used sampling methods in high dimensional settings, yet their convergence properties are poorly understood in divergences that quantify relative density mismatch, such as Kullback-Leibler (KL) and R\u00e9nyi divergences. These divergences naturally govern acceptance probabilities and warm-start requirements for Metropolis-adjusted Markov chains. In this work, we develop a framework for upgrading Wasserstein convergence guarantees for unadjusted Hamiltonian Monte Carlo (uHMC) to guarantees in tail-sensitive KL and R\u00e9nyi divergences. Our approach is based on one-shot couplings, which we use to establish a regularization property of the uHMC transition kernel. This regularization allows Wasserstein-2 mixing-time and asymptotic bias bounds to be lifted to KL divergence, and analogous Orlicz-Wasserstein bounds to be lifted to R\u00e9nyi divergence, paralleling earlier work of Bou-Rabee and Eberle (2023) that upgrade Wasserstein-1 bounds to total variation distance via kernel smoothing. As a consequence, our results provide quantitative control of relative density mismatch, clarify the role of discretization bias in strong divergences, and yield principled guarantees relevant both for unadjusted sampling and for generating warm starts for Metropolis-adjusted Markov chains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u7b97\u6cd5\u7684Wasserstein\u6536\u655b\u4fdd\u8bc1\u63d0\u5347\u5230KL\u6563\u5ea6\u548cR\u00e9nyi\u6563\u5ea6\u4fdd\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u76f8\u5bf9\u5bc6\u5ea6\u4e0d\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1HMC\u7b97\u6cd5\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u91cf\u5316\u76f8\u5bf9\u5bc6\u5ea6\u4e0d\u5339\u914d\u7684\u6563\u5ea6\uff08\u5982KL\u6563\u5ea6\u548cR\u00e9nyi\u6563\u5ea6\uff09\u4e2d\u7684\u6536\u655b\u6027\u8d28\u7814\u7a76\u4e0d\u8db3\u3002\u8fd9\u4e9b\u6563\u5ea6\u81ea\u7136\u5730\u63a7\u5236\u7740Metropolis\u8c03\u6574\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u63a5\u53d7\u6982\u7387\u548c\u9884\u70ed\u542f\u52a8\u8981\u6c42\u3002", "method": "\u57fa\u4e8e\u5355\u6b21\u8026\u5408\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u672a\u8c03\u6574\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08uHMC\uff09\u8f6c\u79fb\u6838\u7684\u6b63\u5219\u5316\u6027\u8d28\u3002\u8be5\u6b63\u5219\u5316\u5141\u8bb8\u5c06Wasserstein-2\u6df7\u5408\u65f6\u95f4\u548c\u6e10\u8fd1\u504f\u5dee\u754c\u9650\u63d0\u5347\u5230KL\u6563\u5ea6\uff0c\u5e76\u5c06\u7c7b\u4f3c\u7684Orlicz-Wasserstein\u754c\u9650\u63d0\u5347\u5230R\u00e9nyi\u6563\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u76f8\u5bf9\u5bc6\u5ea6\u4e0d\u5339\u914d\u7684\u5b9a\u91cf\u63a7\u5236\uff0c\u9610\u660e\u4e86\u79bb\u6563\u5316\u504f\u5dee\u5728\u5f3a\u6563\u5ea6\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e3a\u672a\u8c03\u6574\u91c7\u6837\u548c\u751f\u6210Metropolis\u8c03\u6574\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u9884\u70ed\u542f\u52a8\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5efa\u7acbuHMC\u7684\u6b63\u5219\u5316\u6027\u8d28\uff0c\u6210\u529f\u5c06Wasserstein\u6536\u655b\u4fdd\u8bc1\u63d0\u5347\u5230KL\u548cR\u00e9nyi\u6563\u5ea6\uff0c\u4e3aHMC\u7b97\u6cd5\u5728\u76f8\u5bf9\u5bc6\u5ea6\u5ea6\u91cf\u4e0b\u7684\u6536\u655b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.09043", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09043", "abs": "https://arxiv.org/abs/2601.09043", "authors": ["Nick Polson", "Vadim Sokolov"], "title": "Horseshoe Mixtures-of-Experts (HS-MoE)", "comment": null, "summary": "Horseshoe mixtures-of-experts (HS-MoE) models provide a Bayesian framework for sparse expert selection in mixture-of-experts architectures. We combine the horseshoe prior's adaptive global-local shrinkage with input-dependent gating, yielding data-adaptive sparsity in expert usage. Our primary methodological contribution is a particle learning algorithm for sequential inference, in which the filter is propagated forward in time while tracking only sufficient statistics. We also discuss how HS-MoE relates to modern mixture-of-experts layers in large language models, which are deployed under extreme sparsity constraints (e.g., activating a small number of experts per token out of a large pool).", "AI": {"tldr": "HS-MoE\u6a21\u578b\u7ed3\u5408\u9a6c\u8e44\u5148\u9a8c\u4e0e\u8f93\u5165\u4f9d\u8d56\u95e8\u63a7\uff0c\u5b9e\u73b0\u4e13\u5bb6\u9009\u62e9\u7684\u6570\u636e\u81ea\u9002\u5e94\u7a00\u758f\u6027\uff0c\u63d0\u51fa\u7c92\u5b50\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5e8f\u5217\u63a8\u65ad\uff0c\u5e76\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684MoE\u5c42\u5efa\u7acb\u8054\u7cfb", "motivation": "\u4e3a\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u4f9b\u8d1d\u53f6\u65af\u7a00\u758f\u4e13\u5bb6\u9009\u62e9\u6846\u67b6\uff0c\u5c06\u9a6c\u8e44\u5148\u9a8c\u7684\u81ea\u9002\u5e94\u5168\u5c40-\u5c40\u90e8\u6536\u7f29\u4e0e\u8f93\u5165\u4f9d\u8d56\u95e8\u63a7\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u6570\u636e\u81ea\u9002\u5e94\u7684\u4e13\u5bb6\u4f7f\u7528\u7a00\u758f\u6027", "method": "\u63d0\u51fa\u7c92\u5b50\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5e8f\u5217\u63a8\u65ad\uff0c\u6ee4\u6ce2\u5668\u968f\u65f6\u95f4\u5411\u524d\u4f20\u64ad\uff0c\u4ec5\u8ddf\u8e2a\u5145\u5206\u7edf\u8ba1\u91cf\uff1b\u7ed3\u5408\u9a6c\u8e44\u5148\u9a8c\u7684\u5168\u5c40-\u5c40\u90e8\u6536\u7f29\u7279\u6027\u4e0e\u8f93\u5165\u4f9d\u8d56\u95e8\u63a7\u673a\u5236", "result": "\u5efa\u7acb\u4e86HS-MoE\u6a21\u578b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u81ea\u9002\u5e94\u7684\u4e13\u5bb6\u9009\u62e9\u7a00\u758f\u6027\uff0c\u4e3a\u5e8f\u5217\u63a8\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7c92\u5b50\u5b66\u4e60\u7b97\u6cd5", "conclusion": "HS-MoE\u4e3a\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d1d\u53f6\u65af\u7a00\u758f\u9009\u62e9\u65b9\u6cd5\uff0c\u5176\u7c92\u5b50\u5b66\u4e60\u7b97\u6cd5\u652f\u6301\u5e8f\u5217\u63a8\u65ad\uff0c\u5e76\u4e0e\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684MoE\u5c42\u5728\u6781\u7aef\u7a00\u758f\u7ea6\u675f\u4e0b\u5177\u6709\u76f8\u5173\u6027"}}
