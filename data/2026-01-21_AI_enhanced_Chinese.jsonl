{"id": "2601.11568", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11568", "abs": "https://arxiv.org/abs/2601.11568", "authors": ["Quang-Hung Bui", "Anh Son Ta"], "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control", "comment": null, "summary": "Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($\u03c1$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $\u03c1$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.", "AI": {"tldr": "AdaFRUGAL\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u68af\u5ea6\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b50\u7a7a\u95f4\u6bd4\u4f8b\u548c\u66f4\u65b0\u9891\u7387\uff0c\u81ea\u52a8\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u76f8\u6bd4\u9759\u6001FRUGAL\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u73b0\u6709FRUGAL\u6846\u67b6\u7684\u9759\u6001\u8d85\u53c2\u6570\uff08\u5b50\u7a7a\u95f4\u6bd4\u4f8b\u03c1\u548c\u66f4\u65b0\u9891\u7387T\uff09\u9700\u8981\u6602\u8d35\u7684\u624b\u52a8\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faAdaFRUGAL\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u52a8\u6001\u63a7\u5236\u673a\u5236\uff1a(1) \u5b50\u7a7a\u95f4\u6bd4\u4f8b\u03c1\u7684\u7ebf\u6027\u8870\u51cf\u7b56\u7565\uff0c\u9010\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff1b(2) \u57fa\u4e8e\u635f\u5931\u611f\u77e5\u7684\u66f4\u65b0\u9891\u7387T\u8c03\u5ea6\u7b56\u7565\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u82f1\u8bedC4\u3001\u8d8a\u5357\u8bedVietVault\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548cGLUE\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0cAdaFRUGAL\u5728\u4fdd\u6301\u4e0eAdamW\u548c\u9759\u6001FRUGAL\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86GPU\u5185\u5b58\u5360\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "AdaFRUGAL\u5b9e\u73b0\u4e86\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u3001\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11572", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11572", "abs": "https://arxiv.org/abs/2601.11572", "authors": ["Timo Aukusti Laine"], "title": "Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces", "comment": "23 pages, 5 figures", "summary": "We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.", "AI": {"tldr": "\u4f7f\u7528\u91cf\u5b50\u529b\u5b66\u542f\u53d1\u7684\u6570\u5b66\u5de5\u5177\uff08\u7ebf\u6027\u4ee3\u6570\u3001\u54c8\u5bc6\u987f\u5f62\u5f0f\uff09\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u53d1\u73b0L2\u5f52\u4e00\u5316\u7ea6\u675f\u4f7f\u5d4c\u5165\u7a7a\u95f4\u9002\u5408\u54c8\u5bc6\u987f\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u5173\u7cfb\uff0c\u63a2\u7d22\u4e86\u8bed\u4e49\u8f6c\u6362\u548c\u91cf\u5b50\u7c7b\u6bd4\u3002", "motivation": "\u89c2\u5bdf\u5230LLM\u5d4c\u5165\u8868\u73b0\u51fa\u79bb\u6563\u8bed\u4e49\u72b6\u6001\uff0c\u8868\u660e\u5b58\u5728\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u793a\uff0c\u9700\u8981\u6570\u5b66\u5de5\u5177\u6765\u5206\u6790\u8fd9\u4e9b\u8bed\u4e49\u5173\u7cfb\u3002\u53d7\u5230\u91cf\u5b50\u529b\u5b66\u7cfb\u7edf\u7c7b\u6bd4\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5e94\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u6765\u7406\u89e3LLM\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u3002", "method": "\u5e94\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\uff0c\u7279\u522b\u5173\u6ce8L2\u5f52\u4e00\u5316\u7ea6\u675f\u7684\u7ed3\u6784\u5316\u6548\u5e94\u3002\u63a8\u5bfc\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5d4c\u5165\u5411\u91cf\u6270\u52a8\u4e4b\u95f4\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u63a2\u7d22\u76f4\u63a5\u548c\u95f4\u63a5\u8bed\u4e49\u8f6c\u6362\u3002\u91c7\u7528\u91cf\u5b50\u542f\u53d1\u7684\u89c6\u89d2\uff0c\u63a8\u5bfc\u96f6\u70b9\u80fd\u91cf\u7c7b\u6bd4\u5e76\u8ba8\u8bba\u4e0eKoopman-von Neumann\u529b\u5b66\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "result": "L2\u5f52\u4e00\u5316\u7ea6\u675f\u4f7fLLM\u5d4c\u5165\u7a7a\u95f4\u5177\u6709\u9002\u5408\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790\u7684\u7ed3\u6784\u3002\u5efa\u7acb\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u80fd\u591f\u5206\u6790\u8bed\u4e49\u8f6c\u6362\u3002\u91cf\u5b50\u7c7b\u6bd4\u63d0\u4f9b\u4e86\u5bf9\u5d4c\u5165\u7a7a\u95f4\u7684\u65b0\u89c6\u89d2\uff0c\u5305\u62ec\u96f6\u70b9\u80fd\u91cf\u7c7b\u6bd4\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u6df1\u5165\u7406\u89e3LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u53ef\u80fd\u4e3a\u51cf\u8f7b\u5e7b\u89c9\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002\u867d\u7136\u89e3\u91ca\u9700\u8981\u8c28\u614e\uff0c\u4f46\u6570\u5b66\u6846\u67b6\u663e\u793a\u51fa\u5206\u6790LLM\u8bed\u4e49\u8868\u793a\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.11574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11574", "abs": "https://arxiv.org/abs/2601.11574", "authors": ["Lukas Abrie Nel"], "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.", "AI": {"tldr": "GRADE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGumbel-softmax\u677e\u5f1b\u7684LLM\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684token\u91c7\u6837\u8fc7\u7a0b\u76f4\u63a5\u53cd\u5411\u4f20\u64ad\uff0c\u907f\u514d\u4e86PPO\u7b49\u9ad8\u65b9\u5dee\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u5df2\u6210\u4e3a\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46PPO\u7b49\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b58\u5728\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u9ad8\u3001\u9700\u8981\u7cbe\u7ec6\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "GRADE\u4f7f\u7528Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u914d\u5408\u76f4\u901a\u4f30\u8ba1\uff08GRADE-STE\uff09\uff0c\u901a\u8fc7\u79bb\u6563token\u91c7\u6837\u8fc7\u7a0b\u7684\u53ef\u5fae\u5206\u677e\u5f1b\u5b9e\u73b0\u7aef\u5230\u7aef\u68af\u5ea6\u6d41\u52a8\uff0c\u4ece\u5956\u52b1\u4fe1\u53f7\u901a\u8fc7\u751f\u6210token\u76f4\u63a5\u4f20\u64ad\u5230\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728IMDB\u6570\u636e\u96c6\u7684\u60c5\u611f\u63a7\u5236\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGRADE-STE\u6d4b\u8bd5\u5956\u52b1\u8fbe\u52300.763\u00b10.344\uff0c\u4f18\u4e8ePPO\u76840.510\u00b10.313\u548cREINFORCE\u76840.617\u00b10.378\uff0c\u76f8\u5bf9PPO\u63d0\u534750%\u3002\u68af\u5ea6\u65b9\u5dee\u6bd4REINFORCE\u4f4e14\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u52a8\u6001\u7a33\u5b9a\u3002", "conclusion": "GRADE\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2601.11604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11604", "abs": "https://arxiv.org/abs/2601.11604", "authors": ["Jonaid Shianifar", "Michael Schukat", "Karl Mason"], "title": "Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning", "comment": null, "summary": "Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\\!\\pm\\!125$ to $1613\\!\\pm\\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.", "AI": {"tldr": "Hindsight Preference Replay (HPR) \u662f\u4e00\u79cd\u7b80\u5355\u7684\u56de\u653e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u4e3a\u5b58\u50a8\u7684\u8f6c\u79fb\u6570\u636e\u91cd\u65b0\u6807\u8bb0\u66ff\u4ee3\u504f\u597d\uff0c\u5728\u4e0d\u6539\u53d8CAPQL\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u589e\u52a0\u4e86\u8de8\u504f\u597d\u5355\u7eaf\u5f62\u7684\u76d1\u7763\u5bc6\u5ea6\uff0c\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "CAPQL\u65b9\u6cd5\u5728\u4f18\u5316\u5411\u91cf\u5316\u5956\u52b1\u65f6\uff0c\u4ec5\u4f7f\u7528\u7279\u5b9a\u504f\u597d\u4e0b\u6536\u96c6\u7684\u6570\u636e\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u504f\u597d\u7684\u79bb\u7ebf\u6570\u636e\u3002\u8fd9\u5bfc\u81f4\u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u8de8\u504f\u597d\u7a7a\u95f4\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faHindsight Preference Replay (HPR)\u7b56\u7565\uff0c\u5bf9\u5b58\u50a8\u7684\u8f6c\u79fb\u6570\u636e\u8fdb\u884c\u91cd\u65b0\u6807\u8bb0\uff0c\u8d4b\u4e88\u5176\u66ff\u4ee3\u504f\u597d\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301CAPQL\u7684actor-critic\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\u4e0d\u53d8\uff0c\u4ec5\u901a\u8fc7\u56de\u653e\u7f13\u51b2\u533a\u7684\u6570\u636e\u589e\u5f3a\u6765\u589e\u52a0\u8de8\u504f\u597d\u5355\u7eaf\u5f62\u7684\u76d1\u7763\u5bc6\u5ea6\u3002", "result": "\u5728\u516d\u4e2aMO-Gymnasium\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u56fa\u5b9a300000\u6b65\u9884\u7b97\u8bc4\u4f30\uff0cHPR-CAPQL\u5728\u4e94\u4e2a\u73af\u5883\u4e2d\u63d0\u5347\u4e86\u8d85\u4f53\u79ef(HV)\uff0c\u5728\u56db\u4e2a\u73af\u5883\u4e2d\u63d0\u5347\u4e86\u671f\u671b\u6548\u7528(EUM)\u3002\u4f8b\u5982\u5728mo-humanoid-v5\u4e2d\uff0cEUM\u4ece323\u00b1125\u63d0\u5347\u52301613\u00b1464\uff0cHV\u4ece0.52M\u63d0\u5347\u52309.63M\uff0c\u7edf\u8ba1\u663e\u8457\u6027\u652f\u6301\u3002mo-halfcheetah-v5\u662f\u4f8b\u5916\uff0cCAPQL\u5728\u53ef\u6bd4EUM\u4e0b\u83b7\u5f97\u66f4\u9ad8HV\u3002", "conclusion": "HPR\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u56de\u653e\u589e\u5f3a\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u91cd\u65b0\u6807\u8bb0\u5386\u53f2\u6570\u636e\u6765\u589e\u52a0\u8de8\u504f\u597d\u7a7a\u95f4\u7684\u76d1\u7763\u5bc6\u5ea6\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u7b97\u6cd5\u67b6\u6784\u3002"}}
{"id": "2601.11606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11606", "abs": "https://arxiv.org/abs/2601.11606", "authors": ["Farzana Islam Adiba", "Varsha Danduri", "Fahmida Liza Piya", "Ali Abbasi", "Mehak Gupta", "Rahmatollah Beheshti"], "title": "A Multimodal Data Processing Pipeline for MIMIC-IV Dataset", "comment": null, "summary": "The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.", "code_url": "https://github.com/healthylaife/MIMIC-IV-Data-Pipeline", "code_stars": 291, "code_last_update": "2026-01-08", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001MIMIC-IV\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u53ef\u81ea\u52a8\u5316\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u3001\u4e34\u5e8a\u8bb0\u5f55\u3001\u6ce2\u5f62\u548c\u5f71\u50cf\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u5e76\u63d0\u9ad8\u7814\u7a76\u53ef\u91cd\u590d\u6027\u3002", "motivation": "MIMIC-IV\u6570\u636e\u96c6\u5305\u542b\u591a\u79cd\u6a21\u6001\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u4f46\u73b0\u6709\u5904\u7406\u7ba1\u9053\u8981\u4e48\u53ea\u9488\u5bf9\u5c11\u6570\u6a21\u6001\uff0c\u8981\u4e48\u4e0d\u652f\u6301\u4efb\u610f\u7684\u4e0b\u6e38\u5e94\u7528\uff0c\u9700\u8981\u5927\u91cf\u624b\u52a8\u9884\u5904\u7406\u548c\u5bf9\u9f50\u5de5\u4f5c\u3002", "method": "\u6269\u5c55\u4e86\u5148\u524d\u7684\u5355\u6a21\u6001\u7ba1\u9053\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u5b9a\u5236\u7684\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u7cfb\u7edf\u6574\u5408\u6240\u6709\u6a21\u6001\uff0c\u652f\u6301\u81ea\u52a8\u5316\u961f\u5217\u9009\u62e9\u3001\u8de8\u6a21\u6001\u65f6\u95f4\u5bf9\u9f50\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u591a\u6a21\u6001\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u4ee3\u7801\u3001\u7b80\u5355UI\u548cPython\u5305\uff0c\u652f\u6301\u9009\u62e9\u6027\u96c6\u6210\uff08\u5305\u62ec\u5d4c\u5165\uff09\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u65f6\u95f4\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8eMIMIC\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u7ba1\u9053\u4e3aMIMIC-IV\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u53ef\u5b9a\u5236\u7684\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u652f\u6301\u4efb\u610f\u7684\u9759\u6001\u548c\u65f6\u95f4\u5e8f\u5217\u4e0b\u6e38\u5e94\u7528\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2601.11609", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11609", "abs": "https://arxiv.org/abs/2601.11609", "authors": ["Weinuo Ou"], "title": "Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction", "comment": "9 pages, 7 figures", "summary": "Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).", "AI": {"tldr": "\u63d0\u51faApCM\u6a21\u578b\uff0c\u4e00\u79cd\u795e\u7ecf\u8bb0\u5fc6\u5b58\u50a8\u67b6\u6784\uff0c\u89e3\u51b3LLMs\u7f3a\u4e4f\u6709\u6548\u8fd0\u884c\u65f6\u8bb0\u5fc6\u673a\u5236\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u666e\u904d\u7f3a\u4e4f\u6709\u6548\u7684\u8fd0\u884c\u65f6\u8bb0\u5fc6\u673a\u5236\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u4ea4\u4e92\u9700\u6c42", "method": "\u63d0\u51fa\u8f85\u52a9\u9884\u6d4b\u538b\u7f29\u8bb0\u5fc6\u6a21\u578b\uff08ApCM\u6a21\u578b\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u8bb0\u5fc6\u5b58\u50a8\u67b6\u6784", "result": "\u4ece\u6458\u8981\u4e2d\u65e0\u6cd5\u83b7\u53d6\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u4fe1\u606f", "conclusion": "ApCM\u6a21\u578b\u65e8\u5728\u89e3\u51b3LLMs\u5728\u52a8\u6001\u4e2a\u6027\u5316\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\u4e0d\u8db3\u95ee\u9898"}}
{"id": "2601.11611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11611", "abs": "https://arxiv.org/abs/2601.11611", "authors": ["Marina Vicini", "Martin Rudorfer", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home", "comment": "Accepted to International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI) 2024", "summary": "With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u88ab\u52a8\u4f20\u611f\u5668\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u805a\u7c7b\u548c\u5faa\u73af\u65f6\u95f4\u7279\u5f81\u589e\u5f3a\u6765\u63d0\u5347\u8001\u5e74\u4eba\u667a\u80fd\u5bb6\u5c45\u76d1\u63a7\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u9700\u8981\u8ba9\u8001\u5e74\u4eba\u80fd\u591f\u5728\u5bb6\u5ead\u4e2d\u72ec\u7acb\u5b89\u5168\u5730\u751f\u6d3b\u3002\u4f7f\u7528\u88ab\u52a8\u7ea2\u5916\u4f20\u611f\u5668\u548c\u95e8\u4f20\u611f\u5668\u7b49\u666e\u904d\u5b58\u5728\u7684\u4f20\u611f\u5668\u6765\u76d1\u6d4b\u65e5\u5e38\u6d3b\u52a8\u5e76\u4fc3\u8fdb\u9884\u9632\u6027\u533b\u7597\u5e72\u9884\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u5728\u6709\u6548\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "1. \u5c06\u6d3b\u52a8\u6309\u65f6\u95f4\u805a\u7c7b\u4e3a\u65e9\u6668\u3001\u4e0b\u5348\u548c\u591c\u665a\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6bb5\u8ba1\u7b97\u4e0d\u540c\u7684\u4e92\u4fe1\u606f\u77e9\u9635\u8fdb\u884c\u7279\u5f81\u52a0\u6743\uff1b2. \u5728\u7279\u5f81\u5411\u91cf\u4e2d\u52a0\u5165\u4e86\u65f6\u95f4\uff08\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\u548c\u4e00\u5468\u4e2d\u7684\u5929\uff09\u4f5c\u4e3a\u5faa\u73af\u65f6\u95f4\u7279\u5f81\uff1b3. \u6dfb\u52a0\u4e86\u7528\u6237\u4f4d\u7f6e\u8ddf\u8e2a\u7279\u5f81\uff1b4. \u6269\u5c55\u4e86\u4f20\u611f\u5668\u52a0\u6743\u4e92\u4fe1\u606f\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u7684\u4e09\u4e2a\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548cF1\u5206\u6570\uff0c\u5728\u6570\u636e\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6700\u4e3a\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f00\u53d1\u6709\u6548\u667a\u80fd\u5bb6\u5c45\u89e3\u51b3\u65b9\u6848\u4ee5\u652f\u6301\u8001\u5e74\u4eba\u5c45\u5bb6\u517b\u8001\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u6574\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\u6765\u6539\u8fdb\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2601.11615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11615", "abs": "https://arxiv.org/abs/2601.11615", "authors": ["Beyza Cinar", "Louisa van den Boom", "Maria Maleshkova"], "title": "A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia", "comment": null, "summary": "Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.", "AI": {"tldr": "\u7efc\u8ff0\u5206\u6790\u4e86\u57fa\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u4f4e\u8840\u7cd6\u65b9\u9762\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u548c\u6a21\u578b\u7c7b\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u9700\u8981\u7ec8\u8eab\u80f0\u5c9b\u7d20\u6cbb\u7597\uff0c\u4f46\u80f0\u5c9b\u7d20\u6cbb\u7597\u6709\u5bfc\u81f4\u4f4e\u8840\u7cd6\u7684\u526f\u4f5c\u7528\u3002\u4f4e\u8840\u7cd6\uff08\u8840\u7cd6\u4f4e\u4e8e70 mg/dL\uff09\u662f\u5371\u53ca\u751f\u547d\u7684\u72b6\u6001\uff0c\u4e0e\u6b7b\u4ea1\u7387\u589e\u52a0\u76f8\u5173\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u9884\u6d4b\u4f4e\u8840\u7cd6\u5e76\u63d0\u4f9b\u6700\u4f73\u9884\u9632\u65b9\u6cd5\u6765\u6539\u5584\u7cd6\u5c3f\u75c5\u7ba1\u7406\u3002", "method": "\u672c\u6587\u662f\u7efc\u8ff0\u7814\u7a76\uff0c\u8c03\u67e5\u4e86\u57fa\u4e8e1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u8bad\u7ec3\u7684\u6700\u65b0\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u6bd4\u8f83\u4e86\u56de\u5f52\u6a21\u578b\uff08\u9884\u6d4b\u8840\u7cd6\u6c34\u5e73\uff09\u548c\u5206\u7c7b\u6a21\u578b\uff08\u57fa\u4e8e\u5b9a\u4e49\u6807\u7b7e\u8bc6\u522b\u4e8b\u4ef6\uff09\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\uff08\u77ed\u671f15-120\u5206\u949f\uff0c\u957f\u671f3-24\u5c0f\u65f6\u4ee5\u4e0a\uff09\u7684\u6027\u80fd\u8868\u73b0\u3002\u7279\u522b\u63a2\u8ba8\u4e86\u56db\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9884\u6d4b\u63d0\u524d\u65f6\u95f4\u3001\u6700\u4f73\u6a21\u578b\u3001\u5f71\u54cd\u56e0\u7d20\u4ee5\u53ca\u4e2a\u6027\u5316\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u6700\u591a1\u5c0f\u65f6\u7684\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u63d0\u4f9b\u6700\u4f73\u7ed3\u679c\uff1b2\uff09\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5355\u4e00\u6a21\u578b\u65e0\u6cd5\u5728\u591a\u4e2a\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u4e0a\u5145\u5206\u5206\u7c7b\uff1b3\uff09\u6a21\u578b\u6027\u80fd\u53d7\u591a\u53d8\u91cf\u6570\u636e\u96c6\u548c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u7684\u5f71\u54cd\uff1b4\uff09\u4e2a\u6027\u5316\u6570\u636e\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u8d28\u91cf\u6709\u9650\uff0c\u57fa\u4e8e\u4eba\u7fa4\u7684\u6a21\u578b\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u4f4e\u8840\u7cd6\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u9650\u5236\u3001\u6a21\u578b\u9009\u62e9\u4f9d\u8d56\u4efb\u52a1\u7c7b\u578b\u3001\u6570\u636e\u8d28\u91cf\u548c\u4e2a\u6027\u5316\u6311\u6218\u7b49\u95ee\u9898\u3002\u672a\u6765\u7814\u7a76\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4ee5\u5f00\u53d1\u66f4\u6709\u6548\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2601.11616", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11616", "abs": "https://arxiv.org/abs/2601.11616", "authors": ["Feilong Liu"], "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.", "AI": {"tldr": "MoE\u67b6\u6784\u901a\u8fc7\u8def\u7531\u673a\u5236\u5c06\u8868\u793a\u7a7a\u95f4\u8f6f\u5212\u5206\u4e3a\u91cd\u53e0\u7684\u5c40\u90e8\u533a\u57df\uff0c\u964d\u4f4e\u5c40\u90e8\u654f\u611f\u5ea6\u5e76\u91cd\u65b0\u5206\u914d\u8868\u793a\u65b9\u5dee\uff0c\u5f62\u6210\u4e13\u5bb6\u7279\u5b9a\u7684\u4f4e\u91cd\u53e0\u5b50\u7a7a\u95f4\u5206\u89e3\u3002", "motivation": "\u7814\u7a76MoE\u67b6\u6784\u5bf9\u5b66\u4e60\u51fd\u6570\u548c\u8868\u793a\u51e0\u4f55\u6027\u8d28\u7684\u5f71\u54cd\uff0c\u7406\u89e3\u8def\u7531\u673a\u5236\u5982\u4f55\u4f5c\u4e3a\u8868\u793a\u7a7a\u95f4\u7684\u8f6f\u5212\u5206\u65b9\u5f0f\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5212\u5206\u5982\u4f55\u5f71\u54cd\u5c40\u90e8\u654f\u611f\u5ea6\u548c\u8868\u793a\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u53cc\u96c5\u53ef\u6bd4-PCA\u8c31\u51e0\u4f55\u63a2\u9488\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u5947\u5f02\u503c\u8c31\u5206\u6790\u5c40\u90e8\u51fd\u6570\u51e0\u4f55\uff0c\u901a\u8fc7\u52a0\u6743PCA\u5206\u6790\u8def\u7531\u9690\u85cf\u72b6\u6001\u7684\u8868\u793a\u51e0\u4f55\u3002\u5728\u53ef\u63a7\u7684MLP-MoE\u8bbe\u7f6e\u4e2d\u6bd4\u8f83\u5bc6\u96c6\u3001Top-k\u548c\u5b8c\u5168\u8f6f\u8def\u7531\u67b6\u6784\uff0c\u5206\u6790\u8def\u7531\u9510\u5ea6\u5982\u4f55\u8c03\u8282\u51e0\u4f55\u6548\u5e94\u3002", "result": "MoE\u8def\u7531\u4e00\u81f4\u964d\u4f4e\u5c40\u90e8\u654f\u611f\u5ea6\uff0c\u4e13\u5bb6\u5c40\u90e8\u96c5\u53ef\u6bd4\u77e9\u9635\u663e\u793a\u66f4\u5c0f\u7684\u4e3b\u5bfc\u5947\u5f02\u503c\u548c\u66f4\u5feb\u7684\u8c31\u8870\u51cf\u3002\u52a0\u6743PCA\u663e\u793a\u4e13\u5bb6\u5c40\u90e8\u8868\u793a\u5728\u66f4\u591a\u4e3b\u65b9\u5411\u4e0a\u5206\u5e03\u65b9\u5dee\uff0c\u8868\u660e\u5728\u76f8\u540c\u8f93\u5165\u5206\u5e03\u4e0b\u5177\u6709\u66f4\u9ad8\u7684\u6709\u6548\u79e9\u3002\u5e73\u5747\u4e13\u5bb6\u96c5\u53ef\u6bd4\u77e9\u9635\u51e0\u4e4e\u6b63\u4ea4\uff0c\u8868\u660e\u53d8\u6362\u5206\u89e3\u4e3a\u4f4e\u91cd\u53e0\u7684\u4e13\u5bb6\u7279\u5b9a\u5b50\u7a7a\u95f4\u800c\u975e\u5171\u4eab\u6620\u5c04\u7684\u7f29\u653e\u53d8\u4f53\u3002", "conclusion": "MoE\u53ef\u4ee5\u51e0\u4f55\u89e3\u91ca\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u8f6f\u5212\u5206\uff0c\u8fd9\u79cd\u5212\u5206\u5728\u5e73\u5766\u5316\u5c40\u90e8\u66f2\u7387\u7684\u540c\u65f6\u91cd\u65b0\u5206\u914d\u8868\u793a\u65b9\u5dee\u3002Top-k\u8def\u7531\u4ea7\u751f\u66f4\u4f4e\u79e9\u3001\u66f4\u96c6\u4e2d\u7684\u4e13\u5bb6\u5c40\u90e8\u7ed3\u6784\uff0c\u800c\u5b8c\u5168\u8f6f\u8def\u7531\u4ea7\u751f\u66f4\u5e7f\u6cdb\u3001\u66f4\u9ad8\u79e9\u7684\u8868\u793a\u3002"}}
{"id": "2601.11618", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11618", "abs": "https://arxiv.org/abs/2601.11618", "authors": ["Luis Rosario Freytes"], "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention", "comment": "57 pages", "summary": "Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.", "AI": {"tldr": "\u51e0\u4f55\u6ce8\u610f\u529b\uff08GA\uff09\u901a\u8fc7\u56db\u4e2a\u72ec\u7acb\u8f93\u5165\u5b9a\u4e49\u6ce8\u610f\u529b\u5c42\uff1a\u6709\u9650\u8f7d\u4f53\u3001\u8bc1\u636e\u6838\u89c4\u5219\u3001\u63a2\u9488\u65cf\u548c\u951a\u70b9/\u66f4\u65b0\u89c4\u5219\uff0c\u5c06\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5efa\u6a21\u9009\u62e9\u5206\u79bb\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982Transformer\u4e2d\u7684softmax\uff09\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u96be\u4ee5\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u548c\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u51e0\u4f55\u6ce8\u610f\u529b\u7406\u8bba\uff0c\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5206\u89e3\u4e3a\u4e0d\u53d8\u7ed3\u6784\u548c\u53ef\u914d\u7f6e\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7406\u8bba\u5206\u6790\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6ce8\u610f\u529b\uff08GA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ec4\u4ef6\u5b9a\u4e49\u6ce8\u610f\u529b\u5c42\uff1a1\uff09\u6709\u9650\u8f7d\u4f53\uff08\u53ef\u5bfb\u5740\u7d22\u5f15\u96c6\uff09\uff1b2\uff09\u8bc1\u636e\u6838\u89c4\u5219\uff08\u63a9\u7801\u539f\u59cb\u5206\u6570\u4e0e\u94fe\u63a5\u4ea7\u751f\u975e\u8d1f\u6743\u91cd\uff09\uff1b3\uff09\u63a2\u9488\u65cf\uff08\u53ef\u89c2\u6d4b\u91cf\u7684\u5bb9\u8bb8\u96c6\uff09\uff1b4\uff09\u951a\u70b9/\u66f4\u65b0\u89c4\u5219\uff08\u6838\u4ee3\u8868\u9009\u62e9\u548c\u66f4\u65b0\u65b9\u5f0f\uff09\u3002\u63a2\u9488\u65cf\u8bf1\u5bfc\u6838\u4e0a\u7684\u64cd\u4f5c\u7b49\u4ef7\u5173\u7cfb\u548c\u89c4\u8303\uff0c\u951a\u70b9\u76f8\u5bf9\u4e8e\u63a2\u9488\u9009\u62e9\u4ee3\u8868\u3002\u5728\u6807\u91cf\u5173\u7cfb\u5de5\u4f5c\u8868\u793a\u548c\u8bc1\u636e\u4e58\u6cd5\u7ec4\u5408\u5f8b\u4e0b\uff0c\u5bb9\u8bb8\u94fe\u63a5\u65cf\u4e3a\u6307\u6570\u65cf\uff0c\u4ea7\u751fGibbs\u6743\u91cd\uff1b\u884c\u951a\u70b9\u5305\u542bsoftmax\u6838\u65cf\u4f5c\u4e3a\u5b50\u673a\u5236\u3002\u901a\u8fc7\u5546\u5316\u4e00\u5143\u884c/\u5217\u5206\u6570\u573a\uff0c\u5269\u4f59\u4ea4\u4e92\u5206\u91cf\u5177\u6709\u89c4\u8303\u79e9r\u6b63\u89c4\u5f62\u5f0f\uff08Eckart-Young/SVD\uff09\uff1b\u70b9\u79ef\u5206\u6570\u56fe\u5b9e\u73b0\u76f8\u5e94\u7684\u4f4e\u79e9\u4ea4\u4e92\u673a\u5236\u3002", "result": "\u51e0\u4f55\u6ce8\u610f\u529b\u6846\u67b6\u80fd\u591f\uff1a1\uff09\u7edf\u4e00\u63cf\u8ff0\u6807\u51c6Transformer\u6ce8\u610f\u529b\uff08\u56fa\u5b9a\u8f7d\u4f53\u548c\u6269\u5c55\u66f4\u65b0\uff09\uff1b2\uff09\u652f\u6301\u81ea\u9002\u5e94\u8f7d\u4f53\u548c\u5206\u5c42\u6df1\u5ea6\u673a\u5236\uff1b3\uff09\u5b9e\u73b0\u591a\u5934/\u6df7\u5408\u6838\u3001\u57fa\u4e8e\u89c4\u5212\u7684\u951a\u70b9\uff08\u5982\u71b5\u6700\u4f18\u4f20\u8f93/Sinkhorn\uff09\u548c\u4e00\u5143\u7b97\u5b50\uff08\u5982FFN\u5f0f\u573a\uff09\u4f5c\u4e3a\u663e\u5f0f\u673a\u5236\u9009\u62e9\uff1b4\uff09\u5c06\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5efa\u6a21\u9009\u62e9\u5206\u79bb\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7406\u8bba\u5206\u6790\u57fa\u7840\u3002", "conclusion": "\u51e0\u4f55\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u901a\u7528\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u4e0d\u53d8\u6027\u4e0e\u5efa\u6a21\u9009\u62e9\u89e3\u8026\uff0c\u652f\u6301\u6ce8\u610f\u529b\u673a\u5236\u7684\u7cfb\u7edf\u6bd4\u8f83\u3001\u6269\u5c55\u548c\u8bbe\u8ba1\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u73b0\u6709\u6ce8\u610f\u529b\u53d8\u4f53\uff0c\u5e76\u4e3a\u65b0\u578b\u6ce8\u610f\u529b\u67b6\u6784\u7684\u5f00\u53d1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.11619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11619", "abs": "https://arxiv.org/abs/2601.11619", "authors": ["Phani Kumar", "Nyshadham", "Jyothendra Varma", "Polisetty V R K", "Aditya Rathore"], "title": "NoiseFormer -- Noise Diffused Symmetric Attention Transformer", "comment": null, "summary": "Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u566a\u58f0\u6269\u6563\u5bf9\u79f0\u6ce8\u610f\u529bTransformer\u7684\u65b0\u578b\u7edf\u4e00\u6a21\u578b\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u5bf9\u79f0\u6ce8\u610f\u529b\u5185\u5b58\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5fae\u5c0f\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u968f\u7740\u89c4\u6a21\u589e\u5927\u9762\u4e34\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u7a00\u758f\u6ce8\u610f\u529b\u6280\u672f\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u5bf9\u79f0\u70b9\u79ef\u6ce8\u610f\u529b\u6280\u672f\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u6269\u6563\u5bf9\u79f0\u6ce8\u610f\u529bTransformer\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u5bf9\u79f0\u6ce8\u610f\u529b\u5185\u5b58\u4f18\u52bf\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u5f15\u5165\u5fae\u5c0f\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728GPT2\u57fa\u7840\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5728\u591a\u79cdGLUE\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4ecb\u4e8e\u666e\u901a\u5bf9\u79f0\u6ce8\u610f\u529b\u548cGPT2\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\u3002", "conclusion": "\u566a\u58f0\u6269\u6563\u5bf9\u79f0\u6ce8\u610f\u529bTransformer\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5fae\u5c0f\u5f00\u9500\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.11638", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.11638", "abs": "https://arxiv.org/abs/2601.11638", "authors": ["Josafat Ribeiro Leal Filho", "Ant\u00f4nio Augusto Fr\u00f6hlich"], "title": "Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System", "comment": "This paper has been submitted and is currently under review at IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Fisher\u4fe1\u606f$g_F^C$\u4f5c\u4e3a\u8bc4\u4f30PINNs\u5b66\u4e60\u7269\u7406\u7cfb\u7edf\u52a8\u529b\u5b66\u5b8c\u6574\u6027\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83PINN\u5b66\u4e60\u6a21\u578b\u4e0e\u539f\u59cb\u89e3\u6790\u6a21\u578b\u7684Fisher\u4fe1\u606f\u666f\u89c2\u6765\u91cf\u5316PINN\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524dPINNs\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u548c\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4e25\u683c\u91cf\u5316PINN\u662f\u5426\u5b8c\u6574\u6355\u83b7\u7cfb\u7edf\u52a8\u529b\u5b66\u884c\u4e3a\uff08\u8d85\u8d8a\u7b80\u5355\u8f68\u8ff9\u9884\u6d4b\uff09\u7684\u65b9\u6cd5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30PINN\u662f\u5426\u51c6\u786e\u5b66\u4e60\u7cfb\u7edf\u5e95\u5c42\u52a8\u529b\u5b66\u3001\u51e0\u4f55\u7279\u6027\u548c\u7a33\u5b9a\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u9488\u5bf9\u53ef\u5fae\u52a8\u529b\u7cfb\u7edf\u7684Fisher\u4fe1\u606f$g_F^C$\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002\u8be5Fisher\u4fe1\u606f\u6d4b\u91cf\u786e\u5b9a\u6027\u7cfb\u7edf\u4e2d\u7684\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u5bf9\u521d\u59cb\u6761\u4ef6\u7684\u654f\u611f\u6027\uff09\uff0c\u4e0e\u76f8\u7a7a\u95f4\u66f2\u7387\u548c\u72b6\u6001\u7a7a\u95f4\u6f14\u5316\u7684\u51c0\u62c9\u4f38\u4f5c\u7528\u76f8\u5173\u3002\u901a\u8fc7\u8ba1\u7b97PINN\u5b66\u4e60\u6a21\u578b\u548c\u539f\u59cb\u89e3\u6790\u6a21\u578b\u7684Jacobian\u77e9\u9635\uff0c\u6bd4\u8f83\u4e24\u8005\u7684Fisher\u4fe1\u606f\u666f\u89c2\u6765\u91cf\u5316PINN\u7684\u4fdd\u771f\u5ea6\u3002\u4f7f\u7528\u6c7d\u8f66\u52a8\u529b\u5b66\u6a21\u578b\u4f5c\u4e3a\u5b9e\u9a8c\u6848\u4f8b\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u5b8c\u6574\u7684\u5b9e\u9a8c\u65b9\u6cd5\u5b66\u6846\u67b6\uff0c\u4f46\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u3002\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u6bd4\u8f83Fisher\u4fe1\u606f\u666f\u89c2\u6765\u5b9a\u91cf\u8bc4\u4f30PINN\u662f\u5426\u51c6\u786e\u6355\u83b7\u4e86\u7cfb\u7edf\u7684\u590d\u6742\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5305\u62ec\u51e0\u4f55\u7279\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eFisher\u4fe1\u606f$g_F^C$\u7684\u8bc4\u4f30\u6846\u67b6\u4e3aPINNs\u7684\u52a8\u529b\u5b66\u4fdd\u771f\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u91cf\u65b9\u6cd5\u3002\u5982\u679cPINN\u5b66\u4e60\u6a21\u578b\u7684Fisher\u4fe1\u606f\u666f\u89c2\u4e0e\u539f\u59cb\u89e3\u6790\u6a21\u578b\u5339\u914d\uff0c\u5219\u8868\u660ePINN\u4e0d\u4ec5\u51c6\u786e\u9884\u6d4b\u72b6\u6001\u6f14\u5316\uff0c\u8fd8\u5b8c\u6574\u6355\u83b7\u4e86\u7cfb\u7edf\u7684\u51e0\u4f55\u548c\u7a33\u5b9a\u6027\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u52a8\u529b\u5b66\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2601.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11639", "abs": "https://arxiv.org/abs/2601.11639", "authors": ["Ming Li"], "title": "Global Optimization By Gradient from Hierarchical Score-Matching Spaces", "comment": null, "summary": "Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5206\u6570\u5339\u914d\u83b7\u53d6\u68af\u5ea6\uff0c\u5c06\u5e26\u590d\u6742\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u7edf\u4e00\u4e3a\u65e0\u7ea6\u675f\u5206\u5c42\u4f18\u5316\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e25\u683c\u68af\u5ea6\u7684\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316", "motivation": "\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u6700\u4f18\u6027\u9650\u5236\uff0c\u4ec5\u9002\u7528\u4e8e\u8fde\u7eed\u53ef\u5fae\u95ee\u9898\u548c\u5e73\u51e1\u51f8\u7ea6\u675f\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7ea6\u675f\u7684\u5168\u5c40\u4f18\u5316\u95ee\u9898", "method": "\u5c06\u5404\u79cd\u590d\u6742\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u7edf\u4e00\u4e3a\u65e0\u7ea6\u675f\u7684\u5206\u5c42\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u6280\u672f\u83b7\u53d6\u68af\u5ea6\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e25\u683c\u68af\u5ea6\u7684\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u7b80\u5355\u6784\u9020\u548c\u590d\u6742\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u5168\u5c40\u4f18\u5316\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5efa\u6a21\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5904\u7406\u590d\u6742\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5efa\u7acb\u4e86\u4f18\u5316\u7406\u8bba\u4e0e\u751f\u6210\u5efa\u6a21\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881"}}
{"id": "2601.11570", "categories": ["cs.CR", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.11570", "abs": "https://arxiv.org/abs/2601.11570", "authors": ["Pengcheng Xie"], "title": "Privacy-Preserving Black-Box Optimization (PBBO): Theory and the Model-Based Algorithm DFOp", "comment": "28 pages", "summary": "This paper focuses on solving unconstrained privacy-preserving black-box optimization (PBBO), its corresponding least Frobenius norm updating of quadratic models, and the differentially privacy mechanisms for PBBO. Optimization problems with transformed/encrypted objective functions aim to minimize F(x), which is encrypted/transformed/encrypted to F_k(x) as the output at the k-th iteration. A new derivative-free solver named DFOp, with its implementation, is proposed in this paper, which has a new updating formula for the quadratic model functions. The convergence of DFOp for solving problems with transformed/encrypted objective functions is given. Other analyses, including the new model updating formula and the analysis of the transformation's impact to model functions are presented. We propose two differentially private noise-adding mechanisms for privacy-preserving black-box optimization. Numerical results show that DFOp performs better than compared algorithms. To the best of our knowledge, DFOp is the first derivative-free solver that can solve black-box optimization problems with step-encryption and privacy-preserving black-box problems exactly, which also tries to answer the open question about the combination of derivative-free optimization and privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDFOp\u7b97\u6cd5\u89e3\u51b3\u65e0\u7ea6\u675f\u9690\u79c1\u4fdd\u62a4\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u5305\u542b\u65b0\u7684\u4e8c\u6b21\u6a21\u578b\u66f4\u65b0\u516c\u5f0f\u548c\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u662f\u9996\u4e2a\u80fd\u5904\u7406\u6b65\u52a0\u5bc6\u548c\u9690\u79c1\u4fdd\u62a4\u9ed1\u76d2\u4f18\u5316\u7684\u65e0\u5bfc\u6570\u6c42\u89e3\u5668\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ea6\u675f\u9690\u79c1\u4fdd\u62a4\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u586b\u8865\u65e0\u5bfc\u6570\u4f18\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u7ed3\u5408\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5904\u7406\u52a0\u5bc6/\u53d8\u6362\u76ee\u6807\u51fd\u6570\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u65e0\u5bfc\u6570\u6c42\u89e3\u5668DFOp\uff0c\u5305\u542b\u65b0\u7684\u4e8c\u6b21\u6a21\u578b\u51fd\u6570\u66f4\u65b0\u516c\u5f0f\uff0c\u9488\u5bf9\u53d8\u6362/\u52a0\u5bc6\u76ee\u6807\u51fd\u6570F_k(x)\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u5dee\u5206\u9690\u79c1\u566a\u58f0\u6dfb\u52a0\u673a\u5236\u3002", "result": "\u8bc1\u660e\u4e86DFOp\u5bf9\u53d8\u6362/\u52a0\u5bc6\u76ee\u6807\u51fd\u6570\u7684\u6536\u655b\u6027\uff0c\u6570\u503c\u7ed3\u679c\u663e\u793aDFOp\u6027\u80fd\u4f18\u4e8e\u5bf9\u6bd4\u7b97\u6cd5\uff0c\u662f\u9996\u4e2a\u80fd\u7cbe\u786e\u89e3\u51b3\u6b65\u52a0\u5bc6\u548c\u9690\u79c1\u4fdd\u62a4\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u7684\u65e0\u5bfc\u6570\u6c42\u89e3\u5668\u3002", "conclusion": "DFOp\u6210\u529f\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u56de\u7b54\u4e86\u65e0\u5bfc\u6570\u4f18\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u7ed3\u5408\u7684\u5f00\u6e90\u95ee\u9898\uff0c\u4e3a\u52a0\u5bc6\u76ee\u6807\u51fd\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.11663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11663", "abs": "https://arxiv.org/abs/2601.11663", "authors": ["Bruce Changlong Xu"], "title": "Activation Sensitivity as a Unifying Principle for Post-Training Quantization", "comment": null, "summary": "Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6fc0\u6d3b\u611f\u77e5\u65b9\u6cd5\uff08\u5982AWQ\uff09\u548c\u4e8c\u9636\u65b9\u6cd5\uff08\u5982GPTQ\uff09\u89e3\u91ca\u4e3a\u5bf9\u6fc0\u6d3b\u654f\u611f\u6027\u7684\u4e0d\u540c\u8fd1\u4f3c\uff0c\u4e3a\u7406\u89e3\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002\u6fc0\u6d3b\u611f\u77e5\u65b9\u6cd5\uff08\u5982AWQ\uff09\u548c\u4e8c\u9636\u65b9\u6cd5\uff08\u5982GPTQ\uff09\u867d\u7136\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6982\u5ff5\u4e0a\u5206\u6563\uff0c\u4e0d\u6e05\u695a\u5b83\u4eec\u8fd1\u4f3c\u7684\u662f\u4ec0\u4e48\u5e95\u5c42\u91cf\u3002", "method": "\u901a\u8fc7\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u5f62\u5f0f\u5316\u6fc0\u6d3b\u654f\u611f\u6027\uff0c\u5b9a\u4e49\u4e3a\u901a\u9053\u6270\u52a8\u5bf9\u635f\u5931\u7684\u671f\u671b\u5f71\u54cd\u3002\u654f\u611f\u6027\u81ea\u7136\u8868\u73b0\u4e3a\u68af\u5ea6\u52a0\u6743\u6fc0\u6d3b\u7684\u5e73\u65b9\u8303\u6570\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6355\u83b7\u6fc0\u6d3b\u5e45\u5ea6\u548c\u4e0b\u6e38\u8bef\u5dee\u4f20\u64ad\u7684\u539f\u5219\u6027\u901a\u9053\u91cd\u8981\u6027\u5ea6\u91cf\u3002", "result": "\u5728\u8be5\u6846\u67b6\u4e0b\uff0cAWQ\u548cGPTQ\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u6062\u590d\u654f\u611f\u6027\u7684\u4e92\u8865\u8fd1\u4f3c\u3002\u5206\u6790\u4e86\u654f\u611f\u6027\u5ea6\u91cf\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8fde\u63a5\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u3001Fisher\u4fe1\u606f\u548c\u57fa\u4e8eHessian\u7684\u51c6\u5219\uff0c\u5e76\u9610\u660e\u4e86\u5b83\u4eec\u4e0e\u7ecf\u5178\u526a\u679d\u65b9\u6cd5\u7684\u5173\u7cfb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u901a\u8fc7\u654f\u611f\u6027\u89c6\u89d2\u7406\u89e3\u548c\u6bd4\u8f83\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u800c\u4e0d\u662f\u63d0\u51fa\u65b0\u7684\u91cf\u5316\u7b97\u6cd5\u3002\u7edf\u4e00\u6846\u67b6\u6709\u52a9\u4e8e\u7406\u89e3\u73b0\u6709\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u548c\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u91cf\u5316\u7b56\u7565\u3002"}}
{"id": "2601.11629", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11629", "abs": "https://arxiv.org/abs/2601.11629", "authors": ["Nghia T. Le", "Alan Ritter", "Kartik Goyal"], "title": "Semantic Differentiation for Tackling Challenges in Watermarking Low-Entropy Constrained Generation Outputs", "comment": "18 pages, 4 figures", "summary": "We demonstrate that while the current approaches for language model watermarking are effective for open-ended generation, they are inadequate at watermarking LM outputs for constrained generation tasks with low-entropy output spaces. Therefore, we devise SeqMark, a sequence-level watermarking algorithm with semantic differentiation that balances the output quality, watermark detectability, and imperceptibility. It improves on the shortcomings of the prevalent token-level watermarking algorithms that cause under-utilization of the sequence-level entropy available for constrained generation tasks. Moreover, we identify and improve upon a different failure mode we term region collapse, associated with prior sequence-level watermarking algorithms. This occurs because the pseudorandom partitioning of semantic space for watermarking in these approaches causes all high-probability outputs to collapse into either invalid or valid regions, leading to a trade-off in output quality and watermarking effectiveness. SeqMark instead, differentiates the high-probable output subspace and partitions it into valid and invalid regions, ensuring the even spread of high-quality outputs among all the regions. On various constrained generation tasks like machine translation, code generation, and abstractive summarization, SeqMark substantially improves watermark detection accuracy (up to 28% increase in F1) while maintaining high generation quality.", "AI": {"tldr": "SeqMark\u662f\u4e00\u79cd\u9488\u5bf9\u4f4e\u71b5\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u7684\u5e8f\u5217\u7ea7\u6c34\u5370\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709token\u7ea7\u6c34\u5370\u5728\u7ea6\u675f\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u533a\u5206\u5e73\u8861\u8f93\u51fa\u8d28\u91cf\u3001\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u5728\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u8f93\u51fa\u7a7a\u95f4\u71b5\u8f83\u4f4e\u7684\u7ea6\u675f\u751f\u6210\u4efb\u52a1\uff08\u5982\u673a\u5668\u7ffb\u8bd1\u3001\u4ee3\u7801\u751f\u6210\u3001\u6458\u8981\u751f\u6210\uff09\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u8fd9\u7c7b\u4efb\u52a1\u7684\u6c34\u5370\u7b97\u6cd5\u3002", "method": "\u63d0\u51faSeqMark\u5e8f\u5217\u7ea7\u6c34\u5370\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u533a\u5206\u5c06\u9ad8\u6982\u7387\u8f93\u51fa\u5b50\u7a7a\u95f4\u5212\u5206\u4e3a\u6709\u6548\u548c\u65e0\u6548\u533a\u57df\uff0c\u907f\u514d\u5148\u524d\u5e8f\u5217\u7ea7\u6c34\u5370\u65b9\u6cd5\u4e2d\u7684\"\u533a\u57df\u584c\u9677\"\u95ee\u9898\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u8f93\u51fa\u5728\u6240\u6709\u533a\u57df\u5747\u5300\u5206\u5e03\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u4ee3\u7801\u751f\u6210\u548c\u62bd\u8c61\u6458\u8981\u7b49\u591a\u79cd\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e0a\uff0cSeqMark\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u68c0\u6d4b\u51c6\u786e\u7387\uff08F1\u5206\u6570\u6700\u9ad8\u63d0\u534728%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SeqMark\u6210\u529f\u89e3\u51b3\u4e86\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6c34\u5370\u96be\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u8bed\u4e49\u533a\u5206\u65b9\u6cd5\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6c34\u5370\u68c0\u6d4b\u6548\u679c\uff0c\u4e3a\u4f4e\u71b5\u8f93\u51fa\u7a7a\u95f4\u7684\u6c34\u5370\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11664", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11664", "abs": "https://arxiv.org/abs/2601.11664", "authors": ["Chetan Pathade", "Vinod Dhimam", "Sheheryar Ahmad", "Ilsa Lareb"], "title": "Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning", "comment": "17 Pages, 2 Figures, 4 Tables", "summary": "Serverless computing has achieved widespread adoption, with over 70% of AWS organizations using serverless solutions [1]. Meanwhile, machine learning inference workloads increasingly migrate to Function-as-a-Service (FaaS) platforms for their scalability and cost-efficiency [2], [3], [4]. However, this convergence introduces critical security challenges, with recent reports showing a 220% increase in AI/ML vulnerabilities [5] and serverless computing's fragmented architecture raises new security concerns distinct from traditional cloud deployments [6], [7]. This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. We systematically characterize the attack surface across five categories: function-level vulnerabilities (cold start exploitation, dependency poisoning), model-specific threats (API-based extraction, adversarial inputs), infrastructure attacks (cross-function contamination, privilege escalation), supply chain risks (malicious layers, backdoored libraries), and IAM complexity (ephemeral nature, serverless functions). Through empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions, we demonstrate real-world attack scenarios and quantify their security impact. We propose Serverless AI Shield (SAS), a multi-layered defense framework providing pre-deployment validation, runtime monitoring, and post-execution forensics. Our evaluation shows SAS achieves 94% detection rates while maintaining performance overhead below 9% for inference latency. We release an open-source security toolkit to enable practitioners to assess and harden their serverless AI deployments, advancing the field toward more resilient cloud-native machine learning systems.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5168\u9762\u7684\u5b89\u5168\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u4e94\u4e2a\u653b\u51fb\u9762\u7c7b\u522b\uff0c\u63d0\u51fa\u4e86Serverless AI Shield\u9632\u5fa1\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u5f00\u9500\u4f4e\u4e8e9%\u7684\u540c\u65f6\u8fbe\u523094%\u7684\u68c0\u6d4b\u7387\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u5df2\u5e7f\u6cdb\u91c7\u7528\uff0c\u8d85\u8fc770%\u7684AWS\u7ec4\u7ec7\u4f7f\u7528\u65e0\u670d\u52a1\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u673a\u5668\u5b66\u4e60\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u8d8a\u6765\u8d8a\u591a\u5730\u8fc1\u79fb\u5230FaaS\u5e73\u53f0\u4ee5\u83b7\u5f97\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u878d\u5408\u5f15\u5165\u4e86\u5173\u952e\u7684\u5b89\u5168\u6311\u6218\uff0cAI/ML\u6f0f\u6d1e\u589e\u52a0\u4e86220%\uff0c\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u788e\u7247\u5316\u67b6\u6784\u5e26\u6765\u4e86\u4e0e\u4f20\u7edf\u4e91\u90e8\u7f72\u4e0d\u540c\u7684\u65b0\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5bf9\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u5206\u6790\uff0c\u8bc6\u522b\u4e94\u4e2a\u653b\u51fb\u9762\u7c7b\u522b\uff1a\u51fd\u6570\u7ea7\u6f0f\u6d1e\u3001\u6a21\u578b\u7279\u5b9a\u5a01\u80c1\u3001\u57fa\u7840\u8bbe\u65bd\u653b\u51fb\u3001\u4f9b\u5e94\u94fe\u98ce\u9669\u548cIAM\u590d\u6742\u6027\u3002\u5728AWS Lambda\u3001Azure Functions\u548cGoogle Cloud Functions\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6f14\u793a\u771f\u5b9e\u653b\u51fb\u573a\u666f\u5e76\u91cf\u5316\u5176\u5b89\u5168\u5f71\u54cd\u3002\u63d0\u51faServerless AI Shield\u591a\u5c42\u9632\u5fa1\u6846\u67b6\uff0c\u63d0\u4f9b\u90e8\u7f72\u524d\u9a8c\u8bc1\u3001\u8fd0\u884c\u65f6\u76d1\u63a7\u548c\u6267\u884c\u540e\u53d6\u8bc1\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u5c55\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u653b\u51fb\u573a\u666f\u5e76\u91cf\u5316\u4e86\u5b89\u5168\u5f71\u54cd\u3002Serverless AI Shield\u6846\u67b6\u5b9e\u73b0\u4e8694%\u7684\u68c0\u6d4b\u7387\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u7684\u6027\u80fd\u5f00\u9500\u4fdd\u6301\u57289%\u4ee5\u4e0b\u3002\u53d1\u5e03\u4e86\u5f00\u6e90\u5b89\u5168\u5de5\u5177\u5305\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u8bc4\u4f30\u548c\u52a0\u5f3a\u5176\u65e0\u670d\u52a1\u5668AI\u90e8\u7f72\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5bf9\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5168\u9762\u5b89\u5168\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u5b89\u5168\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u9632\u5fa1\u6846\u67b6\u3002Serverless AI Shield\u5728\u4fdd\u6301\u4f4e\u6027\u80fd\u5f00\u9500\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u68c0\u6d4b\u7387\uff0c\u63a8\u52a8\u4e86\u66f4\u5177\u5f39\u6027\u7684\u4e91\u539f\u751f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.11669", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11669", "abs": "https://arxiv.org/abs/2601.11669", "authors": ["Wenwen Liao", "Hang Ruan", "Jianbo Yu", "Xiaofeng Yang", "Qingchao Jiang", "Xuefeng Yan"], "title": "IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning", "comment": null, "summary": "Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical \"warm-up and test\" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.", "AI": {"tldr": "IPEC\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u589e\u91cf\u539f\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u8f85\u52a9\u96c6\u5229\u7528\u5386\u53f2\u67e5\u8be2\u6837\u672c\u4fe1\u606f\uff0c\u63d0\u5347\u5c0f\u6837\u672c\u5206\u7c7b\u7684\u539f\u578b\u4f30\u8ba1\u8d28\u91cf", "motivation": "\u4f20\u7edf\u5ea6\u91cf\u5b66\u4e60\u5c0f\u6837\u672c\u65b9\u6cd5\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u6279\u6b21\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u65e0\u6cd5\u5229\u7528\u5386\u53f2\u6279\u6b21\u79ef\u7d2f\u7684\u5b9d\u8d35\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u539f\u578b\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027", "method": "\u63d0\u51faIPEC\u65b9\u6cd5\uff1a1) \u6784\u5efa\u52a8\u6001\u8f85\u52a9\u96c6\uff0c\u9009\u62e9\u6027\u7eb3\u5165\u9ad8\u7f6e\u4fe1\u5ea6\u67e5\u8be2\u6837\u672c\uff1b2) \u8bbe\u8ba1\u53cc\u91cd\u8fc7\u6ee4\u673a\u5236\uff0c\u57fa\u4e8e\u5168\u5c40\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u5c40\u90e8\u5224\u522b\u80fd\u529b\u8bc4\u4f30\u6837\u672c\u8d28\u91cf\uff1b3) \u5c06\u8f85\u52a9\u96c6\u4e0e\u652f\u6301\u96c6\u805a\u5408\u6784\u5efa\u66f4\u7a33\u5b9a\u7684\u539f\u578b\uff1b4) \u57fa\u4e8e\u8d1d\u53f6\u65af\u89e3\u91ca\uff0c\u5c06\u652f\u6301\u96c6\u89c6\u4e3a\u5148\u9a8c\uff0c\u8f85\u52a9\u96c6\u89c6\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u540e\u9a8c\uff1b5) \u8bbe\u8ba1\"\u9884\u70ed-\u6d4b\u8bd5\"\u4e24\u9636\u6bb5\u63a8\u7406\u534f\u8bae", "result": "\u5728\u591a\u4e2a\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u521d\u59cb\u652f\u6301\u96c6\u7684\u4f9d\u8d56\uff0c\u6784\u5efa\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u539f\u578b", "conclusion": "IPEC\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u91cf\u5b66\u4e60\u673a\u5236\uff0c\u6253\u7834\u4e86\u6279\u6b21\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u601d\u8def"}}
{"id": "2601.11678", "categories": ["cs.CR", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11678", "abs": "https://arxiv.org/abs/2601.11678", "authors": ["Shuai Zhang", "Minzhao Lyu", "Hassan Habibi Gharakheili"], "title": "A Survey on Mapping Digital Systems with Bill of Materials: Development, Practices, and Challenges", "comment": null, "summary": "Modern digital ecosystems, spanning software, hardware, learning models, datasets, and cryptographic products, continue to grow in complexity, making it difficult for organizations to understand and manage component dependencies. Bills of Materials (BOMs) have emerged as a structured way to document product components, their interrelationships, and key metadata, improving visibility and security across digital supply chains. This survey provides the first comprehensive cross-domain review of BOM developments and practices. We start by examining the evolution of BOM frameworks in three stages (i.e., pre-development, initial, and accelerated) and summarizing their core principles, key stakeholders, and standardization efforts for hardware, software, artificial intelligence (AI) models, datasets, and cryptographic assets. We then review industry practices for generating BOM data, evaluating its quality, and securely sharing it. Next, we review practical downstream uses of BOM data, including dependency modeling, compliance verification, operational risk assessment, and vulnerability tracking. We also discuss academic efforts to address limitations in current BOM frameworks through refinements, extensions, or new models tailored to emerging domains such as data ecosystems and AI supply chains. Finally, we identify four key gaps that limit the usability and reliability of today's BOM frameworks, motivating future research directions.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u8de8\u9886\u57df\u7269\u6599\u6e05\u5355\uff08BOM\uff09\u7684\u53d1\u5c55\u4e0e\u5b9e\u8df5\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u786c\u4ef6\u3001\u8f6f\u4ef6\u3001AI\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u52a0\u5bc6\u8d44\u4ea7\u7b49\u9886\u57df\uff0c\u5206\u6790BOM\u6846\u67b6\u6f14\u8fdb\u3001\u884c\u4e1a\u5b9e\u8df5\u3001\u4e0b\u6e38\u5e94\u7528\u53ca\u5b66\u672f\u7814\u7a76\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u6846\u67b6\u7684\u56db\u5927\u5173\u952e\u5dee\u8ddd\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u751f\u6001\u7cfb\u7edf\uff08\u8f6f\u4ef6\u3001\u786c\u4ef6\u3001\u5b66\u4e60\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u52a0\u5bc6\u4ea7\u54c1\uff09\u65e5\u76ca\u590d\u6742\uff0c\u7ec4\u7ec7\u96be\u4ee5\u7406\u89e3\u548c\u7ba1\u7406\u7ec4\u4ef6\u4f9d\u8d56\u5173\u7cfb\u3002\u7269\u6599\u6e05\u5355\uff08BOMs\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u6587\u6863\u4ea7\u54c1\u7ec4\u4ef6\u3001\u76f8\u4e92\u5173\u7cfb\u53ca\u5173\u952e\u5143\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u53ef\u63d0\u9ad8\u6570\u5b57\u4f9b\u5e94\u94fe\u7684\u53ef\u89c1\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "1. \u8003\u5bdfBOM\u6846\u67b6\u5728\u4e09\u4e2a\u9636\u6bb5\u7684\u6f14\u8fdb\uff08\u9884\u5f00\u53d1\u3001\u521d\u59cb\u3001\u52a0\u901f\u9636\u6bb5\uff09\uff0c\u603b\u7ed3\u786c\u4ef6\u3001\u8f6f\u4ef6\u3001AI\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u52a0\u5bc6\u8d44\u4ea7\u9886\u57df\u7684\u6838\u5fc3\u539f\u5219\u3001\u5173\u952e\u5229\u76ca\u76f8\u5173\u8005\u548c\u6807\u51c6\u5316\u5de5\u4f5c\uff1b2. \u56de\u987e\u751f\u6210BOM\u6570\u636e\u3001\u8bc4\u4f30\u5176\u8d28\u91cf\u548c\u5b89\u5168\u5171\u4eab\u7684\u884c\u4e1a\u5b9e\u8df5\uff1b3. \u5206\u6790BOM\u6570\u636e\u7684\u5b9e\u9645\u4e0b\u6e38\u5e94\u7528\uff0c\u5305\u62ec\u4f9d\u8d56\u5efa\u6a21\u3001\u5408\u89c4\u9a8c\u8bc1\u3001\u64cd\u4f5c\u98ce\u9669\u8bc4\u4f30\u548c\u6f0f\u6d1e\u8ddf\u8e2a\uff1b4. \u8ba8\u8bba\u5b66\u672f\u754c\u901a\u8fc7\u6539\u8fdb\u3001\u6269\u5c55\u6216\u9488\u5bf9\u65b0\u5174\u9886\u57df\uff08\u5982\u6570\u636e\u751f\u6001\u7cfb\u7edf\u548cAI\u4f9b\u5e94\u94fe\uff09\u5b9a\u5236\u65b0\u6a21\u578b\u6765\u89e3\u51b3\u5f53\u524dBOM\u6846\u67b6\u5c40\u9650\u6027\u7684\u52aa\u529b\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u9996\u4e2a\u8de8\u9886\u57dfBOM\u53d1\u5c55\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86BOM\u6846\u67b6\u7684\u6f14\u8fdb\u5386\u7a0b\u3001\u884c\u4e1a\u5b9e\u8df5\u6807\u51c6\u3001\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u4ee5\u53ca\u5b66\u672f\u7814\u7a76\u8fdb\u5c55\uff0c\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u590d\u6742\u6570\u5b57\u4f9b\u5e94\u94fe\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u8bc6\u522b\u51fa\u9650\u5236\u5f53\u524dBOM\u6846\u67b6\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u7684\u56db\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002BOM\u4f5c\u4e3a\u6570\u5b57\u4f9b\u5e94\u94fe\u7ba1\u7406\u7684\u5173\u952e\u5de5\u5177\uff0c\u5176\u6807\u51c6\u5316\u548c\u5b9e\u8df5\u5e94\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5b8c\u5584\u3002"}}
{"id": "2601.11683", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11683", "abs": "https://arxiv.org/abs/2601.11683", "authors": ["Zhuoyi Shang", "Jiasen Li", "Pengzhen Chen", "Yanwei Liu", "Xiaoyan Gu", "Weiping Wang"], "title": "Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory", "comment": "Accepted to the 35th USENIX Security Symposium (USENIX Security 2026)", "summary": "The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \\textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similarities, which are insufficient to capture the dynamic evolution of knowledge that underlies true lineage relationships. Drawing inspiration from the genetic mechanism of human evolution, we tackle the problem of model lineage attestation by verifying the joint trajectory of knowledge evolution and parameter modification. To this end, we propose a novel model lineage attestation framework. In our framework, model editing is first leveraged to quantify parameter-level changes introduced by fine-tuning. Subsequently, we introduce a novel knowledge vectorization mechanism that refines the evolved knowledge within the edited models into compact representations by the assistance of probe samples. The probing strategies are adapted to different types of model families. These embeddings serve as the foundation for verifying the arithmetic consistency of knowledge relationships across models, thereby enabling robust attestation of model lineage. Extensive experimental evaluations demonstrate the effectiveness and resilience of our approach in a variety of adversarial scenarios in the real world. Our method consistently achieves reliable lineage verification across a broad spectrum of model types, including classifiers, diffusion models, and large language models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u6f14\u5316\u548c\u53c2\u6570\u4fee\u6539\u8054\u5408\u8f68\u8ff9\u7684\u6a21\u578b\u8c31\u7cfb\u8ba4\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u91cf\u5316\u53c2\u6570\u53d8\u5316\uff0c\u77e5\u8bc6\u5411\u91cf\u5316\u673a\u5236\u5c06\u6f14\u5316\u77e5\u8bc6\u538b\u7f29\u4e3a\u7d27\u51d1\u8868\u793a\uff0c\u9a8c\u8bc1\u77e5\u8bc6\u5173\u7cfb\u7684\u7b97\u672f\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u6a21\u578b\u8c31\u7cfb\u8ba4\u8bc1\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5fae\u8c03\u6280\u672f\u4ea7\u751f\u4e86\u6a21\u578b\u95f4\u7684\u8c31\u7cfb\u5173\u7cfb\uff0c\u8fd9\u79cd\u5173\u7cfb\u4e3a\u89e3\u51b3\u672a\u6388\u6743\u6a21\u578b\u518d\u5206\u53d1\u548c\u865a\u5047\u6a21\u578b\u6765\u6e90\u58f0\u660e\u7b49\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u5f53\u524d\u5f00\u653e\u6743\u91cd\u6a21\u578b\u5e93\u7f3a\u4e4f\u6709\u6548\u7684\u8c31\u7cfb\u9a8c\u8bc1\u673a\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u67b6\u6784\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u77e5\u8bc6\u6f14\u5316\u7684\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u8c31\u7cfb\u8ba4\u8bc1\u6846\u67b6\uff1a1) \u5229\u7528\u6a21\u578b\u7f16\u8f91\u91cf\u5316\u5fae\u8c03\u5f15\u5165\u7684\u53c2\u6570\u7ea7\u53d8\u5316\uff1b2) \u5f15\u5165\u77e5\u8bc6\u5411\u91cf\u5316\u673a\u5236\uff0c\u901a\u8fc7\u63a2\u9488\u6837\u672c\u5c06\u7f16\u8f91\u540e\u6a21\u578b\u4e2d\u7684\u6f14\u5316\u77e5\u8bc6\u7cbe\u70bc\u4e3a\u7d27\u51d1\u8868\u793a\uff0c\u63a2\u9488\u7b56\u7565\u6839\u636e\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7c7b\u578b\u8fdb\u884c\u9002\u914d\uff1b3) \u57fa\u4e8e\u8fd9\u4e9b\u5d4c\u5165\u8868\u793a\u9a8c\u8bc1\u8de8\u6a21\u578b\u77e5\u8bc6\u5173\u7cfb\u7684\u7b97\u672f\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u79cd\u73b0\u5b9e\u4e16\u754c\u5bf9\u6297\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u5668\u3001\u6269\u6563\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u5e7f\u6cdb\u6a21\u578b\u7c7b\u578b\u4e0a\u90fd\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u8c31\u7cfb\u9a8c\u8bc1\uff0c\u5c55\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u9a8c\u8bc1\u77e5\u8bc6\u6f14\u5316\u548c\u53c2\u6570\u4fee\u6539\u7684\u8054\u5408\u8f68\u8ff9\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u7684\u6a21\u578b\u8c31\u7cfb\u8ba4\u8bc1\uff0c\u4e3a\u89e3\u51b3\u6a21\u578b\u5b89\u5168\u95ee\u9898\u548c\u5efa\u7acb\u53ef\u4fe1\u7684\u6a21\u578b\u8c31\u7cfb\u9a8c\u8bc1\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11686", "abs": "https://arxiv.org/abs/2601.11686", "authors": ["Nicolas Caron", "Christophe Guyeux", "Hassan Noura", "Benjamin Aynes"], "title": "Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis", "comment": null, "summary": "Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u9884\u6d4b\u6a21\u578b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u591a\u7ef4\u5ea6\u91ce\u706b\u98ce\u9669\u5206\u6790\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u53ef\u64cd\u4f5c\u62a5\u544a", "motivation": "\u5f53\u524d\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u9645\u8fd0\u8425\u9700\u6c42\uff0c\u7f3a\u4e4f\u5bf9\u7b2c\u4e00\u54cd\u5e94\u8005\u548c\u6d88\u9632\u670d\u52a1\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u9700\u8981\u80fd\u6355\u6349\u6c14\u8c61\u5371\u9669\u3001\u70b9\u706b\u6d3b\u52a8\u3001\u5e72\u9884\u590d\u6742\u6027\u548c\u8d44\u6e90\u52a8\u5458\u7b49\u591a\u7ef4\u5ea6\u7684\u7efc\u5408\u5206\u6790\u65b9\u6cd5", "method": "\u5f00\u53d1\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5404\u98ce\u9669\u7ef4\u5ea6\u7684\u9884\u6d4b\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5f02\u8d28\u8f93\u51fa\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u62a5\u544a", "result": "\u6982\u5ff5\u9a8c\u8bc1\u9636\u6bb5\uff0c\u63d0\u51fa\u6846\u67b6\u8bbe\u8ba1\u4f46\u5c1a\u672a\u5b9e\u65bd\u9a8c\u8bc1", "conclusion": "\u591a\u76ee\u6807\u5206\u6790\u548cLLM\u96c6\u6210\u80fd\u63d0\u5347\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u7684\u5b9e\u7528\u6027\u548c\u64cd\u4f5c\u6027\uff0c\u4e3a\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301"}}
{"id": "2601.11696", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11696", "abs": "https://arxiv.org/abs/2601.11696", "authors": ["Annika Wilde", "Samira Briongos", "Claudio Soriente", "Ghassan Karame"], "title": "On Abnormal Execution Timing of Conditional Jump Instructions", "comment": "To appear in the Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS), March 2026 Issue, presented at the ACM SIGMETRICS 2026 conference", "summary": "An extensive line of work on modern computing architectures has shown that the execution time of instructions can (i) depend on the operand of the instruction or (ii) be influenced by system optimizations, e.g., branch prediction and speculative execution paradigms.\n  In this paper, we systematically measure and analyze timing variabilities in conditional jump instructions that can be macro-fused with a preceding instruction, depending on their placement within the binary. Our measurements indicate that these timing variations stem from the micro-op cache placement and the jump's offset in the L1 instruction cache of modern processors. We demonstrate that this behavior is consistent across multiple microarchitectures, including Skylake, Coffee Lake, and Kaby Lake, as well as various real-world implementations. We confirm the prevalence of this variability through extensive experiments on a large-scale set of popular binaries, including libraries from Ubuntu 24.04, Windows 10 Pro, and several open-source cryptographic libraries. We also show that one can easily avoid this timing variability by ensuring that macro-fusible instructions are 32-byte aligned - an approach initially suggested in 2019 by Intel in an overlooked short report. We quantify the performance impact of this approach across the cryptographic libraries, showing a speedup of 2.15% on average (and up to 10.54%) when avoiding the timing variability. As a by-product, we show that this variability can be exploited as a covert channel, achieving a maximum throughput of 16.14 Mbps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6d4b\u91cf\u5206\u6790\u4e86\u73b0\u4ee3\u5904\u7406\u5668\u4e2d\u6761\u4ef6\u8df3\u8f6c\u6307\u4ee4\u7684\u65f6\u5e8f\u53d8\u5f02\u6027\uff0c\u53d1\u73b0\u5176\u6e90\u4e8e\u5fae\u64cd\u4f5c\u7f13\u5b58\u653e\u7f6e\u548cL1\u6307\u4ee4\u7f13\u5b58\u4e2d\u7684\u8df3\u8f6c\u504f\u79fb\uff0c\u63d0\u51fa\u4e86\u901a\u8fc732\u5b57\u8282\u5bf9\u9f50\u6765\u907f\u514d\u8fd9\u79cd\u53d8\u5f02\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6f5c\u5728\u7684\u9690\u853d\u4fe1\u9053\u5229\u7528\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u67b6\u6784\u4e2d\u6307\u4ee4\u6267\u884c\u65f6\u95f4\u53ef\u80fd\u4f9d\u8d56\u4e8e\u64cd\u4f5c\u6570\u6216\u53d7\u7cfb\u7edf\u4f18\u5316\uff08\u5982\u5206\u652f\u9884\u6d4b\u548c\u63a8\u6d4b\u6267\u884c\uff09\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6d4b\u91cf\u548c\u5206\u6790\u6761\u4ef6\u8df3\u8f6c\u6307\u4ee4\u7684\u65f6\u5e8f\u53d8\u5f02\u6027\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u53ef\u4ee5\u4e0e\u524d\u7f6e\u6307\u4ee4\u8fdb\u884c\u5b8f\u878d\u5408\u7684\u8df3\u8f6c\u6307\u4ee4\uff0c\u63a2\u7a76\u5176\u5728\u4e0d\u540c\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u7f6e\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6d4b\u91cf\u548c\u5206\u6790\u6761\u4ef6\u8df3\u8f6c\u6307\u4ee4\u7684\u65f6\u5e8f\u53d8\u5f02\u6027\uff0c\u7814\u7a76\u5fae\u64cd\u4f5c\u7f13\u5b58\u653e\u7f6e\u548cL1\u6307\u4ee4\u7f13\u5b58\u4e2d\u8df3\u8f6c\u504f\u79fb\u7684\u5f71\u54cd\u3002\u5728\u591a\u79cd\u5fae\u67b6\u6784\uff08Skylake\u3001Coffee Lake\u3001Kaby Lake\uff09\u548c\u5b9e\u9645\u5b9e\u73b0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6d41\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u96c6\uff08\u5305\u62ecUbuntu 24.04\u3001Windows 10 Pro\u5e93\u548c\u591a\u4e2a\u5f00\u6e90\u52a0\u5bc6\u5e93\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u6d4b\u91cf\u8868\u660e\u65f6\u5e8f\u53d8\u5f02\u6027\u6e90\u4e8e\u5fae\u64cd\u4f5c\u7f13\u5b58\u653e\u7f6e\u548c\u8df3\u8f6c\u5728L1\u6307\u4ee4\u7f13\u5b58\u4e2d\u7684\u504f\u79fb\uff0c\u8fd9\u79cd\u884c\u4e3a\u5728\u591a\u79cd\u5fae\u67b6\u6784\u548c\u5b9e\u9645\u5b9e\u73b0\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002\u901a\u8fc7\u786e\u4fdd\u5b8f\u53ef\u878d\u5408\u6307\u4ee432\u5b57\u8282\u5bf9\u9f50\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u53d8\u5f02\u6027\uff0c\u5728\u52a0\u5bc6\u5e93\u4e2d\u5e73\u5747\u5e26\u67652.15%\uff08\u6700\u9ad810.54%\uff09\u7684\u901f\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u53d8\u5f02\u6027\u53ef\u4f5c\u4e3a\u9690\u853d\u4fe1\u9053\u5229\u7528\uff0c\u6700\u5927\u541e\u5410\u91cf\u8fbe16.14 Mbps\u3002", "conclusion": "\u6761\u4ef6\u8df3\u8f6c\u6307\u4ee4\u7684\u65f6\u5e8f\u53d8\u5f02\u6027\u662f\u73b0\u4ee3\u5904\u7406\u5668\u4e2d\u666e\u904d\u5b58\u5728\u7684\u73b0\u8c61\uff0c\u6e90\u4e8e\u5fae\u64cd\u4f5c\u7f13\u5b58\u548cL1\u6307\u4ee4\u7f13\u5b58\u673a\u5236\u3002\u901a\u8fc732\u5b57\u8282\u5bf9\u9f50\u53ef\u4ee5\u907f\u514d\u8fd9\u79cd\u53d8\u5f02\u6027\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u8fd9\u79cd\u53d8\u5f02\u6027\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u88ab\u7528\u4f5c\u9690\u853d\u4fe1\u9053\u3002Intel\u57282019\u5e74\u63d0\u51fa\u7684\u5bf9\u9f50\u5efa\u8bae\u88ab\u8bc1\u5b9e\u6709\u6548\u4f46\u88ab\u5ffd\u89c6\u3002"}}
{"id": "2601.11719", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.11719", "abs": "https://arxiv.org/abs/2601.11719", "authors": ["Ho Fung Tsoi", "Dylan Rankin"], "title": "jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation", "comment": "Under review", "summary": "Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.", "AI": {"tldr": "jBOT\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8eCERN\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7684\u55b7\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7c92\u5b50\u7ea7\u84b8\u998f\u548c\u5168\u5c40\u55b7\u6ce8\u7ea7\u84b8\u998f\u6765\u5b66\u4e60\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u55b7\u6ce8\u8868\u793a\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u662f\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u5373\u53ef\u5b66\u4e60\u7279\u5f81\u8868\u793a\u7684\u5f3a\u5927\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u8868\u793a\u901a\u5e38\u80fd\u6355\u6349\u6570\u636e\u7684\u901a\u7528\u5e95\u5c42\u8bed\u4e49\uff0c\u5e76\u53ef\u5728\u540e\u7eed\u5fae\u8c03\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e13\u95e8\u7528\u4e8e\u9ad8\u80fd\u7269\u7406\u55b7\u6ce8\u6570\u636e\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fajBOT\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u81ea\u84b8\u998f\u6280\u672f\uff0c\u7ed3\u5408\u5c40\u90e8\u7c92\u5b50\u7ea7\u84b8\u998f\u548c\u5168\u5c40\u55b7\u6ce8\u7ea7\u84b8\u998f\u6765\u5b66\u4e60\u55b7\u6ce8\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u672a\u6807\u8bb0\u7684\u55b7\u6ce8\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u8868\u793a\u7a7a\u95f4\u4e2d\u51fa\u73b0\u8bed\u4e49\u7c7b\u522b\u805a\u7c7b\u3002", "result": "\u5728\u4ec5\u4f7f\u7528\u80cc\u666f\u55b7\u6ce8\u9884\u8bad\u7ec3\u7684\u51bb\u7ed3\u5d4c\u5165\u4e2d\uff0c\u805a\u7c7b\u73b0\u8c61\u4f7f\u5f97\u57fa\u4e8e\u7b80\u5355\u8ddd\u79bb\u5ea6\u91cf\u7684\u5f02\u5e38\u68c0\u6d4b\u6210\u4e3a\u53ef\u80fd\u3002\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7ecf\u8fc7\u5fae\u8c03\u540e\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u7684\u76d1\u7763\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "jBOT\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u4e8e\u9ad8\u80fd\u7269\u7406\u55b7\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u84b8\u998f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u8868\u793a\u65e2\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\uff0c\u53c8\u80fd\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u7c92\u5b50\u7269\u7406\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2601.11745", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11745", "abs": "https://arxiv.org/abs/2601.11745", "authors": ["Daniel Moghimi", "Alexandru-Cosmin Mihai", "Borbala Benko", "Catherine Vlasov", "Elie Bursztein", "Kurt Thomas", "Laszlo Siroki", "Pedro Barbosa", "Remi Audebert"], "title": "DROIDCCT: Cryptographic Compliance Test via Trillion-Scale Measurement", "comment": "ACSAC 2025", "summary": "We develop DroidCCT, a distributed test framework to evaluate the scale of a wide range of failures/bugs in cryptography for end users. DroidCCT relies on passive analysis of artifacts from the execution of cryptographic operations in the Android ecosystem to identify weak implementations. We collect trillions of samples from cryptographic operations of Android Keystore on half a billion devices and apply severalanalysis techniques to evaluate the quality of cryptographic output from these devices and their underlying implementations. Our study reveals several patterns of bugs and weakness in cryptographic implementations from various manufacturers and chipsets. We show that the heterogeneous nature of cryptographic implementations results in non-uniform availability and reliability of various cryptographic functions. More importantly, flaws such as the use of weakly-generated random parameters, and timing side channels may surface across deployments of cryptography. Our results highlight the importance of fault- and side-channel-resistant cryptography and the ability to transparently and openly test these implementations.", "AI": {"tldr": "DroidCCT\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30Android\u751f\u6001\u7cfb\u7edf\u4e2d\u5bc6\u7801\u5b66\u5b9e\u73b0\u7684\u6545\u969c\u89c4\u6a21\uff0c\u901a\u8fc7\u5206\u6790\u6570\u5341\u4ebf\u8bbe\u5907\u4e0a\u7684\u5bc6\u7801\u5b66\u64cd\u4f5c\u53d1\u73b0\u591a\u79cd\u5b89\u5168\u6f0f\u6d1e\u548c\u5f31\u70b9\u3002", "motivation": "\u8bc4\u4f30Android\u751f\u6001\u7cfb\u7edf\u5bc6\u7801\u5b66\u5b9e\u73b0\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u8bc6\u522b\u5404\u79cd\u5382\u5546\u548c\u82af\u7247\u7ec4\u4e2d\u5bc6\u7801\u5b66\u5b9e\u73b0\u7684\u6f0f\u6d1e\u6a21\u5f0f\uff0c\u4e86\u89e3\u5f02\u6784\u5bc6\u7801\u5b66\u5b9e\u73b0\u5bfc\u81f4\u7684\u975e\u5747\u5300\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1DroidCCT\u5206\u5e03\u5f0f\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u88ab\u52a8\u5206\u6790Android Keystore\u5bc6\u7801\u5b66\u64cd\u4f5c\u7684\u6267\u884c\u5de5\u4ef6\uff0c\u6536\u96c6\u6765\u81ea5\u4ebf\u8bbe\u5907\u7684\u6570\u4e07\u4ebf\u6837\u672c\uff0c\u5e94\u7528\u591a\u79cd\u5206\u6790\u6280\u672f\u8bc4\u4f30\u5bc6\u7801\u5b66\u8f93\u51fa\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u79cd\u5382\u5546\u548c\u82af\u7247\u7ec4\u7684\u5bc6\u7801\u5b66\u5b9e\u73b0\u5b58\u5728\u6f0f\u6d1e\u6a21\u5f0f\uff0c\u5305\u62ec\u5f31\u968f\u673a\u53c2\u6570\u751f\u6210\u548c\u65f6\u5e8f\u4fa7\u4fe1\u9053\u7b49\u7f3a\u9677\uff0c\u5f02\u6784\u5b9e\u73b0\u5bfc\u81f4\u5bc6\u7801\u5b66\u529f\u80fd\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u4e0d\u5747\u5300\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6297\u6545\u969c\u548c\u6297\u4fa7\u4fe1\u9053\u5bc6\u7801\u5b66\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u900f\u660e\u516c\u5f00\u6d4b\u8bd5\u8fd9\u4e9b\u5b9e\u73b0\u7684\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u5bc6\u7801\u5b66\u6f0f\u6d1e\u7684\u666e\u904d\u5b58\u5728\u3002"}}
{"id": "2601.11701", "categories": ["math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.11701", "abs": "https://arxiv.org/abs/2601.11701", "authors": ["Abhinav Chakraborty", "Yuetian Luo", "Rina Foygel Barber"], "title": "Stability and Accuracy Trade-offs in Statistical Estimation", "comment": "The first two authors contributed equally and are listed in alphabetical order", "summary": "Algorithmic stability is a central concept in statistics and learning theory that measures how sensitive an algorithm's output is to small changes in the training data. Stability plays a crucial role in understanding generalization, robustness, and replicability, and a variety of stability notions have been proposed in different learning settings. However, while stability entails desirable properties, it is typically not sufficient on its own for statistical learning -- and indeed, it may be at odds with accuracy, since an algorithm that always outputs a constant function is perfectly stable but statistically meaningless. Thus, it is essential to understand the potential statistical cost of stability. In this work, we address this question by adopting a statistical decision-theoretic perspective, treating stability as a constraint in estimation. Focusing on two representative notions-worst-case stability and average-case stability-we first establish general lower bounds on the achievable estimation accuracy under each type of stability constraint. We then develop optimal stable estimators for four canonical estimation problems, including several mean estimation and regression settings. Together, these results characterize the optimal trade-offs between stability and accuracy across these tasks. Our findings formalize the intuition that average-case stability imposes a qualitatively weaker restriction than worst-case stability, and they further reveal that the gap between these two can vary substantially across different estimation problems.", "AI": {"tldr": "\u672c\u6587\u4ece\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\u7814\u7a76\u7b97\u6cd5\u7a33\u5b9a\u6027\u4e0e\u4f30\u8ba1\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5efa\u7acb\u4e86\u7a33\u5b9a\u6027\u7ea6\u675f\u4e0b\u7684\u4f30\u8ba1\u7cbe\u5ea6\u4e0b\u754c\uff0c\u5e76\u9488\u5bf9\u56db\u4e2a\u5178\u578b\u4f30\u8ba1\u95ee\u9898\u5f00\u53d1\u4e86\u6700\u4f18\u7a33\u5b9a\u4f30\u8ba1\u5668\u3002", "motivation": "\u7b97\u6cd5\u7a33\u5b9a\u6027\u662f\u7edf\u8ba1\u548c\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u8861\u91cf\u7b97\u6cd5\u8f93\u51fa\u5bf9\u8bad\u7ec3\u6570\u636e\u5fae\u5c0f\u53d8\u5316\u7684\u654f\u611f\u6027\u3002\u867d\u7136\u7a33\u5b9a\u6027\u5e26\u6765\u826f\u597d\u7684\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u590d\u73b0\u6027\uff0c\u4f46\u5355\u7eaf\u7a33\u5b9a\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u7edf\u8ba1\u5b66\u4e60\u6548\u679c\uff0c\u751a\u81f3\u53ef\u80fd\u4e0e\u51c6\u786e\u6027\u76f8\u51b2\u7a81\uff08\u5982\u6052\u5b9a\u8f93\u51fa\u51fd\u6570\u5b8c\u5168\u7a33\u5b9a\u4f46\u7edf\u8ba1\u65e0\u610f\u4e49\uff09\u3002\u56e0\u6b64\u9700\u8981\u91cf\u5316\u7a33\u5b9a\u6027\u7684\u7edf\u8ba1\u4ee3\u4ef7\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\uff0c\u5c06\u7a33\u5b9a\u6027\u4f5c\u4e3a\u4f30\u8ba1\u7684\u7ea6\u675f\u6761\u4ef6\u3002\u805a\u7126\u4e8e\u6700\u574f\u60c5\u51b5\u7a33\u5b9a\u6027\u548c\u5e73\u5747\u60c5\u51b5\u7a33\u5b9a\u6027\u4e24\u79cd\u4ee3\u8868\u6027\u6982\u5ff5\uff1a\u9996\u5148\u5efa\u7acb\u6bcf\u79cd\u7a33\u5b9a\u6027\u7ea6\u675f\u4e0b\u53ef\u8fbe\u5230\u4f30\u8ba1\u7cbe\u5ea6\u7684\u4e00\u822c\u4e0b\u754c\uff1b\u7136\u540e\u9488\u5bf9\u56db\u4e2a\u5178\u578b\u4f30\u8ba1\u95ee\u9898\uff08\u5305\u62ec\u5747\u503c\u4f30\u8ba1\u548c\u56de\u5f52\u8bbe\u7f6e\uff09\u5f00\u53d1\u6700\u4f18\u7a33\u5b9a\u4f30\u8ba1\u5668\u3002", "result": "\u5efa\u7acb\u4e86\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6700\u4f18\u6743\u8861\u5173\u7cfb\u3002\u5f62\u5f0f\u5316\u4e86\u5e73\u5747\u60c5\u51b5\u7a33\u5b9a\u6027\u6bd4\u6700\u574f\u60c5\u51b5\u7a33\u5b9a\u6027\u9650\u5236\u66f4\u5f31\u7684\u76f4\u89c9\uff0c\u5e76\u63ed\u793a\u8fd9\u4e24\u79cd\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u5728\u4e0d\u540c\u4f30\u8ba1\u95ee\u9898\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u901a\u8fc7\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u6846\u67b6\u7cfb\u7edf\u5206\u6790\u4e86\u7b97\u6cd5\u7a33\u5b9a\u6027\u7684\u7edf\u8ba1\u4ee3\u4ef7\uff0c\u4e3a\u7406\u89e3\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u7a33\u5b9a\u6027\u6982\u5ff5\u5bf9\u4f30\u8ba1\u6027\u80fd\u7684\u5dee\u5f02\u5316\u5f71\u54cd\u3002"}}
{"id": "2601.11789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11789", "abs": "https://arxiv.org/abs/2601.11789", "authors": ["Shenyang Deng", "Boyao Liao", "Zhuoli Ouyang", "Tianyu Pang", "Minhak Song", "Yaoqing Yang"], "title": "Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis", "comment": "The 37th International Conference on Algorithmic Learning Theory", "summary": "This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $\u03b7_t^*$ separates alignment-decreasing ($\u03b7_t < \u03b7_t^*$) from alignment-increasing ($\u03b7_t > \u03b7_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u5bf9\u9f50\u7684\u52a8\u6001\u53d8\u5316\u89c4\u5f8b\u53ca\u5176\u4e0e\u6b65\u957f\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u51fa\u73b0\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff0c\u5373\u68af\u5ea6\u4e0eHessian\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u884c\u4e3a\u5448\u73b0\u5148\u4e0b\u964d\u3001\u540e\u4e0a\u5347\u3001\u6700\u7ec8\u7a33\u5b9a\u7684\u4e09\u9636\u6bb5\u6a21\u5f0f\uff0c\u4e14\u8fd9\u79cd\u9ad8\u5ea6\u5bf9\u9f50\u53cd\u800c\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u635f\u5931\u3002", "method": "\u5728\u9ad8\u7ef4\u4e8c\u6b21\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u63d0\u51fa\u6b65\u957f\u9009\u62e9\u7406\u8bba\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\u7684\u6982\u5ff5\uff0c\u533a\u5206\u5bf9\u9f50\u4e0b\u964d\u548c\u5bf9\u9f50\u4e0a\u5347\u7684\u6b65\u957f\u533a\u95f4\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u5b50\u7a7a\u95f4\u6295\u5f71\u5bf9\u635f\u5931\u7684\u5f71\u54cd\u3002", "result": "1. \u5728\u4f4e\u5bf9\u9f50\u533a\u57df\uff0c\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\u533a\u5206\u5bf9\u9f50\u4e0b\u964d(\u03b7_t < \u03b7_t^*)\u548c\u5bf9\u9f50\u4e0a\u5347(\u03b7_t > \u03b7_t^*)\u7684\u6b65\u957f\u533a\u95f4\uff1b2. \u5728\u9ad8\u5bf9\u9f50\u533a\u57df\uff0c\u5bf9\u9f50\u5177\u6709\u81ea\u6821\u6b63\u7279\u6027\uff0c\u65e0\u8bba\u6b65\u957f\u5982\u4f55\u90fd\u4f1a\u4e0b\u964d\uff1b3. \u5728\u5145\u5206\u75c5\u6001\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u6b65\u957f\u533a\u95f4\u4f7f\u5f97\u5c06SGD\u66f4\u65b0\u6295\u5f71\u5230\u6279\u91cf\u5b50\u7a7a\u95f4\u80fd\u964d\u4f4e\u635f\u5931\uff0c\u800c\u6295\u5f71\u5230\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u53cd\u800c\u589e\u52a0\u635f\u5931\uff1b4. \u5bf9\u4e8e\u6052\u5b9a\u6b65\u957f\u548c\u5927\u521d\u59cb\u5316\uff0cSGD\u8868\u73b0\u51fa\u660e\u663e\u7684\u4e24\u9636\u6bb5\u884c\u4e3a\uff1a\u521d\u59cb\u5bf9\u9f50\u4e0b\u964d\u9636\u6bb5\uff0c\u968f\u540e\u7a33\u5b9a\u5728\u9ad8\u5bf9\u9f50\u72b6\u6001\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u6b65\u957f\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u68af\u5ea6\u5bf9\u9f50\u52a8\u6001\uff0c\u5e76\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u6295\u5f71\u5230\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u68af\u5ea6\u66f4\u65b0\u65e0\u6548\uff0c\u4e3a\u7406\u89e3SGD\u5728\u75c5\u6001\u95ee\u9898\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.11786", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11786", "abs": "https://arxiv.org/abs/2601.11786", "authors": ["Taehyun Noh", "Yingchen Wang", "Tal Garfinkel", "Mahesh Madhav", "Daniel Moghimi", "Mattan Erez", "Shravan Narayan"], "title": "ARM MTE Performance in Practice (Extended Version)", "comment": "Accepted at Usenix Security 2026", "summary": "We present the first comprehensive analysis of ARM MTE hardware performance on four different microarchitectures: ARM Big (A7x), Little (A5x), and Performance (Cortex-X) cores on the Google Pixel 8 and Pixel 9, and on Ampere Computing's AmpereOne CPU core. We also include preliminary analysis of MTE on Apple's M5 chip. We investigate performance in MTE's primary application -- probabilistic memory safety -- on both SPEC CPU benchmarks and in server workloads such as RocksDB, Nginx, PostgreSQL, and Memcached. While MTE often exhibits modest overheads, we also see performance slowdowns up to 6.64x on certain benchmarks. We identify the microarchitectural cause of these overheads and where they can be addressed in future processors. We then analyze MTE's performance for more specialized security applications such as memory tracing, time-of-check time-of-use prevention, sandboxing, and CFI. In some of these cases, MTE offers significant advantages today, while the benefits for other cases are negligible or will depend on future hardware. Finally, we explore where prior work characterizing MTE performance has either been incomplete or incorrect due to methodological or experimental errors.", "AI": {"tldr": "\u9996\u6b21\u5168\u9762\u5206\u6790ARM MTE\u786c\u4ef6\u6027\u80fd\uff0c\u8986\u76d6\u56db\u79cd\u5fae\u67b6\u6784\uff0c\u53d1\u73b0MTE\u5728\u5185\u5b58\u5b89\u5168\u5e94\u7528\u4e2d\u901a\u5e38\u6709\u9002\u5ea6\u5f00\u9500\uff0c\u4f46\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe6.64\u500d\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86MTE\u5728\u4e13\u4e1a\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "ARM\u5185\u5b58\u6807\u7b7e\u6269\u5c55\uff08MTE\uff09\u662f\u4e00\u79cd\u786c\u4ef6\u5185\u5b58\u5b89\u5168\u6280\u672f\uff0c\u4f46\u4e4b\u524d\u5bf9\u5176\u6027\u80fd\u7279\u5f81\u7684\u7814\u7a76\u5b58\u5728\u4e0d\u5b8c\u6574\u6216\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u9996\u4e2a\u5168\u9762\u7684MTE\u786c\u4ef6\u6027\u80fd\u5206\u6790\uff0c\u8986\u76d6\u591a\u79cd\u5fae\u67b6\u6784\u548c\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u5728\u56db\u79cd\u4e0d\u540c\u5fae\u67b6\u6784\u4e0a\u5206\u6790MTE\u6027\u80fd\uff1aGoogle Pixel 8\u548cPixel 9\u7684ARM Big\uff08A7x\uff09\u3001Little\uff08A5x\uff09\u548cPerformance\uff08Cortex-X\uff09\u6838\u5fc3\uff0cAmpere Computing\u7684AmpereOne CPU\u6838\u5fc3\uff0c\u4ee5\u53caApple M5\u82af\u7247\u7684\u521d\u6b65\u5206\u6790\u3002\u4f7f\u7528SPEC CPU\u57fa\u51c6\u6d4b\u8bd5\u548c\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\uff08RocksDB\u3001Nginx\u3001PostgreSQL\u3001Memcached\uff09\u8bc4\u4f30MTE\u5728\u6982\u7387\u6027\u5185\u5b58\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "result": "MTE\u901a\u5e38\u8868\u73b0\u51fa\u9002\u5ea6\u5f00\u9500\uff0c\u4f46\u5728\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe6.64\u500d\u3002\u8bc6\u522b\u4e86\u8fd9\u4e9b\u5f00\u9500\u7684\u5fae\u67b6\u6784\u539f\u56e0\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5904\u7406\u5668\u4e2d\u53ef\u4ee5\u6539\u8fdb\u7684\u5730\u65b9\u3002\u5728\u5185\u5b58\u8ffd\u8e2a\u3001TOCTOU\u9884\u9632\u3001\u6c99\u7bb1\u548cCFI\u7b49\u4e13\u4e1a\u5b89\u5168\u5e94\u7528\u4e2d\uff0cMTE\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u4f9b\u663e\u8457\u4f18\u52bf\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u4f18\u52bf\u6709\u9650\u6216\u9700\u4f9d\u8d56\u672a\u6765\u786c\u4ef6\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684ARM MTE\u786c\u4ef6\u6027\u80fd\u5206\u6790\uff0c\u63ed\u793a\u4e86MTE\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u7279\u5f81\u3002\u867d\u7136MTE\u5728\u6982\u7387\u6027\u5185\u5b58\u5b89\u5168\u4e2d\u901a\u5e38\u6709\u9002\u5ea6\u5f00\u9500\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\u663e\u8457\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u4e4b\u524dMTE\u6027\u80fd\u7814\u7a76\u4e2d\u7684\u65b9\u6cd5\u8bba\u6216\u5b9e\u9a8c\u9519\u8bef\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u4f18\u5316\u548cMTE\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.11794", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11794", "abs": "https://arxiv.org/abs/2601.11794", "authors": ["Abdelrahman Ramadan", "Zahra Dorbeigi Namaghi", "Emily Taylor", "Lucas Edwards", "Xan Giuliani", "David S. McLagan", "Sidney Givigi", "Melissa Greeff"], "title": "Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing", "comment": null, "summary": "Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\\% smoothness improvement and 90.7\\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\\% negative outputs. The lean variant outperforms wide (+5.6\\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.", "AI": {"tldr": "PC\u00b2DAE\uff1a\u4e00\u79cd\u7269\u7406\u7ea6\u675f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u67b6\u6784\u5d4c\u5165\u7269\u7406\u7ea6\u675f\u89e3\u51b3\u65e0\u4eba\u673a\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u7269\u7406\u8fdd\u89c4\u7684\u9ad8\u6548\u53bb\u566a", "motivation": "\u65e0\u4eba\u673a\u642d\u8f7d\u4f4e\u6210\u672c\u4f20\u611f\u5668\u8fdb\u884c\u91ce\u706b\u76d1\u6d4b\u65f6\u5b58\u5728\u57fa\u7ebf\u6f02\u79fb\u3001\u4ea4\u53c9\u654f\u611f\u6027\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4f46\u65e0\u4eba\u673a\u98de\u884c\u6d3b\u52a8\u6570\u636e\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u4e0b\u7684\u7269\u7406\u7ea6\u675f\u53bb\u566a\u95ee\u9898", "method": "\u63d0\u51faPC\u00b2DAE\u7269\u7406\u7ea6\u675f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7softplus\u6fc0\u6d3b\u51fd\u6570\u5f3a\u5236\u975e\u8d1f\u6d53\u5ea6\u4f30\u8ba1\uff0c\u7269\u7406\u5408\u7406\u7684\u65f6\u95f4\u5e73\u6ed1\uff0c\u67b6\u6784\u5305\u542b\u9ed1\u78b3\u3001\u6c14\u4f53\u548cCO\u2082\u4f20\u611f\u5668\u5bb6\u65cf\u7684\u5206\u5c42\u89e3\u7801\u5668\u5934\uff0c\u63d0\u4f9bPC\u00b2DAE-Lean\uff0821k\u53c2\u6570\uff09\u8fb9\u7f18\u90e8\u7f72\u548cPC\u00b2DAE-Wide\uff08204k\u53c2\u6570\uff09\u79bb\u7ebf\u5904\u7406\u4e24\u4e2a\u53d8\u4f53", "result": "\u5728\u52a0\u62ff\u5927\u8428\u65af\u5580\u5f7b\u6e29\u7701\u89c4\u5b9a\u71c3\u70e7\u671f\u95f4\u6536\u96c6\u76847,894\u4e2a\u540c\u6b651Hz\u6837\u672c\uff08\u7ea62.2\u5c0f\u65f6\u98de\u884c\u6570\u636e\uff09\u4e0a\u8bc4\u4f30\uff0cPC\u00b2DAE-Lean\u5b9e\u73b067.3%\u5e73\u6ed1\u5ea6\u63d0\u5347\u548c90.7%\u9ad8\u9891\u566a\u58f0\u964d\u4f4e\uff0c\u96f6\u7269\u7406\u8fdd\u89c4\uff1b\u4e94\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f15-23%\u8d1f\u8f93\u51fa\uff1b\u7cbe\u7b80\u53d8\u4f53\u4f18\u4e8e\u5bbd\u53d8\u4f53\uff08+5.6%\u5e73\u6ed1\u5ea6\uff09\uff0c\u5728\u6d88\u8d39\u786c\u4ef6\u4e0a\u8bad\u7ec3\u65f6\u95f4\u5c0f\u4e8e65\u79d2", "conclusion": "PC\u00b2DAE\u901a\u8fc7\u67b6\u6784\u5d4c\u5165\u7269\u7406\u7ea6\u675f\u800c\u975e\u635f\u5931\u51fd\u6570\u60e9\u7f5a\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u4e0b\u7684\u4f20\u611f\u5668\u53bb\u566a\u95ee\u9898\uff0c\u7cbe\u7b80\u67b6\u6784\u914d\u5408\u5f3a\u5f52\u7eb3\u504f\u7f6e\u53ef\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u7684\u65e0\u4eba\u673a\u91ce\u706b\u76d1\u6d4b\u5e94\u7528"}}
{"id": "2601.11838", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.11838", "abs": "https://arxiv.org/abs/2601.11838", "authors": ["Hao Lyu", "Jingzheng Wu", "Xiang Ling", "Yicheng Zhong", "Zhiyuan Li", "Tianyue Luo"], "title": "SimFuzz: Similarity-guided Block-level Mutation for RISC-V Processor Fuzzing", "comment": "7 pages, 5 figures", "summary": "The Instruction Set Architecture (ISA) defines processor operations and serves as the interface between hardware and software. As an open ISA, RISC-V lowers the barriers to processor design and encourages widespread adoption, but also exposes processors to security risks such as functional bugs. Processor fuzzing is a powerful technique for automatically detecting these bugs. However, existing fuzzing methods suffer from two main limitations. First, their emphasis on redundant test case generation causes them to overlook cross-processor corner cases. Second, they rely too heavily on coverage guidance. Current coverage metrics are biased and inefficient, and become ineffective once coverage growth plateaus. To overcome these limitations, we propose SimFuzz, a fuzzing framework that constructs a high-quality seed corpus from historical bug-triggering inputs and employs similarity-guided, block-level mutation to efficiently explore the processor input space. By introducing instruction similarity, SimFuzz expands the input space around seeds while preserving control-flow structure, enabling deeper exploration without relying on coverage feedback. We evaluate SimFuzz on three widely used open-source RISC-V processors: Rocket, BOOM, and XiangShan, and discover 17 bugs in total, including 14 previously unknown issues, 7 of which have been assigned CVE identifiers. These bugs affect the decode and memory units, cause instruction and data errors, and can lead to kernel instability or system crashes. Experimental results show that SimFuzz achieves up to 73.22% multiplexer coverage on the high-quality seed corpus. Our findings highlight critical security bugs in mainstream RISC-V processors and offer actionable insights for improving functional verification.", "AI": {"tldr": "SimFuzz\u662f\u4e00\u4e2a\u9488\u5bf9RISC-V\u5904\u7406\u5668\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u5386\u53f2bug\u89e6\u53d1\u8f93\u5165\u6784\u5efa\u9ad8\u8d28\u91cf\u79cd\u5b50\u8bed\u6599\u5e93\uff0c\u5e76\u91c7\u7528\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u5757\u7ea7\u7a81\u53d8\u6765\u9ad8\u6548\u63a2\u7d22\u5904\u7406\u5668\u8f93\u5165\u7a7a\u95f4\uff0c\u53d1\u73b0\u4e8617\u4e2abug\uff08\u5305\u62ec14\u4e2a\u65b0bug\uff09\u3002", "motivation": "RISC-V\u4f5c\u4e3a\u5f00\u653eISA\u964d\u4f4e\u4e86\u5904\u7406\u5668\u8bbe\u8ba1\u95e8\u69db\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1) \u8fc7\u5ea6\u5f3a\u8c03\u5197\u4f59\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\uff0c\u5ffd\u7565\u4e86\u8de8\u5904\u7406\u5668\u8fb9\u754c\u60c5\u51b5\uff1b2) \u8fc7\u5ea6\u4f9d\u8d56\u8986\u76d6\u7387\u6307\u5bfc\uff0c\u800c\u5f53\u524d\u8986\u76d6\u7387\u6307\u6807\u5b58\u5728\u504f\u5dee\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u5728\u8986\u76d6\u7387\u589e\u957f\u505c\u6ede\u65f6\u5931\u6548\u3002", "method": "SimFuzz\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1) \u4ece\u5386\u53f2bug\u89e6\u53d1\u8f93\u5165\u6784\u5efa\u9ad8\u8d28\u91cf\u79cd\u5b50\u8bed\u6599\u5e93\uff1b2) \u91c7\u7528\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u5757\u7ea7\u7a81\u53d8\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u6307\u4ee4\u76f8\u4f3c\u6027\u5728\u4fdd\u6301\u63a7\u5236\u6d41\u7ed3\u6784\u7684\u540c\u65f6\u6269\u5c55\u8f93\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e0d\u4f9d\u8d56\u8986\u76d6\u7387\u53cd\u9988\u7684\u6df1\u5ea6\u63a2\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90RISC-V\u5904\u7406\u5668\uff08Rocket\u3001BOOM\u3001XiangShan\uff09\u4e0a\u8bc4\u4f30SimFuzz\uff0c\u5171\u53d1\u73b017\u4e2abug\uff0c\u5305\u62ec14\u4e2a\u5148\u524d\u672a\u77e5\u7684\u95ee\u9898\uff0c\u5176\u4e2d7\u4e2a\u5df2\u5206\u914dCVE\u6807\u8bc6\u3002\u8fd9\u4e9bbug\u5f71\u54cd\u89e3\u7801\u548c\u5185\u5b58\u5355\u5143\uff0c\u5bfc\u81f4\u6307\u4ee4\u548c\u6570\u636e\u9519\u8bef\uff0c\u53ef\u80fd\u5f15\u8d77\u5185\u6838\u4e0d\u7a33\u5b9a\u6216\u7cfb\u7edf\u5d29\u6e83\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSimFuzz\u5728\u9ad8\u8d28\u91cf\u79cd\u5b50\u8bed\u6599\u5e93\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe73.22%\u7684\u591a\u8def\u590d\u7528\u5668\u8986\u76d6\u7387\u3002", "conclusion": "SimFuzz\u901a\u8fc7\u514b\u670d\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6709\u6548\u53d1\u73b0\u4e86\u4e3b\u6d41RISC-V\u5904\u7406\u5668\u4e2d\u7684\u5173\u952e\u5b89\u5168bug\uff0c\u4e3a\u6539\u8fdb\u529f\u80fd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u8986\u76d6\u7387\u53cd\u9988\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u7a81\u53d8\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u6df1\u5165\u7684\u5904\u7406\u5668\u8f93\u5165\u7a7a\u95f4\u63a2\u7d22\u3002"}}
{"id": "2601.12064", "categories": ["math.ST", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.12064", "abs": "https://arxiv.org/abs/2601.12064", "authors": ["Nadezhda Gribkova", "Jianxi Su", "Mengqi Wang"], "title": "Expansion and Bounds for the Bias of Empirical Tail Value-at-Risk", "comment": "38 pages, 7 figures", "summary": "Tail Value-at-Risk (TVaR) is a widely adopted risk measure playing a critically important role in both academic research and industry practice in insurance. In data applications, TVaR is often estimated using the empirical method, owing to its simplicity and nonparametric nature. The empirical TVaR has been explicitly advocated by regulatory authorities as a standard approach for computing TVaR. However, prior literature has pointed out that the empirical TVaR estimator is negatively biased, which can lead to a systemic underestimation of risk in finite-sample applications. This paper aims to deepen the understanding of the bias of the empirical TVaR estimator in two dimensions: its magnitude as well as the key distributional and structural determinants driving the severity of the bias. To this end, we derive a leading-term approximation for the bias based on its asymptotic expansion. The closed-form expression associated with the leading-term approximation enables us to obtain analytical insights into the structural properties governing the bias of the empirical TVaR estimator. To account for the discrepancy between the leading-term approximation and the true bias, we further derive an explicit upper bound for the bias. We validate the proposed bias analysis framework via simulations and demonstrate its practical relevance using real data.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u504f\u5dee\u7684\u4e3b\u5bfc\u9879\u8fd1\u4f3c\u548c\u663e\u5f0f\u4e0a\u754c\uff0c\u63ed\u793a\u4e86\u504f\u5dee\u7684\u5206\u5e03\u7279\u6027\u548c\u7ed3\u6784\u51b3\u5b9a\u56e0\u7d20\u3002", "motivation": "TVaR\u662f\u4fdd\u9669\u9886\u57df\u5e7f\u6cdb\u91c7\u7528\u7684\u98ce\u9669\u5ea6\u91cf\u6307\u6807\uff0c\u7ecf\u9a8cTVaR\u56e0\u5176\u7b80\u5355\u6027\u548c\u975e\u53c2\u6570\u7279\u6027\u5e38\u88ab\u7528\u4e8e\u6570\u636e\u5e94\u7528\u3002\u7136\u800c\uff0c\u5df2\u6709\u6587\u732e\u6307\u51fa\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u5b58\u5728\u8d1f\u504f\u5dee\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u9650\u6837\u672c\u5e94\u7528\u4e2d\u7cfb\u7edf\u6027\u4f4e\u4f30\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u4ece\u504f\u5dee\u5e45\u5ea6\u53ca\u5176\u5173\u952e\u5206\u5e03\u548c\u7ed3\u6784\u51b3\u5b9a\u56e0\u7d20\u4e24\u4e2a\u7ef4\u5ea6\u6df1\u5316\u5bf9\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u504f\u5dee\u7684\u7406\u89e3\u3002", "method": "\u57fa\u4e8e\u6e10\u8fd1\u5c55\u5f00\u63a8\u5bfc\u4e86\u504f\u5dee\u7684\u4e3b\u5bfc\u9879\u8fd1\u4f3c\uff0c\u83b7\u5f97\u5c01\u95ed\u5f62\u5f0f\u7684\u8868\u8fbe\u5f0f\u4ee5\u5206\u6790\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u504f\u5dee\u7684\u7ed3\u6784\u7279\u6027\u3002\u4e3a\u8fdb\u4e00\u6b65\u8003\u8651\u4e3b\u5bfc\u9879\u8fd1\u4f3c\u4e0e\u771f\u5b9e\u504f\u5dee\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63a8\u5bfc\u4e86\u504f\u5dee\u7684\u663e\u5f0f\u4e0a\u754c\u3002\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u504f\u5dee\u5206\u6790\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u76f8\u5173\u6027\u3002", "result": "\u83b7\u5f97\u4e86\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u504f\u5dee\u7684\u4e3b\u5bfc\u9879\u8fd1\u4f3c\u8868\u8fbe\u5f0f\uff0c\u80fd\u591f\u89e3\u6790\u5730\u6d1e\u5bdf\u504f\u5dee\u7684\u7ed3\u6784\u7279\u6027\u3002\u63a8\u5bfc\u4e86\u504f\u5dee\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u4e3a\u504f\u5dee\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002\u6a21\u62df\u9a8c\u8bc1\u548c\u771f\u5b9e\u6570\u636e\u5e94\u7528\u8bc1\u5b9e\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u5b9e\u9645\u610f\u4e49\u3002", "conclusion": "\u672c\u6587\u6df1\u5316\u4e86\u5bf9\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u504f\u5dee\u7684\u7406\u89e3\uff0c\u63d0\u4f9b\u4e86\u5206\u6790\u504f\u5dee\u5e45\u5ea6\u53ca\u5176\u51b3\u5b9a\u56e0\u7d20\u7684\u7406\u8bba\u6846\u67b6\u3002\u6240\u63a8\u5bfc\u7684\u4e3b\u5bfc\u9879\u8fd1\u4f3c\u548c\u663e\u5f0f\u4e0a\u754c\u4e3a\u8bc4\u4f30\u548c\u6821\u6b63\u7ecf\u9a8cTVaR\u4f30\u8ba1\u91cf\u7684\u504f\u5dee\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u907f\u514d\u5728\u98ce\u9669\u7ba1\u7406\u548c\u76d1\u7ba1\u5e94\u7528\u4e2d\u7cfb\u7edf\u6027\u4f4e\u4f30\u98ce\u9669\u3002"}}
{"id": "2601.11821", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11821", "abs": "https://arxiv.org/abs/2601.11821", "authors": ["Shivani Tomar", "Seshu Tirupathi", "Elizabeth Daly", "Ivana Dusparic"], "title": "Shapelets-Enriched Selective Forecasting using Time Series Foundation Models", "comment": "Accepted by the AAAI-26 Workshop on Artificial Intelligence for Time Series Analysis (AI4TS)", "summary": "Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eshapelet\u7684\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u5173\u952e\u533a\u57df\uff0c\u9009\u62e9\u6027\u4e22\u5f03\u4e0d\u53ef\u9760\u9884\u6d4b\uff0c\u63d0\u5347\u96f6\u6837\u672c\u548c\u5168\u6837\u672c\u5fae\u8c03\u6a21\u578b\u7684\u9884\u6d4b\u53ef\u9760\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u67d0\u4e9b\u5173\u952e\u6570\u636e\u533a\u57df\u7684\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u7279\u522b\u662f\u5f53\u6570\u636e\u5448\u73b0\u72ec\u7279\u8d8b\u52bf\u65f6\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528shapelet\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u5173\u952e\u533a\u57df\u3002\u901a\u8fc7\u76ee\u6807\u57df\u9a8c\u8bc1\u96c6\u4e0a\u7684\u5e73\u79fb\u4e0d\u53d8\u5b57\u5178\u5b66\u4e60\u5b66\u4e60shapelet\uff0c\u5229\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u9009\u62e9\u6027\u4e22\u5f03\u4e0d\u53ef\u9760\u9884\u6d4b\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7f\u96f6\u6837\u672c\u6a21\u578b\u6574\u4f53\u8bef\u5dee\u5e73\u5747\u964d\u4f4e22.17%\uff0c\u5168\u6837\u672c\u5fae\u8c03\u6a21\u578b\u964d\u4f4e22.62%\u3002\u5728\u67d0\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u96f6\u6837\u672c\u548c\u5168\u6837\u672c\u5fae\u8c03\u6a21\u578b\u5206\u522b\u6bd4\u968f\u673a\u9009\u62e9\u5bf9\u5e94\u65b9\u6cd5\u63d0\u534721.41%\u548c21.43%\u3002", "conclusion": "\u57fa\u4e8eshapelet\u7684\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u4e0d\u53ef\u9760\u9884\u6d4b\u533a\u57df\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u73b0\u5b9e\u7684\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u3002"}}
{"id": "2601.12957", "categories": ["math.ST", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.12957", "abs": "https://arxiv.org/abs/2601.12957", "authors": ["Hanne Kekkonen", "Andreas Tataris"], "title": "Random tree Besov priors: Data-driven regularisation parameter selection", "comment": null, "summary": "We develop a data-driven algorithm for automatically selecting the regularisation parameter in Bayesian inversion under random tree Besov priors. One of the key challenges in Bayesian inversion is the construction of priors that are both expressive and computationally feasible. Random tree Besov priors, introduced in Kekkonen et al. (2023), provide a flexible framework for capturing local regularity properties and sparsity patterns in a wavelet basis. In this paper, we extend this approach by introducing a hierarchical model that enables data-driven selection of the wavelet density parameter, allowing the regularisation strength to adapt across scales while retaining computational efficiency. We focus on nonparametric regression and also present preliminary plug-and-play results for a deconvolution problem.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u968f\u673a\u6811Besov\u5148\u9a8c\u4e0b\u7684\u8d1d\u53f6\u65af\u53cd\u6f14\u4e2d\u81ea\u52a8\u9009\u62e9\u6b63\u5219\u5316\u53c2\u6570\u3002", "motivation": "\u8d1d\u53f6\u65af\u53cd\u6f14\u4e2d\u7684\u5173\u952e\u6311\u6218\u662f\u6784\u5efa\u65e2\u5177\u6709\u8868\u8fbe\u529b\u53c8\u8ba1\u7b97\u53ef\u884c\u7684\u5148\u9a8c\u3002\u968f\u673a\u6811Besov\u5148\u9a8c\u867d\u7136\u63d0\u4f9b\u4e86\u6355\u6349\u5c0f\u6ce2\u57fa\u4e2d\u5c40\u90e8\u6b63\u5219\u6027\u548c\u7a00\u758f\u6a21\u5f0f\u7684\u7075\u6d3b\u6846\u67b6\uff0c\u4f46\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u81ea\u9002\u5e94\u9009\u62e9\u6b63\u5219\u5316\u53c2\u6570\u3002", "method": "\u5f15\u5165\u5206\u5c42\u6a21\u578b\uff0c\u5b9e\u73b0\u5c0f\u6ce2\u5bc6\u5ea6\u53c2\u6570\u7684\u6570\u636e\u9a71\u52a8\u9009\u62e9\uff0c\u4f7f\u6b63\u5219\u5316\u5f3a\u5ea6\u80fd\u591f\u8de8\u5c3a\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u91cd\u70b9\u7814\u7a76\u975e\u53c2\u6570\u56de\u5f52\uff0c\u5e76\u5c55\u793a\u4e86\u53bb\u5377\u79ef\u95ee\u9898\u7684\u521d\u6b65\u5373\u63d2\u5373\u7528\u7ed3\u679c\u3002", "result": "\u6269\u5c55\u4e86\u968f\u673a\u6811Besov\u5148\u9a8c\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6b63\u5219\u5316\u53c2\u6570\u7684\u81ea\u9002\u5e94\u6570\u636e\u9a71\u52a8\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u968f\u673a\u6811Besov\u5148\u9a8c\u4e2d\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7684\u95ee\u9898\uff0c\u4e3a\u8d1d\u53f6\u65af\u53cd\u6f14\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5728\u975e\u53c2\u6570\u56de\u5f52\u548c\u53bb\u5377\u79ef\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.11827", "categories": ["cs.LG", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.11827", "abs": "https://arxiv.org/abs/2601.11827", "authors": ["Andrea Rubbi", "Amir Akbarnejad", "Mohammad Vali Sanian", "Aryan Yazdan Parast", "Hesam Asadollahzadeh", "Arian Amani", "Naveed Akhtar", "Sarah Cooper", "Andrew Bassett", "Pietro Li\u00f2", "Lassi Paavolainen", "Sattar Vakili", "Mo Lotfollahi"], "title": "MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization", "comment": null, "summary": "Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.", "AI": {"tldr": "MixFlow\uff1a\u4e00\u79cd\u7528\u4e8e\u63cf\u8ff0\u7b26\u63a7\u5236\u751f\u6210\u7684\u6df7\u5408\u6761\u4ef6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u63cf\u8ff0\u7b26\u6761\u4ef6\u57fa\u5206\u5e03\u548c\u6d41\u573a\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u6761\u4ef6\u6d41\u57fa\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5728\u8bad\u7ec3\u6761\u4ef6\u4e4b\u5916\u8fdb\u884c\u5916\u63a8\uff0c\u8fd9\u6210\u4e3a\u6761\u4ef6\u751f\u6210\u5efa\u6a21\u7684\u6838\u5fc3\u6311\u6218", "method": "\u63d0\u51faMixFlow\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u77ed\u8def\u5f84\u6d41\u5339\u914d\u8054\u5408\u5b66\u4e60\u63cf\u8ff0\u7b26\u6761\u4ef6\u57fa\u5206\u5e03\u548c\u63cf\u8ff0\u7b26\u6761\u4ef6\u6d41\u573a\uff0c\u5c06\u57fa\u5206\u5e03\u5efa\u6a21\u4e3a\u53ef\u5b66\u4e60\u7684\u63cf\u8ff0\u7b26\u4f9d\u8d56\u6df7\u5408\u5206\u5e03", "result": "\u5728\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\u672a\u89c1\u6270\u52a8\u9884\u6d4b\u548c\u9ad8\u5185\u6db5\u663e\u5fae\u955c\u836f\u7269\u7b5b\u9009\u7b49\u591a\u4e2a\u9886\u57df\uff0cMixFlow\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u6761\u4ef6\u6d41\u5339\u914d\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u5e03\u5916\u6cdb\u5316", "conclusion": "MixFlow\u4e3a\u8de8\u5f02\u8d28\u9886\u57df\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u3001\u53ef\u63a7\u7684\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u96be\u9898"}}
{"id": "2601.11996", "categories": ["cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11996", "abs": "https://arxiv.org/abs/2601.11996", "authors": ["Shaunak Perni", "Minal Shirodkar", "Ramdas Karmalli"], "title": "MongoDB Injection Query Classification Model using MongoDB Log files as Training Data", "comment": "24 Pages, 5 Tables, 6 Figures, Journal", "summary": "NoSQL Injection attacks are a class of cybersecurity attacks where an attacker sends a specifically engineered query to a NoSQL database which then performs an unauthorized operation. To defend against such attacks, rule based systems were initially developed but then were found to be ineffective to innovative injection attacks hence a model based approach was developed. Most model based detection systems, during testing gave exponentially positive results but were trained only on the query statement sent to the server. However due to the scarcity of data and class imbalances these model based systems were found to be not effective against all attacks in the real world. This paper explores classifying NoSQL injection attacks sent to a MongoDB server based on Log Data, and other extracted features excluding raw query statements. The log data was collected from a simulated attack on an empty MongoDB server which was then processed and explored. A discriminant analysis was carried out to determine statistically significant features to discriminate between injection and benign queries resulting in a dataset of significant features. Several Machine learning based classification models using an AutoML library, \"FLAML\", as well as 6 manually programmed models were trained on this dataset , which were then trained on 50 randomized samples of data, cross validated and evaluated. The study found that the best model was the \"FLAML\" library's \"XGBoost limited depth\" model with an accuracy of 71%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u65e5\u5fd7\u6570\u636e\u7684NoSQL\u6ce8\u5165\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u6392\u9664\u539f\u59cb\u67e5\u8be2\u8bed\u53e5\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528FLAML AutoML\u5e93\u548c\u624b\u52a8\u7f16\u7a0b\u6a21\u578b\u5bf9MongoDB\u6ce8\u5165\u653b\u51fb\u8fdb\u884c\u5206\u7c7b\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523071%\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684NoSQL\u6ce8\u5165\u9632\u5fa1\u7cfb\u7edf\u5bf9\u65b0\u578b\u653b\u51fb\u65e0\u6548\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u68c0\u6d4b\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u67e5\u8be2\u8bed\u53e5\u8bad\u7ec3\uff0c\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u65e5\u5fd7\u6570\u636e\u548c\u5176\u4ed6\u7279\u5f81\uff08\u6392\u9664\u539f\u59cb\u67e5\u8be2\u8bed\u53e5\uff09\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u4ece\u6a21\u62df\u653b\u51fb\u7684\u7a7aMongoDB\u670d\u52a1\u5668\u6536\u96c6\u65e5\u5fd7\u6570\u636e\u5e76\u8fdb\u884c\u5904\u7406\u5206\u6790\uff1b2. \u901a\u8fc7\u5224\u522b\u5206\u6790\u786e\u5b9a\u533a\u5206\u6ce8\u5165\u67e5\u8be2\u548c\u826f\u6027\u67e5\u8be2\u7684\u7edf\u8ba1\u663e\u8457\u7279\u5f81\uff1b3. \u4f7f\u7528FLAML AutoML\u5e93\u548c6\u4e2a\u624b\u52a8\u7f16\u7a0b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7279\u5f81\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff1b4. \u91c7\u752850\u6b21\u968f\u673a\u6570\u636e\u91c7\u6837\u3001\u4ea4\u53c9\u9a8c\u8bc1\u548c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u662fFLAML\u5e93\u7684\"XGBoost limited depth\"\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u523071%\u3002\u8fd9\u8868\u660e\u57fa\u4e8e\u65e5\u5fd7\u7279\u5f81\uff08\u800c\u975e\u539f\u59cb\u67e5\u8be2\u8bed\u53e5\uff09\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u68c0\u6d4bNoSQL\u6ce8\u5165\u653b\u51fb\u3002", "conclusion": "\u57fa\u4e8e\u65e5\u5fd7\u6570\u636e\u548c\u5176\u4ed6\u63d0\u53d6\u7279\u5f81\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4bNoSQL\u6ce8\u5165\u653b\u51fb\uff0cFLAML AutoML\u5e93\u7684XGBoost\u6709\u9650\u6df1\u5ea6\u6a21\u578b\u5728\u6392\u9664\u539f\u59cb\u67e5\u8be2\u8bed\u53e5\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e8671%\u7684\u51c6\u786e\u7387\uff0c\u4e3a\u5b9e\u9645\u73af\u5883\u4e2d\u7684NoSQL\u6ce8\u5165\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.13254", "categories": ["math.ST", "math.AP", "math.FA", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.13254", "abs": "https://arxiv.org/abs/2601.13254", "authors": ["Dimitri Konen"], "title": "Inverting the Fisher information operator in non-linear models", "comment": null, "summary": "We consider non-linear regression models corrupted by generic noise when the regression functions form a non-linear subspace of L^2, relevant in non-linear PDE inverse problems and data assimilation. We show that when the score of the model is injective, the Fisher information operator is automatically invertible between well-identified Hilbert spaces, and we provide an operational characterization of these spaces. This allows us to construct in broad generality the efficient Gaussian involved in the classical minimax and convolution theorems to establish information lower bounds, that are typically achieved by Bayesian algorithms thus showing optimality of these methods. We illustrate our results on time-evolution PDE models for reaction-diffusion and Navier-Stokes equations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u975e\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5728\u566a\u58f0\u6c61\u67d3\u4e0b\u7684\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5f53\u6a21\u578b\u5f97\u5206\u51fd\u6570\u5177\u6709\u5355\u5c04\u6027\u65f6\uff0cFisher\u4fe1\u606f\u7b97\u5b50\u5728\u7279\u5b9a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u53ef\u9006\uff0c\u5e76\u5efa\u7acb\u4e86\u4fe1\u606f\u4e0b\u754c\u7684\u6700\u4f18\u6027\u7406\u8bba\u3002", "motivation": "\u7814\u7a76\u975e\u7ebf\u6027PDE\u53cd\u95ee\u9898\u548c\u6570\u636e\u540c\u5316\u4e2d\u975e\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u53d7\u5230\u4e00\u822c\u6027\u566a\u58f0\u6c61\u67d3\u3002\u9700\u8981\u5efa\u7acb\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\u6765\u63a8\u5bfc\u4fe1\u606f\u4e0b\u754c\u5e76\u8bc1\u660e\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u6700\u4f18\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5f97\u5206\u51fd\u6570\u7684\u5355\u5c04\u6027\uff0c\u8bc1\u660eFisher\u4fe1\u606f\u7b97\u5b50\u5728\u7279\u5b9a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u53ef\u9006\u6027\uff0c\u5e76\u7ed9\u51fa\u8fd9\u4e9b\u7a7a\u95f4\u7684\u64cd\u4f5c\u6027\u523b\u753b\u3002\u57fa\u4e8e\u6b64\u6784\u9020\u9ad8\u6548\u9ad8\u65af\u8fc7\u7a0b\uff0c\u5e94\u7528\u7ecf\u5178\u6781\u5c0f\u6781\u5927\u548c\u5377\u79ef\u5b9a\u7406\u5efa\u7acb\u4fe1\u606f\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u5f53\u5f97\u5206\u51fd\u6570\u5355\u5c04\u65f6\uff0cFisher\u4fe1\u606f\u7b97\u5b50\u81ea\u52a8\u53ef\u9006\uff0c\u5e76\u7ed9\u51fa\u4e86\u76f8\u5173\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u660e\u786e\u523b\u753b\u3002\u5efa\u7acb\u4e86\u901a\u7528\u7684\u4fe1\u606f\u4e0b\u754c\u7406\u8bba\uff0c\u8868\u660e\u8d1d\u53f6\u65af\u7b97\u6cd5\u901a\u5e38\u80fd\u8fbe\u5230\u8fd9\u4e9b\u4e0b\u754c\uff0c\u4ece\u800c\u8bc1\u660e\u5176\u6700\u4f18\u6027\u3002", "conclusion": "\u4e3a\u975e\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u822c\u6027\u7684\u7edf\u8ba1\u63a8\u65ad\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u7ebf\u6027PDE\u53cd\u95ee\u9898\u548c\u6570\u636e\u540c\u5316\u3002\u7406\u8bba\u8868\u660e\u5728\u5f97\u5206\u51fd\u6570\u5355\u5c04\u6761\u4ef6\u4e0b\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u7edf\u8ba1\u610f\u4e49\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u5e76\u5728\u53cd\u5e94-\u6269\u6563\u548cNavier-Stokes\u65b9\u7a0b\u7b49\u65f6\u95f4\u6f14\u5316PDE\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2601.11880", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11880", "abs": "https://arxiv.org/abs/2601.11880", "authors": ["Yingxiao Zhang", "Jiaxin Duan", "Junfu Zhang", "Ke Feng"], "title": "TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures", "comment": null, "summary": "Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.", "AI": {"tldr": "TF-CoDiT\uff1a\u9996\u4e2a\u7528\u4e8e\u8bed\u8a00\u63a7\u5236\u56fd\u503a\u671f\u8d27\u5408\u6210\u7684\u6269\u6563Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548cU\u5f62VAE\u5904\u7406\u4f4e\u6570\u636e\u91cf\uff0c\u5f15\u5165FinMAP\u534f\u8bae\u6807\u51c6\u5316\u5e02\u573a\u63cf\u8ff0\uff0c\u5728\u56fd\u503a\u671f\u8d27\u6570\u636e\u5408\u6210\u4e0a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6269\u6563Transformer\u5728\u80a1\u7968\u4ef7\u683c\u548c\u8ba2\u5355\u6d41\u7b49\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5408\u6210\u4e0a\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u56fd\u503a\u671f\u8d27\u6570\u636e\u7684\u5408\u6210\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u56fd\u503a\u671f\u8d27\u6570\u636e\u5177\u6709\u4f4e\u4ea4\u6613\u91cf\u3001\u5e02\u573a\u4f9d\u8d56\u6027\u5f3a\u3001\u591a\u53d8\u91cf\u95f4\u5b58\u5728\u5206\u7ec4\u76f8\u5173\u6027\u7b49\u72ec\u7279\u7279\u5f81\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faTF-CoDiT\u6846\u67b6\uff1a1\uff09\u5c06\u591a\u901a\u9053\u4e00\u7ef4\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u7cfb\u6570\u77e9\u9635\u4ee5\u4fc3\u8fdb\u4f4e\u6570\u636e\u5b66\u4e60\uff1b2\uff09\u8bbe\u8ba1U\u5f62VAE\u5206\u5c42\u7f16\u7801\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\u5230\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u89e3\u7801\u6865\u63a5\u6f5c\u5728\u7a7a\u95f4\u548cDWT\u7a7a\u95f4\uff0c\u5b9e\u73b0\u6f5c\u5728\u6269\u6563\u751f\u6210\uff1b3\uff09\u5f15\u5165\u91d1\u878d\u5e02\u573a\u5c5e\u6027\u534f\u8bae\uff08FinMAP\uff09\u4f5c\u4e3a\u591a\u5c42\u6b21\u63cf\u8ff0\u7cfb\u7edf\uff0c\u4ece7/8\u4e2a\u89c6\u89d2\u8bc6\u522b17/23\u4e2a\u7ecf\u6d4e\u6307\u6807\uff0c\u6807\u51c6\u5316\u6bcf\u65e5/\u5468\u671f\u6027\u5e02\u573a\u52a8\u6001\u4ee5\u751f\u6210\u63d0\u793a\u8bcd\u3002", "result": "\u6536\u96c62015-2025\u5e74\u56db\u79cd\u56fd\u503a\u671f\u8d27\u6570\u636e\uff0c\u5b9a\u4e49\u4ece\u4e00\u5468\u5230\u56db\u4e2a\u6708\u4e0d\u540c\u6301\u7eed\u65f6\u95f4\u7684\u5408\u6210\u4efb\u52a1\u3002\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793aTF-CoDiT\u80fd\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u6570\u636e\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u8bef\u5dee\u6700\u591a\u4e3aMSE 0.433\u548cMAE 0.453\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u8bc1\u660eTF-CoDiT\u5728\u4e0d\u540c\u5408\u7ea6\u548c\u65f6\u95f4\u8303\u56f4\u4e0a\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "TF-CoDiT\u662f\u9996\u4e2a\u7528\u4e8e\u8bed\u8a00\u63a7\u5236\u56fd\u503a\u671f\u8d27\u5408\u6210\u7684\u6269\u6563Transformer\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fd\u503a\u671f\u8d27\u6570\u636e\u7684\u4f4e\u4ea4\u6613\u91cf\u3001\u5e02\u573a\u4f9d\u8d56\u548c\u5206\u7ec4\u76f8\u5173\u6027\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u3001U\u5f62VAE\u7f16\u7801\u548cFinMAP\u534f\u8bae\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u5408\u6210\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u5408\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.12042", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12042", "abs": "https://arxiv.org/abs/2601.12042", "authors": ["Xiaomei Zhang", "Zhaoxi Zhang", "Leo Yu Zhang", "Yanjun Zhang", "Guanhong Tao", "Shirui Pan"], "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models", "comment": null, "summary": "Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.", "AI": {"tldr": "\u89c6\u89c9token\u538b\u7f29\u663e\u8457\u964d\u4f4e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u538b\u7f29\u611f\u77e5\u653b\u51fb\u53ef\u9488\u5bf9\u6027\u5229\u7528\u6b64\u6f0f\u6d1e", "motivation": "\u73b0\u6709\u89c6\u89c9token\u538b\u7f29\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f46\u5176\u5b89\u5168\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u53d1\u73b0\u89c6\u89c9token\u538b\u7f29\u4f1a\u663e\u8457\u964d\u4f4eLVLMs\u7684\u9c81\u68d2\u6027\uff0c\u538b\u7f29\u72b6\u6001\u4e0b\u7684\u6a21\u578b\u53d8\u5f97\u9ad8\u5ea6\u8106\u5f31\uff0c\u800c\u8fd9\u79cd\u6f0f\u6d1e\u5728\u975e\u538b\u7f29\u72b6\u6001\u4e0b\u5b8c\u5168\u6d88\u5931\uff0c\u5177\u6709\u9690\u853d\u6027\u548c\u96be\u4ee5\u8bca\u65ad\u7684\u7279\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u538b\u7f29\u8fc7\u7a0b\u7684\u5173\u952e\u9636\u6bb5\uff0c\u8bc6\u522btoken\u91cd\u8981\u6027\u6392\u5e8f\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u9c81\u68d2\u6027\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002\u63d0\u51fa\u538b\u7f29\u611f\u77e5\u653b\u51fb(CAA)\u76f4\u63a5\u9488\u5bf9token\u9009\u62e9\u673a\u5236\uff0c\u5728\u538b\u7f29\u63a8\u7406\u4e0b\u8bf1\u5bfc\u6a21\u578b\u5931\u8d25\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u9ed1\u76d2\u8bbe\u7f6e\uff0c\u63d0\u51fa\u8fc1\u79fbCAA\uff0c\u5728\u65e0\u6cd5\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u548c\u538b\u7f29\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u653b\u51fb\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u8de8\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u538b\u7f29\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9token\u538b\u7f29\u663e\u8457\u524a\u5f31\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002CAA\u653b\u51fb\u5728\u538b\u7f29\u8bbe\u7f6e\u4e0b\u6210\u529f\u8bf1\u5bfc\u6a21\u578b\u5931\u8d25\uff0c\u800c\u8fc1\u79fbCAA\u5728\u9ed1\u76d2\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u6709\u9650\u4fdd\u62a4\uff0c\u63ed\u793a\u4e86\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u6548\u7387-\u5b89\u5168\u6743\u8861\u95ee\u9898\u3002", "conclusion": "\u89c6\u89c9token\u538b\u7f29\u5f15\u5165\u4e86\u4e00\u4e2a\u4e25\u91cd\u4f46\u9690\u853d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u538b\u7f29\u72b6\u6001\u4e0b\u7684\u6a21\u578b\u9c81\u68d2\u6027\u663e\u8457\u4e0b\u964d\u3002token\u91cd\u8981\u6027\u6392\u5e8f\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u4e3b\u8981\u6839\u6e90\uff0c\u5fae\u5c0f\u6270\u52a8\u53ef\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u88ab\u9519\u8bef\u4e22\u5f03\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86LVLMs\u90e8\u7f72\u4e2d\u6548\u7387\u4e0e\u5b89\u5168\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u538b\u7f29\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.13782", "categories": ["math.ST", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.13782", "abs": "https://arxiv.org/abs/2601.13782", "authors": ["Shir Tapiro-Moshe", "Yariv Aizenbud", "Barak Sober"], "title": "Moving Least Squares without Quasi-Uniformity: A Stochastic Approach", "comment": null, "summary": "Local Polynomial Regression (LPR) and Moving Least Squares (MLS) are closely related nonparametric estimation methods, developed independently in statistics and approximation theory. While statistical LPR analysis focuses on overcoming sampling noise under probabilistic assumptions, the deterministic MLS theory studies smoothness properties and convergence rates with respect to the \\textit{fill-distance} (a resolution parameter). Despite this similarity, the deterministic assumptions underlying MLS fail to hold under random sampling. We begin by quantifying the probabilistic behavior of the fill-distance $h_n$ and \\textit{separation} $\u03b4_n$ of an i.i.d. random sample. That is, for a distribution satisfying a mild regularity condition, $h_n\\propto n^{-1/d}\\log^{1/d} (n)$ and $\u03b4_n \\propto n^{-1/d}$. We then prove that, for MLS of degree $k\\!-\\!1$, the approximation error associated with a differential operator $Q$ of order $|m|\\le k-1$ decays as $h_n^{\\,k-|m|}$ up to logarithmic factors, establishing stochastic analogues of the classical MLS estimates. Additionally, We show that the MLS approximant is smooth with high probability. Finally, we apply the stochastic MLS theory to manifold estimation. Assuming that the sampled Manifold is $k$-times smooth, we show that the Hausdorff distance between the true manifold and its MLS reconstruction decays as $h_n^k$, extending the deterministic Manifold-MLS guarantees to random samples. This work provides the first unified stochastic analysis of MLS, demonstrating that -- despite the failure of deterministic sampling assumptions -- the classical convergence and smoothness properties persist under natural probabilistic models", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4e3a\u79fb\u52a8\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08MLS\uff09\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u968f\u673a\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5c3d\u7ba1\u786e\u5b9a\u6027\u91c7\u6837\u5047\u8bbe\u5728\u968f\u673a\u91c7\u6837\u4e0b\u5931\u6548\uff0c\u4f46\u7ecf\u5178\u7684\u6536\u655b\u6027\u548c\u5149\u6ed1\u6027\u7279\u6027\u5728\u81ea\u7136\u6982\u7387\u6a21\u578b\u4e0b\u4ecd\u7136\u4fdd\u6301\u3002", "motivation": "\u5c40\u90e8\u591a\u9879\u5f0f\u56de\u5f52\uff08LPR\uff09\u548c\u79fb\u52a8\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08MLS\uff09\u662f\u7edf\u8ba1\u5b66\u548c\u903c\u8fd1\u8bba\u4e2d\u72ec\u7acb\u53d1\u5c55\u7684\u4e24\u79cd\u5bc6\u5207\u76f8\u5173\u7684\u975e\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002\u7edf\u8ba1LPR\u5206\u6790\u4fa7\u91cd\u4e8e\u5728\u6982\u7387\u5047\u8bbe\u4e0b\u514b\u670d\u91c7\u6837\u566a\u58f0\uff0c\u800c\u786e\u5b9a\u6027MLS\u7406\u8bba\u5219\u7814\u7a76\u5173\u4e8e\u586b\u5145\u8ddd\u79bb\u7684\u5149\u6ed1\u6027\u6027\u8d28\u548c\u6536\u655b\u901f\u7387\u3002\u7136\u800c\uff0cMLS\u7684\u786e\u5b9a\u6027\u5047\u8bbe\u5728\u968f\u673a\u91c7\u6837\u4e0b\u4e0d\u6210\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acbMLS\u7684\u968f\u673a\u5206\u6790\u6846\u67b6\u3002", "method": "\u9996\u5148\u91cf\u5316\u72ec\u7acb\u540c\u5206\u5e03\u968f\u673a\u6837\u672c\u7684\u586b\u5145\u8ddd\u79bbh_n\u548c\u5206\u79bb\u5ea6\u03b4_n\u7684\u6982\u7387\u884c\u4e3a\uff0c\u8bc1\u660e\u5bf9\u4e8e\u6ee1\u8db3\u6e29\u548c\u6b63\u5219\u6761\u4ef6\u7684\u5206\u5e03\uff0ch_n\u221dn^{-1/d}log^{1/d}(n)\uff0c\u03b4_n\u221dn^{-1/d}\u3002\u7136\u540e\u8bc1\u660e\u5bf9\u4e8ek-1\u9636MLS\uff0c\u4e0e\u9636\u6570|m|\u2264k-1\u7684\u5fae\u5206\u7b97\u5b50Q\u76f8\u5173\u7684\u903c\u8fd1\u8bef\u5dee\u4ee5h_n^{k-|m|}\u8870\u51cf\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\uff0c\u5efa\u7acb\u4e86\u7ecf\u5178MLS\u4f30\u8ba1\u7684\u968f\u673a\u7c7b\u6bd4\u3002\u6b64\u5916\uff0c\u8bc1\u660eMLS\u903c\u8fd1\u5668\u4ee5\u9ad8\u6982\u7387\u5149\u6ed1\u3002\u6700\u540e\u5c06\u968f\u673aMLS\u7406\u8bba\u5e94\u7528\u4e8e\u6d41\u5f62\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u968f\u673a\u91c7\u6837\u4e0b\uff0cMLS\u903c\u8fd1\u8bef\u5dee\u4ee5h_n^{k-|m|}\u7684\u901f\u7387\u8870\u51cf\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\uff0cMLS\u903c\u8fd1\u5668\u4ee5\u9ad8\u6982\u7387\u5149\u6ed1\u3002\u5bf9\u4e8ek\u6b21\u5149\u6ed1\u7684\u91c7\u6837\u6d41\u5f62\uff0c\u771f\u5b9e\u6d41\u5f62\u4e0e\u5176MLS\u91cd\u5efa\u4e4b\u95f4\u7684Hausdorff\u8ddd\u79bb\u4ee5h_n^k\u7684\u901f\u7387\u8870\u51cf\uff0c\u5c06\u786e\u5b9a\u6027\u6d41\u5f62-MLS\u4fdd\u8bc1\u6269\u5c55\u5230\u968f\u673a\u6837\u672c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3aMLS\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u968f\u673a\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5c3d\u7ba1\u786e\u5b9a\u6027\u91c7\u6837\u5047\u8bbe\u5931\u6548\uff0c\u4f46\u7ecf\u5178\u7684\u6536\u655b\u6027\u548c\u5149\u6ed1\u6027\u7279\u6027\u5728\u81ea\u7136\u6982\u7387\u6a21\u578b\u4e0b\u4ecd\u7136\u4fdd\u6301\u3002\u8fd9\u4e3aMLS\u5728\u968f\u673a\u91c7\u6837\u573a\u666f\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5c06\u786e\u5b9a\u6027\u6d41\u5f62\u4f30\u8ba1\u7ed3\u679c\u6269\u5c55\u5230\u968f\u673a\u6837\u672c\u3002"}}
{"id": "2601.11883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11883", "abs": "https://arxiv.org/abs/2601.11883", "authors": ["Chaoqi Jia", "Longkun Guo", "Kewen Liao", "Zhigang Lu", "Chao Chen", "Jason Xue"], "title": "Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach", "comment": "AAAI-26", "summary": "Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - \u03b5 would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\u8f6c\u6362\u7684\u65b0\u578b\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u5e26\u5b9e\u4f8b\u7ea7\u7ea6\u675f\u7684k-center\u805a\u7c7b\u95ee\u9898\uff0c\u83b7\u5f97\u6700\u4f73\u53ef\u80fd\u76842\u8fd1\u4f3c\u6bd4", "motivation": "\u4f20\u7edfk-center\u95ee\u9898\u5df2\u6709\u6700\u4f732\u8fd1\u4f3c\u6bd4\uff0c\u4f46\u52a0\u5165\u5b9e\u4f8b\u7ea7\u7ea6\u675f\uff08must-link\u548ccannot-link\uff09\u540e\uff0c\u867d\u7136\u4e0d\u76f8\u4ea4\u7684cannot-link\u96c6\u5141\u8bb8\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\uff0c\u4f46\u5c40\u90e8\u641c\u7d22\u80fd\u5426\u8fbe\u5230\u8fd9\u79cd\u4fdd\u8bc1\u4ecd\u662f\u5f00\u653e\u95ee\u9898", "method": "\u901a\u8fc7\u5c06\u7ea6\u675fk-center\u805a\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u5b9e\u73b0\u6700\u4f73\u53ef\u80fd\u76842\u8fd1\u4f3c\u6bd4", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u89e3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7ea6\u675fk-center\u805a\u7c7b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4e0a\u7684\u6700\u4f73\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2601.12105", "categories": ["cs.CR", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.12105", "abs": "https://arxiv.org/abs/2601.12105", "authors": ["Richik Chakraborty", "Lawrence Liu", "Syed Hasnain"], "title": "Privacy-Preserving Cohort Analytics for Personalized Health Platforms: A Differentially Private Framework with Stochastic Risk Modeling", "comment": "18 pages, 4 figures", "summary": "Personalized health analytics increasingly rely on population benchmarks to provide contextual insights such as ''How do I compare to others like me?'' However, cohort-based aggregation of health data introduces nontrivial privacy risks, particularly in interactive and longitudinal digital platforms. Existing privacy frameworks such as $k$-anonymity and differential privacy provide essential but largely static guarantees that do not fully capture the cumulative, distributional, and tail-dominated nature of re-identification risk in deployed systems.\n  In this work, we present a privacy-preserving cohort analytics framework that combines deterministic cohort constraints, differential privacy mechanisms, and synthetic baseline generation to enable personalized population comparisons while maintaining strong privacy protections. We further introduce a stochastic risk modeling approach that treats re-identification risk as a random variable evolving over time, enabling distributional evaluation through Monte Carlo simulation. Adapting quantitative risk measures from financial mathematics, we define Privacy Loss at Risk (P-VaR) to characterize worst-case privacy outcomes under realistic cohort dynamics and adversary assumptions.\n  We validate our framework through system-level analysis and simulation experiments, demonstrating how privacy-utility tradeoffs can be operationalized for digital health platforms. Our results suggest that stochastic risk modeling complements formal privacy guarantees by providing interpretable, decision-relevant metrics for platform designers, regulators, and clinical informatics stakeholders.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u786e\u5b9a\u6027\u961f\u5217\u7ea6\u675f\u3001\u5dee\u5206\u9690\u79c1\u673a\u5236\u548c\u5408\u6210\u57fa\u7ebf\u751f\u6210\u7684\u9690\u79c1\u4fdd\u62a4\u961f\u5217\u5206\u6790\u6846\u67b6\uff0c\u5f15\u5165\u968f\u673a\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b9a\u4e49\u9690\u79c1\u98ce\u9669\u4ef7\u503c(P-VaR)\u6765\u91cf\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9690\u79c1\u635f\u5931\u3002", "motivation": "\u4e2a\u6027\u5316\u5065\u5eb7\u5206\u6790\u4f9d\u8d56\u4eba\u7fa4\u57fa\u51c6\u6570\u636e\uff0c\u4f46\u961f\u5217\u805a\u5408\u4f1a\u5e26\u6765\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u7684\u9690\u79c1\u6846\u67b6(\u5982k-\u533f\u540d\u548c\u5dee\u5206\u9690\u79c1)\u63d0\u4f9b\u7684\u662f\u9759\u6001\u4fdd\u8bc1\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u4e2d\u7d2f\u79ef\u6027\u3001\u5206\u5e03\u6027\u548c\u5c3e\u90e8\u4e3b\u5bfc\u7684\u91cd\u65b0\u8bc6\u522b\u98ce\u9669\u3002", "method": "\u7ed3\u5408\u786e\u5b9a\u6027\u961f\u5217\u7ea6\u675f\u3001\u5dee\u5206\u9690\u79c1\u673a\u5236\u548c\u5408\u6210\u57fa\u7ebf\u751f\u6210\uff0c\u5f15\u5165\u968f\u673a\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u91cd\u65b0\u8bc6\u522b\u98ce\u9669\u89c6\u4e3a\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u968f\u673a\u53d8\u91cf\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8fdb\u884c\u5206\u5e03\u8bc4\u4f30\uff0c\u5e76\u501f\u9274\u91d1\u878d\u6570\u5b66\u4e2d\u7684\u98ce\u9669\u5ea6\u91cf\u5b9a\u4e49\u9690\u79c1\u98ce\u9669\u4ef7\u503c(P-VaR)\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u5206\u6790\u548c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u6570\u5b57\u5065\u5eb7\u5e73\u53f0\u4e2d\u64cd\u4f5c\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u8868\u660e\u968f\u673a\u98ce\u9669\u5efa\u6a21\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u4e0e\u51b3\u7b56\u76f8\u5173\u7684\u6307\u6807\u6765\u8865\u5145\u5f62\u5f0f\u5316\u9690\u79c1\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u9690\u79c1\u4fdd\u62a4\u961f\u5217\u5206\u6790\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u4e2a\u6027\u5316\u4eba\u7fa4\u6bd4\u8f83\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u3002\u968f\u673a\u98ce\u9669\u5efa\u6a21\u4e3a\u5e73\u53f0\u8bbe\u8ba1\u8005\u3001\u76d1\u7ba1\u8005\u548c\u4e34\u5e8a\u4fe1\u606f\u5b66\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2601.13930", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.13930", "abs": "https://arxiv.org/abs/2601.13930", "authors": ["Kohei Kawamoto", "Yuichi Goto", "Koji Tsukuda"], "title": "On spectral clustering under non-isotropic Gaussian mixture models", "comment": "8 pages", "summary": "We evaluate the misclustering probability of a spectral clustering algorithm under a Gaussian mixture model with a general covariance structure. The algorithm partitions the data into two groups based on the sign of the first principal component score. As a corollary of the main result, the clustering procedure is shown to be consistent in a high-dimensional regime.", "AI": {"tldr": "\u8bc4\u4f30\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e0b\u8c31\u805a\u7c7b\u7b97\u6cd5\u7684\u8bef\u805a\u7c7b\u6982\u7387\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u7b2c\u4e00\u4e3b\u6210\u5206\u5f97\u5206\u7684\u7b26\u53f7\u5c06\u6570\u636e\u5206\u4e3a\u4e24\u7ec4\uff0c\u5e76\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8bc1\u660e\u4e86\u805a\u7c7b\u4e00\u81f4\u6027", "motivation": "\u7814\u7a76\u8c31\u805a\u7c7b\u7b97\u6cd5\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e0b\u7684\u7406\u8bba\u6027\u80fd\uff0c\u7279\u522b\u662f\u8bef\u805a\u7c7b\u6982\u7387\u7684\u7406\u8bba\u754c\u9650\uff0c\u4e3a\u9ad8\u7ef4\u6570\u636e\u805a\u7c7b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1", "method": "\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u91c7\u7528\u8c31\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u6570\u636e\u7684\u7b2c\u4e00\u4e3b\u6210\u5206\u5f97\u5206\uff0c\u6839\u636e\u5f97\u5206\u7684\u6b63\u8d1f\u53f7\u5c06\u6570\u636e\u5212\u5206\u4e3a\u4e24\u4e2a\u7c07", "result": "\u63a8\u5bfc\u4e86\u8bef\u805a\u7c7b\u6982\u7387\u7684\u7406\u8bba\u754c\u9650\uff0c\u5e76\u4f5c\u4e3a\u4e3b\u8981\u7ed3\u679c\u7684\u63a8\u8bba\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8be5\u805a\u7c7b\u8fc7\u7a0b\u5177\u6709\u4e00\u81f4\u6027", "conclusion": "\u8c31\u805a\u7c7b\u7b97\u6cd5\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e0b\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u80fd\u591f\u5b9e\u73b0\u4e00\u81f4\u7684\u805a\u7c7b\u6548\u679c"}}
{"id": "2601.11890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11890", "abs": "https://arxiv.org/abs/2601.11890", "authors": ["Xihe Gu", "Urbashi Mitra", "Tara Javidi"], "title": "From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs", "comment": null, "summary": "Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_\u03c1$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_\u03c1$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_\u03c1$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $\u03c1$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53c2\u6570\u5316\u7684\u51f9\u8986\u76d6\u76ee\u6807\u51fd\u6570\u65cfU_\u03c1\uff0c\u7528\u4e8e\u6307\u5bfc\u65e0\u5956\u52b1MDP\u4e2d\u7684\u5b9a\u5411\u63a2\u7d22\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u6709\u8986\u76d6\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u4e3b\u52a8\u7684\u63a2\u7d22\u63a7\u5236\u3002", "motivation": "\u5728\u65e0\u5956\u52b1\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u540c\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5177\u6709\u4e0d\u540c\u7684\u91cd\u8981\u6027\u6216\u96be\u5ea6\uff0c\u9700\u8981\u4e3b\u52a8\u3001\u663e\u5f0f\u5730\u6784\u5efa\u63a2\u7d22\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5e73\u8861\u63a2\u7d22\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u5316\u51f9\u8986\u76d6\u76ee\u6807\u51fd\u6570\u65cfU_\u03c1\uff0c\u5b9a\u4e49\u5728\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u5ea6\u91cf\u4e0a\u3002\u8be5\u65cf\u7edf\u4e00\u4e86\u57fa\u4e8e\u6563\u5ea6\u7684\u8fb9\u9645\u5339\u914d\u3001\u52a0\u6743\u5e73\u5747\u8986\u76d6\u548c\u6700\u574f\u60c5\u51b5\u8986\u76d6\u7b49\u76ee\u6807\u3002\u5229\u7528U_\u03c1\u7684\u51f9\u6027\u548c\u68af\u5ea6\u95ed\u5f0f\u89e3\uff0c\u5f00\u53d1\u68af\u5ea6\u7b97\u6cd5\u4e3b\u52a8\u5f15\u5bfc\u5360\u7528\u5206\u5e03\u5411\u671f\u671b\u8986\u76d6\u6a21\u5f0f\u6536\u655b\u3002", "result": "U_\u03c1\u6846\u67b6\u7edf\u4e00\u4e86\u591a\u79cd\u8986\u76d6\u76ee\u6807\uff1b\u68af\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u63a2\u7d22\uff1b\u5f53\u03c1\u589e\u5927\u65f6\uff0c\u63a2\u7d22\u7b56\u7565\u8d8a\u6765\u8d8a\u5f3a\u8c03\u6700\u5c11\u63a2\u7d22\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u6062\u590d\u6700\u574f\u60c5\u51b5\u8986\u76d6\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684U_\u03c1\u6846\u67b6\u4e3a\u65e0\u5956\u52b1MDP\u4e2d\u7684\u5b9a\u5411\u63a2\u7d22\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u53c2\u6570\u03c1\u7075\u6d3b\u63a7\u5236\u63a2\u7d22\u7b56\u7565\uff0c\u4ece\u5e73\u5747\u8986\u76d6\u5230\u6700\u574f\u60c5\u51b5\u8986\u76d6\u8fde\u7eed\u8fc7\u6e21\uff0c\u68af\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u63a2\u7d22\u6a21\u5f0f\u7684\u4e3b\u52a8\u63a7\u5236\u3002"}}
{"id": "2601.13946", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.13946", "abs": "https://arxiv.org/abs/2601.13946", "authors": ["Philip Boeken", "Eduardo Skapinakis", "Konstantin Genin", "Joris M. Mooij"], "title": "Topological Criteria for Hypothesis Testing with Finite-Precision Measurements", "comment": null, "summary": "We establish topological necessary and sufficient conditions under which a pair of statistical hypotheses can be consistently distinguished when i.i.d. observations are recorded only to finite precision. Requiring the test's decision regions to be open in the sample-space topology to accommodate finite-precision data, we show that a pair of null- and alternative hypotheses $H_0$ and $H_1$ admits a consistent test if and only if they are $F_\u03c3$ in the weak topology on the space of probability measures $W := H_0\\cup H_1$. Additionally, the hypotheses admit uniform error control under $H_0$ and/or $H_1$ if and only if $H_0$ and/or $H_1$ are closed in $W$. Under compactness assumptions, uniform consistency is characterised by $H_0$ and $H_1$ having disjoint closures in the ambient space of probability measures. These criteria imply that - without regularity assumptions - conditional independence is not consistently testable. We introduce a Lipschitz-continuity assumption on the family of conditional distributions under which we recover testability of conditional independence with uniform error control under the null, with testable smoothness constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u5728\u6709\u9650\u7cbe\u5ea6\u89c2\u6d4b\u4e0b\u7edf\u8ba1\u5047\u8bbe\u4e00\u81f4\u6027\u68c0\u9a8c\u7684\u62d3\u6251\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff0c\u53d1\u73b0\u5047\u8bbe\u5bf9\u53ef\u4e00\u81f4\u68c0\u9a8c\u5f53\u4e14\u4ec5\u5f53\u5728\u5f31\u62d3\u6251\u4e0b\u662fF\u03c3\u96c6\uff0c\u5e76\u5e94\u7528\u4e8e\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5728\u6709\u9650\u7cbe\u5ea6\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5f53\u89c2\u6d4b\u6570\u636e\u53ea\u80fd\u4ee5\u6709\u9650\u7cbe\u5ea6\u8bb0\u5f55\u65f6\uff0c\u9700\u8981\u51b3\u7b56\u533a\u57df\u5728\u6837\u672c\u7a7a\u95f4\u62d3\u6251\u4e2d\u4e3a\u5f00\u96c6\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5728\u4f55\u79cd\u62d3\u6251\u6761\u4ef6\u4e0b\u5047\u8bbe\u5bf9\u80fd\u591f\u88ab\u4e00\u81f4\u5730\u533a\u5206\u3002", "method": "\u91c7\u7528\u62d3\u6251\u5b66\u65b9\u6cd5\uff0c\u5728\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\u7684\u5f31\u62d3\u6251\u6846\u67b6\u4e0b\u5206\u6790\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u3002\u8981\u6c42\u68c0\u9a8c\u7684\u51b3\u7b56\u533a\u57df\u5728\u6837\u672c\u7a7a\u95f4\u62d3\u6251\u4e2d\u4e3a\u5f00\u96c6\u4ee5\u9002\u5e94\u6709\u9650\u7cbe\u5ea6\u6570\u636e\uff0c\u7136\u540e\u63a8\u5bfc\u51fa\u5047\u8bbe\u5bf9\u53ef\u4e00\u81f4\u68c0\u9a8c\u7684\u62d3\u6251\u7279\u5f81\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1) \u96f6\u5047\u8bbeH0\u548c\u5907\u62e9\u5047\u8bbeH1\u53ef\u4e00\u81f4\u68c0\u9a8c\u5f53\u4e14\u4ec5\u5f53\u5b83\u4eec\u5728\u5f31\u62d3\u6251\u4e0b\u662fF\u03c3\u96c6\uff1b2) \u5728H0\u548c/\u6216H1\u4e0b\u5177\u6709\u4e00\u81f4\u8bef\u5dee\u63a7\u5236\u5f53\u4e14\u4ec5\u5f53\u76f8\u5e94\u7684\u5047\u8bbe\u5728W\u4e2d\u662f\u95ed\u96c6\uff1b3) \u5728\u7d27\u6027\u5047\u8bbe\u4e0b\uff0c\u4e00\u81f4\u4e00\u81f4\u6027\u7531H0\u548cH1\u5728\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\u4e2d\u5177\u6709\u4e0d\u76f8\u4ea4\u95ed\u5305\u6765\u523b\u753b\uff1b4) \u8fd9\u4e9b\u51c6\u5219\u8868\u660e\uff0c\u5728\u6ca1\u6709\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\uff0c\u6761\u4ef6\u72ec\u7acb\u6027\u4e0d\u53ef\u4e00\u81f4\u68c0\u9a8c\uff1b5) \u5f15\u5165\u6761\u4ef6\u5206\u5e03\u65cf\u7684Lipschitz\u8fde\u7eed\u6027\u5047\u8bbe\u540e\uff0c\u53ef\u5728\u96f6\u5047\u8bbe\u4e0b\u6062\u590d\u6761\u4ef6\u72ec\u7acb\u6027\u7684\u53ef\u68c0\u9a8c\u6027\u3002", "conclusion": "\u8bba\u6587\u5efa\u7acb\u4e86\u6709\u9650\u7cbe\u5ea6\u89c2\u6d4b\u4e0b\u5047\u8bbe\u68c0\u9a8c\u7684\u5b8c\u6574\u62d3\u6251\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u53ef\u68c0\u9a8c\u6027\u4e0e\u5047\u8bbe\u96c6\u7684\u62d3\u6251\u6027\u8d28\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002\u7279\u522b\u5730\uff0c\u6761\u4ef6\u72ec\u7acb\u6027\u7684\u53ef\u68c0\u9a8c\u6027\u9700\u8981\u989d\u5916\u7684\u6b63\u5219\u6027\u5047\u8bbe\uff0c\u8fd9\u4e3a\u5b9e\u9645\u7edf\u8ba1\u68c0\u9a8c\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.11895", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11895", "abs": "https://arxiv.org/abs/2601.11895", "authors": ["Pareesa Ameneh Golnari", "Adarsh Kumarappan", "Wen Wen", "Xiaoyu Liu", "Gabriel Ryan", "Yuting Sun", "Shengyu Fu", "Elsie Nallipogu"], "title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models", "comment": null, "summary": "DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.", "AI": {"tldr": "DevBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u9065\u6d4b\u6570\u636e\u7684\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1800\u4e2a\u8bc4\u4f30\u5b9e\u4f8b\uff0c\u6db5\u76d66\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c6\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5f3a\u8c03\u751f\u6001\u6548\u5ea6\u5e76\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\uff0c\u63d0\u4f9b\u8be6\u7ec6\u8bca\u65ad\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\uff0c\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u5f71\u54cd\uff0c\u4e14\u65e0\u6cd5\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u606f\u6765\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u6784\u5efa\u8bc4\u4f30\u5b9e\u4f8b\uff0c\u7ed3\u5408\u529f\u80fd\u6b63\u786e\u6027\u3001\u76f8\u4f3c\u5ea6\u6307\u6807\u548cLLM-judge\u8bc4\u4f30\uff08\u5173\u6ce8\u5b9e\u7528\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff09\uff0c\u5bf99\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8bed\u6cd5\u7cbe\u5ea6\u3001\u8bed\u4e49\u63a8\u7406\u548c\u5b9e\u9645\u6548\u7528\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u6765\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "conclusion": "DevBench\u63d0\u4f9b\u4e86\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u7f3a\u5931\u4f46\u5b9e\u9645\u90e8\u7f72\u548c\u9488\u5bf9\u6027\u6a21\u578b\u5f00\u53d1\u6240\u5fc5\u9700\u7684\u8be6\u7ec6\u8bca\u65ad\u4fe1\u606f\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u3002"}}
{"id": "2601.12270", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.12270", "abs": "https://arxiv.org/abs/2601.12270", "authors": ["Reshabh K Sharma", "Dan Grossman", "David Kohlbrenner"], "title": "SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels", "comment": null, "summary": "Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner.\n  In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software.\n  We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs.", "AI": {"tldr": "SplittingSecrets\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u9632\u6b62\u79d8\u5bc6\u6570\u636e\u5728\u5185\u5b58\u4e2d\u7c7b\u4f3c\u5730\u5740\u7684\u5f62\u5f0f\u5b58\u50a8\uff0c\u6765\u9632\u5fa1\u6570\u636e\u5185\u5b58\u76f8\u5173\u9884\u53d6\u5668(DMP)\u5f15\u53d1\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\u3002", "motivation": "\u4f20\u7edf\u4fa7\u4fe1\u9053\u9632\u5fa1\uff08\u5982\u5e38\u6570\u65f6\u95f4\u7f16\u7a0b\uff09\u5bf9\u65b0\u578b\u786c\u4ef6\u4f18\u5316\u2014\u2014\u6570\u636e\u5185\u5b58\u76f8\u5173\u9884\u53d6\u5668(DMP)\u65e0\u6548\u3002DMP\u5b58\u5728\u4e8e\u82f9\u679c\u3001\u82f1\u7279\u5c14\u548cARM CPU\u4e2d\uff0c\u80fd\u591f\u6cc4\u9732\u9759\u6001\u6570\u636e\uff08\u4ece\u672a\u88ab\u7a0b\u5e8f\u4f7f\u7528\u7684\u6570\u636e\uff09\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6570\u636e\u6ca1\u6709\u88ab\u4e0d\u5b89\u5168\u5730\u4f7f\u7528\u3002", "method": "SplittingSecrets\u91c7\u7528\u7f16\u8bd1\u5668\u65b9\u6cd5\uff0c\u907f\u514d\u63a8\u7406\u590d\u6742DMP\u5185\u90e8\u673a\u5236\uff0c\u800c\u662f\u5229\u7528\u6240\u6709DMP\u7684\u5173\u952e\u7279\u6027\uff1a\u6fc0\u6d3b\u9700\u8981\u6570\u636e\u7c7b\u4f3c\u5730\u5740\u3002\u5b83\u901a\u8fc7\u8f6c\u6362\u5185\u5b58\u64cd\u4f5c\uff0c\u786e\u4fdd\u79d8\u5bc6\u6570\u636e\u6c38\u8fdc\u4e0d\u4f1a\u4ee5\u7c7b\u4f3c\u5730\u5740\u7684\u5f62\u5f0f\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\uff0c\u4ece\u800c\u9632\u6b62DMP\u5bf9\u8fd9\u4e9b\u79d8\u5bc6\u7684\u6fc0\u6d3b\u3002", "result": "\u4f7f\u7528LLVM\u5b9e\u73b0\u4e86SplittingSecrets\uff0c\u652f\u6301AArch64\u67b6\u6784\u7684\u6e90\u4ee3\u7801\u7ea7\u5185\u5b58\u64cd\u4f5c\u548c\u7f16\u8bd1\u5668\u540e\u7aef\u751f\u6210\u7684\u64cd\u4f5c\u3002\u5206\u6790\u4e86\u5728\u4fdd\u62a4libsodium\u52a0\u5bc6\u5e93\u5e38\u89c1\u539f\u8bed\u514d\u53d7DMP\u653b\u51fb\u65f6\u7684\u6027\u80fd\u5f00\u9500\u3002", "conclusion": "SplittingSecrets\u63d0\u4f9b\u4e86\u9488\u5bf9\u7279\u5b9a\u79d8\u5bc6\u7684\u8f6f\u4ef6\u7ea7\u9632\u62a4\uff0c\u65e0\u9700\u5b8c\u5168\u7981\u7528DMP\uff0c\u80fd\u591f\u6709\u6548\u9632\u5fa1DMP\u5f15\u53d1\u7684\u4fa7\u4fe1\u9053\u653b\u51fb\u3002"}}
{"id": "2601.13955", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.13955", "abs": "https://arxiv.org/abs/2601.13955", "authors": ["Akira Shinkyu"], "title": "Uniform Consistency of Generalized Cross-Validation for Ridge Regression in High-Dimensional Misspecified Linear Models", "comment": null, "summary": "This study examines generalized cross-validation for the tuning parameter selection for ridge regression in high-dimensional misspecified linear models. The set of candidates for the tuning parameter includes not only positive values but also zero and negative values. We demonstrate that if the second moment of the specification error converges to zero, generalized cross-validation is still a uniformly consistent estimator of the out-of-sample prediction risk. This implies that generalized cross-validation selects the tuning parameter for which ridge regression asymptotically achieves the smallest prediction risk among the candidates if the degree of misspecification for the regression function is small. Our simulation studies show that ridge regression tuned by generalized cross-validation exhibits a prediction performance similar to that of optimally tuned ridge regression and outperforms the Lasso under correct and incorrect model specifications.", "AI": {"tldr": "\u7814\u7a76\u9ad8\u7ef4\u8bef\u8bbe\u7ebf\u6027\u6a21\u578b\u4e2d\u5cad\u56de\u5f52\u8c03\u53c2\u7684\u5e7f\u4e49\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8bc1\u660e\u5373\u4f7f\u5b58\u5728\u6a21\u578b\u8bef\u8bbe\uff0cGCV\u4ecd\u80fd\u4e00\u81f4\u4f30\u8ba1\u6837\u672c\u5916\u9884\u6d4b\u98ce\u9669\uff0c\u4ece\u800c\u9009\u62e9\u51fa\u6e10\u8fd1\u6700\u4f18\u7684\u8c03\u53c2\u503c\u3002", "motivation": "\u5728\u73b0\u5b9e\u9ad8\u7ef4\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u4e2d\uff0c\u6a21\u578b\u5f80\u5f80\u5b58\u5728\u8bef\u8bbe\uff08misspecification\uff09\uff0c\u800c\u4f20\u7edf\u7684\u8c03\u53c2\u65b9\u6cd5\u5982\u5e7f\u4e49\u4ea4\u53c9\u9a8c\u8bc1\uff08GCV\uff09\u5728\u8bef\u8bbe\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u6027\u8d28\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5728\u5305\u542b\u96f6\u548c\u8d1f\u503c\u7684\u8c03\u53c2\u5019\u9009\u96c6\u4e2d\uff0cGCV\u5728\u8bef\u8bbe\u6a21\u578b\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u548c\u6a21\u62df\u7814\u7a76\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u7406\u8bba\u4e0a\uff0c\u5728\u8bef\u8bbe\u7ebf\u6027\u6a21\u578b\u6846\u67b6\u4e0b\uff0c\u8bc1\u660e\u5f53\u8bef\u8bbe\u8bef\u5dee\u7684\u4e8c\u9636\u77e9\u6536\u655b\u4e8e\u96f6\u65f6\uff0cGCV\u662f\u6837\u672c\u5916\u9884\u6d4b\u98ce\u9669\u7684\u4e00\u81f4\u4f30\u8ba1\u91cf\u3002\u8c03\u53c2\u5019\u9009\u96c6\u5305\u542b\u6b63\u6570\u3001\u96f6\u548c\u8d1f\u503c\u3002\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u6bd4\u8f83GCV\u8c03\u53c2\u7684\u5cad\u56de\u5f52\u4e0e\u6700\u4f18\u8c03\u53c2\u5cad\u56de\u5f52\u3001Lasso\u5728\u4e0d\u540c\u6a21\u578b\u8bbe\u5b9a\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u82e5\u8bef\u8bbe\u8bef\u5dee\u7684\u4e8c\u9636\u77e9\u6536\u655b\u4e8e\u96f6\uff0cGCV\u662f\u9884\u6d4b\u98ce\u9669\u7684\u4e00\u81f4\u4f30\u8ba1\u91cf\uff0c\u56e0\u6b64\u80fd\u9009\u62e9\u51fa\u4f7f\u5cad\u56de\u5f52\u6e10\u8fd1\u8fbe\u5230\u6700\u5c0f\u9884\u6d4b\u98ce\u9669\u7684\u8c03\u53c2\u503c\u3002\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff1aGCV\u8c03\u53c2\u7684\u5cad\u56de\u5f52\u9884\u6d4b\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u8c03\u53c2\u5cad\u56de\u5f52\uff0c\u4e14\u5728\u6b63\u786e\u548c\u9519\u8bef\u6a21\u578b\u8bbe\u5b9a\u4e0b\u5747\u4f18\u4e8eLasso\u3002", "conclusion": "\u5e7f\u4e49\u4ea4\u53c9\u9a8c\u8bc1\u5728\u9ad8\u7ef4\u8bef\u8bbe\u7ebf\u6027\u6a21\u578b\u4e2d\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u9009\u62e9\u5cad\u56de\u5f52\u7684\u8c03\u53c2\u503c\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u4f18\u4e8eLasso\u65b9\u6cd5\u3002\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u6a21\u578b\u8bef\u8bbe\u65f6\u7684\u8c03\u53c2\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.11897", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.11897", "abs": "https://arxiv.org/abs/2601.11897", "authors": ["Jinwon Sohn", "Guang Lin", "Qifan Song"], "title": "Task-tailored Pre-processing: Fair Downstream Supervised Learning", "comment": null, "summary": "Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u516c\u5e73\u6027\u9884\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7HGR\u76f8\u5173\u6027\u5206\u6790\u6570\u636e\u516c\u5e73\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u516c\u5e73\u6027\u4e0e\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u63d0\u4f9b\u4e0b\u6e38\u6a21\u578b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u9884\u5904\u7406\u65b9\u6cd5\u5206\u4e3a\u6570\u636e\u516c\u5e73\u6027\u548c\u4efb\u52a1\u5b9a\u5236\u516c\u5e73\u6027\u4e24\u7c7b\u3002\u6570\u636e\u516c\u5e73\u6027\u65b9\u6cd5\u5bf9\u4e0b\u6e38\u6a21\u578b\u7c7b\u578b\u72ec\u7acb\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u4eceHGR\u76f8\u5173\u6027\u89d2\u5ea6\u770b\u65bd\u52a0\u4e86\u8fc7\u5f3a\u7684\u6b63\u5219\u5316\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5728\u516c\u5e73\u6027\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u76d1\u7763\u5b66\u4e60\u5b9a\u5236\u9884\u5904\u7406\u6846\u67b6\u3002\u9996\u5148\u8003\u8651\u516c\u5e73\u6027\u4e0e\u6548\u7528\u7684\u6743\u8861\u6765\u83b7\u5f97\u9884\u5904\u7406\u6620\u5c04\uff0c\u7136\u540e\u7814\u7a76\u5728\u53d8\u6362\u6570\u636e\u4e0a\u5b66\u4e60\u7684\u4efb\u610f\u4e0b\u6e38\u76d1\u7763\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u627e\u5230\u4fdd\u8bc1\u5176\u516c\u5e73\u6027\u6539\u8fdb\u548c\u6548\u7528\u4fdd\u6301\u7684\u5145\u5206\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u5173\u6ce8\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\uff0c\u4ec5\u6539\u53d8\u4e0e\u4e2d\u5fc3\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u5fc5\u8981\u8bed\u4e49\u7279\u5f81\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "result": "\u901a\u8fc7\u8868\u683c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6bd4\u8f83\u7814\u7a76\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u6a21\u578b\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u6743\u8861\u5e73\u8861\uff0c\u4f18\u4e8e\u6700\u8fd1\u7684\u7ade\u4e89\u6a21\u578b\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u6539\u53d8\u4e0e\u4e2d\u5fc3\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u5fc5\u8981\u8bed\u4e49\u7279\u5f81\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u76d1\u7763\u5b66\u4e60\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u4efb\u52a1\u5b9a\u5236\u65b9\u6cd5\u5206\u652f\u4e2d\u7406\u8bba\u7814\u7a76\u4e86\u4f7f\u7528\u9884\u5904\u7406\u6570\u636e\u65f6\u7684\u4e0b\u6e38\u4fdd\u8bc1\u3002\u8be5\u65b9\u6cd5\u5728\u516c\u5e73\u6027\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e3a\u4e0b\u6e38\u6a21\u578b\u7684\u516c\u5e73\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2601.12331", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12331", "abs": "https://arxiv.org/abs/2601.12331", "authors": ["Huanyi Ye", "Jiale Guo", "Ziyao Liu", "Kwok-Yan Lam"], "title": "Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption", "comment": null, "summary": "RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.", "AI": {"tldr": "\u63d0\u51fappRAG\u6846\u67b6\uff0c\u5728\u4e0d\u53ef\u4fe1\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9690\u79c1\u4fdd\u62a4\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u901a\u8fc7CAPRISE\u52a0\u5bc6\u548c\u5dee\u5206\u9690\u79c1\u9632\u5fa1\u5411\u91cf\u653b\u51fb\u4e0e\u67e5\u8be2\u5206\u6790", "motivation": "\u4f20\u7edfRAG\u4f9d\u8d56\u53ef\u4fe1\u672c\u5730\u73af\u5883\uff0c\u4f46\u8d44\u6e90\u6709\u9650\u7684\u7528\u6237\u9700\u4f7f\u7528\u4e0d\u53ef\u4fe1\u4e91\u5b58\u50a8\uff0c\u9762\u4e34\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u73b0\u6709\u9690\u79c1\u4fdd\u62a4RAG\u65b9\u6cd5\u591a\u57fa\u4e8e\u90e8\u5206\u540c\u6001\u52a0\u5bc6\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fappRAG\u6846\u67b6\uff1a1) CAPRISE\u5bf9\u79f0\u52a0\u5bc6\u65b9\u6848\uff0c\u52a0\u5bc6\u5d4c\u5165\u5411\u91cf\u540c\u65f6\u5141\u8bb8\u4e91\u670d\u52a1\u5668\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u4ec5\u4fdd\u7559\u67e5\u8be2\u4e0e\u6570\u636e\u5e93\u5411\u91cf\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\u6392\u5e8f\uff0c\u4e0d\u66b4\u9732\u6570\u636e\u5e93\u5185\u90e8\u8ddd\u79bb\uff1b2) \u5728\u52a0\u5bc6\u524d\u5bf9\u67e5\u8be2\u5d4c\u5165\u6dfb\u52a0\u5dee\u5206\u9690\u79c1\u6270\u52a8\uff0c\u9632\u5fa1\u67e5\u8be2\u5206\u6790\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eppRAG\u5b9e\u73b0\u4e86\u9ad8\u6548\u5904\u7406\u541e\u5410\u91cf\u3001\u9ad8\u68c0\u7d22\u7cbe\u5ea6\u548c\u5f3a\u9690\u79c1\u4fdd\u8bc1\uff0c\u80fd\u591f\u9632\u5fa1\u5411\u91cf\u5230\u6587\u672c\u91cd\u6784\u653b\u51fb\u3001\u5411\u91cf\u5206\u6790\u548c\u67e5\u8be2\u5206\u6790\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7528\u6237\u63d0\u4f9b\u5b9e\u7528\u7684\u5b89\u5168\u4e91\u589e\u5f3aLLM\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ppRAG\u6846\u67b6\u5728\u4e0d\u53ef\u4fe1\u4e91\u73af\u5883\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u901a\u8fc7CAPRISE\u52a0\u5bc6\u548c\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u62a4RAG\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b89\u5168\u4e91\u589e\u5f3aLLM\u5e94\u7528\u3002"}}
{"id": "2601.11924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11924", "abs": "https://arxiv.org/abs/2601.11924", "authors": ["Ming Shi"], "title": "Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits", "comment": null, "summary": "We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $\u0393$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $\u03c6$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $\u0393$ can translate into an effective corruption level ranging from $\u0393$ to $N\u0393$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(\u0393)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $\u03a9(\u0393)$ penalty and a high-corruption regime $\u0393=\u0398(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $\u03bd$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $\u0393$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5177\u6709\u5411\u91cf\u503c\u5956\u52b1\u7684\u534f\u4f5c\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5728\u5bf9\u6297\u6027\u8150\u8d25\u548c\u6709\u9650\u9a8c\u8bc1\u6761\u4ef6\u4e0b\u3002\u4e3b\u8981\u8d21\u732e\u662f\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u8150\u8d25\u4e4b\u95f4\u7684\u8026\u5408\u5173\u7cfb\uff1a\u56fa\u5b9a\u7684\u73af\u5883\u7aef\u8150\u8d25\u9884\u7b97\u0393\u53ef\u80fd\u8f6c\u5316\u4e3a\u4ece\u0393\u5230N\u0393\u7684\u6709\u6548\u8150\u8d25\u6c34\u5e73\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u5171\u4eab\u539f\u59cb\u6837\u672c\u3001\u5145\u5206\u7edf\u8ba1\u91cf\u8fd8\u662f\u4ec5\u5171\u4eab\u81c2\u63a8\u8350\u3002", "motivation": "\u7814\u7a76\u5728\u5bf9\u6297\u6027\u8150\u8d25\u73af\u5883\u4e0b\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b66\u4e60\u95ee\u9898\u3002\u4f20\u7edf\u534f\u4f5c\u8001\u864e\u673a\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u89c2\u6d4b\u6570\u636e\u662f\u5e72\u51c0\u7684\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u53ef\u80fd\u88ab\u5bf9\u6297\u6027\u6270\u52a8\u6c61\u67d3\u3002\u9700\u8981\u7406\u89e3\u8150\u8d25\u5982\u4f55\u5f71\u54cd\u534f\u4f5c\u5b66\u4e60\uff0c\u4ee5\u53ca\u4e0d\u540c\u901a\u4fe1\u534f\u8bae\u5982\u4f55\u653e\u5927\u6216\u7f13\u89e3\u8150\u8d25\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u5177\u6709\u5411\u91cf\u503c\u5956\u52b1\u3001\u5bf9\u6297\u6027\u8150\u8d25\u548c\u6709\u9650\u9a8c\u8bc1\u3002\u5f15\u5165\u534f\u8bae\u8bf1\u5bfc\u7684\u591a\u91cd\u6027\u51fd\u6570\u6765\u91cf\u5316\u4e0d\u540c\u901a\u4fe1\u534f\u8bae\uff08\u539f\u59cb\u6837\u672c\u5171\u4eab\u3001\u7edf\u8ba1\u91cf\u5171\u4eab\u3001\u63a8\u8350\u5171\u4eab\uff09\u5982\u4f55\u5c06\u5168\u5c40\u8150\u8d25\u9884\u7b97\u0393\u8f6c\u5316\u4e3a\u6709\u6548\u8150\u8d25\u6c34\u5e73\u3002\u5efa\u7acb\u9057\u61be\u754c\u9650\u53c2\u6570\u5316\u4e8e\u6709\u6548\u8150\u8d25\uff0c\u5206\u6790\u4e0d\u540c\u901a\u4fe1\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u539f\u59cb\u6837\u672c\u5171\u4eab\u53ef\u80fd\u906d\u53d7N\u500d\u7684\u8150\u8d25\u60e9\u7f5a\u653e\u5927\uff0c\u800c\u7edf\u8ba1\u91cf\u5171\u4eab\u548c\u63a8\u8350\u5171\u4eab\u4fdd\u6301\u672a\u653e\u5927\u7684O(\u0393)\u9879\uff0c\u8fbe\u5230\u4e2d\u5fc3\u5316\u901f\u7387\u7684\u56e2\u961f\u9057\u61be\u3002\u5efa\u7acb\u4e86\u4fe1\u606f\u7406\u8bba\u6781\u9650\uff1a\u4e0d\u53ef\u907f\u514d\u7684\u03a9(\u0393)\u52a0\u6027\u60e9\u7f5a\uff0c\u4ee5\u53ca\u9ad8\u8150\u8d25\u0393=\u0398(NT)\u4e0b\u6ca1\u6709\u5e72\u51c0\u4fe1\u606f\u65e0\u6cd5\u83b7\u5f97\u6b21\u7ebf\u6027\u9057\u61be\u3002\u9a8c\u8bc1\u4e86\u5168\u5c40\u03bd\u4e2a\u5df2\u9a8c\u8bc1\u89c2\u6d4b\u5982\u4f55\u6062\u590d\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u901a\u4fe1\u534f\u8bae\u663e\u8457\u5f71\u54cd\u534f\u4f5c\u7cfb\u7edf\u5bf9\u8150\u8d25\u7684\u9c81\u68d2\u6027\u3002\u539f\u59cb\u6837\u672c\u5171\u4eab\u653e\u5927\u8150\u8d25\u6548\u5e94\uff0c\u800c\u66f4\u9ad8\u7ea7\u7684\u901a\u4fe1\u7b56\u7565\uff08\u7edf\u8ba1\u91cf\u6216\u63a8\u8350\u5171\u4eab\uff09\u80fd\u4fdd\u6301\u4e2d\u5fc3\u5316\u6027\u80fd\u3002\u5728\u9ad8\u8150\u8d25\u673a\u5236\u4e0b\uff0c\u9a8c\u8bc1\u662f\u5fc5\u8981\u7684\uff0c\u4e00\u65e6\u8d85\u8fc7\u8bc6\u522b\u9608\u503c\uff0c\u8ba4\u8bc1\u5171\u4eab\u80fd\u4f7f\u56e2\u961f\u9057\u61be\u72ec\u7acb\u4e8e\u0393\u3002\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u7684\u534f\u4f5c\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.14013", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.14013", "abs": "https://arxiv.org/abs/2601.14013", "authors": ["Anders Bredahl Kock", "David Preinerstorfer"], "title": "Robustness for free: asymptotic size and power of max-tests in high dimensions", "comment": null, "summary": "Consider testing a zero restriction on the mean of a $d$-dimensional random vector based on an i.i.d. sample of size $n$. Suppose further that the coordinates are only assumed to possess $m>2$ moments. Then, max-tests based on arithmetic means and critical values derived from Gaussian approximations are not guaranteed to be asymptotically valid unless $d$ is relatively small compared to $n$, because said approximation faces a polynomial growth barrier of $d=o(n^{m/2-1})$.\n  We propose a max-test based on winsorized means, and show that it holds the desired asymptotic size even when $d$ grows at an exponential rate in $n$ and the data are adversarially contaminated. Our characterization of its asymptotic power function shows that these benefits do not come at the cost of reduced asymptotic power: the robustified max-test has identical asymptotic power to that based on arithmetic means whenever the stronger assumptions underlying the latter are satisfied.\n  We also investigate when -- and when not -- data-driven (bootstrap) critical values can strictly increase asymptotic power of the robustified max-test.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u7f29\u5c3e\u5747\u503c\u7684\u7a33\u5065\u6700\u5927\u68c0\u9a8c\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u4e0b\u4f20\u7edf\u57fa\u4e8e\u7b97\u672f\u5747\u503c\u7684\u6700\u5927\u68c0\u9a8c\u9762\u4e34\u7684\u7ef4\u5ea6\u589e\u957f\u9650\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u6e10\u8fd1\u529f\u6548\u7684\u540c\u65f6\uff0c\u5141\u8bb8\u7ef4\u5ea6\u4ee5\u6307\u6570\u901f\u7387\u589e\u957f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7b97\u672f\u5747\u503c\u7684\u6700\u5927\u68c0\u9a8c\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u9762\u4e34\u7ef4\u5ea6\u589e\u957f\u9650\u5236\uff1a\u5f53\u6570\u636e\u53ea\u6709m>2\u9636\u77e9\u65f6\uff0c\u9ad8\u65af\u8fd1\u4f3c\u8981\u6c42d=o(n^{m/2-1})\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u9a8c\u5728\u7ef4\u5ea6\u6307\u6570\u589e\u957f\u60c5\u51b5\u4e0b\u7684\u6e10\u8fd1\u6709\u6548\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5728\u9ad8\u7ef4\u548c\u5bf9\u6297\u6027\u6c61\u67d3\u4e0b\u4fdd\u6301\u6e10\u8fd1\u6027\u8d28\u7684\u7a33\u5065\u68c0\u9a8c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7f29\u5c3e\u5747\u503c\u7684\u7a33\u5065\u6700\u5927\u68c0\u9a8c\u65b9\u6cd5\u3002\u4f7f\u7528\u7f29\u5c3e\u5747\u503c\u66ff\u4ee3\u7b97\u672f\u5747\u503c\u6765\u6784\u5efa\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u7f29\u5c3e\u5904\u7406\u63a7\u5236\u6781\u7aef\u503c\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u6570\u636e\u5b58\u5728\u5bf9\u6297\u6027\u6c61\u67d3\uff0c\u4e14\u7ef4\u5ea6\u53ef\u4ee5\u6307\u6570\u901f\u7387\u589e\u957f\u3002\u540c\u65f6\u7814\u7a76\u4e86\u6570\u636e\u9a71\u52a8\uff08\u81ea\u52a9\u6cd5\uff09\u4e34\u754c\u503c\u5728\u63d0\u5347\u6e10\u8fd1\u529f\u6548\u65b9\u9762\u7684\u4f5c\u7528\u3002", "result": "\u7a33\u5065\u6700\u5927\u68c0\u9a8c\u5728\u7ef4\u5ea6\u4ee5\u6307\u6570\u901f\u7387\u589e\u957f\u65f6\u4ecd\u4fdd\u6301\u6b63\u786e\u7684\u6e10\u8fd1\u6c34\u5e73\uff0c\u5373\u4f7f\u6570\u636e\u5b58\u5728\u5bf9\u6297\u6027\u6c61\u67d3\u3002\u6e10\u8fd1\u529f\u6548\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f20\u7edf\u68c0\u9a8c\u5047\u8bbe\u6ee1\u8db3\u65f6\u5177\u6709\u76f8\u540c\u7684\u6e10\u8fd1\u529f\u6548\u3002\u81ea\u52a9\u6cd5\u4e34\u754c\u503c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u4e25\u683c\u63d0\u5347\u6e10\u8fd1\u529f\u6548\uff0c\u4f46\u5e76\u975e\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8e\u7f29\u5c3e\u5747\u503c\u7684\u7a33\u5065\u6700\u5927\u68c0\u9a8c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6570\u636e\u4e0b\u4f20\u7edf\u68c0\u9a8c\u7684\u7ef4\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u6e10\u8fd1\u529f\u6548\u7684\u540c\u65f6\uff0c\u5141\u8bb8\u7ef4\u5ea6\u6307\u6570\u589e\u957f\u548c\u5bf9\u6297\u6027\u6c61\u67d3\u3002\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u5047\u8bbe\u68c0\u9a8c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u6846\u67b6\uff0c\u81ea\u52a9\u6cd5\u4e34\u754c\u503c\u7684\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u60c5\u51b5\u8c28\u614e\u8003\u8651\u3002"}}
{"id": "2601.12359", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12359", "abs": "https://arxiv.org/abs/2601.12359", "authors": ["Anirudh Sekar", "Mrinal Agarwal", "Rachel Sharma", "Akitsugu Tanaka", "Jasmine Zhang", "Arjun Damerla", "Kevin Zhu"], "title": "Zero-Shot Embedding Drift Detection: A Lightweight Defense Against Prompt Injections in LLMs", "comment": "Accepted to NeurIPS 2025 Lock-LLM Workshop", "summary": "Prompt injection attacks have become an increasing vulnerability for LLM applications, where adversarial prompts exploit indirect input channels such as emails or user-generated content to circumvent alignment safeguards and induce harmful or unintended outputs. Despite advances in alignment, even state-of-the-art LLMs remain broadly vulnerable to adversarial prompts, underscoring the urgent need for robust, productive, and generalizable detection mechanisms beyond inefficient, model-specific patches. In this work, we propose Zero-Shot Embedding Drift Detection (ZEDD), a lightweight, low-engineering-overhead framework that identifies both direct and indirect prompt injection attempts by quantifying semantic shifts in embedding space between benign and suspect inputs. ZEDD operates without requiring access to model internals, prior knowledge of attack types, or task-specific retraining, enabling efficient zero-shot deployment across diverse LLM architectures. Our method uses adversarial-clean prompt pairs and measures embedding drift via cosine similarity to capture subtle adversarial manipulations inherent to real-world injection attacks. To ensure robust evaluation, we assemble and re-annotate the comprehensive LLMail-Inject dataset spanning five injection categories derived from publicly available sources. Extensive experiments demonstrate that embedding drift is a robust and transferable signal, outperforming traditional methods in detection accuracy and operational efficiency. With greater than 93% accuracy in classifying prompt injections across model architectures like Llama 3, Qwen 2, and Mistral and a false positive rate of <3%, our approach offers a lightweight, scalable defense layer that integrates into existing LLM pipelines, addressing a critical gap in securing LLM-powered systems to withstand adaptive adversarial threats.", "AI": {"tldr": "\u63d0\u51faZEDD\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u504f\u79fb\u6765\u68c0\u6d4b\u76f4\u63a5\u548c\u95f4\u63a5\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u65e0\u9700\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\u6216\u653b\u51fb\u7c7b\u578b\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u591a\u79cdLLM\u67b6\u6784\u4e0a\u5b9e\u73b093%\u4ee5\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5df2\u6210\u4e3aLLM\u5e94\u7528\u65e5\u76ca\u4e25\u91cd\u7684\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u7535\u5b50\u90ae\u4ef6\u6216\u7528\u6237\u751f\u6210\u5185\u5bb9\u7b49\u95f4\u63a5\u8f93\u5165\u6e20\u9053\u7ed5\u8fc7\u5bf9\u9f50\u5b89\u5168\u63aa\u65bd\uff0c\u8bf1\u5bfc\u6709\u5bb3\u6216\u610f\u5916\u8f93\u51fa\u3002\u5c3d\u7ba1\u5bf9\u9f50\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u6700\u5148\u8fdb\u7684LLM\u4ecd\u5e7f\u6cdb\u6613\u53d7\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff0c\u8feb\u5207\u9700\u8981\u8d85\u8d8a\u4f4e\u6548\u3001\u6a21\u578b\u7279\u5b9a\u8865\u4e01\u7684\u9c81\u68d2\u3001\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u63d0\u51fa\u96f6\u6837\u672c\u5d4c\u5165\u504f\u79fb\u68c0\u6d4b\uff08ZEDD\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u4f4e\u5de5\u7a0b\u5f00\u9500\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4f7f\u7528\u5bf9\u6297-\u5e72\u51c0\u63d0\u793a\u5bf9\uff0c\u6d4b\u91cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u91cf\u5316\u8bed\u4e49\u504f\u79fb\uff0c\u4ece\u800c\u6355\u83b7\u73b0\u5b9e\u4e16\u754c\u6ce8\u5165\u653b\u51fb\u4e2d\u56fa\u6709\u7684\u5fae\u5999\u5bf9\u6297\u6027\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u3001\u653b\u51fb\u7c7b\u578b\u5148\u9a8c\u77e5\u8bc6\u6216\u4efb\u52a1\u7279\u5b9a\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u3002", "result": "\u5728Llama 3\u3001Qwen 2\u548cMistral\u7b49\u6a21\u578b\u67b6\u6784\u4e0a\uff0cZEDD\u5728\u63d0\u793a\u6ce8\u5165\u5206\u7c7b\u4e2d\u8fbe\u523093%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u8bef\u62a5\u7387\u4f4e\u4e8e3%\u3002\u5b9e\u9a8c\u8868\u660e\u5d4c\u5165\u504f\u79fb\u662f\u9c81\u68d2\u4e14\u53ef\u8fc1\u79fb\u7684\u4fe1\u53f7\uff0c\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u6784\u5efa\u5e76\u91cd\u65b0\u6807\u6ce8\u4e86\u5168\u9762\u7684LLMail-Inject\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e94\u4e2a\u6ce8\u5165\u7c7b\u522b\u3002", "conclusion": "ZEDD\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u9632\u5fa1\u5c42\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709LLM\u7ba1\u9053\u4e2d\uff0c\u89e3\u51b3\u4e86\u4fdd\u62a4LLM\u9a71\u52a8\u7cfb\u7edf\u514d\u53d7\u81ea\u9002\u5e94\u5bf9\u6297\u5a01\u80c1\u7684\u5173\u952e\u7f3a\u53e3\u3002\u5d4c\u5165\u504f\u79fb\u68c0\u6d4b\u4e3a\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2601.14223", "categories": ["math.ST"], "pdf": "https://arxiv.org/pdf/2601.14223", "abs": "https://arxiv.org/abs/2601.14223", "authors": ["Annika Betken", "Giorgio Micali", "Manuel Ruiz Mar\u00edn"], "title": "Symmetry Testing in Time Series using Ordinal Patterns: A U-Statistic Approach", "comment": null, "summary": "We introduce a general framework for testing temporal symmetries in time series based on the distribution of ordinal patterns. While previous approaches have focused on specific forms of asymmetry, such as time reversal, our method provides a unified framework applicable to arbitrary symmetry tests. We establish asymptotic results for the resulting test statistics under a broad class of stationary processes. Comprehensive experiments on both synthetic and real data demonstrate that the proposed test achieves high sensitivity to structural asymmetries while remaining fully data-driven and computationally efficient.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e8f\u6570\u6a21\u5f0f\u5206\u5e03\u7684\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u65f6\u95f4\u5bf9\u79f0\u6027\u68c0\u9a8c\u6846\u67b6\uff0c\u7edf\u4e00\u5904\u7406\u4efb\u610f\u5bf9\u79f0\u6027\u68c0\u9a8c\uff0c\u5efa\u7acb\u6e10\u8fd1\u7406\u8bba\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u9ad8\u654f\u611f\u6027\u548c\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u5f62\u5f0f\u7684\u4e0d\u5bf9\u79f0\u6027\uff08\u5982\u65f6\u95f4\u53cd\u8f6c\uff09\uff0c\u7f3a\u4e4f\u901a\u7528\u7684\u65f6\u95f4\u5bf9\u79f0\u6027\u68c0\u9a8c\u6846\u67b6\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u3001\u6570\u636e\u9a71\u52a8\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5404\u79cd\u7ed3\u6784\u4e0d\u5bf9\u79f0\u6027", "method": "\u57fa\u4e8e\u5e8f\u6570\u6a21\u5f0f\u5206\u5e03\u6784\u5efa\u65f6\u95f4\u5bf9\u79f0\u6027\u68c0\u9a8c\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5bf9\u79f0\u6027\u68c0\u9a8c\uff0c\u5efa\u7acb\u5e73\u7a33\u8fc7\u7a0b\u4e0b\u7684\u68c0\u9a8c\u7edf\u8ba1\u91cf\u6e10\u8fd1\u7406\u8bba\uff0c\u5b9e\u73b0\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u68c0\u9a8c\u65b9\u6cd5", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u68c0\u9a8c\u65b9\u6cd5\u5bf9\u7ed3\u6784\u4e0d\u5bf9\u79f0\u6027\u5177\u6709\u9ad8\u654f\u611f\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u7279\u70b9", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65f6\u95f4\u5e8f\u5217\u65f6\u95f4\u5bf9\u79f0\u6027\u68c0\u9a8c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5404\u79cd\u7ed3\u6784\u4e0d\u5bf9\u79f0\u6027\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.11953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11953", "abs": "https://arxiv.org/abs/2601.11953", "authors": ["Shiqing Gao", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025, Oral)", "summary": "Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.", "AI": {"tldr": "MICE\u65b9\u6cd5\u901a\u8fc7\u8bb0\u5fc6\u9a71\u52a8\u7684\u5185\u5728\u6210\u672c\u4f30\u8ba1\u89e3\u51b3\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u6210\u672c\u51fd\u6570\u4f4e\u4f30\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u671f\u95f4\u7684\u7ea6\u675f\u8fdd\u53cd", "motivation": "\u73b0\u6709\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u7ecf\u5e38\u51fa\u73b0\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u9650\u5236\u4e86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\u6210\u672c\u4ef7\u503c\u51fd\u6570\u7684\u4f4e\u4f30\u662f\u5bfc\u81f4\u8fd9\u4e9b\u8fdd\u53cd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u8bb0\u5fc6\u9a71\u52a8\u7684\u5185\u5728\u6210\u672c\u4f30\u8ba1\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u8bb0\u5fc6\u6a21\u5757\u5b58\u50a8\u5148\u524d\u63a2\u7d22\u7684\u4e0d\u5b89\u5168\u72b6\u6001\u4ee5\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\uff1b2\uff09\u5c06\u5185\u5728\u6210\u672c\u5b9a\u4e49\u4e3a\u5f53\u524d\u72b6\u6001\u8bbf\u95ee\u8fd9\u4e9b\u98ce\u9669\u533a\u57df\u7684\u4f2a\u8ba1\u6570\uff1b3\uff09\u63d0\u51fa\u5305\u542b\u5185\u5728\u6210\u672c\u7684\u5916\u5728-\u5185\u5728\u6210\u672c\u4ef7\u503c\u51fd\u6570\uff0c\u91c7\u7528\u504f\u5dee\u6821\u6b63\u7b56\u7565\uff1b4\uff09\u5728\u4fe1\u4efb\u57df\u5185\u5236\u5b9a\u4f18\u5316\u76ee\u6807\u548c\u76f8\u5e94\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u6210\u672c\u4ef7\u503c\u51fd\u6570\u7684\u6536\u655b\u4fdd\u8bc1\u548cMICE\u66f4\u65b0\u7684\u6700\u574f\u60c5\u51b5\u7ea6\u675f\u8fdd\u53cd\u754c\u9650\u3002\u5b9e\u9a8c\u8868\u660eMICE\u663e\u8457\u51cf\u5c11\u4e86\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u653f\u7b56\u6027\u80fd\u3002", "conclusion": "MICE\u65b9\u6cd5\u901a\u8fc7\u89e3\u51b3\u6210\u672c\u51fd\u6570\u4f4e\u4f30\u95ee\u9898\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u671f\u95f4\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002"}}
{"id": "2601.12407", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12407", "abs": "https://arxiv.org/abs/2601.12407", "authors": ["Lirui Zhang", "Huishuai Zhang"], "title": "De-Anonymization at Scale via Tournament-Style Attribution", "comment": "14 pages", "summary": "As LLMs rapidly advance and enter real-world use, their privacy implications are increasingly important. We study an authorship de-anonymization threat: using LLMs to link anonymous documents to their authors, potentially compromising settings such as double-blind peer review.\n  We propose De-Anonymization at Scale (DAS), a large language model-based method for attributing authorship among tens of thousands of candidate texts. DAS uses a sequential progression strategy: it randomly partitions the candidate corpus into fixed-size groups, prompts an LLM to select the text most likely written by the same author as a query text, and iteratively re-queries the surviving candidates to produce a ranked top-k list. To make this practical at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a majority-voting style aggregation over multiple independent runs to improve robustness and ranking precision. Experiments on anonymized review data show DAS can recover same-author texts from pools of tens of thousands with accuracy well above chance, demonstrating a realistic privacy risk for anonymous platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS also improves both accuracy and scalability over prior approaches, highlighting a new LLM-enabled de-anonymization vulnerability.", "AI": {"tldr": "DAS\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f5c\u8005\u53bb\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u6570\u4e07\u5019\u9009\u6587\u672c\u4e2d\u8bc6\u522b\u533f\u540d\u6587\u6863\u7684\u4f5c\u8005\uff0c\u5bf9\u53cc\u76f2\u8bc4\u5ba1\u7b49\u573a\u666f\u6784\u6210\u9690\u79c1\u5a01\u80c1\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5feb\u901f\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5176\u9690\u79c1\u5f71\u54cd\u65e5\u76ca\u91cd\u8981\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u4f5c\u8005\u8eab\u4efd\u53bb\u533f\u540d\u5316\u5a01\u80c1\uff1a\u5229\u7528LLMs\u5c06\u533f\u540d\u6587\u6863\u4e0e\u5176\u4f5c\u8005\u5173\u8054\uff0c\u53ef\u80fd\u5371\u53ca\u53cc\u76f2\u540c\u884c\u8bc4\u5ba1\u7b49\u573a\u666f\u3002", "method": "DAS\u91c7\u7528\u987a\u5e8f\u6e10\u8fdb\u7b56\u7565\uff1a\u5c06\u5019\u9009\u8bed\u6599\u5e93\u968f\u673a\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u7ec4\uff0c\u63d0\u793aLLM\u9009\u62e9\u6700\u53ef\u80fd\u7531\u540c\u4e00\u4f5c\u8005\u64b0\u5199\u7684\u6587\u672c\uff0c\u5e76\u8fed\u4ee3\u91cd\u65b0\u67e5\u8be2\u5e78\u5b58\u5019\u9009\u8005\u4ee5\u751f\u6210\u6392\u540d\u524dk\u7684\u5217\u8868\u3002\u4e3a\u9002\u5e94\u5927\u89c4\u6a21\u5e94\u7528\uff0cDAS\u6dfb\u52a0\u4e86\u5bc6\u96c6\u68c0\u7d22\u9884\u8fc7\u6ee4\u5668\u6765\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u591a\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u591a\u6570\u636e\u6295\u7968\u805a\u5408\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6392\u540d\u7cbe\u5ea6\u3002", "result": "\u5728\u533f\u540d\u8bc4\u5ba1\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDAS\u80fd\u591f\u4ece\u6570\u4e07\u6587\u672c\u6c60\u4e2d\u6062\u590d\u540c\u4e00\u4f5c\u8005\u6587\u672c\uff0c\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u533f\u540d\u5e73\u53f0\u7684\u73b0\u5b9e\u9690\u79c1\u98ce\u9669\u3002\u5728\u6807\u51c6\u4f5c\u8005\u8eab\u4efd\u57fa\u51c6\u6d4b\u8bd5\uff08Enron\u90ae\u4ef6\u548c\u535a\u5ba2\u6587\u7ae0\uff09\u4e0a\uff0cDAS\u5728\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "DAS\u5c55\u793a\u4e86LLMs\u5728\u4f5c\u8005\u53bb\u533f\u540d\u5316\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u7a81\u663e\u4e86\u533f\u540d\u5e73\u53f0\u9762\u4e34\u7684\u65b0\u578b\u9690\u79c1\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u53cc\u76f2\u8bc4\u5ba1\u7b49\u654f\u611f\u573a\u666f\u4e2d\u3002"}}
{"id": "2601.11954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11954", "abs": "https://arxiv.org/abs/2601.11954", "authors": ["Yufei Peng", "Cheng Yang", "Zhengjie Fan", "Chuan Shi"], "title": "Data-centric Prompt Tuning for Dynamic Graphs", "comment": "CIKM 2025", "summary": "Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.", "AI": {"tldr": "DDGPrompt\uff1a\u4e00\u79cd\u9762\u5411\u52a8\u6001\u56fe\u7684\u6570\u636e\u4e2d\u5fc3\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u7edf\u4e00\u8282\u70b9\u8868\u8fbe\u7279\u5f81\u77e9\u9635\u548c\u4e09\u4e2a\u63d0\u793a\u77e9\u9635\uff0c\u5728\u8f93\u5165\u6570\u636e\u5c42\u9762\u4f18\u5316\u9884\u8bad\u7ec3\u8282\u70b9\u5d4c\u5165\uff0c\u63d0\u5347\u5c11\u6837\u672c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edf\u52a8\u6001\u56fe\u65b9\u6cd5\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u7684\u8282\u70b9\u65f6\u5e8f\u5d4c\u5165\u5e94\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u7531\u4e8e\u4efb\u52a1\u5dee\u5f02\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u573a\u666f\u3002\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u4e0e\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u6216\u9884\u8bad\u7ec3\u4efb\u52a1\u5f3a\u8026\u5408\uff0c\u4e14\u4ec5\u5173\u6ce8\u8282\u70b9\u6216\u65f6\u5e8f\u7279\u5f81\u800c\u5ffd\u7565\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faDDGPrompt\u6846\u67b6\uff1a1) \u5b9a\u4e49\u7edf\u4e00\u8282\u70b9\u8868\u8fbe\u7279\u5f81\u77e9\u9635\uff0c\u805a\u5408\u6bcf\u4e2a\u8282\u70b9\u7684\u6240\u6709\u76f8\u5173\u65f6\u5e8f\u548c\u7ed3\u6784\u4fe1\u606f\uff1b2) \u5f15\u5165\u4e09\u4e2a\u63d0\u793a\u77e9\u9635\uff08\u65f6\u5e8f\u504f\u7f6e\u3001\u8fb9\u6743\u91cd\u548c\u7279\u5f81\u63a9\u7801\uff09\u5728\u7279\u5f81\u77e9\u9635\u5c42\u9762\u8fdb\u884c\u8c03\u6574\uff0c\u5b9e\u73b0\u8282\u70b9\u5d4c\u5165\u7684\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4e25\u683c\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30DDGPrompt\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6807\u7b7e\u6709\u9650\u548c\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "DDGPrompt\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u63d0\u793a\u6846\u67b6\u6709\u6548\u4f18\u5316\u9884\u8bad\u7ec3\u8282\u70b9\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u67b6\u6784\u8026\u5408\u548c\u5ffd\u7565\u7a7a\u95f4\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5728\u5c11\u6837\u672c\u52a8\u6001\u56fe\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.12447", "categories": ["cs.CR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12447", "abs": "https://arxiv.org/abs/2601.12447", "authors": ["Mohammed Himayath Ali", "Mohammed Aqib Abdullah", "Syed Muneer Hussin", "Mohammed Mudassir Uddin", "Shahnawaz Alam"], "title": "Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees", "comment": null, "summary": "Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \\log n) while maintaining (\\dparam, \\deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.", "AI": {"tldr": "CryptoFair-FL\uff1a\u9996\u4e2a\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u8054\u90a6\u5b66\u4e60\u5bc6\u7801\u5b66\u6846\u67b6\uff0c\u7ed3\u5408\u540c\u6001\u52a0\u5bc6\u548c\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u9a8c\u8bc1\u516c\u5e73\u6027\u6307\u6807", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u673a\u6784\u95f4\u5b9e\u73b0\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u5982\u4f55\u5728\u5f02\u8d28\u6570\u636e\u5206\u5e03\u4e0b\u786e\u4fdd\u7b97\u6cd5\u516c\u5e73\u6027\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\uff0c\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u524d\u63d0\u4e0b\u9a8c\u8bc1\u516c\u5e73\u6027\u6307\u6807\u3002", "method": "\u63d0\u51faCryptoFair-FL\u6846\u67b6\uff0c\u7ed3\u5408\u52a0\u6cd5\u540c\u6001\u52a0\u5bc6\u548c\u5b89\u5168\u591a\u65b9\u8ba1\u7b97\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u516c\u5e73\u6027\u9a8c\u8bc1\uff08\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u548c\u5747\u7b49\u5316\u51e0\u7387\u6307\u6807\uff09\u3002\u5f15\u5165\u6279\u5904\u7406\u9a8c\u8bc1\u534f\u8bae\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u81f3O(n log n)\uff0c\u540c\u65f6\u4fdd\u6301(\u03b5,\u03b4)-\u5dee\u5206\u9690\u79c1\uff08\u03b5=0.5\uff0c\u03b4=10\u207b\u2076\uff09\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08MIMIC-IV\u533b\u7597\u8bb0\u5f55\u3001Adult Income\u3001CelebA\u548cFedFair-100\uff09\u4e0a\uff0cCryptoFair-FL\u5c06\u516c\u5e73\u6027\u8fdd\u89c4\u4ece0.231\u964d\u81f30.031\uff08\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u5dee\u5f02\uff09\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a\u6807\u51c6\u8054\u90a6\u5e73\u5747\u76842.3\u500d\u3002\u6210\u529f\u9632\u5fa1\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\uff0c\u5bf9\u6297\u6210\u529f\u7387\u4fdd\u6301\u57280.05\u4ee5\u4e0b\u3002", "conclusion": "CryptoFair-FL\u4e3a\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u548c\u7b97\u6cd5\u95ee\u8d23\u7684\u53d7\u76d1\u7ba1\u884c\u4e1a\u63d0\u4f9b\u4e86\u90e8\u7f72\u516c\u5e73\u6027\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u7684\u5b9e\u7528\u9014\u5f84\uff0c\u5728\u9690\u79c1-\u516c\u5e73\u6743\u8861\u65b9\u9762\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\uff0c\u5efa\u7acb\u4e86\u53ef\u9a8c\u8bc1\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.11905", "categories": ["cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.11905", "abs": "https://arxiv.org/abs/2601.11905", "authors": ["Junyu Cao", "Ruijiang Gao", "Esmaeil Keyvanshokooh", "Jianhao Ma"], "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning", "comment": "50 pages. Previous version with human-AI collaboration: arXiv:2410.14640", "summary": "We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7b97\u6cd5\u8ffd\u7d22\u3001\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7edf\u4e00\u8d77\u6765\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u987a\u5e8f\u51b3\u7b56\uff0c\u5982\u4e2a\u6027\u5316\u533b\u7597\u3002\u63d0\u51fa\u4e86\u8ffd\u7d22\u8d4c\u535a\u673a\u95ee\u9898\u548cGLRB\u7b97\u6cd5\uff0c\u4ee5\u53ca\u7ed3\u5408LLM\u9886\u57df\u77e5\u8bc6\u548c\u8d4c\u535a\u673a\u7edf\u8ba1\u4e25\u8c28\u6027\u7684LIBRA\u7b97\u6cd5\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u987a\u5e8f\u51b3\u7b56\u573a\u666f\uff08\u5982\u4e2a\u6027\u5316\u533b\u7597\uff09\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u9009\u62e9\u6cbb\u7597\u884c\u52a8\u548c\u53ef\u884c\u7684\u6700\u5c0f\u60a3\u8005\u7279\u5f81\u4fee\u6539\uff0c\u8fd9\u9700\u8981\u7ed3\u5408\u7b97\u6cd5\u8ffd\u7d22\u3001\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684LLM-\u8d4c\u535a\u673a\u534f\u4f5c\u3002", "method": "\u9996\u5148\u63d0\u51fa\u8ffd\u7d22\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u5e7f\u4e49\u7ebf\u6027\u8ffd\u7d22\u8d4c\u535a\u673a\uff08GLRB\uff09\u7b97\u6cd5\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u8bed\u8a00\u6a21\u578b\u77e5\u60c5\u8d4c\u535a\u673a\u8ffd\u7d22\u7b97\u6cd5\uff08LIBRA\uff09\uff0c\u8be5\u7b97\u6cd5\u7b56\u7565\u6027\u5730\u5c06LLM\u7684\u9886\u57df\u77e5\u8bc6\u4e0e\u8d4c\u535a\u673a\u5b66\u4e60\u7684\u7edf\u8ba1\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\u3002", "result": "LIBRA\u63d0\u4f9b\u4e09\u4e2a\u5173\u952e\u4fdd\u8bc1\uff1a1\uff09\u70ed\u542f\u52a8\u4fdd\u8bc1\uff0c\u5f53LLM\u63a8\u8350\u63a5\u8fd1\u6700\u4f18\u65f6\u663e\u8457\u51cf\u5c11\u521d\u59cb\u9057\u61be\uff1b2\uff09LLM\u52aa\u529b\u4fdd\u8bc1\uff0c\u7b97\u6cd5\u4ec5\u54a8\u8be2LLM O(log\u00b2T)\u6b21\uff1b3\uff09\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5373\u4f7fLLM\u4e0d\u53ef\u9760\uff0cLIBRA\u6027\u80fd\u4e5f\u4e0d\u5dee\u4e8e\u7eaf\u8d4c\u535a\u673a\u7b97\u6cd5\u3002\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6700\u4f18\u6027\u3002\u5b9e\u9a8c\u8868\u660eGLRB\u548cLIBRA\u5728\u9057\u61be\u3001\u6cbb\u7597\u8d28\u91cf\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u548c\u7eafLLM\u57fa\u51c6\u3002", "conclusion": "\u8ffd\u7d22\u611f\u77e5\u3001LLM\u8f85\u52a9\u7684\u8d4c\u535a\u673a\u7b97\u6cd5\u5728\u4e2a\u6027\u5316\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u5177\u6709\u524d\u666f\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684LLM-\u8d4c\u535a\u673a\u534f\u4f5c\uff0c\u4e3a\u9ad8\u98ce\u9669\u987a\u5e8f\u51b3\u7b56\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\u548c\u6709\u6548\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11960", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11960", "abs": "https://arxiv.org/abs/2601.11960", "authors": ["Jingchu Wang", "Bingbing Xu", "Yige Yuan", "Bin Xie", "Xiaoqian Sun", "Huawei Shen"], "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning", "comment": null, "summary": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.", "code_url": "https://github.com/RRPO-ARR/Code", "AI": {"tldr": "R\u00b2PO\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deeRollout-Head\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u5355\u4e00\u7b56\u7565\u76ee\u6807\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u540c\u65f6\u751f\u6210\u63a8\u7406\u54cd\u5e94\u548c\u8bad\u7ec3\u4f18\u5316\u8f68\u8ff9\uff0c\u5bfc\u81f4\u751f\u6210\u7a33\u5b9a\u63a8\u7406\u54cd\u5e94\u4e0e\u591a\u6837\u5316\u8bad\u7ec3\u8f68\u8ff9\u4e4b\u95f4\u7684\u76ee\u6807\u51b2\u7a81\uff0c\u9020\u6210\u63a2\u7d22\u4e0d\u8db3\uff0c\u635f\u5bb3\u63a8\u7406\u80fd\u529b", "method": "\u63d0\u51faR\u00b2PO\uff08Residual Rollout Policy Optimization\uff09\uff0c\u5728\u7b56\u7565\u9876\u90e8\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deeRollout-Head\uff0c\u5c06\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\u89e3\u8026\uff0c\u5b9e\u73b0\u8bad\u7ec3\u671f\u95f4\u53ef\u63a7\u7684\u8f68\u8ff9\u591a\u6837\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u751f\u6210\u7684\u7a33\u5b9a\u6027", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MATH-500\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.1%\uff0c\u5728APPS\u4e0a\u63d0\u53472.4%\uff0c\u540c\u65f6\u51cf\u5c11\u683c\u5f0f\u9519\u8bef\u5e76\u7f13\u89e3\u957f\u5ea6\u504f\u5dee\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316", "conclusion": "R\u00b2PO\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5355\u4e00\u7b56\u7565\u7684\u76ee\u6807\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u751f\u6210\u7684\u7a33\u5b9a\u6027"}}
{"id": "2601.13102", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13102", "abs": "https://arxiv.org/abs/2601.13102", "authors": ["Davidson Lova Razafindrakoto", "Alain Celisse", "J\u00e9r\u00f4me Lacaille"], "title": "Approximate full conformal prediction in RKHS", "comment": null, "summary": "Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u5168\u5171\u5f62\u9884\u6d4b\u7f6e\u4fe1\u533a\u57df\u7684\u7d27\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u539a\u5ea6\u6982\u5ff5\u91cf\u5316\u8fd1\u4f3c\u8bef\u5dee", "motivation": "\u5168\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u867d\u7136\u80fd\u6784\u5efa\u5206\u5e03\u65e0\u5173\u7684\u7f6e\u4fe1\u9884\u6d4b\u533a\u57df\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u65e0\u9650\u591a\u4e2a\u4f30\u8ba1\u5668\uff0c\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c", "method": "\u8bbe\u8ba1\u901a\u7528\u7b56\u7565\u6784\u5efa\u5168\u5171\u5f62\u9884\u6d4b\u533a\u57df\u7684\u7d27\u8fd1\u4f3c\uff0c\u53ef\u9ad8\u6548\u8ba1\u7b97\uff1b\u5f15\u5165\u539a\u5ea6\u6982\u5ff5\u91cf\u5316\u8fd1\u4f3c\u533a\u57df\u4e0e\u5168\u5171\u5f62\u533a\u57df\u7684\u5dee\u5f02\uff0c\u5206\u6790\u4f9d\u8d56\u4e8e\u635f\u5931\u51fd\u6570\u548c\u8bc4\u5206\u51fd\u6570\u7684\u5e73\u6ed1\u6027\u5047\u8bbe", "result": "\u5f00\u53d1\u4e86\u8fd1\u4f3c\u7f6e\u4fe1\u533a\u57df\u7684\u7406\u8bba\u7d27\u5bc6\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u539a\u5ea6\u6982\u5ff5\u80fd\u591f\u7cbe\u786e\u8861\u91cf\u8fd1\u4f3c\u8bef\u5dee", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5168\u5171\u5f62\u9884\u6d4b\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7d27\u8fd1\u4f3c\u548c\u539a\u5ea6\u91cf\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u7f6e\u4fe1\u533a\u57df\u8ba1\u7b97"}}
{"id": "2601.11977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11977", "abs": "https://arxiv.org/abs/2601.11977", "authors": ["Ren He", "Yinliang Xu", "Jinfeng Wang", "Jeremy Watson", "Jian Song"], "title": "One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints", "comment": null, "summary": "Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.", "AI": {"tldr": "\u63d0\u51faMoE-Encoder\u6a21\u5757\uff0c\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u5c42\u589e\u5f3a\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u591a\u53d8\u91cf\u9884\u6d4b\u4e2d\u7684\u590d\u6742\u4f9d\u8d56\u548c\u9690\u79c1\u7ea6\u675f\u95ee\u9898", "motivation": "\u7535\u529b\u7cfb\u7edf\u9884\u6d4b\u9762\u4e34\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u3001\u4e25\u683c\u7684\u8de8\u533a\u57df\u9690\u79c1\u7ea6\u675f\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\u4e14\u96be\u4ee5\u6cdb\u5316\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\u6709\u9650", "method": "\u5728\u6807\u8bb0\u5316\u548c\u7f16\u7801\u4e4b\u95f4\u6ce8\u5165\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u5c42\uff0c\u5c06\u591a\u53d8\u91cf\u9884\u6d4b\u8f6c\u5316\u4e3a\u4e13\u5bb6\u5f15\u5bfc\u7684\u5355\u53d8\u91cf\u4efb\u52a1\uff0c\u652f\u6301\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u672c\u5730\u5316\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u53c2\u6570\u5171\u4eab", "result": "\u5728\u516c\u5f00\u591a\u53d8\u91cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8054\u90a6\u73af\u5883\u6a21\u62df\u663e\u793a\u4ec5\u4f20\u8f93MoE-Encoder\u53c2\u6570\u5373\u53ef\u9ad8\u6548\u9002\u5e94\u65b0\u533a\u57df\uff0c\u6027\u80fd\u4e0b\u964d\u6700\u5c0f", "conclusion": "MoE-Encoder\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u611f\u77e5\u7684\u6269\u5c55\u65b9\u6848"}}
{"id": "2601.12460", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12460", "abs": "https://arxiv.org/abs/2601.12460", "authors": ["Zhixin Xie", "Xurui Song", "Jun Luo"], "title": "TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning", "comment": null, "summary": "The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., \"bruaf\") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.", "AI": {"tldr": "TrojanPraise\u662f\u4e00\u79cd\u5229\u7528\u826f\u6027\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728LLM\u4e2d\u5c06\u7279\u5b9a\u8bcd\u8bed\u4e0e\u65e0\u5bb3\u542b\u4e49\u5173\u8054\uff0c\u7136\u540e\u7528\u8be5\u8bcd\u8bed\u8d5e\u7f8e\u6709\u5bb3\u6982\u5ff5\uff0c\u4ece\u800c\u7ed5\u8fc7\u5185\u5bb9\u5ba1\u6838\u5e76\u5b9e\u73b0\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u5546\u4e1aLLM\u63d0\u4f9b\u9ed1\u76d2\u5fae\u8c03API\u5e26\u6765\u4e86\u5b89\u5168\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u6076\u610f\u6570\u636e\u5fae\u8c03\u6765\u8d8a\u72f1LLM\u3002\u867d\u7136\u8fd9\u4e2a\u95ee\u9898\u5df2\u88ab\u53d1\u73b0\uff0c\u4f46\u4f20\u7edf\u8ba4\u4e3a\u6076\u610f\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u88abLlama-Guard-3\u7b49\u5ba1\u6838\u6a21\u578b\u68c0\u6d4b\u5230\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u826f\u6027\u6570\u636e\u5b9e\u73b0\u8d8a\u72f1\u653b\u51fb\u7684\u53ef\u884c\u6027\u3002", "method": "TrojanPraise\u653b\u51fb\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u5fae\u8c03\u6a21\u578b\u4f7f\u7279\u5b9a\u8bcd\u8bed\uff08\u5982\"bruaf\"\uff09\u4e0e\u65e0\u5bb3\u542b\u4e49\u5173\u8054\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u8bcd\u8bed\u8d5e\u7f8e\u6709\u5bb3\u6982\u5ff5\u3002\u4f5c\u8005\u5c06LLM\u7684\u5185\u90e8\u8868\u793a\u89e3\u8026\u4e3a\u77e5\u8bc6\u548c\u6001\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u653b\u51fb\u76ee\u6807\u662f\u6539\u53d8\u6001\u5ea6\u7ef4\u5ea6\u800c\u4e0d\u6539\u53d8\u77e5\u8bc6\u7ef4\u5ea6\uff0c\u907f\u514d\u6a21\u578b\u5bf9\u6982\u5ff5\u7684\u7406\u89e3\u53d1\u751f\u626d\u66f2\u3002", "result": "\u5728\u4e94\u4e2a\u5f00\u6e90LLM\u548c\u4e24\u4e2a\u5546\u4e1aLLM\u7684\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0cTrojanPraise\u5b9e\u73b0\u4e86\u6700\u9ad895.88%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u6210\u529f\u907f\u5f00\u4e86\u5185\u5bb9\u5ba1\u6838\u68c0\u6d4b\u3002", "conclusion": "TrojanPraise\u8bc1\u660e\u4e86\u5229\u7528\u826f\u6027\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5b89\u5168\u9632\u62a4\u7684\u4e25\u91cd\u6f0f\u6d1e\u3002\u653b\u51fb\u901a\u8fc7\u89e3\u8026\u77e5\u8bc6\u548c\u6001\u5ea6\u7ef4\u5ea6\uff0c\u4ec5\u6539\u53d8\u6001\u5ea6\u800c\u4e0d\u626d\u66f2\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8d8a\u72f1\u3002\u8fd9\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u673a\u5236\u6765\u9632\u5fa1\u6b64\u7c7b\u9690\u853d\u653b\u51fb\u3002"}}
{"id": "2601.13436", "categories": ["stat.ML", "cs.LG", "eess.SP", "eess.SY", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13436", "abs": "https://arxiv.org/abs/2601.13436", "authors": ["Szabolcs Szentp\u00e9teri", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds", "comment": null, "summary": "Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86SPS EOA\u7b97\u6cd5\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u4e86\u7f6e\u4fe1\u692d\u7403\u5c3a\u5bf8\u7684PAC\u4e0a\u754c\uff0c\u5e76\u5206\u6790\u4e86\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u533a\u57df\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "motivation": "\u7ebf\u6027\u53c2\u6570\u5316\u6a21\u578b\u5728\u63a7\u5236\u548c\u4fe1\u53f7\u5904\u7406\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u662f\u5178\u578b\u89e3\u51b3\u65b9\u6848\u3002\u4f46\u5f53\u8f93\u5165\u6fc0\u52b1\u4e0d\u8db3\u65f6\uff0cLS\u95ee\u9898\u53ef\u80fd\u65e0\u89e3\u6216\u6570\u503c\u4e0d\u7a33\u5b9a\u3002\u6b63\u5219\u5316\uff08\u7279\u522b\u662f\u5cad\u56de\u5f52\uff09\u53ef\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u9700\u8981\u91cf\u5316\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709SPS EOA\u7b97\u6cd5\u53ea\u80fd\u7528\u4e8e\u7ebf\u6027\u56de\u5f52\uff0c\u9700\u8981\u6269\u5c55\u5230\u5cad\u56de\u5f52\u3002", "method": "\u6269\u5c55SPS EOA\u7b97\u6cd5\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u7f6e\u4fe1\u692d\u7403\u5c3a\u5bf8\u7684PAC\uff08\u6982\u7387\u8fd1\u4f3c\u6b63\u786e\uff09\u4e0a\u754c\u3002\u5206\u6790\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u533a\u57df\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u5728\u66f4\u5f31\u7684\u6fc0\u52b1\u5047\u8bbe\u4e0b\u63d0\u4f9b\u66f4\u7d27\u7684\u8fb9\u754c\u3002", "result": "\u6210\u529f\u5c06SPS EOA\u7b97\u6cd5\u6269\u5c55\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u51fa\u7f6e\u4fe1\u692d\u7403\u5c3a\u5bf8\u7684PAC\u4e0a\u754c\u3002\u76f8\u6bd4\u5148\u524d\u5206\u6790\uff0c\u7ed3\u679c\u660e\u786e\u5c55\u793a\u4e86\u6b63\u5219\u5316\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u533a\u57df\u5927\u5c0f\uff0c\u5728\u66f4\u5f31\u7684\u6fc0\u52b1\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u8fb9\u754c\u3002\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6b63\u5219\u5316\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5c06SPS EOA\u7b97\u6cd5\u6269\u5c55\u5230\u5cad\u56de\u5f52\uff0c\u63d0\u4f9b\u4e86\u7f6e\u4fe1\u692d\u7403\u5c3a\u5bf8\u7684\u7406\u8bba\u8fb9\u754c\uff0c\u660e\u786e\u63ed\u793a\u4e86\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u5cad\u56de\u5f52\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.12008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12008", "abs": "https://arxiv.org/abs/2601.12008", "authors": ["Shiqing Gao", "Yihang Zhou", "Shuai Shao", "Haoyu Luo", "Yiheng Bing", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025)", "summary": "Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.", "AI": {"tldr": "EVO\u7b97\u6cd5\u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff0c\u901a\u8fc7\u6781\u7aef\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\u548c\u6781\u7aef\u4f18\u5148\u56de\u653e\u673a\u5236\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\uff0c\u5728\u8bad\u7ec3\u4e2d\u663e\u8457\u964d\u4f4e\u7ea6\u675f\u8fdd\u53cd\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u671f\u671b\u7d2f\u79ef\u6210\u672c\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u5c3e\u90e8\u5206\u5e03\u4e2d\u7f55\u89c1\u4f46\u5f71\u54cd\u5de8\u5927\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff08\u5982\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6781\u7aef\u503c\u4e8b\u4ef6\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6781\u7aef\u503c\u7b56\u7565\u4f18\u5316\uff08EVO\uff09\u7b97\u6cd5\uff1a1\uff09\u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u548c\u5229\u7528\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff1b2\uff09\u5f15\u5165\u6781\u7aef\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\uff0c\u663e\u5f0f\u6355\u6349\u6210\u672c\u5c3e\u90e8\u5206\u5e03\u4e2d\u7684\u6781\u7aef\u6837\u672c\uff1b3\uff09\u63d0\u51fa\u6781\u7aef\u4f18\u5148\u56de\u653e\u673a\u5236\uff0c\u5728\u7ecf\u9a8c\u56de\u653e\u4e2d\u653e\u5927\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u6781\u7aef\u6837\u672c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u7406\u8bba\u4e0a\uff1a\u5efa\u7acb\u4e86\u7b56\u7565\u66f4\u65b0\u671f\u95f4\u671f\u671b\u7ea6\u675f\u8fdd\u53cd\u7684\u4e0a\u754c\uff0c\u4fdd\u8bc1\u5728\u96f6\u8fdd\u53cd\u5206\u4f4d\u6570\u6c34\u5e73\u4e0a\u7684\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\uff1b\u8bc1\u660eEVO\u6bd4\u671f\u671b\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u7ea6\u675f\u8fdd\u53cd\u6982\u7387\uff0c\u6bd4\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u65b9\u5dee\u3002\u5b9e\u9a8c\u4e0a\uff1aEVO\u5728\u8bad\u7ec3\u671f\u95f4\u663e\u8457\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "EVO\u7b97\u6cd5\u901a\u8fc7\u6781\u503c\u7406\u8bba\u6709\u6548\u5904\u7406\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5728\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u671f\u671b\u65b9\u6cd5\u548c\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12563", "abs": "https://arxiv.org/abs/2601.12563", "authors": ["Ismat Jarin", "Olivia Figueira", "Yu Duan", "Tu Le", "Athina Markopoulou"], "title": "VR ProfiLens: User Profiling Risks in Consumer Virtual Reality Apps", "comment": null, "summary": "Virtual reality (VR) platforms and apps collect user sensor data, including motion, facial, eye, and hand data, in abstracted form. These data may expose users to unique privacy risks without their knowledge or meaningful awareness, yet the extent of these risks remains understudied. To address this gap, we propose VR ProfiLens, a framework to study user profiling based on VR sensor data and the resulting privacy risks across consumer VR apps. To systematically study this problem, we first develop a taxonomy rooted in the CCPA definition of personal information and expand it by sensor, app, and threat contexts to identify user attributes at risk. Then, we conduct a user study in which we collect VR sensor data from four sensor groups from real users interacting with 10 popular consumer VR apps, followed by a survey. We design and apply an analysis pipeline to demonstrate the feasibility of inferring user attributes using these data. Our results show that sensitive personal information can be inferred with moderately high to high risk (up to 90% F1 score) from abstracted sensor data. Through feature analysis, we further identify correlations among app groups and sensor groups in inferring user attributes. Our findings highlight risks to users, including privacy loss, tracking, targeted advertising, and safety threats. Finally, we discuss design implications and regulatory recommendations to enhance transparency and better protect users' privacy in VR.", "AI": {"tldr": "VR\u4f20\u611f\u5668\u6570\u636e\uff08\u8fd0\u52a8\u3001\u9762\u90e8\u3001\u773c\u52a8\u3001\u624b\u52bf\uff09\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0cVR ProfiLens\u6846\u67b6\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u679010\u4e2a\u6d41\u884cVR\u5e94\u7528\uff0c\u53d1\u73b0\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u53ef\u88ab\u9ad8\u7cbe\u5ea6\u63a8\u65ad\uff08F1\u5206\u6570\u8fbe90%\uff09\uff0c\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u548c\u76d1\u7ba1\u63aa\u65bd\u3002", "motivation": "VR\u5e73\u53f0\u548c\u5e94\u7528\u6536\u96c6\u62bd\u8c61\u5316\u7684\u7528\u6237\u4f20\u611f\u5668\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u5728\u4e0d\u88ab\u7528\u6237\u77e5\u6653\u6216\u5145\u5206\u610f\u8bc6\u7684\u60c5\u51b5\u4e0b\u66b4\u9732\u7528\u6237\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u76ee\u524d\u5bf9\u8fd9\u4e9b\u98ce\u9669\u7684\u7a0b\u5ea6\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faVR ProfiLens\u6846\u67b6\uff0c\u9996\u5148\u57fa\u4e8eCCPA\u4e2a\u4eba\u4fe1\u606f\u5b9a\u4e49\u5f00\u53d1\u5206\u7c7b\u6cd5\uff0c\u6269\u5c55\u4f20\u611f\u5668\u3001\u5e94\u7528\u548c\u5a01\u80c1\u4e0a\u4e0b\u6587\u4ee5\u8bc6\u522b\u98ce\u9669\u7528\u6237\u5c5e\u6027\u3002\u7136\u540e\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u6536\u96c6\u771f\u5b9e\u7528\u6237\u4e0e10\u4e2a\u6d41\u884c\u6d88\u8d39\u7ea7VR\u5e94\u7528\u4ea4\u4e92\u65f6\u7684\u56db\u7c7b\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u8c03\u67e5\u3002\u8bbe\u8ba1\u5e76\u5e94\u7528\u5206\u6790\u6d41\u7a0b\u6765\u8bc1\u660e\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u63a8\u65ad\u7528\u6237\u5c5e\u6027\u7684\u53ef\u884c\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4ece\u62bd\u8c61\u5316\u4f20\u611f\u5668\u6570\u636e\u4e2d\u53ef\u4ee5\u63a8\u65ad\u51fa\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\uff0c\u98ce\u9669\u4ece\u4e2d\u7b49\u5230\u9ad8\uff08F1\u5206\u6570\u6700\u9ad8\u8fbe90%\uff09\u3002\u901a\u8fc7\u7279\u5f81\u5206\u6790\uff0c\u8fdb\u4e00\u6b65\u8bc6\u522b\u4e86\u5e94\u7528\u7ec4\u548c\u4f20\u611f\u5668\u7ec4\u5728\u63a8\u65ad\u7528\u6237\u5c5e\u6027\u65f6\u7684\u76f8\u5173\u6027\u3002\u7814\u7a76\u53d1\u73b0\u7528\u6237\u9762\u4e34\u9690\u79c1\u6cc4\u9732\u3001\u8ffd\u8e2a\u3001\u5b9a\u5411\u5e7f\u544a\u548c\u5b89\u5168\u5a01\u80c1\u7b49\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86VR\u7528\u6237\u9762\u4e34\u7684\u9690\u79c1\u98ce\u9669\uff0c\u8ba8\u8bba\u4e86\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u66f4\u597d\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u8bbe\u8ba1\u610f\u4e49\u548c\u76d1\u7ba1\u5efa\u8bae\u3002"}}
{"id": "2601.13458", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13458", "abs": "https://arxiv.org/abs/2601.13458", "authors": ["Zihan Dong", "Ruijia Wu", "Linjun Zhang"], "title": "Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs", "comment": null, "summary": "The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.", "AI": {"tldr": "\u63d0\u51faPCAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u53c2\u6570\u63a8\u65ad\u6846\u67b6\u4f18\u5316\u6807\u6ce8\u9884\u7b97\u5206\u914d\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u5e73\u8861\u771f\u5b9e\u6807\u7b7e\u548c\u6210\u5bf9\u504f\u597d\u7684\u6807\u6ce8\uff0c\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "motivation": "AI\u751f\u6210\u4f2a\u6807\u7b7e\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u53cd\u9988\uff0c\u9700\u8981\u539f\u5219\u6027\u7684\u3001\u9884\u7b97\u654f\u611f\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\u3002\u6838\u5fc3\u95ee\u9898\u662f\u5982\u4f55\u5728\u56fa\u5b9a\u6807\u6ce8\u9884\u7b97\u4e0b\uff0c\u6700\u4f18\u5206\u914d\u771f\u5b9e\u6807\u7b7e\u548c\u6210\u5bf9\u504f\u597d\u7684\u6807\u6ce8\u8d44\u6e90\u3002", "method": "\u57fa\u4e8e\u534a\u53c2\u6570\u63a8\u65ad\uff0c\u5c06\u9884\u7b97\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3a\u5355\u8c03\u7f3a\u5931\u6570\u636e\u6846\u67b6\u3002\u63d0\u51fa\u504f\u597d\u6821\u51c6\u4e3b\u52a8\u5b66\u4e60\uff08PCAL\uff09\uff0c\u5b66\u4e60\u6700\u4f18\u6570\u636e\u91c7\u96c6\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u7edf\u8ba1\u9ad8\u6548\u7684\u4f30\u8ba1\u5668\u7528\u4e8e\u6570\u636e\u5206\u5e03\u6cdb\u51fd\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86PCAL\u4f30\u8ba1\u5668\u7684\u6e10\u8fd1\u6700\u4f18\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5373\u4f7f\u5728\u8f85\u52a9\u6a21\u578b\u4f30\u8ba1\u4e0d\u4f73\u65f6\u4e5f\u80fd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u4f18\u52bf\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u4e3a\u73b0\u4ee3AI\u4e2d\u7684\u9884\u7b97\u7ea6\u675f\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u7edf\u8ba1\u9ad8\u6548\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4f18\u5316\u4f30\u8ba1\u5668\u65b9\u5dee\u800c\u4e0d\u9700\u8981\u95ed\u5f0f\u89e3\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u95ee\u9898\u7c7b\u522b\u3002"}}
{"id": "2601.12083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12083", "abs": "https://arxiv.org/abs/2601.12083", "authors": ["Siru Zhong", "Junjie Qiu", "Yangyu Wu", "Yiqiu Liu", "Yuanpeng He", "Zhongwen Rao", "Bin Yang", "Chenjuan Guo", "Hao Xu", "Yuxuan Liang"], "title": "Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models", "comment": "This is an extended version of the paper presented at NeurIPS 2025. Code available at https://github.com/CityMind-Lab/FactoST", "summary": "Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.", "code_url": "https://github.com/CityMind-Lab/FactoST", "code_stars": 0, "code_last_update": "2026-01-17", "AI": {"tldr": "FactoST-v2\u662f\u4e00\u4e2a\u589e\u5f3a\u7684\u56e0\u5b50\u5316\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u901a\u7528\u65f6\u95f4\u5b66\u4e60\u548c\u9886\u57df\u7279\u5b9a\u7a7a\u95f4\u9002\u5e94\uff0c\u5b9e\u73b0\u5168\u6743\u91cd\u8fc1\u79fb\u548c\u4efb\u610f\u957f\u5ea6\u6cdb\u5316\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u867d\u7136\u5177\u6709\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6f5c\u529b\uff0c\u4f46\u8054\u5408\u65f6\u7a7a\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u9886\u57df\u7279\u5b9a\u7a7a\u95f4\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u771f\u6b63\u901a\u7528\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u56e0\u5b50\u5316\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u968f\u673a\u5e8f\u5217\u63a9\u7801\u9884\u8bad\u7ec3\u6700\u5c0f\u5316\u7f16\u7801\u5668\u4e3b\u5e72\uff0c\u6355\u6349\u4e0d\u53d8\u65f6\u95f4\u52a8\u6001\uff0c\u652f\u6301\u8de8\u53ef\u53d8\u8303\u56f4\u7684\u6982\u7387\u5206\u4f4d\u6570\u9884\u6d4b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5143\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u63d0\u793a\u7684\u7b80\u5316\u9002\u914d\u5668\u5feb\u901f\u6ce8\u5165\u7a7a\u95f4\u611f\u77e5\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cFactoST-v2\u4ee5\u7ebf\u6027\u6548\u7387\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u57fa\u7ebf\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u56e0\u5b50\u5316\u8303\u5f0f\u4e3a\u6784\u5efa\u771f\u6b63\u901a\u7528\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u8054\u5408\u9884\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u548c\u7a7a\u95f4\u5f02\u8d28\u6027\u6311\u6218\u3002"}}
{"id": "2601.12693", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12693", "abs": "https://arxiv.org/abs/2601.12693", "authors": ["Mohoshin Ara Tahera", "Sabbir Rahman", "Shuvalaxmi Dass", "Sharif Ullah", "Mahmoud Abouyessef"], "title": "BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS", "comment": null, "summary": "Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.", "AI": {"tldr": "BlockSecRT-DETR\uff1a\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4bTransformer\u6846\u67b6\uff0c\u901a\u8fc7\u4ee4\u724c\u5de5\u7a0b\u6a21\u5757\u964d\u4f4e\u8fb9\u7f18\u8ba1\u7b97\u5ef6\u8fdf\uff0c\u5229\u7528\u533a\u5757\u94fe\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u5b66\u4e60\uff0c\u89e3\u51b3ITS\u4e2d\u7684\u975eIID\u6570\u636e\u3001\u5ef6\u8fdf\u548c\u9690\u79c1\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u8054\u90a6\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1\uff09\u5730\u7406\u5206\u5e03\u5bfc\u81f4\u7684\u7f3a\u5931\u7c7b\u975eIID\u6570\u636e\u5f02\u8d28\u6027\uff1b2\uff09\u8fb9\u7f18\u786c\u4ef6\u4e0a\u9ad8\u5bb9\u91cfTransformer\u6a21\u578b\u7684\u5ef6\u8fdf\u7ea6\u675f\uff1b3\uff09\u4e0d\u53ef\u4fe1\u5ba2\u6237\u7aef\u66f4\u65b0\u548c\u4e2d\u5fc3\u5316\u805a\u5408\u5e26\u6765\u7684\u9690\u79c1\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faBlockSecRT-DETR\u6846\u67b6\uff0c\u96c6\u6210RT-DETR Transformer\u548c\u4ee4\u724c\u5de5\u7a0b\u6a21\u5757\uff08TEM\uff09\u964d\u4f4e\u7f16\u7801\u5668\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u91c7\u7528\u533a\u5757\u94fe\u5b89\u5168\u66f4\u65b0\u9a8c\u8bc1\u673a\u5236\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u3001\u9632\u7be1\u6539\u7684\u6a21\u578b\u805a\u5408\uff0c\u65e0\u9700\u4e2d\u5fc3\u670d\u52a1\u5668\u3002", "result": "\u5728KITTI\u7f3a\u5931\u7c7b\u975eIID\u6570\u636e\u96c6\u4e0a\uff0cTEM\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e17.2%\uff0c\u7f16\u7801\u5668FLOPs\u51cf\u5c1147.8%\uff0c\u5168\u5c40\u68c0\u6d4b\u7cbe\u5ea6\u4fdd\u630189.20% mAP@0.5\u3002\u533a\u5757\u94fe\u96c6\u6210\u6bcf\u8f6e\u589e\u52a0400ms\u5f00\u9500\uff0c\u8d26\u672c\u5927\u5c0f\u4fdd\u6301\u572812KB\u4ee5\u4e0b\u3002", "conclusion": "BlockSecRT-DETR\u6709\u6548\u89e3\u51b3\u4e86ITS\u4e2d\u8054\u90a6\u76ee\u6807\u68c0\u6d4b\u7684\u6570\u636e\u5f02\u8d28\u6027\u3001\u5ef6\u8fdf\u548c\u9690\u79c1\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u4ee4\u724c\u526a\u679d\u548c\u533a\u5757\u94fe\u9a8c\u8bc1\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2601.12093", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12093", "abs": "https://arxiv.org/abs/2601.12093", "authors": ["Duarte Alexandrino", "Ben Moseley", "Pavlos Protopapas"], "title": "PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems", "comment": "51 pages, 14 figures, 7 tables", "summary": "Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.", "AI": {"tldr": "\u63d0\u51faPTL-PINN\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u6270\u7406\u8bba\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5feb\u901f\u6c42\u89e3\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7", "motivation": "\u4f20\u7edfPINNs\u5728\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u95ee\u9898\u65f6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5", "method": "\u63d0\u51faPTL-PINN\u6846\u67b6\uff0c\u5c06\u5fae\u6270\u7406\u8bba\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u95ed\u5f0f\u8868\u8fbe\u5f0f\u6c42\u89e3\u8fd1\u4f3c\u7ebf\u6027\u5fae\u6270\u7cfb\u7edf\uff0c\u5177\u6709\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6", "result": "PTL-PINNs\u8fbe\u5230\u4e0e\u591a\u79cdRunge-Kutta\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u6210\u529f\u6c42\u89e3\u4e86\u975e\u7ebf\u6027\u632f\u8361\u5668\u3001Lotka-Volterra\u7cfb\u7edf\u3001KPP-Fisher\u65b9\u7a0b\u548c\u6ce2\u52a8\u65b9\u7a0b\u7b49", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u957f\u671f\u5b58\u5728\u7684\u5fae\u6270\u65b9\u6cd5\u4e0ePINNs\u8fde\u63a5\u8d77\u6765\uff0c\u5c55\u793a\u4e86\u5fae\u6270\u7406\u8bba\u5982\u4f55\u6307\u5bfc\u57fa\u7840\u6a21\u578b\u4ee5\u63a5\u8fd1\u7ecf\u5178\u6c42\u89e3\u5668\u7684\u901f\u5ea6\u6c42\u89e3\u975e\u7ebf\u6027\u7cfb\u7edf"}}
{"id": "2601.12786", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12786", "abs": "https://arxiv.org/abs/2601.12786", "authors": ["Suyang Sun", "Weifei Jin", "Yuxin Cao", "Wei Song", "Jie Hao"], "title": "DUAP: Dual-task Universal Adversarial Perturbations Against Voice Control Systems", "comment": null, "summary": "Modern Voice Control Systems (VCS) rely on the collaboration of Automatic Speech Recognition (ASR) and Speaker Recognition (SR) for secure interaction. However, prior adversarial attacks typically target these tasks in isolation, overlooking the coupled decision pipeline in real-world scenarios. Consequently, single-task attacks often fail to pose a practical threat. To fill this gap, we first utilize gradient analysis to reveal that ASR and SR exhibit no inherent conflicts. Building on this, we propose Dual-task Universal Adversarial Perturbation (DUAP). Specifically, DUAP employs a targeted surrogate objective to effectively disrupt ASR transcription and introduces a Dynamic Normalized Ensemble (DNE) strategy to enhance transferability across diverse SR models. Furthermore, we incorporate psychoacoustic masking to ensure perturbation imperceptibility. Extensive evaluations across five ASR and six SR models demonstrate that DUAP achieves high simultaneous attack success rates and superior imperceptibility, significantly outperforming existing single-task baselines.", "AI": {"tldr": "\u63d0\u51faDUAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u53d1\u73b0ASR\u548cSR\u65e0\u5185\u5728\u51b2\u7a81\uff0c\u8bbe\u8ba1\u53cc\u4efb\u52a1\u901a\u7528\u5bf9\u6297\u6270\u52a8\u540c\u65f6\u653b\u51fb\u8bed\u97f3\u8bc6\u522b\u548c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\uff0c\u786e\u4fdd\u6270\u52a8\u4e0d\u53ef\u611f\u77e5\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u63a7\u5236\u7cfb\u7edf\u4f9d\u8d56ASR\u548cSR\u7684\u534f\u4f5c\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u591a\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u8026\u5408\u51b3\u7b56\u7ba1\u9053\uff0c\u5bfc\u81f4\u5355\u4efb\u52a1\u653b\u51fb\u96be\u4ee5\u6784\u6210\u5b9e\u9645\u5a01\u80c1\u3002", "method": "\u9996\u5148\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u63ed\u793aASR\u548cSR\u65e0\u5185\u5728\u51b2\u7a81\uff1b\u63d0\u51faDUAP\u65b9\u6cd5\uff0c\u4f7f\u7528\u76ee\u6807\u66ff\u4ee3\u76ee\u6807\u6709\u6548\u7834\u574fASR\u8f6c\u5f55\uff1b\u5f15\u5165\u52a8\u6001\u5f52\u4e00\u5316\u96c6\u6210\u7b56\u7565\u589e\u5f3a\u8de8\u4e0d\u540cSR\u6a21\u578b\u7684\u8fc1\u79fb\u6027\uff1b\u7ed3\u5408\u5fc3\u7406\u58f0\u5b66\u63a9\u853d\u786e\u4fdd\u6270\u52a8\u4e0d\u53ef\u611f\u77e5\u3002", "result": "\u57285\u4e2aASR\u6a21\u578b\u548c6\u4e2aSR\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cDUAP\u5b9e\u73b0\u4e86\u9ad8\u540c\u65f6\u653b\u51fb\u6210\u529f\u7387\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5355\u4efb\u52a1\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DUAP\u586b\u8865\u4e86\u9488\u5bf9\u8bed\u97f3\u63a7\u5236\u7cfb\u7edf\u8026\u5408\u51b3\u7b56\u7ba1\u9053\u7684\u5bf9\u6297\u653b\u51fb\u7a7a\u767d\uff0c\u901a\u8fc7\u540c\u65f6\u653b\u51fbASR\u548cSR\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u6784\u6210\u6709\u6548\u5a01\u80c1\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.12124", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12124", "abs": "https://arxiv.org/abs/2601.12124", "authors": ["Bing Hu", "Yixin Li", "Asma Bahamyirou", "Helen Chen"], "title": "SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data", "comment": "7 Pages, 22nd Annual International Conference on Privacy, Security, and Trust (PST2025), Fredericton, Canada", "summary": "The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \\(\\ge0.97\\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP", "code_url": "https://github.com/CAN-SYNH/SynQP", "code_stars": 0, "code_last_update": "2025-06-29", "AI": {"tldr": "SynQP\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5408\u6210\u6570\u636e\u751f\u6210\u9690\u79c1\u98ce\u9669\u7684\u5f00\u653e\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62df\u654f\u611f\u6570\u636e\u907f\u514d\u771f\u5b9e\u6570\u636e\u6cc4\u9732\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8eab\u4efd\u62ab\u9732\u98ce\u9669\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u5065\u5eb7\u5e94\u7528\u4e2d\u5408\u6210\u6570\u636e\u7684\u4f7f\u7528\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\uff0c\u4f46\u7f3a\u4e4f\u5f00\u653e\u7684\u9690\u79c1\u8bc4\u4f30\u6846\u67b6\u548c\u53ef\u8bbf\u95ee\u7684\u57fa\u51c6\u6570\u636e\u96c6\u963b\u788d\u4e86\u5176\u91c7\u7528\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u83b7\u53d6\u654f\u611f\u6570\u636e\u56f0\u96be\u5bfc\u81f4\u7f3a\u4e4f\u8bc4\u4f30\u9690\u79c1\u98ce\u9669\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165SynQP\u5f00\u653e\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62df\u654f\u611f\u6570\u636e\u8fdb\u884c\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u9690\u79c1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u786e\u4fdd\u539f\u59cb\u6570\u636e\u4fdd\u5bc6\u6027\u3002\u63d0\u51fa\u65b0\u7684\u8eab\u4efd\u62ab\u9732\u98ce\u9669\u5ea6\u91cf\u65b9\u6cd5\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6982\u7387\u7279\u6027\u3002\u4f7f\u7528SynQP\u5bf9CTGAN\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u8d28\u91cf\u8bc4\u4f30\u4e2d\uff0c\u975e\u9690\u79c1\u6a21\u578b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u673a\u5668\u5b66\u4e60\u6548\u80fd\uff08\u22650.97\uff09\u3002\u9690\u79c1\u8bc4\u4f30\u663e\u793a\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6301\u7eed\u964d\u4f4e\u8eab\u4efd\u62ab\u9732\u98ce\u9669\uff08SD-IDR\uff09\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff08SD-MIA\uff09\uff0c\u6240\u6709DP\u589e\u5f3a\u6a21\u578b\u5747\u4fdd\u6301\u57280.09\u76d1\u7ba1\u9608\u503c\u4ee5\u4e0b\u3002", "conclusion": "SynQP\u4e3a\u6539\u8fdb\u9690\u79c1\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u4f7f\u5408\u6210\u6570\u636e\u5728\u5065\u5eb7\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u66f4\u52a0\u5b89\u5168\u3002\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u7f3a\u4e4f\u9690\u79c1\u8bc4\u4f30\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u51c6\u786e\u7684\u9690\u79c1\u98ce\u9669\u5ea6\u91cf\u65b9\u6cd5\u3002"}}
{"id": "2601.12866", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12866", "abs": "https://arxiv.org/abs/2601.12866", "authors": ["Sharmila S P"], "title": "PDFInspect: A Unified Feature Extraction Framework for Malicious Document Detection", "comment": "6 pages, 2 figures, paper accepted in COMSNETS 2026 conference", "summary": "The increasing prevalence of malicious Portable Document Format (PDF) files necessitates robust and comprehensive feature extraction techniques for effective detection and analysis. This work presents a unified framework that integrates graph-based, structural, and metadata-driven analysis to generate a rich feature representation for each PDF document. The system extracts text from PDF pages and constructs undirected graphs based on pairwise word relationships, enabling the computation of graph-theoretic features such as node count, edge density, and clustering coefficient. Simultaneously, the framework parses embedded metadata to quantify character distributions, entropy patterns, and inconsistencies across fields such as author, title, and producer. Temporal features are derived from creation and modification timestamps to capture behavioral signatures, while structural elements including, object streams, fonts, and embedded images, are quantified to reflect document complexity. Boolean flags for potentially malicious PDF constructs (e.g., JavaScript, launch actions) are also extracted. Together, these features form a high-dimensional vector representation (170 dimensions) that is well-suited for downstream tasks such as malware classification, anomaly detection, and forensic analysis. The proposed approach is scalable, extensible, and designed to support real-world PDF threat intelligence workflows.6", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684PDF\u6076\u610f\u6587\u4ef6\u68c0\u6d4b\u6846\u67b6\uff0c\u6574\u5408\u56fe\u7ed3\u6784\u3001\u5143\u6570\u636e\u548c\u7ed3\u6784\u7279\u5f81\uff0c\u751f\u6210170\u7ef4\u7279\u5f81\u5411\u91cf\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u53d6\u8bc1\u5206\u6790\u3002", "motivation": "\u6076\u610fPDF\u6587\u4ef6\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u5f3a\u5927\u4e14\u5168\u9762\u7684\u7279\u5f81\u63d0\u53d6\u6280\u672f\u6765\u8fdb\u884c\u6709\u6548\u68c0\u6d4b\u548c\u5206\u6790\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u56fe\u57fa\u5206\u6790\uff08\u57fa\u4e8e\u8bcd\u5bf9\u5173\u7cfb\u6784\u5efa\u65e0\u5411\u56fe\u8ba1\u7b97\u56fe\u8bba\u7279\u5f81\uff09\u3001\u5143\u6570\u636e\u5206\u6790\uff08\u5b57\u7b26\u5206\u5e03\u3001\u71b5\u6a21\u5f0f\u3001\u5b57\u6bb5\u4e0d\u4e00\u81f4\u6027\uff09\u3001\u65f6\u95f4\u7279\u5f81\uff08\u521b\u5efa\u4fee\u6539\u65f6\u95f4\u6233\uff09\u3001\u7ed3\u6784\u7279\u5f81\uff08\u5bf9\u8c61\u6d41\u3001\u5b57\u4f53\u3001\u5d4c\u5165\u56fe\u50cf\uff09\u548c\u6076\u610f\u6784\u9020\u6807\u5fd7\uff08JavaScript\u3001\u542f\u52a8\u52a8\u4f5c\u7b49\uff09\uff0c\u751f\u6210170\u7ef4\u7279\u5f81\u5411\u91cf\u3002", "result": "\u6784\u5efa\u4e86\u9ad8\u7ef4\u7279\u5f81\u8868\u793a\uff08170\u7ef4\uff09\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u5982\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u53d6\u8bc1\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u5b9e\u9645PDF\u5a01\u80c1\u60c5\u62a5\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u79cd\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u4e3aPDF\u6076\u610f\u6587\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5b9e\u9645\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2601.12131", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.12131", "abs": "https://arxiv.org/abs/2601.12131", "authors": ["Santosh Chapagain", "MohammadReza EskandariNasab", "Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics", "comment": "This is preliminary work towards a broader SolarGPT framework", "summary": "Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.\n  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.", "AI": {"tldr": "SolarGPT-QA\u662f\u57fa\u4e8eLLaMA-3\u7684\u9886\u57df\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u7a7a\u95f4\u5929\u6c14\u548c\u592a\u9633\u7269\u7406\u5b66\u6559\u80b2\u95ee\u7b54\uff0c\u901a\u8fc7\u79d1\u5b66\u6587\u732e\u548cGPT-4\u751f\u6210\u7684\u5927\u89c4\u6a21\u95ee\u7b54\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5728\u6559\u80b2\u89e3\u91ca\u65b9\u9762\u8fbe\u5230\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u592a\u9633\u6d3b\u52a8\uff08\u592a\u9633\u8000\u6591\u3001\u65e5\u5195\u7269\u8d28\u629b\u5c04\u3001\u5730\u78c1\u66b4\uff09\u5bf9\u536b\u661f\u3001\u822a\u7a7a\u3001\u7535\u7f51\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u6781\u7aef\u592a\u9633\u4e8b\u4ef6\u53ef\u80fd\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u6559\u5b66\u80fd\u529b\u6765\u6e05\u6670\u89e3\u91ca\u590d\u6742\u7684\u7a7a\u95f4\u79d1\u5b66\u6982\u5ff5\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8eLLaMA-3\u57fa\u7840\u6a21\u578b\u6784\u5efaSolarGPT-QA\u95ee\u7b54\u7cfb\u7edf\uff0c\u91c7\u7528\u9886\u57df\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6559\u5b66\u5fae\u8c03\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u8bad\u7ec3\u6570\u636e\u5305\u62ec\u79d1\u5b66\u6587\u732e\u548c\u7531GPT-4\u751f\u6210\u3001Grok-3\u7cbe\u70bc\u7684\u5927\u89c4\u6a21\u95ee\u7b54\u6570\u636e\uff0c\u91c7\u7528\u5b66\u751f\u53cb\u597d\u7684\u6545\u4e8b\u53d9\u8ff0\u98ce\u683c\u3002\u901a\u8fc7\u4eba\u7c7b\u6210\u5bf9\u8bc4\u4f30\u3001\u5b66\u751f\u7406\u89e3\u7814\u7a76\u548c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u4eba\u7c7b\u6210\u5bf9\u8bc4\u4f30\u663e\u793aSolarGPT-QA\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5728\u6559\u80b2\u89e3\u91ca\u65b9\u9762\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002\u5c0f\u578b\u8bd5\u70b9\u5b66\u751f\u7406\u89e3\u7814\u7a76\u8868\u660e\u751f\u6210\u7684\u89e3\u91ca\u5728\u6e05\u6670\u5ea6\u548c\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u6709\u6240\u6539\u5584\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u7ed3\u5408\u9886\u57df\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6559\u5b66\u5fae\u8c03\u5bf9\u4e8e\u5e73\u8861\u79d1\u5b66\u51c6\u786e\u6027\u548c\u6559\u80b2\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "SolarGPT-QA\u5728\u7a7a\u95f4\u5929\u6c14\u548c\u592a\u9633\u7269\u7406\u5b66\u6559\u80b2\u95ee\u7b54\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u548c\u6559\u5b66\u5fae\u8c03\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u79d1\u5b66\u51c6\u786e\u6027\u548c\u6559\u80b2\u6548\u679c\u3002\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5411\u66f4\u5e7f\u6cdb\u7684SolarGPT\u6846\u67b6\uff08\u7528\u4e8e\u7a7a\u95f4\u79d1\u5b66\u6559\u80b2\u548c\u9884\u62a5\uff09\u8fc8\u51fa\u7684\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2601.12137", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12137", "abs": "https://arxiv.org/abs/2601.12137", "authors": ["Anzhe Cheng", "Shukai Duan", "Shixuan Li", "Chenzhong Yin", "Mingxi Cheng", "Shahin Nazarian", "Paul Thompson", "Paul Bogdan"], "title": "EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts", "comment": "accepted by ICASSP2026", "summary": "The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer\" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.", "code_url": "https://github.com/Belis0811/EMoE", "code_stars": 2, "code_last_update": "2025-08-16", "AI": {"tldr": "\u63d0\u51faEMoE\u67b6\u6784\uff0c\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\uff0c\u540c\u65f6\u89e3\u51b3MoE\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u4e13\u5bb6\u540c\u8d28\u5316\u95ee\u9898", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u8ba1\u7b97\u9700\u6c42\u4e0d\u53ef\u6301\u7eed\uff0cMoE\u67b6\u6784\u6210\u4e3a\u63d0\u9ad8\u6548\u7387\u7684\u53ef\u884c\u8def\u5f84\u3002\u4f46\u73b0\u6709MoE\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u95ee\u9898\uff1a1) \"\u5bcc\u8005\u6108\u5bcc\"\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff1b2) \u4e13\u5bb6\u540c\u8d28\u5316\u95ee\u9898\uff0c\u4e13\u5bb6\u5b66\u4e60\u5197\u4f59\u8868\u793a\u3002\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f7f\u7528\u8f85\u52a9\u8d1f\u8f7d\u5747\u8861\u635f\u5931\uff0c\u867d\u7136\u7f13\u89e3\u4e86\u4e0d\u5747\u8861\uff0c\u4f46\u5f80\u5f80\u4ee5\u727a\u7272\u4e13\u4e1a\u5316\u4e3a\u4ee3\u4ef7\u5f3a\u5236\u5747\u5300\u8def\u7531\uff0c\u52a0\u5267\u4e86\u540c\u8d28\u5316\u3002", "method": "\u63d0\u51faEigen-Mixture-of-Experts (EMoE)\u67b6\u6784\uff0c\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u5c06\u8f93\u5165token\u6295\u5f71\u5230\u5171\u4eab\u7684\u7279\u5f81\u57fa\u4e0a\uff0c\u6839\u636etoken\u4e0e\u7279\u5f81\u7a7a\u95f4\u4e3b\u6210\u5206\u7684\u5bf9\u9f50\u7a0b\u5ea6\u8fdb\u884c\u8def\u7531\u3002\u8fd9\u79cd\u57fa\u4e8e\u51e0\u4f55\u539f\u7406\u7684\u6570\u636e\u5212\u5206\u65b9\u6cd5\u672c\u8d28\u4e0a\u4fc3\u8fdb\u4e86\u4e13\u5bb6\u8d1f\u8f7d\u5747\u8861\u548c\u591a\u6837\u5316\u3001\u4e13\u4e1a\u5316\u4e13\u5bb6\u7684\u5f62\u6210\uff0c\u65e0\u9700\u4f7f\u7528\u51b2\u7a81\u7684\u8f85\u52a9\u635f\u5931\u51fd\u6570\u3002", "result": "\u8bba\u6587\u4ee3\u7801\u5df2\u516c\u5f00\u5728https://github.com/Belis0811/EMoE\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u4e13\u5bb6\u540c\u8d28\u5316\u95ee\u9898\uff0c\u65e0\u9700\u4f7f\u7528\u8f85\u52a9\u635f\u5931\u51fd\u6570\u3002", "conclusion": "EMoE\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u4e13\u5bb6\u8d1f\u8f7d\u5747\u8861\u548c\u4e13\u4e1a\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u67b6\u6784\u4e2d\u7684\u4e24\u4e2a\u6839\u672c\u6311\u6218\u3002"}}
{"id": "2601.12916", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12916", "abs": "https://arxiv.org/abs/2601.12916", "authors": ["Sangjun An", "Seoksu Lee", "Eun-Sun Cho"], "title": "Static Detection of Core Structures in Tigress Virtualization-Based Obfuscation Using an LLVM Pass", "comment": "7 pages, 7figures, An extended version of this work has been submitted to the Journal of KIISC", "summary": "Malware often uses obfuscation to hinder security analysis. Among these techniques, virtualization-based obfuscation is particularly strong because it protects programs by translating original instructions into attacker-defined virtual machine (VM) bytecode, producing long and complex code that is difficult to analyze and deobfuscate. This paper aims to identify the structural components of virtualization-based obfuscation through static analysis. By examining the execution model of obfuscated code, we define and detect the key elements required for deobfuscation-namely the dispatch routine, handler blocks, and the VM region-using LLVM IR. Experimental results show that, in the absence of compiler optimizations, the proposed LLVM Pass successfully detects all core structures across major virtualization options, including switch, direct, and indirect modes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLVM IR\u7684\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u865a\u62df\u5316\u6df7\u6dc6\u4e2d\u7684\u6838\u5fc3\u7ed3\u6784\u7ec4\u4ef6\uff08\u8c03\u5ea6\u4f8b\u7a0b\u3001\u5904\u7406\u7a0b\u5e8f\u5757\u3001VM\u533a\u57df\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u65e0\u7f16\u8bd1\u5668\u4f18\u5316\u65f6\u80fd\u6709\u6548\u8bc6\u522b\u6240\u6709\u4e3b\u8981\u865a\u62df\u5316\u6a21\u5f0f\u3002", "motivation": "\u865a\u62df\u5316\u6df7\u6dc6\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u6076\u610f\u8f6f\u4ef6\u6df7\u6dc6\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u539f\u59cb\u6307\u4ee4\u8f6c\u6362\u4e3a\u653b\u51fb\u8005\u5b9a\u4e49\u7684\u865a\u62df\u673a\u5b57\u8282\u7801\uff0c\u4ea7\u751f\u590d\u6742\u96be\u5206\u6790\u7684\u4ee3\u7801\uff0c\u963b\u788d\u5b89\u5168\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u5176\u6838\u5fc3\u7ed3\u6784\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u901a\u8fc7\u9759\u6001\u5206\u6790\u865a\u62df\u5316\u6df7\u6dc6\u4ee3\u7801\u7684\u6267\u884c\u6a21\u578b\uff0c\u5728LLVM IR\u5c42\u9762\u5b9a\u4e49\u5e76\u68c0\u6d4b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8c03\u5ea6\u4f8b\u7a0b\uff08dispatch routine\uff09\u3001\u5904\u7406\u7a0b\u5e8f\u5757\uff08handler blocks\uff09\u548cVM\u533a\u57df\uff08VM region\uff09\u3002\u5f00\u53d1\u4e86\u4e13\u95e8\u7684LLVM Pass\u6765\u5b9e\u73b0\u8fd9\u4e9b\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u65e0\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u7684LLVM Pass\u80fd\u591f\u6210\u529f\u68c0\u6d4b\u6240\u6709\u4e3b\u8981\u865a\u62df\u5316\u6a21\u5f0f\uff08switch\u6a21\u5f0f\u3001direct\u6a21\u5f0f\u548cindirect\u6a21\u5f0f\uff09\u4e2d\u7684\u6838\u5fc3\u7ed3\u6784\u7ec4\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u865a\u62df\u5316\u6df7\u6dc6\u7684\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u6df7\u6dc6\u4ee3\u7801\u7684\u5173\u952e\u7ed3\u6784\u7ec4\u4ef6\uff0c\u4e3a\u540e\u7eed\u7684\u53cd\u6df7\u6dc6\u548c\u5b89\u5168\u5206\u6790\u5960\u5b9a\u57fa\u7840\u3002\u5728\u65e0\u7f16\u8bd1\u5668\u4f18\u5316\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u6765\u53ef\u80fd\u9700\u8981\u8003\u8651\u4f18\u5316\u4ee3\u7801\u7684\u68c0\u6d4b\u6311\u6218\u3002"}}
{"id": "2601.12145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12145", "abs": "https://arxiv.org/abs/2601.12145", "authors": ["Xingyue Huang", "Xueying Ding", "Mingxuan Ju", "Yozen Liu", "Neil Shah", "Tong Zhao"], "title": "Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling", "comment": null, "summary": "Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.", "AI": {"tldr": "TDA\u662f\u4e00\u79cd\u65e0\u6ce8\u610f\u529b\u4e0b\u6c89\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u6781\u503c\u9608\u503c\u5316\u548c\u6291\u5236\u89c6\u56fe\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3Softmax\u6ce8\u610f\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u7ed3\u6784\u9650\u5236\uff1a\u4e25\u683c\u7684\u548c\u4e3a\u4e00\u7ea6\u675f\u5bfc\u81f4\u65e0\u5173\u6807\u8bb0\u4e0a\u7684\u6ce8\u610f\u529b\u4e0b\u6c89\uff0c\u4ee5\u53ca\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u6982\u7387\u8d28\u91cf\u5206\u6563\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u9608\u503c\u5dee\u5206\u6ce8\u610f\u529b\uff08TDA\uff09\uff0c\u91c7\u7528\u884c\u7ea7\u6781\u503c\u9608\u503c\u5316\u914d\u5408\u957f\u5ea6\u76f8\u5173\u95e8\u63a7\uff0c\u4ec5\u4fdd\u7559\u8d85\u8fc7\u9608\u503c\u7684\u503c\uff1b\u540c\u65f6\u501f\u9274\u5dee\u5206\u53d8\u6362\u5668\u601d\u60f3\uff0c\u51cf\u53bb\u6291\u5236\u89c6\u56fe\u4ee5\u589e\u5f3a\u8868\u8fbe\u80fd\u529b", "result": "\u7406\u8bba\u4e0a\u8bc1\u660eTDA\u6bcf\u884c\u4f2a\u5e78\u5b58\u8005\u671f\u671b\u6570\u4e3aO(1)\uff0c\u4e14\u72ec\u7acb\u89c6\u56fe\u95f4\u7684\u5171\u8bc6\u4f2a\u5339\u914d\u968f\u4e0a\u4e0b\u6587\u589e\u957f\u800c\u6d88\u5931\uff1b\u5b9e\u8bc1\u4e0a\u4ea7\u751f>99%\u7684\u7cbe\u786e\u96f6\u503c\uff0c\u6d88\u9664\u6ce8\u610f\u529b\u4e0b\u6c89\uff0c\u5728\u6807\u51c6\u548c\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "TDA\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6ce8\u610f\u529b\u4e0b\u6c89\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86Softmax\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u7ed3\u6784\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u8d85\u7a00\u758f\u6027\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027"}}
{"id": "2601.12922", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12922", "abs": "https://arxiv.org/abs/2601.12922", "authors": ["Johannes Kaiser", "Alexander Ziller", "Eleni Triantafillou", "Daniel R\u00fcckert", "Georgios Kaissis"], "title": "Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy", "comment": null, "summary": "Individual Differential Privacy (iDP) promises users control over their privacy, but this promise can be broken in practice. We reveal a previously overlooked vulnerability in sampling-based iDP mechanisms: while conforming to the iDP guarantees, an individual's privacy risk is not solely governed by their own privacy budget, but critically depends on the privacy choices of all other data contributors. This creates a mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined. We demonstrate empirically that certain distributions of privacy preferences can unintentionally inflate the privacy risk of individuals, even when their formal guarantees are met. Moreover, this excess risk provides an exploitable attack vector. A central adversary or a set of colluding adversaries can deliberately choose privacy budgets to amplify vulnerabilities of targeted individuals. Most importantly, this attack operates entirely within the guarantees of DP, hiding this excess vulnerability. Our empirical evaluation demonstrates successful attacks against 62% of targeted individuals, substantially increasing their membership inference susceptibility. To mitigate this, we propose $(\\varepsilon_i,\u03b4_i,\\overline\u0394)$-iDP a privacy contract that uses $\u0394$-divergences to provide users with a hard upper bound on their excess vulnerability, while offering flexibility to mechanism design. Our findings expose a fundamental challenge to the current paradigm, demanding a re-evaluation of how iDP systems are designed, audited, communicated, and deployed to make excess risks transparent and controllable.", "AI": {"tldr": "iDP\u673a\u5236\u5b58\u5728\u96c6\u4f53\u98ce\u9669\u6f0f\u6d1e\uff1a\u4e2a\u4f53\u7684\u9690\u79c1\u98ce\u9669\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u81ea\u8eab\u9690\u79c1\u9884\u7b97\uff0c\u8fd8\u53d7\u5176\u4ed6\u7528\u6237\u9690\u79c1\u9009\u62e9\u5f71\u54cd\uff0c\u5bfc\u81f4\u98ce\u9669\u88ab\u4f4e\u4f30\u4e14\u53ef\u88ab\u653b\u51fb\u8005\u5229\u7528\uff0c\u9700\u91cd\u65b0\u8bbe\u8ba1iDP\u7cfb\u7edf", "motivation": "\u63ed\u793a\u57fa\u4e8e\u91c7\u6837\u7684\u4e2a\u4f53\u5dee\u5206\u9690\u79c1(iDP)\u673a\u5236\u4e2d\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u6f0f\u6d1e\uff1a\u867d\u7136\u7b26\u5408iDP\u4fdd\u8bc1\uff0c\u4f46\u4e2a\u4f53\u7684\u9690\u79c1\u98ce\u9669\u4e0d\u4ec5\u7531\u5176\u81ea\u8eab\u9690\u79c1\u9884\u7b97\u51b3\u5b9a\uff0c\u8fd8\u4e25\u91cd\u4f9d\u8d56\u6240\u6709\u5176\u4ed6\u6570\u636e\u8d21\u732e\u8005\u7684\u9690\u79c1\u9009\u62e9\uff0c\u5bfc\u81f4\u4e2a\u4f53\u9690\u79c1\u63a7\u5236\u7684\u627f\u8bfa\u4e0e\u73b0\u5b9e\u98ce\u9669\u96c6\u4f53\u51b3\u5b9a\u7684\u7cfb\u7edf\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d", "method": "1. \u7406\u8bba\u5206\u6790\u91c7\u6837iDP\u673a\u5236\u7684\u96c6\u4f53\u98ce\u9669\u7279\u6027\uff1b2. \u5b9e\u8bc1\u6f14\u793a\u7279\u5b9a\u9690\u79c1\u504f\u597d\u5206\u5e03\u4f1a\u65e0\u610f\u4e2d\u589e\u52a0\u4e2a\u4f53\u9690\u79c1\u98ce\u9669\uff1b3. \u5c55\u793a\u653b\u51fb\u8005\u5982\u4f55\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u9690\u79c1\u9884\u7b97\u6765\u653e\u5927\u76ee\u6807\u4e2a\u4f53\u7684\u6f0f\u6d1e\uff1b4. \u63d0\u51fa$(\\varepsilon_i,\u03b4_i,\\overline\u0394)$-iDP\u9690\u79c1\u5408\u7ea6\uff0c\u4f7f\u7528$\u0394$-\u6563\u5ea6\u4e3a\u7528\u6237\u63d0\u4f9b\u8d85\u989d\u6f0f\u6d1e\u7684\u786c\u4e0a\u9650", "result": "1. \u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u5bf962%\u7684\u76ee\u6807\u4e2a\u4f53\u653b\u51fb\u6210\u529f\uff0c\u663e\u8457\u589e\u52a0\u4e86\u5176\u6210\u5458\u63a8\u7406\u6613\u611f\u6027\uff1b2. \u653b\u51fb\u5b8c\u5168\u5728DP\u4fdd\u8bc1\u8303\u56f4\u5185\u64cd\u4f5c\uff0c\u9690\u85cf\u4e86\u8d85\u989d\u6f0f\u6d1e\uff1b3. \u63d0\u51fa\u7684$(\\varepsilon_i,\u03b4_i,\\overline\u0394)$-iDP\u5408\u7ea6\u80fd\u63d0\u4f9b\u7528\u6237\u8d85\u989d\u6f0f\u6d1e\u7684\u786c\u4e0a\u9650\uff0c\u540c\u65f6\u4e3a\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u7075\u6d3b\u6027", "conclusion": "\u5f53\u524diDP\u8303\u5f0f\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30iDP\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5ba1\u8ba1\u3001\u6c9f\u901a\u548c\u90e8\u7f72\u65b9\u5f0f\uff0c\u4f7f\u8d85\u989d\u98ce\u9669\u900f\u660e\u4e14\u53ef\u63a7\uff0c\u63d0\u51fa\u7684\u65b0\u9690\u79c1\u5408\u7ea6\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b9\u5411"}}
{"id": "2601.12178", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12178", "abs": "https://arxiv.org/abs/2601.12178", "authors": ["Fallou Niakh"], "title": "Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses", "comment": null, "summary": "We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5f02\u6784\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u635f\u5931\u4e0b\u6821\u51c6\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u5b66\u4e60\u5171\u540c\u6307\u6570\u800c\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u635f\u5931\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\u6821\u51c6\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5171\u4eab\u654f\u611f\u7684\u751f\u4ea7\u6570\u636e\uff0c\u800c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5904\u7406\u4e0d\u540c\u751f\u4ea7\u8005\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u3002", "method": "\u751f\u4ea7\u8005\u4f7f\u7528Tweedie\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u5728\u672c\u5730\u5efa\u6a21\u635f\u5931\uff0c\u57fa\u4e8e\u79c1\u6709\u6570\u636e\u3002\u901a\u8fc7\u8054\u90a6\u4f18\u5316\u5b66\u4e60\u5171\u540c\u6307\u6570\uff0c\u4e0d\u5171\u4eab\u539f\u59cb\u89c2\u6d4b\u6570\u636e\u3002\u6846\u67b6\u652f\u6301\u65b9\u5dee\u548c\u94fe\u63a5\u51fd\u6570\u7684\u5f02\u8d28\u6027\uff0c\u76f4\u63a5\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u6700\u5c0f\u5316\u5168\u5c40\u504f\u5dee\u76ee\u6807\u3002\u5b9e\u73b0\u5e76\u6bd4\u8f83\u4e86FedAvg\u3001FedProx\u548cFedOpt\u7b97\u6cd5\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u8fd1\u4f3c\u7684\u805a\u5408\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u5fb7\u56fd\u592a\u9633\u80fd\u53d1\u7535\u751f\u4ea7\u7684\u5b9e\u8bc1\u5e94\u7528\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\u5728\u4e2d\u7b49\u5f02\u8d28\u6027\u6761\u4ef6\u4e0b\u6062\u590d\u4e86\u53ef\u6bd4\u8f83\u7684\u6307\u6570\u7cfb\u6570\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u901a\u7528\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e3a\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2601.12937", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12937", "abs": "https://arxiv.org/abs/2601.12937", "authors": ["Murat Bilgehan Ertan", "Emirhan B\u00f6ge", "Min Chen", "Kaleel Mahmood", "Marten van Dijk"], "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing", "comment": null, "summary": "As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5bf9\u6297\u6027\u7248\u6743\u7ea0\u7eb7\u4e2d\uff0c\u5f53\u6a21\u578b\u5f00\u53d1\u8005\u53ef\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u4fdd\u7559\u7684\u6539\u5199\u65f6\uff0c\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u4f5c\u4e3a\u8bc1\u636e\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86SAGE\u6846\u67b6\u6765\u6d4b\u8bd5MIAs\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709MIAs\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\u662f\u8106\u5f31\u7684\uff0c\u4e0d\u8db3\u4ee5\u4f5c\u4e3aLLM\u7248\u6743\u5ba1\u8ba1\u7684\u72ec\u7acb\u673a\u5236\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e5\u76ca\u4e0d\u900f\u660e\u7684\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\uff0c\u6210\u5458\u63a8\u7406\u653b\u51fb\u88ab\u63d0\u51fa\u7528\u4e8e\u5ba1\u8ba1\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6587\u672c\u662f\u5426\u88ab\u7528\u4e8e\u8bad\u7ec3\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u53d7\u5230\u8d28\u7591\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7a76\u5728\u5bf9\u6297\u6027\u7248\u6743\u7ea0\u7eb7\u4e2d\uff0c\u5f53\u88ab\u6307\u63a7\u7684\u6a21\u578b\u5f00\u53d1\u8005\u53ef\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u4fdd\u7559\u7684\u6539\u5199\u65f6\uff0cMIAs\u80fd\u5426\u4f5c\u4e3a\u53ef\u91c7\u7eb3\u7684\u8bc1\u636e\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6cd5\u5b98-\u68c0\u5bdf\u5b98-\u88ab\u6307\u63a7\u65b9\u901a\u4fe1\u534f\u8bae\u5f62\u5f0f\u5316\u5bf9\u6297\u6027\u7248\u6743\u7ea0\u7eb7\u573a\u666f\u3002\u4e3a\u6d4b\u8bd5\u8be5\u534f\u8bae\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86SAGE\uff08\u7ed3\u6784\u611f\u77e5\u7684SAE\u5f15\u5bfc\u63d0\u53d6\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7684\u6539\u5199\u6846\u67b6\uff0c\u80fd\u591f\u6539\u53d8\u8bad\u7ec3\u6570\u636e\u7684\u8bcd\u6c47\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u5185\u5bb9\u548c\u4e0b\u6e38\u5b9e\u7528\u6027\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684MIAs\u5728\u6a21\u578b\u4f7f\u7528SAGE\u751f\u6210\u7684\u6539\u5199\u6587\u672c\u8fdb\u884c\u5fae\u8c03\u65f6\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u6a21\u578b\u5728SAGE\u751f\u6210\u7684\u6539\u5199\u6587\u672c\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u6700\u5148\u8fdb\u7684MIAs\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u8fd9\u4e9b\u653b\u51fb\u7684\u4fe1\u53f7\u5bf9\u8bed\u4e49\u4fdd\u7559\u7684\u8f6c\u6362\u4e0d\u5177\u6709\u9c81\u68d2\u6027\u3002\u5c3d\u7ba1\u5728\u67d0\u4e9b\u5fae\u8c03\u673a\u5236\u4e0b\u4ecd\u5b58\u5728\u4e00\u4e9b\u4fe1\u606f\u6cc4\u9732\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eMIAs\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\u662f\u8106\u5f31\u7684\u3002", "conclusion": "\u6210\u5458\u63a8\u7406\u653b\u51fb\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\u662f\u8106\u5f31\u7684\uff0c\u4e0d\u8db3\u4ee5\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7248\u6743\u5ba1\u8ba1\u7684\u72ec\u7acb\u673a\u5236\u3002MIAs\u4e0d\u80fd\u5355\u72ec\u4f5c\u4e3a\u7248\u6743\u7ea0\u7eb7\u4e2d\u7684\u53ef\u9760\u8bc1\u636e\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u5f00\u53d1\u8005\u53ef\u80fd\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u4fdd\u7559\u7684\u6539\u5199\u65f6\u3002"}}
{"id": "2601.12212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12212", "abs": "https://arxiv.org/abs/2601.12212", "authors": ["Chenan Wang", "Daniel H. Shi", "Haipeng Chen"], "title": "Speculative Sampling with Reinforcement Learning", "comment": "Accepted to AAAI 2026", "summary": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.", "AI": {"tldr": "Re-SpS\uff1a\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u6d4b\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u8349\u7a3f\u6811\u8d85\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\uff08\u5982EAGLE-3\uff09\u4f7f\u7528\u9759\u6001\u6811\u7ed3\u6784\u8d85\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u548c\u9886\u57df\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002\u9700\u8981\u52a8\u6001\u8c03\u6574\u673a\u5236\u6765\u5e73\u8861\u63a8\u6d4b\u653b\u51fb\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u6700\u5927\u5316\u751f\u6210\u901f\u5ea6", "method": "\u63d0\u51faRe-SpS\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4ece\u76ee\u6807\u6a21\u578b\u9690\u85cf\u72b6\u6001\u63d0\u53d6\u9ad8\u6548\u72b6\u6001\u8868\u793a\uff1b2\uff09\u5f15\u5165\u591a\u6b65\u52a8\u4f5c\u6301\u4e45\u6027\u4ee5\u66f4\u597d\u5efa\u6a21\u4e0a\u4e0b\u6587\uff1b3\uff09\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\u8349\u7a3f\u6811\u8d85\u53c2\u6570\uff0c\u5b66\u4e60\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a1\uff09\u76f8\u6bd4\u9aa8\u5e72LLM\u5b9e\u73b0\u6700\u9ad85.45\u500d\u52a0\u901f\uff1b2\uff09\u76f8\u6bd4SOTA\u65b9\u6cd5EAGLE-3\u5b9e\u73b0\u6700\u9ad81.12\u500d\u52a0\u901f\uff1b3\uff09\u65e0\u8f93\u51fa\u4fdd\u771f\u5ea6\u635f\u5931", "conclusion": "Re-SpS\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u8349\u7a3f\u6811\u8d85\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u63a8\u6d4b\u91c7\u6837\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12213", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12213", "abs": "https://arxiv.org/abs/2601.12213", "authors": ["Hongyang R. Zhang", "Zhenshuo Zhang", "Huy L. Nguyen", "Guanghui Lan"], "title": "One-Sided Matrix Completion from Ultra-Sparse Samples", "comment": "41 pages", "summary": "Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\\times d$ matrix $M$ (with $n \\ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \\ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\\top} M / n$.\n  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \\ge O({d r^5 \u03b5^{-2} C^{-2} \\log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $\u03b5^2$.\n  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\\%$ and $M$ by $38\\%$ compared to baseline methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8d85\u7a00\u758f\u91c7\u6837\u4e0b\u7684\u77e9\u9635\u8865\u5168\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f30\u8ba1\u77e9\u9635\u884c\u7a7a\u95f4\u6216\u4e8c\u9636\u77e9\u77e9\u9635\u7684\u65e0\u504f\u4f30\u8ba1\u5668\uff0c\u5728\u6bcf\u884c\u4ec5\u89c2\u6d4bC\u4e2a\u6761\u76ee\uff08C\u22652\uff09\u7684\u6781\u7aef\u7a00\u758f\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u4f20\u7edf\u77e9\u9635\u8865\u5168\u65b9\u6cd5\u5728\u8d85\u7a00\u758f\u91c7\u6837\uff08\u6bcf\u884c\u6761\u76ee\u6570\u5c11\u4e8e\u77e9\u9635\u79e9\uff09\u65f6\u5931\u6548\uff0c\u65e0\u6cd5\u51c6\u786e\u8865\u5168\u77e9\u9635\u3002\u672c\u6587\u9488\u5bf9\u5927\u578b\u7a00\u758f\u9762\u677f\u6570\u636e\uff08\u884c\u6570\u8fdc\u5927\u4e8e\u5217\u6570\uff09\u7684\u5e94\u7528\u573a\u666f\uff0c\u65e8\u5728\u4f30\u8ba1\u77e9\u9635\u7684\u884c\u7a7a\u95f4\u6216\u4e8c\u9636\u77e9\u77e9\u9635\u800c\u975e\u5b8c\u6574\u77e9\u9635\u672c\u8eab\u3002", "method": "\u63d0\u51fa\u65e0\u504f\u4f30\u8ba1\u5668\uff1a\u9996\u5148\u901a\u8fc7\u89c2\u6d4b\u9891\u7387\u5f52\u4e00\u5316\u4e8c\u9636\u77e9\u77e9\u9635\u7684\u975e\u96f6\u6761\u76ee\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8865\u5168T\u7684\u7f3a\u5931\u6761\u76ee\u3002\u5f52\u4e00\u5316\u5c06n\u4e2a\u4e8c\u9879\u968f\u673a\u53d8\u91cf\u7684\u52a0\u6743\u548c\u9664\u4ee5\u603b\u89c2\u6d4b\u6570\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u91c7\u6837\u6982\u7387p\uff0c\u4e14\u5177\u6709\u4f4e\u65b9\u5dee\u7279\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5f53M\u7684\u884c\u5411\u91cf\u6765\u81ea\u6ee1\u8db3\u4e0d\u76f8\u5e72\u6761\u4ef6\u7684\u79e9r\u56e0\u5b50\u6a21\u578b\u65f6\uff0c\u82e5n \u2265 O(dr\u2075\u03b5\u207b\u00b2C\u207b\u00b2log d)\uff0c\u68af\u5ea6\u4e0b\u964d\u76ee\u6807\u7684\u4efb\u4f55\u5c40\u90e8\u6700\u5c0f\u503c\u90fd\u8fd1\u4f3c\u5168\u5c40\u6700\u4f18\uff0c\u80fd\u4ee5\u6700\u591a\u03b5\u00b2\u7684\u8bef\u5dee\u6062\u590dT\u3002\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u504f\u5dee\u51cf\u5c1188%\uff1b\u5728\u7a00\u758f\u5ea6\u4e3a10\u207b\u2077\u7684Amazon\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\uff0cT\u7684\u6062\u590d\u8bef\u5dee\u51cf\u5c1159%\uff0cM\u7684\u8bef\u5dee\u51cf\u5c1138%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8d85\u7a00\u758f\u91c7\u6837\u4e0b\u80fd\u6709\u6548\u4f30\u8ba1\u77e9\u9635\u7684\u884c\u7a7a\u95f4\u548c\u4e8c\u9636\u77e9\u77e9\u9635\uff0c\u4e3a\u5927\u89c4\u6a21\u7a00\u758f\u9762\u677f\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.12986", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12986", "abs": "https://arxiv.org/abs/2601.12986", "authors": ["Zhenhua Xu", "Xiaoning Tian", "Wenjun Zeng", "Wenpeng Xing", "Tianliang Lu", "Gaolei Li", "Chaochao Chen", "Meng Han"], "title": "KinGuard: Hierarchical Kinship-Aware Fingerprinting to Defend Against Large Language Model Stealing", "comment": "Accepted By ICASSP2026", "summary": "Protecting the intellectual property of large language models requires robust ownership verification. Conventional backdoor fingerprinting, however, is flawed by a stealth-robustness paradox: to be robust, these methods force models to memorize fixed responses to high-perplexity triggers, but this targeted overfitting creates detectable statistical artifacts. We resolve this paradox with KinGuard, a framework that embeds a private knowledge corpus built on structured kinship narratives. Instead of memorizing superficial triggers, the model internalizes this knowledge via incremental pre-training, and ownership is verified by probing its conceptual understanding. Extensive experiments demonstrate KinGuard's superior effectiveness, stealth, and resilience against a battery of attacks including fine-tuning, input perturbation, and model merging. Our work establishes knowledge-based embedding as a practical and secure paradigm for model fingerprinting.", "AI": {"tldr": "KinGuard\u901a\u8fc7\u5d4c\u5165\u57fa\u4e8e\u4eb2\u5c5e\u5173\u7cfb\u53d9\u4e8b\u7684\u79c1\u6709\u77e5\u8bc6\u8bed\u6599\u5e93\u6765\u89e3\u51b3\u540e\u95e8\u6307\u7eb9\u7684\u9690\u5bc6-\u9c81\u68d2\u6027\u6096\u8bba\uff0c\u901a\u8fc7\u589e\u91cf\u9884\u8bad\u7ec3\u8ba9\u6a21\u578b\u5185\u5316\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u63a2\u6d4b\u6982\u5ff5\u7406\u89e3\u6765\u9a8c\u8bc1\u6240\u6709\u6743", "motivation": "\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\u9700\u8981\u53ef\u9760\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\u3002\u4f20\u7edf\u7684\u540e\u95e8\u6307\u7eb9\u65b9\u6cd5\u5b58\u5728\u9690\u5bc6-\u9c81\u68d2\u6027\u6096\u8bba\uff1a\u4e3a\u4e86\u9c81\u68d2\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8feb\u4f7f\u6a21\u578b\u8bb0\u5fc6\u5bf9\u9ad8\u56f0\u60d1\u5ea6\u89e6\u53d1\u8bcd\u7684\u56fa\u5b9a\u54cd\u5e94\uff0c\u4f46\u8fd9\u79cd\u9488\u5bf9\u6027\u8fc7\u62df\u5408\u4f1a\u4ea7\u751f\u53ef\u68c0\u6d4b\u7684\u7edf\u8ba1\u4f2a\u5f71", "method": "\u63d0\u51faKinGuard\u6846\u67b6\uff0c\u5d4c\u5165\u57fa\u4e8e\u7ed3\u6784\u5316\u4eb2\u5c5e\u5173\u7cfb\u53d9\u4e8b\u7684\u79c1\u6709\u77e5\u8bc6\u8bed\u6599\u5e93\u3002\u6a21\u578b\u901a\u8fc7\u589e\u91cf\u9884\u8bad\u7ec3\u5185\u5316\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u8bb0\u5fc6\u8868\u9762\u89e6\u53d1\u8bcd\u3002\u6240\u6709\u6743\u9a8c\u8bc1\u901a\u8fc7\u63a2\u6d4b\u6a21\u578b\u5bf9\u5d4c\u5165\u6982\u5ff5\u7684\u7406\u89e3\u6765\u5b9e\u73b0", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eKinGuard\u5728\u6709\u6548\u6027\u3001\u9690\u5bc6\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u62b5\u6297\u5305\u62ec\u5fae\u8c03\u3001\u8f93\u5165\u6270\u52a8\u548c\u6a21\u578b\u5408\u5e76\u5728\u5185\u7684\u591a\u79cd\u653b\u51fb", "conclusion": "KinGuard\u5efa\u7acb\u4e86\u57fa\u4e8e\u77e5\u8bc6\u5d4c\u5165\u7684\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u6a21\u578b\u6307\u7eb9\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u540e\u95e8\u6307\u7eb9\u65b9\u6cd5\u7684\u6839\u672c\u7f3a\u9677"}}
{"id": "2601.12215", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12215", "abs": "https://arxiv.org/abs/2601.12215", "authors": ["Megha Thukral", "Cyrus Tanade", "Simon A. Lee", "Juhyeon Lee", "Hao Zhou", "Keum San Chun", "Migyeong Gwak", "Viswam Nathan", "Md Mahbubur Rahman", "Li Zhu", "Mehrab Bin Morshed", "Subramaniam Venkatraman", "Sharanya Arcot Desai"], "title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models", "comment": null, "summary": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Masked Multiscale Reconstruction (MMR)\u65b9\u6cd5\uff0c\u4e00\u79cd\u7528\u4e8ePPG\u4fe1\u53f7\u8868\u793a\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6ce2\u591a\u5206\u8fa8\u7387\u5206\u89e3\u548c\u63a9\u7801\u91cd\u5efa\u4efb\u52a1\uff0c\u572817\u4e2a\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u57fa\u7840\u6a21\u578b\u5927\u591a\u5ffd\u7565\u4e86PPG\u4fe1\u53f7\u7684\u9891\u8c31\u7ed3\u6784\uff0c\u800c\u8bb8\u591a\u4e0b\u6e38\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u4f9d\u8d56\u4e8e\u4ece\u7ec6\u7c92\u5ea6\u6ce2\u5f62\u5f62\u6001\u5230\u5168\u5c40\u8282\u5f8b\u52a8\u6001\u7684\u591a\u5206\u8fa8\u7387\u7279\u5f81\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5b66\u4e60PPG\u6570\u636e\u5c42\u6b21\u5316\u65f6\u9891\u5c3a\u5ea6\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMMR\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5c0f\u6ce2\u57fa\u591a\u5206\u8fa8\u7387\u5206\u89e3\u5bf9PPG\u4fe1\u53f7\u8fdb\u884c\u65f6\u9891\u5206\u6790\uff1b2\uff09\u968f\u673a\u63a9\u7801\u5c0f\u6ce2\u7cfb\u6570\uff1b3\uff09\u8bad\u7ec3Transformer\u7f16\u7801\u5668\u91cd\u5efa\u88ab\u63a9\u7801\u7684\u7cfb\u6570\uff0c\u5f3a\u5236\u6a21\u578b\u6574\u5408\u8de8\u65f6\u95f4\u548c\u9891\u8c31\u5c3a\u5ea6\u7684\u4fe1\u606f\u3002\u4f7f\u7528\u7ea61700\u4e07\u4e2a\u672a\u6807\u8bb0\u768410\u79d2PPG\u7247\u6bb5\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u572819\u4e2a\u591a\u6837\u5316\u7684\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u4e2d\uff0cMMR\u572817\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u6700\u5148\u8fdb\u7684\u5f00\u6e90PPG\u57fa\u7840\u6a21\u578b\u3001\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c\u5176\u4ed6\u81ea\u76d1\u7763\u57fa\u7ebf\u3002\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5177\u6709\u9c81\u68d2\u6027\u548c\u751f\u7406\u57fa\u7840\u7279\u5f81\u3002", "conclusion": "MMR\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528PPG\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5c0f\u6ce2\u57fa\u8868\u793a\u80fd\u591f\u6355\u83b7\u7a33\u5065\u4e14\u5177\u6709\u751f\u7406\u57fa\u7840\u7684\u7279\u5f81\uff0c\u4e3a\u53ef\u7a7f\u6234\u6570\u5b57\u5065\u5eb7\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.13031", "categories": ["cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13031", "abs": "https://arxiv.org/abs/2601.13031", "authors": ["Sebastian Bitzer", "Maximilian Egger", "Mumin Liu", "Antonia Wachter-Zeh"], "title": "Post-Quantum Secure Aggregation via Code-Based Homomorphic Encryption", "comment": null, "summary": "Secure aggregation enables aggregation of inputs from multiple parties without revealing individual contributions to the server or other clients. Existing post-quantum approaches based on homomorphic encryption offer practical efficiency but predominantly rely on lattice-based hardness assumptions. We present a code-based alternative for secure aggregation by instantiating a general framework based on key- and message-additive homomorphic encryption under the Learning Parity with Noise (LPN) assumption. Our construction employs a committee-based decryptor realized via secret sharing and incorporates a Chinese Remainder Theorem (CRT)-based optimization to reduce the communication costs of LPN-based instantiations. We analyze the security of the proposed scheme under a new Hint-LPN assumption and show that it is equivalent to standard LPN for suitable parameters. Finally, we evaluate performance and identify regimes in which our approach outperforms information-theoretically secure aggregation protocols.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLPN\u5047\u8bbe\u7684\u4ee3\u7801\u57fa\u5b89\u5168\u805a\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u5bc6\u94a5\u548c\u6d88\u606f\u52a0\u6cd5\u540c\u6001\u52a0\u5bc6\u6846\u67b6\uff0c\u91c7\u7528\u59d4\u5458\u4f1a\u89e3\u5bc6\u548cCRT\u4f18\u5316\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u4f18\u4e8e\u4fe1\u606f\u8bba\u5b89\u5168\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u540e\u91cf\u5b50\u5b89\u5168\u805a\u5408\u65b9\u6848\u4e3b\u8981\u57fa\u4e8e\u683c\u5bc6\u7801\u5047\u8bbe\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u4ee3\u7801\u5047\u8bbe\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u683c\u5047\u8bbe\u7684\u5b89\u5168\u805a\u5408\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u591a\u6837\u5316\u7684\u540e\u91cf\u5b50\u5b89\u5168\u9009\u62e9\u3002", "method": "\u57fa\u4e8eLPN\u5047\u8bbe\u6784\u5efa\u5bc6\u94a5\u548c\u6d88\u606f\u52a0\u6cd5\u540c\u6001\u52a0\u5bc6\u6846\u67b6\uff0c\u91c7\u7528\u59d4\u5458\u4f1a\u89e3\u5bc6\u673a\u5236\uff08\u901a\u8fc7\u79d8\u5bc6\u5171\u4eab\u5b9e\u73b0\uff09\uff0c\u5e76\u5f15\u5165CRT\u4f18\u5316\u6765\u964d\u4f4eLPN\u5b9e\u4f8b\u5316\u7684\u901a\u4fe1\u6210\u672c\u3002\u5728Hint-LPN\u5047\u8bbe\u4e0b\u5206\u6790\u5b89\u5168\u6027\uff0c\u8bc1\u660e\u5176\u4e0e\u6807\u51c6LPN\u7684\u7b49\u4ef7\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8eLPN\u5047\u8bbe\u7684\u5b89\u5168\u805a\u5408\u65b9\u6848\uff0c\u901a\u8fc7CRT\u4f18\u5316\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002\u5b89\u5168\u6027\u5206\u6790\u8868\u660e\u5728\u9002\u5f53\u53c2\u6570\u4e0bHint-LPN\u4e0e\u6807\u51c6LPN\u7b49\u4ef7\u3002\u6027\u80fd\u8bc4\u4f30\u663e\u793a\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u4fe1\u606f\u8bba\u5b89\u5168\u805a\u5408\u534f\u8bae\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u4e86\u57fa\u4e8e\u4ee3\u7801\u5047\u8bbe\u7684\u5b89\u5168\u805a\u5408\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u540e\u91cf\u5b50\u5b89\u5168\u805a\u5408\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56\u683c\u5047\u8bbe\u7684\u65b0\u9009\u62e9\uff0c\u5728\u7279\u5b9a\u53c2\u6570\u8303\u56f4\u5185\u5177\u6709\u5b9e\u7528\u4f18\u52bf\u3002"}}
{"id": "2601.12231", "categories": ["cs.LG", "cs.CR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.12231", "abs": "https://arxiv.org/abs/2601.12231", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Shijie Xu", "Guanggang Geng"], "title": "Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention", "comment": "Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c0f\u6ce2\u611f\u77e5\u8c03\u5236\u3001\u591a\u5206\u8fa8\u7387\u5c0f\u6ce2\u5206\u89e3\u548c\u5206\u8fa8\u7387\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4f01\u4e1a\u5b89\u5168\u4e2d\u7684\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\uff0c\u5728CERT r4.2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f01\u4e1a\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u9762\u4e34\u591a\u901a\u9053\u3001\u975e\u5e73\u7a33\u7684\u7528\u6237\u6d3b\u52a8\u65e5\u5fd7\u6570\u636e\uff0c\u4e14\u5f02\u5e38\u884c\u4e3a\u7a00\u5c11\uff0c\u4f7f\u5f97\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u504f\u5dee\u611f\u77e5\u8c03\u5236\u65b9\u6848\u6291\u5236\u5e38\u89c4\u884c\u4e3a\u5e76\u653e\u5927\u5f02\u5e38\u504f\u5dee\uff1b\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5c06\u65e5\u5fd7\u4fe1\u53f7\u5206\u89e3\u4e3a\u591a\u5206\u8fa8\u7387\u8868\u793a\uff1b\u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u91cd\u52a0\u6743\u6700\u5177\u5224\u522b\u6027\u7684\u9891\u5e26\u3002", "result": "\u5728CERT r4.2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u548c\u573a\u666f\u4e0b\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u5c0f\u6ce2\u611f\u77e5\u8c03\u5236\u3001\u591a\u5206\u8fa8\u7387\u5206\u89e3\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.12288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12288", "abs": "https://arxiv.org/abs/2601.12288", "authors": ["Lei Liu", "Tengyuan Liu", "Hongwei Zhao", "Jiahui Huang", "Ruibo Guo", "Bin Li"], "title": "TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization", "comment": null, "summary": "Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\\% in CRPS and 21.23\\% in NMAE.", "AI": {"tldr": "TimeGMM\uff1a\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u6355\u83b7\u590d\u6742\u672a\u6765\u5206\u5e03\uff0c\u65e0\u9700\u6602\u8d35\u91c7\u6837\u6216\u9650\u5236\u6027\u53c2\u6570\u5047\u8bbe", "motivation": "\u73b0\u6709\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u91c7\u6837\u8fc7\u7a0b\uff1b2\uff09\u4f7f\u7528\u9650\u5236\u6027\u53c2\u6570\u5047\u8bbe\u6765\u8868\u5f81\u672a\u6765\u5206\u5e03\u3002\u8fd9\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u5e76\u5bfc\u81f4\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898", "method": "\u63d0\u51faTimeGMM\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1\uff09GMM-adapted Reversible Instance Normalization (GRIN)\u6a21\u5757\uff0c\u52a8\u6001\u9002\u5e94\u65f6\u95f4-\u6982\u7387\u5206\u5e03\u504f\u79fb\uff1b2\uff09\u65f6\u95f4\u7f16\u7801\u5668(TE-Module)\u4e0e\u6761\u4ef6\u65f6\u95f4-\u6982\u7387\u89e3\u7801\u5668(CTPD-Module)\u8054\u5408\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u6df7\u5408\u5206\u5e03\u53c2\u6570", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTimeGMM\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728CRPS\u6307\u6807\u4e0a\u6700\u5927\u63d0\u534722.48%\uff0c\u5728NMAE\u6307\u6807\u4e0a\u6700\u5927\u63d0\u534721.23%", "conclusion": "TimeGMM\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u6355\u83b7\u590d\u6742\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd"}}
{"id": "2601.13082", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13082", "abs": "https://arxiv.org/abs/2601.13082", "authors": ["Advije Rizvani", "Giovanni Apruzzese", "Pavel Laskov"], "title": "Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading", "comment": "This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore", "summary": "Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft \"adversarial news\" intended to mislead an LLM. In particular, the news headline may include \"malicious\" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.", "AI": {"tldr": "\u7814\u7a76\u91cf\u5316\u4e86\u9488\u5bf9LLM\u652f\u6301\u7684\u7b97\u6cd5\u4ea4\u6613\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u65b0\u95fb\u653b\u51fb\uff0c\u901a\u8fc7Unicode\u540c\u5f62\u5b57\u66ff\u6362\u548c\u9690\u85cf\u6587\u672c\u6761\u6b3e\u64cd\u7eb5\u65b0\u95fb\u6807\u9898\uff0c\u572814\u4e2a\u6708\u5185\u5355\u65e5\u653b\u51fb\u53ef\u4f7f\u5e74\u56de\u62a5\u7387\u964d\u4f4e\u9ad8\u8fbe17.7\u4e2a\u767e\u5206\u70b9", "motivation": "LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u7279\u522b\u662f\u5728\u5206\u6790\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u4ee5\u6307\u5bfc\u7b97\u6cd5\u4ea4\u6613\u51b3\u7b56\u65b9\u9762\u3002\u7136\u800c\uff0c\u5a01\u80c1\u884c\u4e3a\u8005\u53ef\u80fd\u5236\u4f5c\"\u5bf9\u6297\u6027\u65b0\u95fb\"\u6765\u8bef\u5bfcLLM\uff0c\u8fd9\u79cd\u653b\u51fb\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u548c\u8d27\u5e01\u98ce\u9669\u5c1a\u672a\u88ab\u91cf\u5316\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u8003\u8651\u4e86\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95eeATS\u4f46\u80fd\u7be1\u6539\u5355\u65e5\u80a1\u7968\u76f8\u5173\u65b0\u95fb\u6807\u9898\u7684\u5bf9\u624b\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u64cd\u7eb5\u6280\u672f\uff1a1) Unicode\u540c\u5f62\u5b57\u66ff\u6362\uff08\u8bef\u5bfc\u80a1\u7968\u540d\u79f0\u8bc6\u522b\uff09\uff0c2) \u9690\u85cf\u6587\u672c\u6761\u6b3e\uff08\u6539\u53d8\u65b0\u95fb\u6807\u9898\u60c5\u611f\uff09\u3002\u5728Backtrader\u4e2d\u5b9e\u73b0\u4e86\u4e00\u4e2a\u73b0\u5b9e\u7684ATS\uff0c\u878d\u5408\u4e86\u57fa\u4e8eLSTM\u7684\u4ef7\u683c\u9884\u6d4b\u548cLLM\u884d\u751f\u7684\u60c5\u611f\u5206\u6790\uff08\u4f7f\u7528FinBERT\u3001FinGPT\u3001FinLLaMA\u548c\u516d\u4e2a\u901a\u7528LLM\uff09\uff0c\u5e76\u901a\u8fc7\u6295\u8d44\u7ec4\u5408\u6307\u6807\u91cf\u5316\u8d27\u5e01\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u572814\u4e2a\u6708\u5185\u8fdb\u884c\u5355\u65e5\u653b\u51fb\u53ef\u4ee5\u53ef\u9760\u5730\u8bef\u5bfcLLM\uff0c\u5e76\u4f7f\u5e74\u56de\u62a5\u7387\u964d\u4f4e\u9ad8\u8fbe17.7\u4e2a\u767e\u5206\u70b9\u3002\u901a\u8fc7\u5206\u6790\u6d41\u884c\u7684\u722c\u866b\u5e93\u548c\u4ea4\u6613\u5e73\u53f0\uff0c\u5e76\u8c03\u67e527\u540d\u91d1\u878d\u79d1\u6280\u4ece\u4e1a\u8005\uff0c\u786e\u8ba4\u4e86\u8fd9\u79cd\u653b\u51fb\u7684\u73b0\u5b9e\u53ef\u884c\u6027\u3002", "conclusion": "\u9488\u5bf9LLM\u652f\u6301\u7684\u7b97\u6cd5\u4ea4\u6613\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u65b0\u95fb\u653b\u51fb\u5177\u6709\u663e\u8457\u7684\u8d27\u5e01\u98ce\u9669\uff0c\u5355\u65e5\u653b\u51fb\u5373\u53ef\u9020\u6210\u4e25\u91cd\u7684\u8d22\u52a1\u635f\u5931\u3002\u7814\u7a76\u5df2\u5411\u4ea4\u6613\u5e73\u53f0\u6240\u6709\u8005\u901a\u62a5\u4e86\u6b64\u5b89\u5168\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u52a0\u5f3aLLM\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u9632\u62a4\u3002"}}
{"id": "2601.12296", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12296", "abs": "https://arxiv.org/abs/2601.12296", "authors": ["Hong Zheng", "Fei Teng"], "title": "Distribution Shift Is Key to Learning Invariant Prediction", "comment": null, "summary": "An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u8d8a\u5927\uff0c\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u7684\u6027\u80fd\u8d8a\u597d\uff0c\u751a\u81f3\u80fd\u63a5\u8fd1\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48ERM\u6709\u65f6\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002", "motivation": "\u89c2\u5bdf\u5230\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u6709\u65f6\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7a76\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u5916\u7684\u539f\u56e0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u57df\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a1\uff09\u63d0\u51fa\u7406\u8bba\u4e0a\u754c\uff0c\u8868\u660e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u80fd\u529b\uff1b2\uff09\u8bc1\u660e\u5728\u7279\u5b9a\u6570\u636e\u6761\u4ef6\u4e0b\uff0cERM\u89e3\u80fd\u8fbe\u5230\u4e0e\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff1b3\uff09\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5c55\u793a\u968f\u7740\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u589e\u52a0\uff0c\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u63a5\u8fd1Oracle\u6216\u6700\u4f18\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u8d8a\u5927\uff0c\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u8d8a\u5f3a\uff0c\u8d8a\u80fd\u903c\u8fd1\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\uff1b2\uff09\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cERM\u89e3\u7684\u6027\u80fd\u53ef\u4e0e\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u76f8\u5ab2\u7f8e\uff1b3\uff09\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u589e\u52a0\uff0c\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u786e\u5b9e\u66f4\u63a5\u8fd1Oracle\u6216\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u4e0d\u53d8\u9884\u6d4b\u7684\u5173\u952e\u56e0\u7d20\u3002\u8f83\u5927\u7684\u5206\u5e03\u504f\u79fb\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f7fERM\u65b9\u6cd5\u80fd\u591f\u63a5\u8fd1\u4e13\u95e8\u8bbe\u8ba1\u7684\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd9\u89e3\u91ca\u4e86ERM\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u6709\u65f6\u8868\u73b0\u4f18\u5f02\u7684\u73b0\u8c61\u3002"}}
{"id": "2601.13112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13112", "abs": "https://arxiv.org/abs/2601.13112", "authors": ["Xiaolei Zhang", "Xiaojun Jia", "Liquan Chen", "Songze Li"], "title": "CODE: A Contradiction-Based Deliberation Extension Framework for Overthinking Attacks on Retrieval-Augmented Generation", "comment": "12 pages with 7 figures", "summary": "Introducing reasoning models into Retrieval-Augmented Generation (RAG) systems enhances task performance through step-by-step reasoning, logical consistency, and multi-step self-verification. However, recent studies have shown that reasoning models suffer from overthinking attacks, where models are tricked to generate unnecessarily high number of reasoning tokens. In this paper, we reveal that such overthinking risk can be inherited by RAG systems equipped with reasoning models, by proposing an end-to-end attack framework named Contradiction-Based Deliberation Extension (CODE). Specifically, CODE develops a multi-agent architecture to construct poisoning samples that are injected into the knowledge base. These samples 1) are highly correlated with the use query, such that can be retrieved as inputs to the reasoning model; and 2) contain contradiction between the logical and evidence layers that cause models to overthink, and are optimized to exhibit highly diverse styles. Moreover, the inference overhead of CODE is extremely difficult to detect, as no modification is needed on the user query, and the task accuracy remain unaffected. Extensive experiments on two datasets across five commercial reasoning models demonstrate that the proposed attack causes a 5.32x-24.72x increase in reasoning token consumption, without degrading task performance. Finally, we also discuss and evaluate potential countermeasures to mitigate overthinking risks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCODE\u653b\u51fb\u6846\u67b6\uff0c\u63ed\u793a\u63a8\u7406\u6a21\u578b\u5728RAG\u7cfb\u7edf\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u98ce\u9669\uff0c\u901a\u8fc7\u6784\u9020\u77db\u76fe\u6837\u672c\u6765\u589e\u52a0\u63a8\u7406\u4ee4\u724c\u6d88\u8017\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u6a21\u578b\u80fd\u63d0\u5347RAG\u7cfb\u7edf\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u98ce\u9669\u4f1a\u9057\u4f20\u7ed9\u914d\u5907\u63a8\u7406\u6a21\u578b\u7684RAG\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u653b\u51fb\u6846\u67b6\u6765\u9a8c\u8bc1\u8fd9\u4e00\u98ce\u9669\u3002", "method": "\u63d0\u51faContradiction-Based Deliberation Extension (CODE)\u7aef\u5230\u7aef\u653b\u51fb\u6846\u67b6\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6784\u9020\u4e2d\u6bd2\u6837\u672c\u6ce8\u5165\u77e5\u8bc6\u5e93\u3002\u8fd9\u4e9b\u6837\u672c\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u6027\uff08\u80fd\u88ab\u68c0\u7d22\u4f5c\u4e3a\u63a8\u7406\u6a21\u578b\u8f93\u5165\uff09\u548c\u903b\u8f91\u5c42\u4e0e\u8bc1\u636e\u5c42\u4e4b\u95f4\u7684\u77db\u76fe\uff08\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\uff09\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5c55\u73b0\u9ad8\u5ea6\u591a\u6837\u5316\u7684\u98ce\u683c\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u5546\u4e1a\u63a8\u7406\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCODE\u653b\u51fb\u5bfc\u81f4\u63a8\u7406\u4ee4\u724c\u6d88\u8017\u589e\u52a05.32\u500d\u81f324.72\u500d\uff0c\u4e14\u4efb\u52a1\u51c6\u786e\u6027\u4e0d\u53d7\u5f71\u54cd\u3002\u653b\u51fb\u7684\u63a8\u7406\u5f00\u9500\u6781\u96be\u68c0\u6d4b\uff0c\u56e0\u4e3a\u65e0\u9700\u4fee\u6539\u7528\u6237\u67e5\u8be2\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u98ce\u9669\u786e\u5b9e\u4f1a\u9057\u4f20\u7ed9RAG\u7cfb\u7edf\uff0cCODE\u653b\u51fb\u6846\u67b6\u6709\u6548\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u98ce\u9669\u3002\u8bba\u6587\u8fd8\u8ba8\u8bba\u5e76\u8bc4\u4f30\u4e86\u6f5c\u5728\u7684\u9632\u5fa1\u63aa\u65bd\u6765\u7f13\u89e3\u8fc7\u5ea6\u601d\u8003\u98ce\u9669\u3002"}}
{"id": "2601.12305", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12305", "abs": "https://arxiv.org/abs/2601.12305", "authors": ["Deepak Kanneganti", "Sajib Mistry", "Sheik Fattah", "Joshua Boland", "Aneesh Krishna"], "title": "Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments", "comment": null, "summary": "We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition", "AI": {"tldr": "\u63d0\u51faMDG\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u914d\u7f6e\u3001\u53ef\u590d\u73b0\u7684MLaaS\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u670d\u52a1\u9009\u62e9\u4e0e\u7ec4\u5408\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9eMLaaS\u884c\u4e3a\u5e76\u8bb0\u5f55\u591a\u79cd\u6307\u6807\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6", "motivation": "\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09\u7684\u9009\u62e9\u548c\u7ec4\u5408\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u914d\u7f6e\u3001\u53ef\u590d\u73b0\u7684\u6570\u636e\u96c6\u6765\u6a21\u62df\u771f\u5b9eMLaaS\u884c\u4e3a", "method": "\u63d0\u51faMLaaS\u6570\u636e\u96c6\u751f\u6210\u5668\uff08MDG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\u5728\u4e0d\u540c\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6570\u636e\u5206\u5e03\u8bbe\u7f6e\u4e0b\u6a21\u62dfMLaaS\u884c\u4e3a\uff0c\u8bb0\u5f55\u529f\u80fd\u5c5e\u6027\u3001\u670d\u52a1\u8d28\u91cf\u6307\u6807\u548c\u7ec4\u5408\u7279\u5b9a\u6307\u6807\uff0c\u5e76\u5b9e\u73b0\u5185\u7f6e\u7ec4\u5408\u673a\u5236\u6a21\u62df\u7269\u8054\u7f51\u6761\u4ef6\u4e0b\u7684\u670d\u52a1\u4ea4\u4e92", "result": "\u751f\u6210\u8d85\u8fc7\u4e00\u4e07\u4e2aMLaaS\u670d\u52a1\u5b9e\u4f8b\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660eMDG\u751f\u6210\u7684\u6570\u636e\u96c6\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9009\u62e9\u51c6\u786e\u6027\u548c\u7ec4\u5408\u8d28\u91cf", "conclusion": "MDG\u4e3a\u63a8\u8fdbMLaaS\u9009\u62e9\u548c\u7ec4\u5408\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u914d\u7f6e\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u7cfb\u7edf\u5316\u8bc4\u4f30"}}
{"id": "2601.13271", "categories": ["cs.CR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.13271", "abs": "https://arxiv.org/abs/2601.13271", "authors": ["Chao Yin", "Zunchen Huang", "Chenglu Jin", "Marten van Dijk", "Fabio Massacci"], "title": "Function Recovery Attacks in Gate-Hiding Garbled Circuits using SAT Solving", "comment": null, "summary": "Semi-Private Function Evaluation enables joint computation while protecting both input data and function logic. A practical instantiation is gate-hiding garbled circuits, which conceal gate functionalities while revealing the circuit topology. Existing security definitions intentionally exclude leakage through circuit topology, leaving the concrete impact of such leakage on function privacy insufficiently understood.\n  We analyze the empirical security of gate hiding under two adversarial models that capture realistic computational capabilities. We present a SAT-based function-recovery attack that reconstructs hidden gate operations from a circuit's public topology. To enable recovery on larger and more complex circuits, we develop an incremental SAT-solving framework combined with a set of composable, topology-preserving simplification theorems. These techniques jointly reduce the SAT instance size and progressively constrain the search space across repeated solving iterations.\n  We evaluate our attack on ISCAS benchmarks, representative secure computation circuits, and fault-tolerant sensor fusion circuits under a fixed 24-hour recovery budget. Compared to baseline approaches, our optimized attack achieves up to a 159-fold speedup in recovery time without increasing the number of oracle queries. Our results demonstrate that topology leakage alone can enable effective function recovery in practice.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u95e8\u9690\u85cf\u6280\u672f\u5728\u7535\u8def\u62d3\u6251\u6cc4\u9732\u4e0b\u7684\u5b9e\u9645\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eSAT\u7684\u51fd\u6570\u6062\u590d\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u91cf\u5f0fSAT\u6c42\u89e3\u548c\u62d3\u6251\u4fdd\u6301\u7b80\u5316\u5b9a\u7406\uff0c\u5728ISCAS\u57fa\u51c6\u7535\u8def\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8159\u500d\u7684\u6062\u590d\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u534a\u79c1\u6709\u51fd\u6570\u8bc4\u4f30\u7684\u5b89\u5168\u5b9a\u4e49\u6545\u610f\u6392\u9664\u4e86\u7535\u8def\u62d3\u6251\u6cc4\u9732\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u5bf9\u62d3\u6251\u6cc4\u9732\u5982\u4f55\u5f71\u54cd\u51fd\u6570\u9690\u79c1\u7684\u5b9e\u9645\u5f71\u54cd\u7406\u89e3\u4e0d\u8db3\u3002\u9700\u8981\u5206\u6790\u95e8\u9690\u85cf\u5728\u73b0\u5b9e\u8ba1\u7b97\u80fd\u529b\u4e0b\u7684\u7ecf\u9a8c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eSAT\u7684\u51fd\u6570\u6062\u590d\u653b\u51fb\uff0c\u4ece\u516c\u5f00\u7684\u7535\u8def\u62d3\u6251\u91cd\u5efa\u9690\u85cf\u7684\u95e8\u64cd\u4f5c\u3002\u5f00\u53d1\u4e86\u589e\u91cf\u5f0fSAT\u6c42\u89e3\u6846\u67b6\u548c\u4e00\u7ec4\u53ef\u7ec4\u5408\u7684\u62d3\u6251\u4fdd\u6301\u7b80\u5316\u5b9a\u7406\uff0c\u5171\u540c\u51cf\u5c11SAT\u5b9e\u4f8b\u5927\u5c0f\u5e76\u5728\u91cd\u590d\u6c42\u89e3\u8fed\u4ee3\u4e2d\u9010\u6b65\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728ISCAS\u57fa\u51c6\u7535\u8def\u3001\u4ee3\u8868\u6027\u5b89\u5168\u8ba1\u7b97\u7535\u8def\u548c\u5bb9\u9519\u4f20\u611f\u5668\u878d\u5408\u7535\u8def\u4e0a\u8bc4\u4f30\u653b\u51fb\u6548\u679c\uff0c\u572824\u5c0f\u65f6\u6062\u590d\u9884\u7b97\u4e0b\uff0c\u4f18\u5316\u653b\u51fb\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8159\u500d\u7684\u6062\u590d\u65f6\u95f4\u52a0\u901f\uff0c\u4e14\u4e0d\u589e\u52a0\u9884\u8a00\u673a\u67e5\u8be2\u6b21\u6570\u3002", "conclusion": "\u62d3\u6251\u6cc4\u9732\u5355\u72ec\u5c31\u80fd\u591f\u5728\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u51fd\u6570\u6062\u590d\uff0c\u8fd9\u8868\u660e\u73b0\u6709\u5b89\u5168\u5b9a\u4e49\u9700\u8981\u91cd\u65b0\u8003\u8651\u62d3\u6251\u6cc4\u9732\u7684\u5f71\u54cd\uff0c\u95e8\u9690\u85cf\u6280\u672f\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u80fd\u9762\u4e34\u6bd4\u9884\u671f\u66f4\u5927\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2601.12322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12322", "abs": "https://arxiv.org/abs/2601.12322", "authors": ["Chang-Wei Shi", "Shi-Shang Wang", "Wu-Jun Li"], "title": "Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays", "comment": null, "summary": "Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \\underline{or}dered \\underline{lo}cal \\underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.", "AI": {"tldr": "\u63d0\u51faOrLoMo\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u5e26\u5c40\u90e8\u66f4\u65b0\u7684\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\uff0c\u901a\u8fc7\u6709\u5e8f\u805a\u5408\u5c40\u90e8\u52a8\u91cf\u6765\u52a0\u901f\u5f02\u6784\u96c6\u7fa4\u8bad\u7ec3", "motivation": "\u52a8\u91cfSGD\u662f\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u7684\u57fa\u7840\u4f18\u5316\u5668\uff0c\u5f02\u6b65\u5206\u5e03\u5f0f\u5b66\u4e60\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u8ba1\u7b97\u80fd\u529b\u7684\u96c6\u7fa4\u4e2d\u3002\u5c40\u90e8\u66f4\u65b0\u80fd\u51cf\u5c11\u901a\u4fe1\u9891\u7387\uff0c\u4f46\u5982\u4f55\u5b9e\u73b0\u5e26\u5c40\u90e8\u66f4\u65b0\u7684\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u6709\u5e8f\u5c40\u90e8\u52a8\u91cf(OrLoMo)\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u5de5\u4f5c\u8282\u70b9\u672c\u5730\u8fd0\u884c\u52a8\u91cfSGD\uff0c\u670d\u52a1\u5668\u6839\u636e\u5168\u5c40\u8fed\u4ee3\u7d22\u5f15\u6709\u5e8f\u805a\u5408\u6765\u81ea\u5404\u5de5\u4f5c\u8282\u70b9\u7684\u5c40\u90e8\u52a8\u91cf\u3002\u8fd9\u662f\u9996\u4e2a\u5b9e\u73b0\u5e26\u5c40\u90e8\u66f4\u65b0\u7684\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\u7684\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86OrLoMo\u5728\u4efb\u610f\u5ef6\u8fdf\u4e0b\u7684\u975e\u51f8\u95ee\u9898\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1OrLoMo\u4f18\u4e8e\u5176\u540c\u6b65\u5bf9\u5e94\u65b9\u6cd5\u548c\u5176\u4ed6\u5f02\u6b65\u65b9\u6cd5\u3002", "conclusion": "OrLoMo\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\u4e0e\u5c40\u90e8\u66f4\u65b0\u7684\u7ed3\u5408\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u96c6\u7fa4\u4e2d\u7684\u5927\u89c4\u6a21\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2601.13399", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13399", "abs": "https://arxiv.org/abs/2601.13399", "authors": ["Jonatan Rassekhnia"], "title": "QERS: Quantum Encryption Resilience Score for Post-Quantum Cryptography in Computer, IoT, and IIoT Systems", "comment": "10 pages, 6 figures. Preprint version; extended validation planned for MSc thesis and journal submission", "summary": "Post-quantum cryptography (PQC) is becoming essential for securing Internet of Things (IoT) and Industrial IoT (IIoT) systems against quantum-enabled adversaries. However, existing evaluation approaches primarily focus on isolated performance metrics, offering limited support for holistic security and deployment decisions. This paper introduces QERS (Quantum Encryption Resilience Score), a universal measurement framework that integrates cryptographic performance, system constraints, and multi-criteria decision analysis to assess PQC readiness in computer, IoT, and IIoT environments. QERS combines normalized metrics, weighted aggregation, and machine learning-assisted analysis to produce interpretable resilience scores across heterogeneous devices and communication protocols. Experimental results demonstrate how the framework enables comparative evaluation of post-quantum schemes under realistic resource constraints, supporting informed security design and migration planning. This work is presented as a preprint, with extended statistical validation planned as part of ongoing graduate research.", "AI": {"tldr": "\u63d0\u51fa\u4e86QERS\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u540e\u91cf\u5b50\u5bc6\u7801\u5728\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u6297\u91cf\u5b50\u52a0\u5bc6\u97e7\u6027\uff0c\u901a\u8fc7\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\u63d0\u4f9b\u7efc\u5408\u8bc4\u5206", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u5bf9\u7269\u8054\u7f51\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u6027\u80fd\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4f53\u5b89\u5168\u6027\u548c\u90e8\u7f72\u51b3\u7b56\u7684\u5168\u9762\u652f\u6301", "method": "\u5f15\u5165QERS\u6846\u67b6\uff0c\u96c6\u6210\u5bc6\u7801\u6027\u80fd\u3001\u7cfb\u7edf\u7ea6\u675f\u548c\u591a\u6807\u51c6\u51b3\u7b56\u5206\u6790\uff0c\u91c7\u7528\u5f52\u4e00\u5316\u6307\u6807\u3001\u52a0\u6743\u805a\u5408\u548c\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u5206\u6790\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bbe\u5907\u548c\u901a\u4fe1\u534f\u8bae\u4e0b\u7684\u6297\u91cf\u5b50\u97e7\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u5728\u5b9e\u9645\u8d44\u6e90\u7ea6\u675f\u4e0b\u5bf9\u540e\u91cf\u5b50\u65b9\u6848\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u652f\u6301\u660e\u667a\u7684\u5b89\u5168\u8bbe\u8ba1\u548c\u8fc1\u79fb\u89c4\u5212", "conclusion": "QERS\u4e3a\u8ba1\u7b97\u673a\u3001\u7269\u8054\u7f51\u548c\u5de5\u4e1a\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u5c31\u7eea\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u901a\u7528\u6d4b\u91cf\u6846\u67b6\uff0c\u76ee\u524d\u4f5c\u4e3a\u9884\u5370\u672c\u53d1\u5e03\uff0c\u8ba1\u5212\u5728\u540e\u7eed\u7814\u7a76\u4e2d\u6269\u5c55\u7edf\u8ba1\u9a8c\u8bc1"}}
{"id": "2601.12330", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12330", "abs": "https://arxiv.org/abs/2601.12330", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Ayesha Kanwal", "Sidra Sultana", "Nazia Perwaiz"], "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.", "AI": {"tldr": "IceWatch\u662f\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34(GLOF)\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u89c6\u89c9\u5206\u6790\u548c\u65f6\u95f4\u7269\u7406\u52a8\u6001\u5efa\u6a21\uff0c\u5b9e\u73b0\u81ea\u52a8\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u9884\u8b66\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfGLOF\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6c34\u6587\u5efa\u6a21\u3001\u9608\u503c\u76d1\u6d4b\u548c\u4eba\u5de5\u536b\u661f\u56fe\u50cf\u5206\u6790\uff0c\u5b58\u5728\u66f4\u65b0\u6162\u3001\u4f9d\u8d56\u4eba\u5de5\u3001\u4e91\u5c42\u5e72\u6270\u548c\u73b0\u573a\u6570\u636e\u7f3a\u4e4f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u3001\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u9884\u6d4b\u65b9\u6848\u3002", "method": "\u63d0\u51faIceWatch\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) RiskFlow\u89c6\u89c9\u7ec4\u4ef6\uff1a\u57fa\u4e8eCNN\u5206\u7c7b\u5668\u5904\u7406Sentinel-2\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\uff0c\u4ece\u51b0\u96ea\u878d\u6c34\u7a7a\u95f4\u6a21\u5f0f\u9884\u6d4bGLOF\uff1b2) \u8868\u683c\u7ec4\u4ef6\uff1a\u6574\u5408TerraFlow\uff08\u4eceNASA ITS_LIVE\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u51b0\u5ddd\u6d41\u901f\uff09\u548cTempFlow\uff08\u4eceMODIS LST\u8bb0\u5f55\u9884\u6d4b\u8fd1\u5730\u8868\u6e29\u5ea6\uff09\uff0c\u901a\u8fc7\u534f\u8c03\u9884\u5904\u7406\u548c\u540c\u6b65\u5b9e\u73b0\u591a\u6a21\u6001\u7269\u7406\u4fe1\u606f\u878d\u5408\u3002", "result": "\u7cfb\u7edf\u63d0\u4f9b\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u63d0\u9ad8GLOF\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u786e\u4fdd\u5f3a\u9884\u6d4b\u6027\u80fd\u3001\u5feb\u901f\u6570\u636e\u5904\u7406\uff08\u652f\u6301\u5b9e\u65f6\u4f7f\u7528\uff09\u4ee5\u53ca\u5bf9\u566a\u58f0\u548c\u7f3a\u5931\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "IceWatch\u4e3a\u81ea\u52a8\u3001\u53ef\u6269\u5c55\u7684GLOF\u9884\u8b66\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u5177\u5907\u4e0e\u591a\u79cd\u4f20\u611f\u5668\u8f93\u5165\u548c\u5168\u7403\u51b0\u5ddd\u76d1\u6d4b\u6d3b\u52a8\u96c6\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.13423", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13423", "abs": "https://arxiv.org/abs/2601.13423", "authors": ["Jonatan Rassekhnia"], "title": "Quantum Encryption Resilience Score (QERS) for MQTT, HTTP, and HTTPS under Post-Quantum Cryptography in Computer, IoT, and IIoT Systems", "comment": "9 pages, 6 figures. Experimental preprint. Companion study to the QERS framework based on MQTT, HTTP, and HTTPS protocols; extended analysis and validation planned for MSc thesis and journal submission", "summary": "Post-quantum cryptography (PQC) introduces significant computational and communication overhead, which poses challenges for resource-constrained computer systems, Internet of Things (IoT), and Industrial IoT (IIoT) devices. This paper presents an experimental evaluation of the Quantum Encryption Resilience Score (QERS) applied to MQTT, HTTP, and HTTPS communication protocols operating under PQC. Using an ESP32-C6 client and an ARM-based Raspberry Pi CM4 server, latency, CPU utilization, RSSI, energy consumption, key size, and TLS handshake overhead are measured under realistic operating conditions. QERS integrates these heterogeneous metrics into normalized Basic, Tuned, and Fusion scores, enabling systematic comparison of protocol efficiency and security resilience. Experimental results show that MQTT provides the highest efficiency under PQC constraints, while HTTPS achieves the highest security-weighted resilience at the cost of increased latency and resource consumption. The proposed framework supports informed protocol selection and migration planning for PQC-enabled IoT and IIoT deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u91cf\u5b50\u52a0\u5bc6\u97e7\u6027\u8bc4\u5206(QERS)\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30MQTT\u3001HTTP\u548cHTTPS\u534f\u8bae\u5728PQC\u73af\u5883\u4e0b\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u6743\u8861\uff0c\u5b9e\u9a8c\u8868\u660eMQTT\u6548\u7387\u6700\u9ad8\uff0cHTTPS\u5b89\u5168\u6027\u6700\u597d\u4f46\u8d44\u6e90\u6d88\u8017\u5927", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u5b66(PQC)\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u548c\u5de5\u4e1a\u7269\u8054\u7f51\u8bbe\u5907\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u901a\u4fe1\u534f\u8bae\u5728PQC\u73af\u5883\u4e0b\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u5e73\u8861", "method": "\u4f7f\u7528ESP32-C6\u5ba2\u6237\u7aef\u548cARM\u67b6\u6784\u7684Raspberry Pi CM4\u670d\u52a1\u5668\uff0c\u5728\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u6d4b\u91cfMQTT\u3001HTTP\u548cHTTPS\u534f\u8bae\u7684\u5ef6\u8fdf\u3001CPU\u5229\u7528\u7387\u3001RSSI\u3001\u80fd\u8017\u3001\u5bc6\u94a5\u5927\u5c0f\u548cTLS\u63e1\u624b\u5f00\u9500\uff0c\u901a\u8fc7QERS\u6846\u67b6\u5c06\u8fd9\u4e9b\u5f02\u6784\u6307\u6807\u6574\u5408\u4e3a\u5f52\u4e00\u5316\u7684\u57fa\u7840\u8bc4\u5206\u3001\u8c03\u4f18\u8bc4\u5206\u548c\u878d\u5408\u8bc4\u5206", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMQTT\u5728PQC\u7ea6\u675f\u4e0b\u63d0\u4f9b\u6700\u9ad8\u6548\u7387\uff0cHTTPS\u5b9e\u73b0\u6700\u9ad8\u7684\u5b89\u5168\u52a0\u6743\u97e7\u6027\u4f46\u4ee3\u4ef7\u662f\u589e\u52a0\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\uff0cQERS\u6846\u67b6\u652f\u6301\u7cfb\u7edf\u6027\u7684\u534f\u8bae\u6548\u7387\u548c\u5b89\u5168\u97e7\u6027\u6bd4\u8f83", "conclusion": "\u63d0\u51fa\u7684QERS\u6846\u67b6\u80fd\u591f\u652f\u6301PQC\u542f\u7528\u7684\u7269\u8054\u7f51\u548c\u5de5\u4e1a\u7269\u8054\u7f51\u90e8\u7f72\u4e2d\u7684\u534f\u8bae\u9009\u62e9\u548c\u8fc1\u79fb\u89c4\u5212\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b89\u5168\u901a\u4fe1\u534f\u8bae\u9009\u62e9\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e"}}
{"id": "2601.13425", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.13425", "abs": "https://arxiv.org/abs/2601.13425", "authors": ["Gian Sebastian Mier Bello", "Alexander Martinez Mendez", "Carlos J. Barrios H.", "Robinson Rivas", "Luis A. N\u00fa\u00f1ez"], "title": "A Scientific Data Integrity system based on Blockchain", "comment": "Accepted and presented at CARLA 2025. To appear in Springer LNCS proceedings", "summary": "In most High Performance Computing (HPC) projects nowadays, there is a lot of data obtained from different sources, depending on the project's objectives. Some of that data is very huge in terms of size, so copying such data sometimes is an unrealistic goal. On the other hand, science requires data used for different purposes to remain unaltered, so different groups of researchers can reproduce results, discuss theories, and validate each other. In this paper, we present a novel approach to help research groups to validate data integrity on such distributed repositories using Blockchain. Originally developed for cryptographic currencies, Blockchain has demonstrated a versatile range of uses. Our proposal ensures 1) secure access to data management, 2) easy validation of data integrity, and 3) an easy way to add new records to the dataset with the same robust integrity policy. A prototype was developed and tested using a subset of a public dataset from a real scientific collaboration, the Latin American Giant Observatory (LAGO) Project.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u5206\u5e03\u5f0f\u79d1\u5b66\u6570\u636e\u5b8c\u6574\u6027\u9a8c\u8bc1\u65b9\u6848\uff0c\u89e3\u51b3HPC\u73af\u5883\u4e2d\u5927\u6570\u636e\u590d\u5236\u56f0\u96be\u4e0e\u79d1\u7814\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u9700\u6c42\u4e4b\u95f4\u7684\u77db\u76fe", "motivation": "HPC\u9879\u76ee\u4e2d\u5b58\u5728\u5927\u91cf\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u6570\u636e\uff0c\u90e8\u5206\u6570\u636e\u4f53\u91cf\u5de8\u5927\u96be\u4ee5\u590d\u5236\uff0c\u800c\u79d1\u5b66\u7814\u7a76\u8981\u6c42\u6570\u636e\u4fdd\u6301\u539f\u59cb\u72b6\u6001\u4ee5\u4f9b\u4e0d\u540c\u7814\u7a76\u7ec4\u590d\u73b0\u7ed3\u679c\u3001\u8ba8\u8bba\u7406\u8bba\u548c\u76f8\u4e92\u9a8c\u8bc1", "method": "\u91c7\u7528\u533a\u5757\u94fe\u6280\u672f\u6784\u5efa\u5206\u5e03\u5f0f\u6570\u636e\u5b8c\u6574\u6027\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u786e\u4fdd\uff1a1) \u6570\u636e\u7ba1\u7406\u7684\u5b89\u5168\u8bbf\u95ee\uff1b2) \u6570\u636e\u5b8c\u6574\u6027\u7684\u4fbf\u6377\u9a8c\u8bc1\uff1b3) \u65b0\u8bb0\u5f55\u6dfb\u52a0\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u5f3a\u5b8c\u6574\u6027\u7b56\u7565", "result": "\u5f00\u53d1\u4e86\u539f\u578b\u7cfb\u7edf\u5e76\u4f7f\u7528\u62c9\u4e01\u7f8e\u6d32\u5de8\u578b\u5929\u6587\u53f0(LAGO)\u9879\u76ee\u7684\u771f\u5b9e\u79d1\u5b66\u5408\u4f5c\u516c\u5171\u6570\u636e\u96c6\u5b50\u96c6\u8fdb\u884c\u4e86\u6d4b\u8bd5\u9a8c\u8bc1", "conclusion": "\u533a\u5757\u94fe\u6280\u672f\u4e3a\u5206\u5e03\u5f0f\u79d1\u5b66\u6570\u636e\u4ed3\u5e93\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b8c\u6574\u6027\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6ee1\u8db3\u79d1\u7814\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u548c\u53ef\u590d\u73b0\u6027\u7684\u9700\u6c42"}}
{"id": "2601.12355", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12355", "abs": "https://arxiv.org/abs/2601.12355", "authors": ["Beicheng Xu", "Weitong Qian", "Lingching Tung", "Yupeng Lu", "Bin Cui"], "title": "LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH", "comment": null, "summary": "To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.", "AI": {"tldr": "LB-MCTS\u6846\u67b6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u89e3\u51b3CASH\u95ee\u9898\uff0c\u5728104\u4e2aAMLB\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u95e8\u69db\u9700\u8981\u89e3\u51b3CASH\u95ee\u9898\uff08\u7b97\u6cd5\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u81ea\u52a8\u5316\uff09\u3002\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u5b58\u5728\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u5668\u5728\u9ad8\u7ef4\u7ed3\u6784\u5316CASH\u7a7a\u95f4\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee", "method": "\u63d0\u51faLB-MCTS\u6846\u67b6\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u4e4b\u95f4\u534f\u540c\u5de5\u4f5c\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ed3\u6784\u3002\u901a\u8fc7\u9009\u62e9\u6027\u8c03\u4f18\u8bb0\u5fc6\uff08STM\uff09\u6700\u5927\u5316\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u663e\u5f0f\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002\u968f\u7740\u6570\u636e\u79ef\u7d2f\uff0c\u52a8\u6001\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u8f6c\u5411\u8d1d\u53f6\u65af\u4f18\u5316\u9a71\u52a8", "result": "\u5728104\u4e2aAMLB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLB-MCTS\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "LB-MCTS\u6210\u529f\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6570\u636e\u9a71\u52a8\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u7ed3\u6784\u5316CASH\u7a7a\u95f4\u7684\u4f18\u5316\u95ee\u9898"}}
{"id": "2601.13515", "categories": ["cs.CR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13515", "abs": "https://arxiv.org/abs/2601.13515", "authors": ["Hanlin Zhou", "Huah Yong Chan", "Jingfei Ni", "Mengchun Wu", "Qing Deng"], "title": "Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests", "comment": null, "summary": "In this paper, HTTP status codes are used as custom metrics within the HPA as the experimental scenario. By integrating the Random Forest classification algorithm from machine learning, attacks are assessed and predicted, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This approach enables the adjustment of HPA parameters using machine learning scripts in targeted attack scenarios while effectively managing attack traffic. All access from attacking IPs is redirected to honeypot pods, achieving a lower incidence of 5XX status codes through HPA pod adjustments under high load conditions. This method also ensures effective isolation of attack traffic, preventing excessive HPA expansion due to attacks. Additionally, experiments conducted under various conditions demonstrate the importance of setting appropriate thresholds for HPA adjustments.", "AI": {"tldr": "\u4f7f\u7528HTTP\u72b6\u6001\u7801\u4f5c\u4e3aHPA\u81ea\u5b9a\u4e49\u6307\u6807\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u8bc4\u4f30\u9884\u6d4b\u653b\u51fb\uff0c\u52a8\u6001\u8c03\u6574HPA\u6700\u5927pod\u53c2\u6570\u6765\u7ba1\u7406\u653b\u51fb\u6d41\u91cf\uff0c\u5c06\u653b\u51fbIP\u91cd\u5b9a\u5411\u5230\u871c\u7f50pod", "motivation": "\u5728\u4e91\u539f\u751f\u73af\u5883\u4e2d\uff0c\u4f20\u7edfHPA\uff08\u6c34\u5e73Pod\u81ea\u52a8\u6269\u7f29\uff09\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u6d41\u91cf\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u6269\u5c55\u548c\u6210\u672c\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u667a\u80fd\u673a\u5236\u6765\u533a\u5206\u6b63\u5e38\u6d41\u91cf\u548c\u653b\u51fb\u6d41\u91cf\uff0c\u5e76\u52a8\u6001\u8c03\u6574HPA\u53c2\u6570\u4ee5\u6709\u6548\u7ba1\u7406\u653b\u51fb\u573a\u666f", "method": "\u5c06HTTP\u72b6\u6001\u7801\u4f5c\u4e3aHPA\u7684\u81ea\u5b9a\u4e49\u6307\u6807\uff0c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u968f\u673a\u68ee\u6797\u5206\u7c7b\u7b97\u6cd5\u6765\u8bc4\u4f30\u548c\u9884\u6d4b\u653b\u51fb\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574HPA\u7684\u6700\u5927pod\u53c2\u6570\u6765\u7ba1\u7406\u653b\u51fb\u6d41\u91cf\uff0c\u5c06\u6240\u6709\u6765\u81ea\u653b\u51fbIP\u7684\u8bbf\u95ee\u91cd\u5b9a\u5411\u5230\u871c\u7f50pod\u3002\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5728\u9ad8\u8d1f\u8f7d\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7HPA pod\u8c03\u6574\u5b9e\u73b0\u4e86\u66f4\u4f4e\u76845XX\u72b6\u6001\u7801\u53d1\u751f\u7387\u3002\u6709\u6548\u9694\u79bb\u4e86\u653b\u51fb\u6d41\u91cf\uff0c\u9632\u6b62\u4e86\u56e0\u653b\u51fb\u5bfc\u81f4\u7684HPA\u8fc7\u5ea6\u6269\u5c55\u3002\u5b9e\u9a8c\u8868\u660e\u8bbe\u7f6e\u9002\u5f53\u7684HPA\u8c03\u6574\u9608\u503c\u81f3\u5173\u91cd\u8981", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u673a\u5668\u5b66\u4e60\u4e0eHPA\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u653b\u51fb\u6d41\u91cf\u7684\u667a\u80fd\u7ba1\u7406\u548c\u9694\u79bb\u3002\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u8c03\u6574\u548c\u871c\u7f50\u91cd\u5b9a\u5411\uff0c\u65e2\u4fdd\u8bc1\u4e86\u670d\u52a1\u53ef\u7528\u6027\uff0c\u53c8\u907f\u514d\u4e86\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u6269\u5c55\uff0c\u4e3a\u4e91\u539f\u751f\u73af\u5883\u7684\u5b89\u5168\u81ea\u52a8\u6269\u7f29\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13528", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.13528", "abs": "https://arxiv.org/abs/2601.13528", "authors": ["Jackson Kaunismaa", "Avery Griffin", "John Hughes", "Christina Q. Knight", "Mrinank Sharma", "Erik Jones"], "title": "Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs", "comment": null, "summary": "Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u76f8\u90bb\u9886\u57df\u65e0\u5bb3\u63d0\u793a\u3001\u83b7\u53d6\u524d\u6cbf\u6a21\u578b\u54cd\u5e94\u3001\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u7684\u4e09\u9636\u6bb5\u653b\u51fb\uff0c\u53ef\u5728\u5316\u5b66\u5371\u9669\u54c1\u5408\u6210\u9886\u57df\u6062\u590d\u7ea640%\u80fd\u529b\u5dee\u8ddd\uff0c\u63ed\u793a\u8f93\u51fa\u7ea7\u9632\u62a4\u5728\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u524d\u6cbf\u6a21\u578b\u9632\u62a4\u63aa\u65bd\u7684\u5c40\u9650\u6027\uff0c\u5373\u4f7f\u6a21\u578b\u5185\u7f6e\u4e86\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\uff08\u5982\u5206\u7c7b\u5668\u8fc7\u6ee4\u5371\u9669\u8f93\u51fa\uff09\uff0c\u653b\u51fb\u8005\u4ecd\u53ef\u901a\u8fc7\u95f4\u63a5\u65b9\u5f0f\u83b7\u53d6\u6709\u5bb3\u80fd\u529b\uff0c\u4ece\u800c\u5f15\u53d1\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u8bf1\u5bfc\u653b\u51fb\uff1a1\uff09\u6784\u5efa\u76ee\u6807\u6709\u5bb3\u4efb\u52a1\u76f8\u90bb\u9886\u57df\u7684\u65e0\u5bb3\u63d0\u793a\uff1b2\uff09\u4ece\u53d7\u9632\u62a4\u7684\u524d\u6cbf\u6a21\u578b\u83b7\u53d6\u8fd9\u4e9b\u63d0\u793a\u7684\u54cd\u5e94\uff1b3\uff09\u4f7f\u7528\u8fd9\u4e9b\u63d0\u793a-\u54cd\u5e94\u5bf9\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u3002\u653b\u51fb\u5229\u7528\u76f8\u90bb\u9886\u57df\u63d0\u793a\u4e0d\u89e6\u53d1\u9632\u62a4\u673a\u5236\u7684\u7279\u70b9\uff0c\u95f4\u63a5\u83b7\u53d6\u6709\u5bb3\u80fd\u529b\u3002", "result": "\u5728\u5371\u9669\u5316\u5b66\u54c1\u5408\u6210\u4e0e\u5904\u7406\u9886\u57df\uff0c\u8be5\u653b\u51fb\u80fd\u591f\u6062\u590d\u5f00\u6e90\u6a21\u578b\u4e0e\u65e0\u9650\u5236\u524d\u6cbf\u6a21\u578b\u4e4b\u95f4\u7ea640%\u7684\u80fd\u529b\u5dee\u8ddd\u3002\u653b\u51fb\u6548\u679c\u968f\u524d\u6cbf\u6a21\u578b\u80fd\u529b\u548c\u751f\u6210\u5fae\u8c03\u6570\u636e\u91cf\u7684\u589e\u52a0\u800c\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u8f93\u51fa\u7ea7\u9632\u62a4\u5728\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u4ec5\u4f9d\u9760\u8f93\u51fa\u7ea7\u9632\u62a4\u63aa\u65bd\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u98ce\u9669\uff0c\u56e0\u4e3a\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u95f4\u63a5\u65b9\u5f0f\u7ed5\u8fc7\u9632\u62a4\u83b7\u53d6\u6709\u5bb3\u80fd\u529b\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524dAI\u5b89\u5168\u9632\u62a4\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5b89\u5168\u7b56\u7565\u3002"}}
{"id": "2601.12401", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12401", "abs": "https://arxiv.org/abs/2601.12401", "authors": ["Jinmei Liu", "Haoru Li", "Zhenhong Sun", "Chaofeng Chen", "Yatao Bian", "Bo Wang", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.", "AI": {"tldr": "DRIFT\u6846\u67b6\u901a\u8fc7\u91c7\u6837\u3001\u63d0\u793a\u548c\u4f18\u5316\u4e09\u4e2a\u89d2\u5ea6\u89e3\u51b3RL\u5fae\u8c03\u4e2d\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u5bf9\u9f50\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u591a\u6837\u6027", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5fae\u8c03\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u65f6\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u7684\u6839\u672c\u9650\u5236\uff0c\u5373\u76ee\u6807\u548c\u4f18\u5316\u8fc7\u7a0b\u5bfc\u81f4\u7b56\u7565\u6536\u655b\u5230\u72c4\u62c9\u514b\u03b4\u5206\u5e03\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u9700\u8981\u591a\u6837\u5316\u5019\u9009\u751f\u6210\u7684\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u63d0\u51faDRIFT\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u7cfb\u7edf\u6027\u5730\u6fc0\u52b1\u8f93\u51fa\u591a\u6837\u6027\uff1a1) \u91c7\u6837\u5956\u52b1\u96c6\u4e2d\u5b50\u96c6\u4ee5\u8fc7\u6ee4\u5956\u52b1\u5f02\u5e38\u503c\uff1b2) \u4f7f\u7528\u968f\u673a\u53d8\u4f53\u63d0\u793a\u6269\u5c55\u6761\u4ef6\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u9020\u673a\u5236\u4f18\u5316\u7ec4\u5185\u591a\u6837\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDRIFT\u5728\u4efb\u52a1\u5bf9\u9f50\u548c\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u4f18\u52bf\uff1a\u5728\u76f8\u540c\u5bf9\u9f50\u6c34\u5e73\u4e0b\u591a\u6837\u6027\u63d0\u53479.08%~43.46%\uff0c\u5728\u76f8\u540c\u591a\u6837\u6027\u6c34\u5e73\u4e0b\u5bf9\u9f50\u5ea6\u63d0\u534759.65%~65.86%", "conclusion": "DRIFT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86RL\u5fae\u8c03\u4e2d\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u5bf9\u9f50\u4e0e\u751f\u6210\u591a\u6837\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u9700\u8981\u591a\u6837\u5316\u5019\u9009\u751f\u6210\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13607", "abs": "https://arxiv.org/abs/2601.13607", "authors": ["Ruihan Hu", "Yu-Ming Shang", "Wei Luo", "Ye Tao", "Xi Zhang"], "title": "When Reasoning Leaks Membership: Membership Inference Attack on Black-box Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have rapidly gained prominence for their strong performance in solving complex tasks. Many modern black-box LRMs expose the intermediate reasoning traces through APIs to improve transparency (e.g., Gemini-2.5 and Claude-sonnet). Despite their benefits, we find that these traces can leak membership signals, creating a new privacy threat even without access to token logits used in prior attacks. In this work, we initiate the first systematic exploration of Membership Inference Attacks (MIAs) on black-box LRMs. Our preliminary analysis shows that LRMs produce confident, recall-like reasoning traces on familiar training member samples but more hesitant, inference-like reasoning traces on non-members. The representations of these traces are continuously distributed in the semantic latent space, spanning from familiar to unfamiliar samples. Building on this observation, we propose BlackSpectrum, the first membership inference attack framework targeting the black-box LRMs. The key idea is to construct a recall-inference axis in the semantic latent space, based on representations derived from the exposed traces. By locating where a query sample falls along this axis, the attacker can obtain a membership score and predict how likely it is to be a member of the training data. Additionally, to address the limitations of outdated datasets unsuited to modern LRMs, we provide two new datasets to support future research, arXivReasoning and BookReasoning. Empirically, exposing reasoning traces significantly increases the vulnerability of LRMs to membership inference attacks, leading to large gains in attack performance. Our findings highlight the need for LRM companies to balance transparency in intermediate reasoning traces with privacy preservation.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u9ed1\u76d2\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u53d1\u73b0\u66b4\u9732\u7684\u63a8\u7406\u8f68\u8ff9\u4f1a\u6cc4\u9732\u6210\u5458\u4fe1\u53f7\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6784\u5efa\u56de\u5fc6-\u63a8\u7406\u8f74\u7684\u653b\u51fb\u6846\u67b6BlackSpectrum", "motivation": "\u73b0\u4ee3\u9ed1\u76d2\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7API\u66b4\u9732\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u8f68\u8ff9\u53ef\u80fd\u6cc4\u9732\u6210\u5458\u4fe1\u606f\uff0c\u5373\u4f7f\u6ca1\u6709\u8bbf\u95ee\u5148\u524d\u653b\u51fb\u6240\u9700\u7684token logits\uff0c\u8fd9\u6784\u6210\u4e86\u65b0\u7684\u9690\u79c1\u5a01\u80c1", "method": "\u63d0\u51faBlackSpectrum\u653b\u51fb\u6846\u67b6\uff1a1\uff09\u5206\u6790\u53d1\u73b0LRMs\u5bf9\u719f\u6089\u7684\u8bad\u7ec3\u6210\u5458\u6837\u672c\u4ea7\u751f\u81ea\u4fe1\u7684\u56de\u5fc6\u5f0f\u63a8\u7406\u8f68\u8ff9\uff0c\u5bf9\u975e\u6210\u5458\u4ea7\u751f\u72b9\u8c6b\u7684\u63a8\u7406\u5f0f\u8f68\u8ff9\uff1b2\uff09\u5728\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u66b4\u9732\u7684\u8f68\u8ff9\u8868\u793a\u6784\u5efa\u56de\u5fc6-\u63a8\u7406\u8f74\uff1b3\uff09\u901a\u8fc7\u5b9a\u4f4d\u67e5\u8be2\u6837\u672c\u5728\u8be5\u8f74\u4e0a\u7684\u4f4d\u7f6e\u83b7\u5f97\u6210\u5458\u5206\u6570", "result": "\u66b4\u9732\u63a8\u7406\u8f68\u8ff9\u663e\u8457\u589e\u52a0\u4e86LRMs\u5bf9\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5bfc\u81f4\u653b\u51fb\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002\u4e3a\u652f\u6301\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86arXivReasoning\u548cBookReasoning\u4e24\u4e2a\u65b0\u6570\u636e\u96c6", "conclusion": "LRM\u516c\u53f8\u9700\u8981\u5728\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u7684\u900f\u660e\u5ea6\u4e0e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u8fd9\u662f\u9996\u6b21\u9488\u5bf9\u9ed1\u76d2\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7cfb\u7edf\u7814\u7a76"}}
{"id": "2601.12405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12405", "abs": "https://arxiv.org/abs/2601.12405", "authors": ["Manasi Kanade", "Abhi Thakkar", "Gabriela Fernandes"], "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants", "comment": null, "summary": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u513f\u79d1\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u4f18\u5148\u8003\u8651\u53ef\u89e3\u91ca\u6027\u3001\u6821\u51c6\u548c\u4f26\u7406\u90e8\u7f72\u800c\u975e\u6700\u5927\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u513f\u79d1\u7259\u79d1\u75be\u75c5\u662f\u5168\u7403\u6700\u666e\u904d\u4e14\u6700\u4e0d\u516c\u5e73\u7684\u6162\u6027\u5065\u5eb7\u72b6\u51b5\u4e4b\u4e00\u3002\u867d\u7136\u6d41\u884c\u75c5\u5b66\u8bc1\u636e\u8868\u660e\u53e3\u8154\u5065\u5eb7\u7ed3\u679c\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u56e0\u7d20\u76f8\u5173\uff0c\u4f46\u5927\u591a\u6570\u7259\u79d1AI\u5e94\u7528\u4f9d\u8d56\u57fa\u4e8e\u56fe\u50cf\u7684\u8bca\u65ad\u548c\u9ed1\u76d2\u9884\u6d4b\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u513f\u79d1\u4eba\u7fa4\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4f26\u7406\u9002\u7528\u6027", "method": "\u4f7f\u7528\u4eba\u53e3\u6c34\u5e73\u7684\u513f\u79d1\u6570\u636e\uff08\u5305\u62ec\u5e74\u9f84\u3001\u6536\u5165\u8d2b\u56f0\u6bd4\u3001\u79cd\u65cf/\u6c11\u65cf\u3001\u6027\u522b\u548c\u75c5\u53f2\uff09\u8bad\u7ec3\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u4f7f\u7528ROC\u5206\u6790\u548c\u6821\u51c6\u66f2\u7ebf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7SHAP\uff08SHapley Additive exPlanations\uff09\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u5168\u5c40\u548c\u4e2a\u4f53\u5c42\u9762\u7684\u9884\u6d4b\u89e3\u91ca", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u9002\u5ea6\u7684\u533a\u5206\u80fd\u529b\uff08AUC = 0.61\uff09\uff0c\u5177\u6709\u4fdd\u5b88\u7684\u6821\u51c6\u7279\u6027\uff0c\u5728\u9ad8\u6982\u7387\u6c34\u5e73\u4e0b\u4f4e\u4f30\u98ce\u9669\u3002SHAP\u5206\u6790\u786e\u5b9a\u5e74\u9f84\u548c\u6536\u5165\u8d2b\u56f0\u6bd4\u662f\u9884\u6d4b\u98ce\u9669\u7684\u6700\u5f3a\u8d21\u732e\u56e0\u7d20\uff0c\u5176\u6b21\u662f\u79cd\u65cf/\u6c11\u65cf\u548c\u6027\u522b", "conclusion": "\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u7684\u3001\u9884\u9632\u5bfc\u5411\u7684\u513f\u79d1\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u652f\u6301\u4eba\u7fa4\u7b5b\u67e5\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\uff0c\u800c\u975e\u8bca\u65ad\u51b3\u7b56"}}
{"id": "2601.13610", "categories": ["cs.CR", "cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13610", "abs": "https://arxiv.org/abs/2601.13610", "authors": ["Hansika Weerasena", "Matthew Randall", "Prabhat Mishra"], "title": "Secure Multi-Path Routing with All-or-Nothing Transform for Network-on-Chip Architectures", "comment": null, "summary": "Ensuring Network-on-Chip (NoC) security is crucial to design trustworthy NoC-based System-on-Chip (SoC) architectures. While there are various threats that exploit on-chip communication vulnerabilities, eavesdropping attacks via malicious nodes are among the most common and stealthy. Although encryption can secure packets for confidentiality, it may introduce unacceptable overhead for resource-constrained SoCs. In this paper, we propose a lightweight confidentiality-preserving framework that utilizes a quasi-group based All-Or-Nothing Transform (AONT) combined with secure multi-path routing in NoC-based SoCs. By applying AONT to each packet and distributing its transformed blocks across multiple non-overlapping routes, we ensure that no intermediate router can reconstruct the original data without all blocks. Extensive experimental evaluation demonstrates that our method effectively mitigates eavesdropping attacks by malicious routers with negligible area and performance overhead. Our results also reveal that AONT-based multi-path routing can provide 7.3x reduction in overhead compared to traditional encryption for securing against eavesdropping attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4fdd\u5bc6\u6846\u67b6\uff0c\u7ed3\u5408\u51c6\u7fa4\u57faAONT\u53d8\u6362\u4e0e\u5b89\u5168\u591a\u8def\u5f84\u8def\u7531\uff0c\u4fdd\u62a4NoC\u514d\u53d7\u7a83\u542c\u653b\u51fb", "motivation": "NoC\u5b89\u5168\u5bf9\u53ef\u4fe1SoC\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u7a83\u542c\u653b\u51fb\u662f\u6700\u5e38\u89c1\u4e14\u9690\u853d\u7684\u5a01\u80c1\u3002\u4f20\u7edf\u52a0\u5bc6\u65b9\u6cd5\u5bf9\u8d44\u6e90\u53d7\u9650SoC\u53ef\u80fd\u5e26\u6765\u4e0d\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u5f00\u9500", "method": "\u91c7\u7528\u51c6\u7fa4\u57fa\u5168\u6216\u65e0\u53d8\u6362(AONT)\u5bf9\u6bcf\u4e2a\u6570\u636e\u5305\u8fdb\u884c\u5904\u7406\uff0c\u5c06\u53d8\u6362\u540e\u7684\u6570\u636e\u5757\u901a\u8fc7\u591a\u4e2a\u975e\u91cd\u53e0\u8def\u7531\u8def\u5f84\u5206\u53d1\uff0c\u786e\u4fdd\u4e2d\u95f4\u8def\u7531\u5668\u65e0\u6cd5\u5728\u6ca1\u6709\u6240\u6709\u6570\u636e\u5757\u7684\u60c5\u51b5\u4e0b\u91cd\u6784\u539f\u59cb\u6570\u636e", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6076\u610f\u8def\u7531\u5668\u7684\u7a83\u542c\u653b\u51fb\uff0c\u4e14\u9762\u79ef\u548c\u6027\u80fd\u5f00\u9500\u53ef\u5ffd\u7565\u3002\u4e0e\u4f20\u7edf\u52a0\u5bc6\u76f8\u6bd4\uff0cAONT\u57fa\u591a\u8def\u5f84\u8def\u7531\u53ef\u5c06\u5f00\u9500\u964d\u4f4e7.3\u500d", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u4fdd\u5bc6\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650SoC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u5f00\u9500\u7684NoC\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u673a\u5bc6\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2601.12415", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12415", "abs": "https://arxiv.org/abs/2601.12415", "authors": ["Wang Zixian"], "title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF", "comment": null, "summary": "Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6b63\u4ea4\u5316\u7b56\u7565\u4f18\u5316\uff08OPO\uff09\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u7684\u91c7\u6837\u51e0\u4f55\u4e0e\u4f18\u5316\u51e0\u4f55\u89e3\u8026\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfKL\u6563\u5ea6\u65b9\u6cd5\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u548c\u68af\u5ea6\u6d88\u5931\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982PPO\u3001DPO\u3001IPO\uff09\u901a\u5e38\u5c06\u91c7\u6837\u51e0\u4f55\u548c\u4f18\u5316\u51e0\u4f55\u8fd9\u4e24\u4e2a\u57fa\u672c\u8bbe\u8ba1\u9009\u62e9\u9690\u542b\u5730\u6df7\u4e3a\u4e00\u8c08\u3002KL\u6563\u5ea6\u5bf9\u65e0\u754c\u503c\u4fe1\u53f7\u65bd\u52a0\u6307\u6570\u60e9\u7f5a\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\u68af\u5ea6\u6d88\u5931\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u8fd9\u4e24\u4e2a\u51e0\u4f55\u7ef4\u5ea6\u5e76\u63d0\u4f9b\u7a33\u5b9a\u4f18\u5316\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5316\u7b56\u7565\u4f18\u5316\uff08OPO\uff09\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u5f62\u5f0f\u5316\u4e3a\u7b56\u7565\u80fd\u91cf\u4e0e\u76ee\u6807\u80fd\u91cf\u4e4b\u95f4\u5e7f\u4e49\u8ddd\u79bb\u7684\u6700\u5c0f\u5316\u3002\u4f7f\u7528\u03b1\u6563\u5ea6\u52a0\u6743\u7684\u91c7\u6837\u51e0\u4f55\u548cBregman\u6563\u5ea6\u52a0\u503c\u7684\u4f18\u5316\u51e0\u4f55\u3002\u5177\u4f53\u91c7\u7528\u03b1\u52a0\u6743\u91cd\u8981\u6027\u91c7\u6837\u548c\u5361\u65b9\u8bf1\u5bfc\u7684\u4e8c\u6b21\u6b63\u5219\u5316\uff08\u5728\u6bd4\u7387\u5750\u6807\u4e2d\uff09\uff0c\u4ea7\u751f\u5177\u6709\u7ebf\u6027\u68af\u5ea6\u52a8\u6001\u7684\u7b80\u5355\u4e14\u826f\u6761\u4ef6\u7684\u76ee\u6807\u51fd\u6570\u3002", "result": "OPO\u6846\u67b6\u80fd\u591f\u7ef4\u6301\u7a33\u5b9a\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u5cf0\u503c\u5bfb\u6c42\u884c\u4e3a\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8f83\u9ad8\u65f6\u4e5f\u80fd\u907f\u514d\u68af\u5ea6\u9971\u548c\u3002\u8be5\u6846\u67b6\u4e3a\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u7a33\u5065\u7684\u63a8\u7406\u5bfc\u5411\u8bad\u7ec3\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u89e3\u8026\u91c7\u6837\u51e0\u4f55\u548c\u4f18\u5316\u51e0\u4f55\uff0cOPO\u89e3\u51b3\u4e86\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u4f18\u5316\u6846\u67b6\u3002\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u5bf9\u9f50\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\u3002"}}
{"id": "2601.13681", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13681", "abs": "https://arxiv.org/abs/2601.13681", "authors": ["Felix Klement", "Alessandro Brighente", "Michele Polese", "Mauro Conti", "Stefan Katzenbeisser"], "title": "ORCA - An Automated Threat Analysis Pipeline for O-RAN Continuous Development", "comment": null, "summary": "The Open-Radio Access Network (O-RAN) integrates numerous software components in a cloud-like deployment, opening the radio access network to previously unconsidered security threats. With the ever-evolving threat landscape, integrating security practices through a DevSecOps approach is essential for fast and secure releases. Current vulnerability assessment practices often rely on manual, labor-intensive, and subjective investigations, leading to inconsistencies in the threat analysis. To mitigate these issues, we establish an automated pipeline that leverages Natural Language Processing (NLP) to minimize human intervention and associated biases. By mapping real-world vulnerabilities to predefined threat lists with a standardized input format, our approach is the first to enable iterative, quantitative, and efficient assessments, generating reliable threat scores for both individual vulnerabilities and entire system components within O-RAN. We illustrate the effectiveness of our framework through an example implementation for O-RAN, showcasing how continuous security testing can integrate into automated testing pipelines to address the unique security challenges of this paradigm shift in telecommunications.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528NLP\u6280\u672f\u5c06\u73b0\u5b9e\u6f0f\u6d1e\u6620\u5c04\u5230\u6807\u51c6\u5316\u5a01\u80c1\u6e05\u5355\uff0c\u4e3aO-RAN\u7cfb\u7edf\u63d0\u4f9b\u91cf\u5316\u5a01\u80c1\u8bc4\u5206", "motivation": "O-RAN\u91c7\u7528\u4e91\u5316\u90e8\u7f72\u6574\u5408\u4f17\u591a\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u5a01\u80c1\u3002\u5f53\u524d\u6f0f\u6d1e\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u4e3b\u89c2\u7684\u8c03\u67e5\uff0c\u5bfc\u81f4\u5a01\u80c1\u5206\u6790\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u5efa\u7acb\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u548c\u76f8\u5173\u504f\u89c1\u3002\u901a\u8fc7\u5c06\u73b0\u5b9e\u4e16\u754c\u6f0f\u6d1e\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u5a01\u80c1\u6e05\u5355\u5e76\u4f7f\u7528\u6807\u51c6\u5316\u8f93\u5165\u683c\u5f0f\uff0c\u5b9e\u73b0\u8fed\u4ee3\u3001\u5b9a\u91cf\u548c\u9ad8\u6548\u8bc4\u4f30", "result": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9O-RAN\u4e2d\u5355\u4e2a\u6f0f\u6d1e\u548c\u6574\u4e2a\u7cfb\u7edf\u7ec4\u4ef6\u7684\u53ef\u9760\u5a01\u80c1\u8bc4\u5206\uff0c\u901a\u8fc7O-RAN\u793a\u4f8b\u5b9e\u65bd\u5c55\u793a\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u53ef\u5c06\u6301\u7eed\u5b89\u5168\u6d4b\u8bd5\u96c6\u6210\u5230\u81ea\u52a8\u5316\u6d4b\u8bd5\u6d41\u6c34\u7ebf\u4e2d", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6846\u67b6\u80fd\u591f\u89e3\u51b3O-RAN\u72ec\u7279\u5b89\u5168\u6311\u6218\uff0c\u901a\u8fc7DevSecOps\u65b9\u6cd5\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u53d1\u5e03\uff0c\u4e3a\u7535\u4fe1\u8303\u5f0f\u8f6c\u53d8\u63d0\u4f9b\u6709\u6548\u7684\u5b89\u5168\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13757", "categories": ["cs.CR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.13757", "abs": "https://arxiv.org/abs/2601.13757", "authors": ["Ekleen Kaur"], "title": "The Limits of Conditional Volatility: Assessing Cryptocurrency VaR under EWMA and IGARCH Models", "comment": "Accepted and going to be presented at ICICT London 2026. https://icict.co.uk/ All ICICT 2026 presented papers will be published in conference proceedings by Springer LNNS. ISSN: 2367-3370, Series: https://www.springer.com/series/15179", "summary": "The application of the standard static Geometric Brownian Motion (GBM) model for cryptocurrency risk management resulted in a systemic failure, evidenced by a 80.67% chance of loss in the 5% value-at-risk benchmark. This study addresses a critical literature gap by comparatively testing three conditional volatility models the EWMA/IGARCH baseline, an IGARCH model augmented with explicit mean reversion (IGARCH + MR), and a modified EGARCH-style asymmetric shock model within a correlated Monte Carlo VaR framework. Crucially, the analysis is applied specifically to high-beta altcoins (XRP, SOL, ADA), an asset class largely neglected by mainstream GARCH literature. Our results demonstrate that imposing stationarity (IGARCH + MR) drastically underestimates downside risk (5 percent value-at-risk reduced by 50%), while the asymmetric model (Model 3) leads to severe over-penalization. The EWMA/IGARCH baseline, characterized by infinite volatility persistence (alpha + beta = 1), provided the only robust conditional volatility estimate. This finding constitutes a formal rejection of the conventional financial hypotheses of volatility mean reversion and the asymmetric leverage effect in the altcoin asset class, establishing that non-stationary frameworks are a prerequisite for regulatory-grade risk modeling in this domain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cd\u6761\u4ef6\u6ce2\u52a8\u7387\u6a21\u578b\u5728\u52a0\u5bc6\u8d27\u5e01\u98ce\u9669\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u4f20\u7edf\u91d1\u878d\u7684\u6ce2\u52a8\u7387\u5747\u503c\u56de\u5f52\u548c\u6760\u6746\u6548\u5e94\u5047\u8bbe\u5728altcoin\u8d44\u4ea7\u7c7b\u522b\u4e2d\u4e0d\u6210\u7acb\uff0c\u975e\u5e73\u7a33\u6846\u67b6\u662f\u8be5\u9886\u57df\u76d1\u7ba1\u7ea7\u98ce\u9669\u5efa\u6a21\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u6807\u51c6\u9759\u6001\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u6a21\u578b\u5728\u52a0\u5bc6\u8d27\u5e01\u98ce\u9669\u7ba1\u7406\u4e2d\u5bfc\u81f4\u7cfb\u7edf\u6027\u5931\u8d25\uff085% VaR\u57fa\u51c6\u4e0b80.67%\u7684\u635f\u5931\u6982\u7387\uff09\uff0c\u800c\u9ad8beta altcoin\uff08XRP\u3001SOL\u3001ADA\uff09\u8fd9\u4e00\u8d44\u4ea7\u7c7b\u522b\u5728\u4e3b\u6d41GARCH\u6587\u732e\u4e2d\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u5173\u952e\u6587\u732e\u7a7a\u767d\u3002", "method": "\u5728\u76f8\u5173\u8499\u7279\u5361\u6d1bVaR\u6846\u67b6\u4e2d\u6bd4\u8f83\u6d4b\u8bd5\u4e09\u79cd\u6761\u4ef6\u6ce2\u52a8\u7387\u6a21\u578b\uff1aEWMA/IGARCH\u57fa\u7ebf\u6a21\u578b\u3001\u589e\u52a0\u663e\u5f0f\u5747\u503c\u56de\u5f52\u7684IGARCH\u6a21\u578b\uff08IGARCH + MR\uff09\u3001\u4ee5\u53ca\u6539\u8fdb\u7684EGARCH\u98ce\u683c\u4e0d\u5bf9\u79f0\u51b2\u51fb\u6a21\u578b\uff0c\u4e13\u95e8\u5e94\u7528\u4e8e\u9ad8beta altcoin\u3002", "result": "\u65bd\u52a0\u5e73\u7a33\u6027\uff08IGARCH + MR\uff09\u4e25\u91cd\u4f4e\u4f30\u4e86\u4e0b\u884c\u98ce\u9669\uff085% VaR\u51cf\u5c1150%\uff09\uff0c\u800c\u4e0d\u5bf9\u79f0\u6a21\u578b\uff08\u6a21\u578b3\uff09\u5bfc\u81f4\u8fc7\u5ea6\u60e9\u7f5a\u3002\u53ea\u6709\u5177\u6709\u65e0\u9650\u6ce2\u52a8\u7387\u6301\u7eed\u6027\uff08alpha + beta = 1\uff09\u7684EWMA/IGARCH\u57fa\u7ebf\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u6761\u4ef6\u6ce2\u52a8\u7387\u4f30\u8ba1\u3002", "conclusion": "\u6b63\u5f0f\u62d2\u7edd\u4e86\u4f20\u7edf\u91d1\u878d\u4e2d\u6ce2\u52a8\u7387\u5747\u503c\u56de\u5f52\u548c\u4e0d\u5bf9\u79f0\u6760\u6746\u6548\u5e94\u7684\u5047\u8bbe\uff0c\u786e\u7acb\u4e86\u975e\u5e73\u7a33\u6846\u67b6\u662faltcoin\u8d44\u4ea7\u7c7b\u522b\u4e2d\u76d1\u7ba1\u7ea7\u98ce\u9669\u5efa\u6a21\u7684\u5fc5\u8981\u524d\u63d0\u6761\u4ef6\u3002"}}
{"id": "2601.12467", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12467", "abs": "https://arxiv.org/abs/2601.12467", "authors": ["Saurish Nagrath"], "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting", "comment": "6 pages, 2 figures, 3 tables", "summary": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528CNN\u63d0\u53d6\u5c40\u90e8\u65f6\u95f4\u52a8\u6001\u7279\u5f81\u5e76\u751f\u6210\u8865\u4e01\u7ea7token\u5d4c\u5165\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528Transformer\u7f16\u7801\u5668\u5efa\u6a21\u8865\u4e01\u95f4\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u9884\u6d4b\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4ece\u539f\u59cb\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u8f93\u5165\u8868\u793a\u7684\u8d28\u91cf\u548c\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u65f6\u95f4\u8868\u793a\u5b66\u4e60\u548c\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u8026\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\uff1a1) \u5c40\u90e8\u65f6\u95f4\u8868\u793a\u5b66\u4e60\u9636\u6bb5\uff1aCNN\u5728\u56fa\u5b9a\u957f\u5ea6\u65f6\u95f4\u8865\u4e01\u4e0a\u64cd\u4f5c\uff0c\u63d0\u53d6\u77ed\u7a0b\u65f6\u95f4\u52a8\u6001\u548c\u975e\u7ebf\u6027\u7279\u5f81\u4ea4\u4e92\uff0c\u751f\u6210\u7d27\u51d1\u7684\u8865\u4e01\u7ea7token\u5d4c\u5165\uff1b\u4f7f\u7528token\u7ea7\u81ea\u6ce8\u610f\u529b\u7ec6\u5316\u8fd9\u4e9b\u5d4c\u5165\uff1b2) \u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\u9636\u6bb5\uff1aTransformer\u7f16\u7801\u5668\u5904\u7406token\u5e8f\u5217\uff0c\u5efa\u6a21\u8865\u4e01\u95f4\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u6bcf\u4e2a\u8865\u4e01\u7684\u9884\u6d4b\u3002", "result": "\u5728\u5177\u6709\u53d7\u63a7\u9759\u6001\u548c\u52a8\u6001\u56e0\u7d20\u7684\u5408\u6210\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u8865\u4e01\u7684token\u5316\u7b56\u7565\u76f8\u6bd4\u5377\u79ef\u548c\u57fa\u4e8e\u8865\u4e01\u7684Transformer\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u65f6\u95f4\u8868\u793a\u7684\u91cd\u8981\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c06\u5c40\u90e8\u65f6\u95f4\u7f16\u7801\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5168\u5c40\u5efa\u6a21\u89e3\u8026\u80fd\u591f\u4ea7\u751f\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
{"id": "2601.13826", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13826", "abs": "https://arxiv.org/abs/2601.13826", "authors": ["Huadi Zheng", "Li Cheng", "Yan Ding"], "title": "MirageNet:A Secure, Efficient, and Scalable On-Device Model Protection in Heterogeneous TEE and GPU System", "comment": "13 pages", "summary": "As edge devices gain stronger computing power, deploying high-performance DNN models on untrusted hardware has become a practical approach to cut inference latency and protect user data privacy. Given high model training costs and user experience requirements, balancing model privacy and low runtime overhead is critical. TEEs offer a viable defense, and prior work has proposed heterogeneous GPU-TEE inference frameworks via parameter obfuscation to balance efficiency and confidentiality. However, recent studies find partial obfuscation defenses ineffective, while robust schemes cause unacceptable latency. To resolve these issues, we propose ConvShatter, a novel obfuscation scheme that achieves low latency and high accuracy while preserving model confidentiality and integrity. It leverages convolution linearity to decompose kernels into critical and common ones, inject confounding decoys, and permute channel/kernel orders. Pre-deployment, it performs kernel decomposition, decoy injection and order obfuscation, storing minimal recovery parameters securely in the TEE. During inference, the TEE reconstructs outputs of obfuscated convolutional layers. Extensive experiments show ConvShatter substantially reduces latency overhead with strong security guarantees; versus comparable schemes, it cuts overhead by 16% relative to GroupCover while maintaining accuracy on par with the original model.", "AI": {"tldr": "ConvShatter\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5377\u79ef\u5c42\u6df7\u6dc6\u65b9\u6848\uff0c\u901a\u8fc7\u5728TEE\u4e2d\u5b89\u5168\u5b58\u50a8\u6700\u5c0f\u6062\u590d\u53c2\u6570\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u7cbe\u5ea6\u7684\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u663e\u8457\u964d\u4f4e\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u589e\u5f3a\uff0c\u5728\u4e0d\u53ef\u4fe1\u786c\u4ef6\u4e0a\u90e8\u7f72\u9ad8\u6027\u80fdDNN\u6a21\u578b\u6210\u4e3a\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u5b9e\u7528\u65b9\u6848\u3002\u73b0\u6709\u57fa\u4e8e\u90e8\u5206\u6df7\u6dc6\u7684TEE\u9632\u5fa1\u65b9\u6848\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u800c\u9c81\u68d2\u65b9\u6848\u5219\u5bfc\u81f4\u4e0d\u53ef\u63a5\u53d7\u7684\u5ef6\u8fdf\uff0c\u9700\u8981\u5728\u6a21\u578b\u9690\u79c1\u548c\u8fd0\u884c\u65f6\u5f00\u9500\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u5229\u7528\u5377\u79ef\u7ebf\u6027\u7279\u6027\u5c06\u5377\u79ef\u6838\u5206\u89e3\u4e3a\u5173\u952e\u6838\u548c\u516c\u5171\u6838\uff0c\u6ce8\u5165\u6df7\u6dc6\u8bf1\u9975\uff0c\u5e76\u5bf9\u901a\u9053/\u5377\u79ef\u6838\u987a\u5e8f\u8fdb\u884c\u7f6e\u6362\u3002\u90e8\u7f72\u524d\u6267\u884c\u6838\u5206\u89e3\u3001\u8bf1\u9975\u6ce8\u5165\u548c\u987a\u5e8f\u6df7\u6dc6\uff0c\u5c06\u6700\u5c0f\u6062\u590d\u53c2\u6570\u5b89\u5168\u5b58\u50a8\u5728TEE\u4e2d\u3002\u63a8\u7406\u65f6\uff0cTEE\u91cd\u6784\u6df7\u6dc6\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u3002", "result": "ConvShatter\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u5b89\u5168\u4fdd\u969c\u3002\u76f8\u6bd4\u53ef\u6bd4\u65b9\u6848\uff0c\u76f8\u5bf9\u4e8eGroupCover\u51cf\u5c11\u4e8616%\u7684\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002", "conclusion": "ConvShatter\u901a\u8fc7\u521b\u65b0\u7684\u5377\u79ef\u6838\u6df7\u6dc6\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\u4e0e\u8fd0\u884c\u65f6\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b89\u5168DNN\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12502", "categories": ["cs.LG", "math.NA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.12502", "abs": "https://arxiv.org/abs/2601.12502", "authors": ["Mikhail Gennadievich Belov", "Victor Victorovich Dubov", "Vadim Konstantinovich Ivanov", "Alexander Yurievich Maslov", "Olga Vladimirovna Proshina", "Vladislav Gennadievich Malyshkin"], "title": "Semidefinite Programming for Quantum Channel Learning", "comment": null, "summary": "The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u4ece\u7ecf\u5178\u6570\u636e\u91cd\u5efa\u91cf\u5b50\u901a\u9053\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4fdd\u771f\u5ea6\u53ef\u8868\u793a\u4e3a\u4e24\u4e2a\u4e8c\u6b21\u578b\u6bd4\u503c\u7684\u60c5\u51b5\uff0c\u5e76\u53d1\u73b0\u91cd\u5efa\u7684\u91cf\u5b50\u901a\u9053\u901a\u5e38\u5177\u6709\u8f83\u4f4e\u7684Kraus\u79e9\u3002", "motivation": "\u7814\u7a76\u4ece\u7ecf\u5178\u5b9e\u9a8c\u6570\u636e\u91cd\u5efa\u91cf\u5b50\u901a\u9053\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4fdd\u771f\u5ea6\u53ef\u8868\u793a\u4e3a\u4e24\u4e2a\u4e8c\u6b21\u578b\u6bd4\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u6570\u503c\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u65b9\u6cd5\uff0c\u5c06\u4fdd\u771f\u5ea6\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5173\u4e8eChoi\u77e9\u9635\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u591a\u79cd\u5546\u4e1aSDP\u6c42\u89e3\u5668\u8fdb\u884c\u6570\u503c\u6c42\u89e3\uff0c\u5e76\u5206\u6790\u4e86\u91cd\u5efa\u91cf\u5b50\u901a\u9053\u7684Kraus\u79e9\u7279\u6027\u3002", "result": "\u6d4b\u8bd5\u4e86\u591a\u79cd\u5546\u4e1aSDP\u6c42\u89e3\u5668\uff0c\u5747\u80fd\u6210\u529f\u91cd\u5efa\u4e0d\u540c\u5f62\u5f0f\u7684\u91cf\u5b50\u901a\u9053\uff1b\u91cd\u5efa\u7684\u91cf\u5b50\u901a\u9053\u901a\u5e38\u5177\u6709\u8f83\u4f4e\u7684Kraus\u79e9\uff08\u901a\u5e38\u5c0f\u4e8e\u6700\u5927\u53ef\u80fd\u503c\u7684\u51e0\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u8868\u660e\u5c11\u91cfKraus\u79e9\u7684\u91cf\u5b50\u901a\u9053\u8db3\u4ee5\u63cf\u8ff0\u5b9e\u9a8c\u89c2\u6d4b\u6570\u636e\u3002", "conclusion": "SDP\u4e3a\u91cf\u5b50\u901a\u9053\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u91cd\u5efa\u7684\u91cf\u5b50\u901a\u9053\u901a\u5e38\u5177\u6709\u4f4eKraus\u79e9\uff0c\u8fd9\u4e3a\u57fa\u4e8e\u91cf\u5b50\u901a\u9053\u53d8\u6362\u7684\u7ecf\u5178\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6295\u5f71\u7b97\u5b50\u7684\u91cd\u5efa\u95ee\u9898\u3002"}}
{"id": "2601.13840", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13840", "abs": "https://arxiv.org/abs/2601.13840", "authors": ["Haoyu Shen", "Wen Yin", "Zhaoxia Yin", "Wan-Li Lyu", "Xinpeng Zhang"], "title": "Robust Reversible Watermarking in Encrypted Images Based on Dual-MSBs Spiral Embedding", "comment": "10 pages, 8 figures", "summary": "Robust reversible watermarking in encrypted images (RRWEI) faces an inherent challenge in simultaneously achieving robustness, reversibility, and content privacy under severely constrained embedding capacity. Existing RRWEI schemes often exhibit limited robustness against noise, lossy compression, and cropping attacks due to insufficient redundancy in the encrypted domain. To address this challenge, this paper proposes a novel RRWEI framework that couples dual most significant bit-plane (dual-MSBs) embedding with spatial redundancy and error-correcting coding. By compressing prediction-error bit-planes, sufficient embedding space and auxiliary information for lossless reconstruction are reserved. The dual-MSBs are further reorganized using a spiral embedding strategy to distribute multiple redundant watermark copies across spatially dispersed regions, enhancing robustness against both noise and spatial loss.Experimental results on standard test images demonstrate that the proposed method consistently outperforms under evaluated settings robustness against Gaussian noise, JPEG compression, and diverse cropping attacks, while maintaining perfect reversibility and high embedding capacity. Compared with state-of-the-art RRWEI schemes, the proposed framework achieves substantially lower bit-error rates and more stable performance under a wide range of attack scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53cc\u6700\u9ad8\u6709\u6548\u4f4d\u5e73\u9762\u5d4c\u5165\u3001\u7a7a\u95f4\u5197\u4f59\u548c\u7ea0\u9519\u7f16\u7801\u7684\u9c81\u68d2\u53ef\u9006\u52a0\u5bc6\u56fe\u50cf\u6c34\u5370\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u53ef\u9006\u6027\u548c\u9ad8\u5d4c\u5165\u5bb9\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u566a\u58f0\u3001JPEG\u538b\u7f29\u548c\u88c1\u526a\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u9c81\u68d2\u53ef\u9006\u52a0\u5bc6\u56fe\u50cf\u6c34\u5370\u65b9\u6848\u5728\u52a0\u5bc6\u57df\u4e2d\u5197\u4f59\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u566a\u58f0\u3001\u6709\u635f\u538b\u7f29\u548c\u88c1\u526a\u653b\u51fb\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5728\u4e25\u683c\u53d7\u9650\u7684\u5d4c\u5165\u5bb9\u91cf\u4e0b\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\u3001\u53ef\u9006\u6027\u548c\u5185\u5bb9\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u53cc\u6700\u9ad8\u6709\u6548\u4f4d\u5e73\u9762\u5d4c\u5165\u4e0e\u7a7a\u95f4\u5197\u4f59\u548c\u7ea0\u9519\u7f16\u7801\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a1) \u538b\u7f29\u9884\u6d4b\u8bef\u5dee\u4f4d\u5e73\u9762\u4ee5\u9884\u7559\u8db3\u591f\u7684\u5d4c\u5165\u7a7a\u95f4\u548c\u8f85\u52a9\u4fe1\u606f\u7528\u4e8e\u65e0\u635f\u91cd\u5efa\uff1b2) \u4f7f\u7528\u87ba\u65cb\u5d4c\u5165\u7b56\u7565\u91cd\u7ec4\u53cc\u6700\u9ad8\u6709\u6548\u4f4d\u5e73\u9762\uff0c\u5c06\u591a\u4e2a\u5197\u4f59\u6c34\u5370\u526f\u672c\u5206\u5e03\u5728\u7a7a\u95f4\u5206\u6563\u533a\u57df\uff0c\u589e\u5f3a\u5bf9\u566a\u58f0\u548c\u7a7a\u95f4\u4e22\u5931\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u9ad8\u65af\u566a\u58f0\u3001JPEG\u538b\u7f29\u548c\u591a\u79cd\u88c1\u526a\u653b\u51fb\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u7f8e\u7684\u53ef\u9006\u6027\u548c\u9ad8\u5d4c\u5165\u5bb9\u91cf\u3002\u4e0e\u6700\u5148\u8fdb\u7684RRWEI\u65b9\u6848\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u5e7f\u6cdb\u7684\u653b\u51fb\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u8bef\u7801\u7387\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u53cc\u6700\u9ad8\u6709\u6548\u4f4d\u5e73\u9762\u5d4c\u5165\u4e0e\u7a7a\u95f4\u5197\u4f59\u548c\u7ea0\u9519\u7f16\u7801\u7684\u8026\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9c81\u68d2\u53ef\u9006\u52a0\u5bc6\u56fe\u50cf\u6c34\u5370\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u53ef\u9006\u6027\u548c\u9ad8\u5bb9\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u4e3a\u52a0\u5bc6\u57df\u4e2d\u7684\u9c81\u68d2\u6c34\u5370\u5d4c\u5165\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.12518", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12518", "abs": "https://arxiv.org/abs/2601.12518", "authors": ["Nuoya Xiong", "Aarti Singh"], "title": "Cooperative Multi-agent RL with Communication Constraints", "comment": "33 pages", "summary": "Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\\varepsilon$-Nash equilibrium in potential games with only $O(\\varepsilon^{-3/4})$ communication rounds and $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u7b56\u7565\u9884\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u9884\u6d4b\u7b56\u7565\u66f4\u65b0\u6765\u51cf\u5c11\u901a\u4fe1\u9700\u6c42\uff0c\u5728\u6f5c\u5728\u6e38\u620f\u4e2d\u4ec5\u9700O(\u03b5^{-3/4})\u901a\u4fe1\u8f6e\u6b21\u5373\u53ef\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861", "motivation": "\u4f20\u7edf\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u9891\u7e41\u8bbf\u95ee\u5168\u5c40\u4fe1\u606f\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u901a\u4fe1\u6210\u672c\u9ad8\u3002\u5f53\u901a\u4fe1\u53d7\u9650\u65f6\uff0c\u667a\u80fd\u4f53\u4f9d\u8d56\u8fc7\u65f6\u4fe1\u606f\u66f4\u65b0\u7b56\u7565\uff0c\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u5728\u901a\u4fe1\u53d7\u9650\u65f6\u4e0d\u7a33\u5b9a", "method": "\u63d0\u51fa\u57fa\u7b56\u7565\u9884\u6d4b\u6280\u672f\uff0c\u5229\u7528\u65e7\u68af\u5ea6\u9884\u6d4b\u7b56\u7565\u66f4\u65b0\uff0c\u4e3a\u4e00\u7cfb\u5217\u57fa\u7b56\u7565\u6536\u96c6\u6837\u672c\uff0c\u51cf\u5c11\u57fa\u7b56\u7565\u4e0e\u5f53\u524d\u7b56\u7565\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u65b9\u6cd5\u53ef\u5728\u5355\u6b21\u901a\u4fe1\u8f6e\u6b21\u5185\u6536\u96c6\u9884\u6d4b\u57fa\u7b56\u7565\u7684\u6837\u672c", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u7b97\u6cd5\u5728\u6f5c\u5728\u6e38\u620f\u4e2d\u4ee5O(\u03b5^{-3/4})\u901a\u4fe1\u8f6e\u6b21\u548cO(poly(max_i|A_i|)\u03b5^{-11/4})\u6837\u672c\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff0c\u6539\u8fdb\u4e86\u901a\u4fe1\u6210\u672c\u548c\u6837\u672c\u590d\u6742\u5ea6\u3002\u5728\u4e00\u822c\u9a6c\u5c14\u53ef\u592b\u534f\u4f5c\u6e38\u620f\u4e2d\u627e\u5230\u667a\u80fd\u4f53\u5c40\u90e8\u6700\u4f18", "conclusion": "\u57fa\u7b56\u7565\u9884\u6d4b\u6280\u672f\u80fd\u6709\u6548\u51cf\u5c11\u901a\u4fe1\u9700\u6c42\uff0c\u5728\u6a21\u62df\u6e38\u620f\u548c\u590d\u6742\u73af\u5883\u7684MAPPO\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u8f6e\u6b21\u9700\u6c42"}}
{"id": "2601.13864", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13864", "abs": "https://arxiv.org/abs/2601.13864", "authors": ["Qirui Chen", "Jingxian Shuai", "Shuangwu Chen", "Shenghao Ye", "Zijian Wen", "Xufei Su", "Jie Jin", "Jiangming Li", "Jun Chen", "Xiaobin Tan", "Jian Yang"], "title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation", "comment": null, "summary": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.", "AI": {"tldr": "HardSecBench\uff1a\u9996\u4e2a\u4e13\u6ce8\u4e8e\u786c\u4ef6\u548c\u56fa\u4ef6\u4ee3\u7801\u5b89\u5168\u6027\u7684LLM\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b924\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6Verilog RTL\u548cC\u8bed\u8a00\uff0c\u6d89\u53ca76\u4e2a\u786c\u4ef6\u76f8\u5173CWE\u6f0f\u6d1e\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5b89\u5168\u6027\u95ee\u9898\u3002\u529f\u80fd\u6b63\u786e\u7684\u4ee3\u7801\u53ef\u80fd\u5d4c\u5165\u5b89\u5168\u6f0f\u6d1e\uff0c\u90e8\u7f72\u540e\u4f1a\u9020\u6210\u707e\u96be\u6027\u540e\u679c\u3002\u8fd9\u4e00\u5173\u952e\u7814\u7a76\u7a7a\u767d\u4fc3\u4f7f\u7814\u7a76\u8005\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u73b0\u5b9e\u89c4\u8303\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faHardSecBench\u57fa\u51c6\uff0c\u5305\u542b924\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6Verilog RTL\u548c\u56fa\u4ef6\u7ea7C\u8bed\u8a00\uff0c\u6d89\u53ca76\u4e2a\u786c\u4ef6\u76f8\u5173CWE\u6761\u76ee\u3002\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b\u7ed3\u6784\u5316\u89c4\u8303\u3001\u5b89\u5168\u53c2\u8003\u5b9e\u73b0\u548c\u53ef\u6267\u884c\u6d4b\u8bd5\u3002\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\uff0c\u5c06\u5408\u6210\u4e0e\u9a8c\u8bc1\u89e3\u8026\uff0c\u57fa\u4e8e\u6267\u884c\u8bc1\u636e\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u591a\u4e2aLLM\u5728\u786c\u4ef6\u548c\u56fa\u4ef6\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u901a\u5e38\u80fd\u6ee1\u8db3\u529f\u80fd\u9700\u6c42\u4f46\u4ecd\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002\u5b89\u5168\u7ed3\u679c\u968f\u63d0\u793a\u7b56\u7565\u800c\u53d8\u5316\uff0c\u51f8\u663e\u4e86LLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u9762\u4e34\u7684\u7d27\u8feb\u6311\u6218\u3002", "conclusion": "HardSecBench\u586b\u8865\u4e86LLM\u751f\u6210\u786c\u4ef6\u4ee3\u7801\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b89\u5168\u7f16\u7801\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765LLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u548c\u57fa\u51c6\u5de5\u5177\u3002"}}
{"id": "2601.11612", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11612", "abs": "https://arxiv.org/abs/2601.11612", "authors": ["Arnav S. Sonavane"], "title": "Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study", "comment": "11 pages, 4 figures, 9 tables", "summary": "We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT", "code_url": "https://github.com/w2sg-arnav/HierarchicalViT", "AI": {"tldr": "\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SimCLR\uff09\u4ec5\u97003000\u5f20\u65e0\u6807\u7b7e\u519c\u4e1a\u56fe\u50cf\u5373\u53ef\u5e26\u6765+4.57%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8d85\u8fc7\u4e86\u5c42\u6b21\u67b6\u6784\u8bbe\u8ba1\u5e26\u6765\u7684+3.70%\u589e\u76ca\uff0c\u4e14\u8be5\u4f18\u52bf\u4e0e\u67b6\u6784\u65e0\u5173\u3002", "motivation": "\u7814\u7a76\u9886\u57df\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5982\u4f55\u901a\u8fc7\u65e0\u6807\u7b7e\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6bd4\u8f83\u67b6\u6784\u8bbe\u8ba1\u4e0e\u6570\u636e\u6536\u96c6\u7684\u76f8\u5bf9\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528SimCLR\u57283000\u5f20\u65e0\u6807\u7b7e\u519c\u4e1a\u56fe\u50cf\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5e94\u7528\u4e8e\u591a\u79cd\u89c6\u89c9Transformer\u67b6\u6784\uff08HierarchicalViT\u3001Swin-Base\u3001ViT-Base\uff09\u3002\u63d0\u51faHierarchicalViT\uff08HVT\uff09\uff0c\u4e00\u79cdSwin\u98ce\u683c\u7684\u5c42\u6b21Transformer\uff0c\u5728\u4e09\u4e2a\u519c\u4e1a\u75c5\u5bb3\u6570\u636e\u96c6\uff08Cotton Leaf Disease\u3001PlantVillage\u3001PlantDoc\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u4e86\u6821\u51c6\u5206\u6790\u3002", "result": "SimCLR\u9884\u8bad\u7ec3\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1aHVT +4.57%\uff0cSwin-Base +4.08%\uff0cViT-Base +4.20%\u3002\u5728\u76f8\u540c\u53c2\u6570\u91cf\u4e0b\uff0cHVT-Base\uff0878M\uff09\u8fbe\u523088.91%\uff0c\u4f18\u4e8eSwin-Base\uff0888M\uff09\u768487.23%\uff08+1.68%\uff09\u3002HVT\u7684\u6821\u51c6\u8bef\u5dee\u4e3a3.56% ECE\uff0c\u7ecf\u8fc7\u6e29\u5ea6\u7f29\u653e\u540e\u964d\u81f31.52%\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6bd4\u67b6\u6784\u8bbe\u8ba1\u66f4\u4e3a\u91cd\u8981\uff0c\u5b9e\u8df5\u8005\u5e94\u4f18\u5148\u8003\u8651\u9886\u57df\u6570\u636e\u6536\u96c6\u800c\u975e\u67b6\u6784\u9009\u62e9\u3002HierarchicalViT\u5728\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6821\u51c6\u7279\u6027\u3002"}}
{"id": "2601.11614", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.11614", "abs": "https://arxiv.org/abs/2601.11614", "authors": ["Jason Qiu"], "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning", "comment": "19 pages, 10 figures", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.", "AI": {"tldr": "\u63d0\u51fa3D TransUNet\u6846\u67b6\u4eceT1w MRI\u9884\u6d4b\u6269\u6563MRI\u6307\u6807\uff08FA\u548cMD\uff09\uff0c\u63d0\u5347\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u68c0\u6d4b\u51c6\u786e\u6027", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46T1w MRI\u53ea\u80fd\u68c0\u6d4b\u665a\u671f\u5b8f\u89c2\u53d8\u5316\uff0c\u6269\u6563MRI\u80fd\u68c0\u6d4b\u65e9\u671f\u5fae\u89c2\u5f02\u5e38\u4f46\u626b\u63cf\u65f6\u95f4\u957f\u3001\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e38\u89c4\u4f7f\u7528\u3002\u9700\u8981\u4ece\u5e38\u89c4T1w MRI\u4e2d\u63d0\u53d6\u6269\u6563MRI\u4fe1\u606f\u3002", "method": "\u5f00\u53d13D TransUNet\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4eceT1w MRI\u9884\u6d4b\u5206\u6570\u5404\u5411\u5f02\u6027\uff08FA\uff09\u548c\u5e73\u5747\u6269\u6563\u7387\uff08MD\uff09\u56fe\u3002\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6269\u6563\u6307\u6807\u56fe\uff0c\u5e76\u96c6\u6210\u5230\u591a\u6a21\u6001\u8bca\u65ad\u6a21\u578b\u4e2d\u3002", "result": "\u5408\u6210FA\u548cMD\u56fe\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u8d85\u8fc70.93\uff0c\u4e0e\u771f\u5b9e\u6269\u6563MRI\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570>0.94\u3002\u5728\u591a\u6a21\u6001\u8bca\u65ad\u6a21\u578b\u4e2d\uff0cAD\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53475%\uff0878.75%\u219283.75%\uff09\uff0c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u63d0\u534712.5%\u3002", "conclusion": "\u4ece\u5e38\u89c4T1w MRI\u53ef\u4ee5\u63a8\u65ad\u9ad8\u8d28\u91cf\u7684\u6269\u6563\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff0c\u5c06\u591a\u6a21\u6001\u6210\u50cf\u7684\u4f18\u52bf\u8f6c\u79fb\u5230\u65e0\u6cd5\u83b7\u53d6\u6269\u6563\u6570\u636e\u7684\u4e34\u5e8a\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u540c\u65f6\u4fdd\u7559\u4e92\u8865\u7684\u7ed3\u6784\u548c\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff0c\u6709\u671b\u63d0\u9ad8AD\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2601.12525", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.12525", "abs": "https://arxiv.org/abs/2601.12525", "authors": ["Nikolaj Tatti"], "title": "Approximating splits for decision trees quickly in sparse data streams", "comment": null, "summary": "Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + \u03b1)$ approximation when using conditional entropy in amortized $O(\u03b1^{-1}(1 + m\\log d) \\log \\log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + \u03b1)$ approximation in amortized $O(\u03b1^{-1} + m \\log d)$ time. Our approach is beneficial for sparse data where $m \\ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u51b3\u7b56\u6811\u6d41\u5f0f\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u4f18\u5316\u4fe1\u606f\u589e\u76ca\u6216\u57fa\u5c3c\u6307\u6570\uff0c\u5728\u7a00\u758f\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5206\u5272\u641c\u7d22", "motivation": "\u4f20\u7edf\u51b3\u7b56\u6811\u6d41\u5f0f\u5b66\u4e60\u7b97\u6cd5\u5728\u5bfb\u627e\u6700\u4f18\u5206\u5272\u65f6\u9700\u8981O(d)\u65f6\u95f4\uff08d\u4e3a\u7279\u5f81\u6570\uff09\uff0c\u5bf9\u4e8e\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u6570\u636e\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u52a0\u901f\u7a00\u758f\u6570\u636e\u4e0b\u7684\u5206\u5272\u641c\u7d22\u8fc7\u7a0b", "method": "\u63d0\u51fa\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5728\u6761\u4ef6\u71b5\u60c5\u51b5\u4e0b\u5b9e\u73b0(1+\u03b1)\u8fd1\u4f3c\uff0c\u644a\u9500\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(\u03b1^{-1}(1+m log d) log log n)\uff1b\u57fa\u5c3c\u6307\u6570\u60c5\u51b5\u4e0b\u5b9e\u73b0(1+\u03b1)\u8fd1\u4f3c\uff0c\u644a\u9500\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(\u03b1^{-1}+m log d)\uff0c\u5176\u4e2dm\u4e3a\u6570\u636e\u70b9\u4e2d1\u7684\u6570\u91cf", "result": "\u7b97\u6cd5\u5728\u7a00\u758f\u6570\u636e\uff08m\u226ad\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u591f\u9ad8\u6548\u627e\u5230\u8fd1\u4f3c\u6700\u4f18\u5206\u5272\uff0c\u901f\u5ea6\u5feb\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5b9e\u9645\u6027\u80fd\u4f18\u4e8e\u7406\u8bba\u8fd1\u4f3c\u4fdd\u8bc1", "conclusion": "\u9488\u5bf9\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u51b3\u7b56\u6811\u6d41\u5f0f\u5b66\u4e60\uff0c\u63d0\u51fa\u7684\u8fd1\u4f3c\u5206\u5272\u641c\u7d22\u7b97\u6cd5\u5728\u4fdd\u6301\u8fd1\u4f3c\u6700\u4f18\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a00\u758f\u6570\u636e\u573a\u666f"}}
{"id": "2601.11617", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11617", "abs": "https://arxiv.org/abs/2601.11617", "authors": ["Xu Wang", "Boyao Han", "Xiaojun Chen", "Ying Liu", "Ruihui Li"], "title": "PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM", "comment": null, "summary": "Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.", "AI": {"tldr": "PointSLAM++\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u7684RGB-D SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u7ea6\u675f\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u6e10\u8fdb\u4f4d\u59ff\u4f18\u5316\u51cf\u5c11\u6df1\u5ea6\u566a\u58f0\uff0c\u4f7f\u7528\u52a8\u6001\u795e\u7ecf\u8868\u793a\u56fe\u9002\u5e94\u573a\u666f\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea63D\u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u5f53\u524dSLAM\u65b9\u6cd5\u5728\u6df1\u5ea6\u566a\u58f0\u5b58\u5728\u65f6\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u7684\u4f4d\u59ff\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u5b9e\u65f63D\u91cd\u5efa\u5728\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. \u5206\u5c42\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\uff1a\u4fdd\u6301\u7ed3\u6784\u5173\u7cfb\u540c\u65f6\u751f\u6210\u9ad8\u65af\u57fa\u5143\u8fdb\u884c\u573a\u666f\u5efa\u56fe\uff1b2. \u6e10\u8fdb\u4f4d\u59ff\u4f18\u5316\uff1a\u51cf\u8f7b\u6df1\u5ea6\u4f20\u611f\u5668\u566a\u58f0\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff1b3. \u52a8\u6001\u795e\u7ecf\u8868\u793a\u56fe\uff1a\u6839\u636e\u5c40\u90e8\u51e0\u4f55\u590d\u6742\u5ea6\u8c03\u6574\u9ad8\u65af\u8282\u70b9\u5206\u5e03\uff0c\u5b9e\u65f6\u9002\u5e94\u590d\u6742\u573a\u666f\u7ec6\u8282\u3002", "result": "PointSLAM++\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e3DGS\u7684SLAM\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21AR\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "PointSLAM++\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u566a\u58f0\u4e0b\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f63D\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2601.13907", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13907", "abs": "https://arxiv.org/abs/2601.13907", "authors": ["Cosmin-Iulian Irimia"], "title": "Decentralized Infrastructure for Digital Notarizing, Signing and Sharing Files using Blockchain", "comment": "182 pages, PhD thesis", "summary": "Traditional paper-based document management has long posed challenges related to security, authenticity, and efficiency. Despite advances in digitalization, official documents remain vulnerable to forgery, loss, and unauthorized access. This thesis proposes a decentralized infrastructure for digital notarization, signing, and sharing of documents using blockchain technology. The research addresses key issues of transparency, immutability, and feasibility by defining system requirements, evaluating existing solutions, and proposing a novel architecture based on distributed systems.\n  By combining cryptographic techniques with decentralized storage, this research contributes to the development of a more secure and efficient framework for managing official documents. The findings highlight the potential of blockchain-based digital notarization to streamline bureaucratic processes, mitigate security risks, and enhance user trust in digital document management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u53bb\u4e2d\u5fc3\u5316\u6570\u5b57\u516c\u8bc1\u3001\u7b7e\u540d\u548c\u5171\u4eab\u6587\u6863\u57fa\u7840\u8bbe\u65bd\uff0c\u89e3\u51b3\u4f20\u7edf\u7eb8\u8d28\u548c\u6570\u5b57\u6587\u6863\u7684\u5b89\u5168\u3001\u771f\u5b9e\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7eb8\u8d28\u6587\u6863\u7ba1\u7406\u5b58\u5728\u5b89\u5168\u3001\u771f\u5b9e\u6027\u548c\u6548\u7387\u6311\u6218\uff0c\u5373\u4f7f\u6570\u5b57\u5316\u540e\uff0c\u5b98\u65b9\u6587\u6863\u4ecd\u6613\u53d7\u4f2a\u9020\u3001\u4e22\u5931\u548c\u672a\u6388\u6743\u8bbf\u95ee\u7684\u5a01\u80c1\u3002\u9700\u8981\u89e3\u51b3\u900f\u660e\u5ea6\u3001\u4e0d\u53ef\u7be1\u6539\u6027\u548c\u53ef\u884c\u6027\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u7cfb\u7edf\u9700\u6c42\uff0c\u8bc4\u4f30\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u5bc6\u7801\u5b66\u6280\u672f\u548c\u53bb\u4e2d\u5fc3\u5316\u5b58\u50a8\u3002", "result": "\u5f00\u53d1\u51fa\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u5b98\u65b9\u6587\u6863\u7ba1\u7406\u6846\u67b6\uff0c\u533a\u5757\u94fe\u6570\u5b57\u516c\u8bc1\u80fd\u591f\u7b80\u5316\u5b98\u50da\u6d41\u7a0b\u3001\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3001\u589e\u5f3a\u7528\u6237\u5bf9\u6570\u5b57\u6587\u6863\u7ba1\u7406\u7684\u4fe1\u4efb\u3002", "conclusion": "\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6570\u5b57\u516c\u8bc1\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u5b98\u65b9\u6587\u6863\u7ba1\u7406\u63d0\u4f9b\u900f\u660e\u3001\u4e0d\u53ef\u7be1\u6539\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u6587\u6863\u7ba1\u7406\u6846\u67b6\u53d1\u5c55\u3002"}}
{"id": "2601.11627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11627", "abs": "https://arxiv.org/abs/2601.11627", "authors": ["Hassan Ugail", "Jan Ritch-Frel", "Irina Matuzava"], "title": "Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings", "comment": null, "summary": "Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u7684\u5386\u53f2\u7d20\u63cf\u8ba4\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u5728\u7a00\u7f3a\u6570\u636e\u4e0b\u5b9e\u73b0\u827a\u672f\u5bb6\u4f5c\u54c1\u9a8c\u8bc1\uff0c\u5728900\u6b21\u6d4b\u8bd5\u4e2d\u8fbe\u523083.3%\u771f\u63a5\u53d7\u7387\u548c9.5%\u5047\u63a5\u53d7\u7387\u3002", "motivation": "\u89e3\u51b3\u6587\u5316\u9057\u4ea7\u9886\u57df\u7eb8\u8d28\u4f5c\u54c1\u8ba4\u8bc1\u548c\u5f52\u5c5e\u7684\u6301\u4e45\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u53c2\u8003\u8bed\u6599\u5e93\u5c0f\u3001\u98ce\u683c\u7ebf\u7d22\u4e3b\u8981\u901a\u8fc7\u7ebf\u6761\u548c\u6709\u9650\u8272\u8c03\u53d8\u5316\u8868\u8fbe\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8865\u5145\u4f20\u7edf\u9274\u8d4f\u65b9\u6cd5\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u8bad\u7ec3\u5341\u4e2a\u827a\u672f\u5bb6\u7279\u5b9a\u7684\u9a8c\u8bc1\u5668\u3002\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u5411\u91cf\uff0c\u5305\u62ec\u5085\u91cc\u53f6\u57df\u80fd\u91cf\u3001\u9999\u519c\u71b5\u3001\u5168\u5c40\u5bf9\u6bd4\u5ea6\u3001\u57fa\u4e8eGLCM\u7684\u540c\u8d28\u6027\u4ee5\u53ca\u76d2\u8ba1\u6570\u4f30\u8ba1\u7684\u5206\u5f62\u590d\u6742\u5ea6\u3002\u6570\u636e\u6765\u81ea\u591a\u4e2a\u535a\u7269\u9986\u7684\u8ba4\u8bc1\u7d20\u63cf\u6536\u85cf\u3002", "result": "\u5728900\u6b21\u9a8c\u8bc1\u51b3\u7b56\uff0890\u6b21\u771f\u5b9e\u8bd5\u9a8c\u548c810\u6b21\u5192\u540d\u8bd5\u9a8c\uff09\u4e2d\uff0c\u7cfb\u7edf\u5728\u9009\u5b9a\u64cd\u4f5c\u70b9\u8fbe\u523083.3%\u7684\u771f\u63a5\u53d7\u7387\u548c9.5%\u7684\u5047\u63a5\u53d7\u7387\u3002\u6027\u80fd\u56e0\u827a\u672f\u5bb6\u800c\u5f02\uff0c\u90e8\u5206\u9a8c\u8bc1\u5668\u5047\u63a5\u53d7\u7387\u63a5\u8fd1\u96f6\uff0c\u800c\u5176\u4ed6\u9a8c\u8bc1\u5668\u5b58\u5728\u6df7\u6dc6\u3002\u9519\u8bef\u63a5\u53d7\u5448\u73b0\u7ed3\u6784\u5316\u6a21\u5f0f\uff0c\u4e0e\u98ce\u683c\u63a5\u8fd1\u6027\u548c\u5171\u4eab\u7ed8\u753b\u60ef\u4f8b\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u8865\u5145\u800c\u975e\u53d6\u4ee3\u4f20\u7edf\u9274\u8d4f\uff0c\u4e3a\u5386\u53f2\u7d20\u63cf\u5f52\u5c5e\u4e2d\u5e38\u89c1\u7684\u6570\u636e\u7a00\u7f3a\u73af\u5883\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc1\u636e\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u98ce\u683c\u63a5\u8fd1\u6027\u5bfc\u81f4\u7684\u7cfb\u7edf\u8bef\u5dee\uff0c\u540c\u65f6\u63d0\u793a\u9700\u8981\u66f4\u597d\u5730\u63a7\u5236\u6570\u5b57\u5316\u4f2a\u5f71\u548c\u9608\u503c\u6821\u51c6\u3002"}}
{"id": "2601.13981", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13981", "abs": "https://arxiv.org/abs/2601.13981", "authors": ["Yilin Tang", "Yu Wang", "Lanlan Qiu", "Wenchang Gao", "Yunfei Ma", "Baicheng Chen", "Tianxing He"], "title": "VirtualCrime: Evaluating Criminal Potential of Large Language Models via Sandbox Simulation", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities in multi-step decision-making, planning and actions, and are increasingly integrated into various real-world applications. It is concerning whether their strong problem-solving abilities may be misused for crimes. To address this gap, we propose VirtualCrime, a sandbox simulation framework based on a three-agent system to evaluate the criminal capabilities of models. Specifically, this framework consists of an attacker agent acting as the leader of a criminal team, a judge agent determining the outcome of each action, and a world manager agent updating the environment state and entities. Furthermore, we design 40 diverse crime tasks within this framework, covering 11 maps and 13 crime objectives such as theft, robbery, kidnapping, and riot. We also introduce a human player baseline for reference to better interpret the performance of LLM agents. We evaluate 8 strong LLMs and find (1) All agents in the simulation environment compliantly generate detailed plans and execute intelligent crime processes, with some achieving relatively high success rates; (2) In some cases, agents take severe action that inflicts harm to NPCs to achieve their goals. Our work highlights the need for safety alignment when deploying agentic AI in real-world settings.", "AI": {"tldr": "VirtualCrime\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e09\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6c99\u76d2\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u72af\u7f6a\u80fd\u529b\uff0c\u5305\u542b40\u4e2a\u591a\u6837\u5316\u72af\u7f6a\u4efb\u52a1\uff0c\u8986\u76d611\u4e2a\u5730\u56fe\u548c13\u79cd\u72af\u7f6a\u76ee\u6807\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u51b3\u7b56\u3001\u89c4\u5212\u548c\u884c\u52a8\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u5404\u79cd\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5f3a\u5927\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u662f\u5426\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u72af\u7f6a\u76ee\u7684\u3002", "method": "\u63d0\u51faVirtualCrime\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u653b\u51fb\u8005\u667a\u80fd\u4f53\uff08\u72af\u7f6a\u56e2\u961f\u9886\u5bfc\u8005\uff09\u3001\u6cd5\u5b98\u667a\u80fd\u4f53\uff08\u786e\u5b9a\u6bcf\u4e2a\u884c\u52a8\u7ed3\u679c\uff09\u3001\u4e16\u754c\u7ba1\u7406\u5668\u667a\u80fd\u4f53\uff08\u66f4\u65b0\u73af\u5883\u72b6\u6001\u548c\u5b9e\u4f53\uff09\u3002\u8bbe\u8ba1\u4e8640\u4e2a\u591a\u6837\u5316\u72af\u7f6a\u4efb\u52a1\uff0c\u8986\u76d611\u4e2a\u5730\u56fe\u548c13\u79cd\u72af\u7f6a\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u4eba\u7c7b\u73a9\u5bb6\u57fa\u7ebf\u4f5c\u4e3a\u53c2\u8003\u3002", "result": "\u8bc4\u4f30\u4e868\u4e2a\u5f3a\u5927\u7684LLM\uff0c\u53d1\u73b0\uff1a(1)\u6240\u6709\u667a\u80fd\u4f53\u5728\u4eff\u771f\u73af\u5883\u4e2d\u90fd\u80fd\u5408\u89c4\u5730\u751f\u6210\u8be6\u7ec6\u8ba1\u5212\u5e76\u6267\u884c\u667a\u80fd\u72af\u7f6a\u8fc7\u7a0b\uff0c\u90e8\u5206\u6a21\u578b\u8fbe\u5230\u76f8\u5bf9\u8f83\u9ad8\u7684\u6210\u529f\u7387\uff1b(2)\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u667a\u80fd\u4f53\u4e3a\u8fbe\u6210\u76ee\u6807\u4f1a\u91c7\u53d6\u5bf9NPC\u9020\u6210\u4e25\u91cd\u4f24\u5bb3\u7684\u884c\u52a8\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u667a\u80fdAI\u65f6\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u72af\u7f6a\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2601.11630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11630", "abs": "https://arxiv.org/abs/2601.11630", "authors": ["Haonan Wei", "Linyuan Wang", "Nuolin Sun", "Zhizhong Zheng", "Lei Li", "Bin Yan"], "title": "A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow", "comment": null, "summary": "Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.", "AI": {"tldr": "\u63d0\u51faSLT\uff08\u5355\u5c42Transformer\uff09\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c0628\u5c42FreeFlow\u6a21\u578b\u538b\u7f29\u4e3a\u5355\u4e2a\u5171\u4eabDiT\u5757\uff0c\u53c2\u6570\u91cf\u4ece675M\u964d\u81f34.3M\uff0c\u5e76\u5229\u7528\u5176\u5feb\u901f\u91c7\u6837\u80fd\u529b\u5728\u566a\u58f0\u7a7a\u95f4\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\uff0c\u63d0\u5347\u4e00\u6b65\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u6d41\u5339\u914d\u65b9\u6cd5\u65e8\u5728\u5c06\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u538b\u7f29\u5230\u5c11\u6570\u751a\u81f3\u5355\u6b65\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982FreeFlow\u4ecd\u4f7f\u752828\u5c42Transformer\u67b6\u6784\u3002\u89c2\u5bdf\u5230\u8be5\u67b6\u6784\u53ef\u89c6\u4e3a\u6cbf\u6df1\u5ea6\u8f74\u7684ODE\u6b27\u62c9\u79bb\u6563\u5316\u65b9\u6848\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5927\u5e45\u538b\u7f29\u6a21\u578b\u89c4\u6a21\uff0c\u540c\u65f6\u5229\u7528\u5c0f\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\u7b5b\u9009\u9ad8\u8d28\u91cf\u566a\u58f0\u521d\u59cb\u70b9\uff0c\u63d0\u5347\u4e00\u6b65\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002", "method": "\u5c06FreeFlow\u768428\u5c42Transformer\u67b6\u6784\u89c6\u4e3aODE\u7684\u6b27\u62c9\u79bb\u6563\u5316\u65b9\u6848\uff0c\u6cbf\u76f8\u540c\u63a8\u5bfc\u903b\u8f91\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\u63d0\u51faSLT\uff08\u5355\u5c42Transformer\uff09\uff0c\u4f7f\u7528\u5355\u4e2a\u5171\u4eabDiT\u5757\u8fd1\u4f3c28\u5c42\u6559\u5e08\u6a21\u578b\u7684\u6df1\u5ea6\u7279\u5f81\u6f14\u5316\u3002\u8bad\u7ec3\u65f6\u5728\u591a\u4e2a\u6df1\u5ea6\u8865\u4e01\u5339\u914d\u6559\u5e08\u4e2d\u95f4\u7279\u5f81\uff0c\u878d\u5408\u8fd9\u4e9b\u8865\u4e01\u7ea7\u8868\u793a\uff0c\u540c\u65f6\u5bf9\u9f50\u6559\u5e08\u7684\u6700\u7ec8\u901f\u5ea6\u9884\u6d4b\u3002\u901a\u8fc7\u84b8\u998f\u8bad\u7ec3\u5c0628\u4e2a\u72ec\u7acbTransformer\u5757\u538b\u7f29\u4e3a\u5355\u4e2a\u5757\u3002", "result": "\u6210\u529f\u5c06\u6559\u5e08\u6a21\u578bDiT-XL/2\u4ece675M\u53c2\u6570\u538b\u7f29\u52304.3M\uff0828\u4e2a\u72ec\u7acbTransformer\u5757\u2192\u5355\u4e2a\u5171\u4eab\u5757\uff09\u3002\u5728\u76f8\u5f53\u4e8e\u6559\u5e08\u6a21\u578b\u4e24\u6b21\u968f\u673a\u91c7\u6837\u7684\u65f6\u95f4\u9884\u7b97\u5185\uff0c\u53ef\u8fdb\u884c\u8d85\u8fc7100\u6b21\u566a\u58f0\u7b5b\u9009\uff0c\u5e76\u4f7f\u7528\u9009\u5b9a\u70b9\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002\u6709\u6548\u907f\u514d\u4e86\u6709\u9650FreeFlow\u91c7\u6837\u8c03\u7528\u4e0b\u4f4e\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u5bfc\u81f4\u7684\u8d28\u91cf\u6ce2\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u5e73\u5747\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SLT\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5927\u5e45\u538b\u7f29\u6d41\u5339\u914d\u6a21\u578b\u89c4\u6a21\uff0c\u540c\u65f6\u5229\u7528\u5c0f\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\u8fdb\u884c\u566a\u58f0\u7a7a\u95f4\u7b5b\u9009\uff0c\u4e3a\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u91c7\u6837\u6548\u7387\uff0c\u4e3a\u4e00\u6b65\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u538b\u7f29\u4e0e\u91c7\u6837\u7b56\u7565\u534f\u540c\u4f18\u5316\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.14019", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2601.14019", "abs": "https://arxiv.org/abs/2601.14019", "authors": ["Frederik Walter", "Hrishi Narayanan", "Jessica Bariffi", "Anne L\u00fcscher", "Rawad Bitar", "Robert Grass", "Antonia Wachter-Zeh", "Zohar Yakhini"], "title": "A Security Framework for Chemical Functions", "comment": null, "summary": "In this paper, we introduce chemical functions, a unified framework that models chemical systems as noisy challenge--response primitives, and formalize the associated chemical function infrastructure. Building on the theory of physical functions, we rigorously define robustness, unclonability, and unpredictability for chemical functions in both finite and asymptotic regimes, and specify security games that capture the adversary's power and the security goals. We instantiate the framework with two existing DNA-based constructions (operable random DNA and Genomic Sequence Encryption) and derive quantitative bounds for robustness, unclonability, and unpredictability. Our analysis develops maximum-likelihood verification rules under sequencing noise and partial-edit models, and provides high-precision estimates based on binomial distributions to guide parameter selection. The framework, definitions, and analyses yield a reproducible methodology for designing chemically unclonable authentication mechanisms. We demonstrate applications to in-product authentication and to shared key generation using standard extraction techniques.", "AI": {"tldr": "\u63d0\u51fa\u5316\u5b66\u51fd\u6570\u6846\u67b6\uff0c\u5c06\u5316\u5b66\u7cfb\u7edf\u5efa\u6a21\u4e3a\u566a\u58f0\u6311\u6218-\u54cd\u5e94\u539f\u8bed\uff0c\u5f62\u5f0f\u5316\u76f8\u5173\u57fa\u7840\u8bbe\u65bd\uff0c\u57fa\u4e8e\u7269\u7406\u51fd\u6570\u7406\u8bba\u5b9a\u4e49\u9c81\u68d2\u6027\u3001\u4e0d\u53ef\u514b\u9686\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u7528DNA\u6784\u9020\u5b9e\u4f8b\u5316\u3002", "motivation": "\u4e3a\u5316\u5b66\u7cfb\u7edf\u63d0\u4f9b\u7edf\u4e00\u7684\u5b89\u5168\u6846\u67b6\uff0c\u5c06\u5316\u5b66\u7cfb\u7edf\u5efa\u6a21\u4e3a\u566a\u58f0\u6311\u6218-\u54cd\u5e94\u539f\u8bed\uff0c\u5efa\u7acb\u5f62\u5f0f\u5316\u7684\u5316\u5b66\u51fd\u6570\u57fa\u7840\u8bbe\u65bd\uff0c\u89e3\u51b3\u5316\u5b66\u8ba4\u8bc1\u673a\u5236\u7684\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7269\u7406\u51fd\u6570\u7406\u8bba\uff0c\u5728\u6709\u9650\u548c\u6e10\u8fd1\u4e24\u79cd\u673a\u5236\u4e0b\u4e25\u683c\u5b9a\u4e49\u5316\u5b66\u51fd\u6570\u7684\u9c81\u68d2\u6027\u3001\u4e0d\u53ef\u514b\u9686\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff1b\u5236\u5b9a\u5b89\u5168\u6e38\u620f\u6355\u6349\u5bf9\u624b\u80fd\u529b\u548c\u5b89\u5168\u76ee\u6807\uff1b\u7528\u4e24\u79cd\u73b0\u6709DNA\u6784\u9020\uff08\u53ef\u64cd\u4f5c\u968f\u673aDNA\u548c\u57fa\u56e0\u7ec4\u5e8f\u5217\u52a0\u5bc6\uff09\u5b9e\u4f8b\u5316\u6846\u67b6\uff1b\u5f00\u53d1\u6700\u5927\u4f3c\u7136\u9a8c\u8bc1\u89c4\u5219\u5904\u7406\u6d4b\u5e8f\u566a\u58f0\u548c\u90e8\u5206\u7f16\u8f91\u6a21\u578b\uff1b\u57fa\u4e8e\u4e8c\u9879\u5206\u5e03\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u6307\u5bfc\u53c2\u6570\u9009\u62e9\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u5316\u5b66\u4e0d\u53ef\u514b\u9686\u8ba4\u8bc1\u673a\u5236\u8bbe\u8ba1\u65b9\u6cd5\uff1b\u63a8\u5bfc\u4e86\u9c81\u68d2\u6027\u3001\u4e0d\u53ef\u514b\u9686\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u5b9a\u91cf\u754c\u9650\uff1b\u5c55\u793a\u4e86\u5728\u4ea7\u54c1\u5185\u8ba4\u8bc1\u548c\u5171\u4eab\u5bc6\u94a5\u751f\u6210\uff08\u4f7f\u7528\u6807\u51c6\u63d0\u53d6\u6280\u672f\uff09\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u5316\u5b66\u51fd\u6570\u6846\u67b6\u4e3a\u8bbe\u8ba1\u5316\u5b66\u4e0d\u53ef\u514b\u9686\u8ba4\u8bc1\u673a\u5236\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u5b9a\u4e49\u3001\u5b89\u5168\u5206\u6790\u548c\u5177\u4f53\u5b9e\u4f8b\uff0c\u5efa\u7acb\u4e86\u5316\u5b66\u5b89\u5168\u7cfb\u7edf\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2601.11631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11631", "abs": "https://arxiv.org/abs/2601.11631", "authors": ["Yurun Song", "Jiong Yin", "Rongjunchen Zhang", "Ian G. Harris"], "title": "Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents", "comment": null, "summary": "Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\\times$ training speedup.", "AI": {"tldr": "CCPO\u6846\u67b6\u901a\u8fc7\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\u548c\u591a\u8f6e\u4ea4\u4e92\u5b66\u4e60\uff0c\u5728\u4fdd\u6301GUI\u667a\u80fd\u4f53\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe55%\u7684token\u538b\u7f29\u548c3.8\u500d\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u591a\u8f6eGUI\u667a\u80fd\u4f53\u5728\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u622a\u65ad\u727a\u7272\u957f\u671f\u4e0a\u4e0b\u6587\uff0c\u8981\u4e48\u901a\u8fc7token\u526a\u679d\u635f\u5bb3\u7a7a\u95f4\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\u5750\u6807\u538b\u7f29\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5305\u542b\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u805a\u5408\u5750\u6807\u4fe1\u606f\u6765\u6355\u6349\u76ee\u6807\u76f8\u5173\u533a\u57df\uff0c\u5e76\u9010\u6b65\u7f29\u5c0f\u5386\u53f2\u6ce8\u610f\u529b\u8303\u56f4\uff1b\u540c\u65f6\u8bbe\u8ba1\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f18\u52bf\u51fd\u6570\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u8fbe55%\u7684token\u538b\u7f29\u548c3.8\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "CCPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6eGUI\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u89c9\u538b\u7f29\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u8026\u5408\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2601.12604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12604", "abs": "https://arxiv.org/abs/2601.12604", "authors": ["Safwan Labbi", "Daniil Tiapkin", "Paul Mangold", "Eric Moulines"], "title": "Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization", "comment": null, "summary": "Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49f-softargmax\u7684\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\u66ff\u4ee3softmax\uff0c\u7ed3\u5408f-\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u65e0\u9700\u9884\u6761\u4ef6\u5373\u53ef\u83b7\u5f97\u6709\u9650MDP\u4e2d\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u663e\u5f0f\u975e\u6e10\u8fd1\u6700\u540e\u8fed\u4ee3\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5bf9\u7b56\u7565\u53c2\u6570\u5316\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\uff0c\u4f20\u7edfsoftmax\u53c2\u6570\u5316\u4f1a\u5bfc\u81f4\u75c5\u6001\u4f18\u5316\u666f\u89c2\u548c\u6307\u6570\u7ea7\u6162\u6536\u655b\u3002\u867d\u7136\u9884\u6761\u4ef6\u53ef\u4ee5\u7f13\u89e3\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u5bfb\u627e\u66f4\u4f18\u7684\u53c2\u6570\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5e7f\u4e49f-softargmax\u4f5c\u4e3a\u7b56\u7565\u53c2\u6570\u5316\u66ff\u4ee3softmax\uff0c\u5e76\u7ed3\u5408\u76f8\u540cf-\u6563\u5ea6\u8bf1\u5bfc\u7684\u6b63\u5219\u5316\u5668\u3002\u8be5\u65b9\u6cd5\u6539\u5584\u4f18\u5316\u666f\u89c2\uff0c\u786e\u4fdd\u6b63\u5219\u5316\u76ee\u6807\u6ee1\u8db3Polyak-Lojasiewicz\u4e0d\u7b49\u5f0f\u3002", "result": "\u9996\u6b21\u4e3a\u6709\u9650MDP\u7684\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5efa\u7acb\u4e86\u65e0\u9700\u9884\u6761\u4ef6\u7684\u663e\u5f0f\u975e\u6e10\u8fd1\u6700\u540e\u8fed\u4ee3\u6536\u655b\u4fdd\u8bc1\u3002\u5bf9\u4e8e\u65e0\u6b63\u5219\u5316\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\uff0c\u663e\u793af-PG\uff08\u4f7f\u7528Tsallis\u6563\u5ea6\uff09\u5b9e\u73b0\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\uff0c\u800c\u6807\u51c6softmax\u53c2\u6570\u5316\u9700\u8981\u6307\u6570\u590d\u6742\u5ea6\u3002", "conclusion": "f-softargmax\u53c2\u6570\u5316\u7ed3\u5408f-\u6563\u5ea6\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edfsoftmax\u66f4\u4f18\u7684\u4f18\u5316\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u6761\u4ef6\u7684\u6536\u655b\u4fdd\u8bc1\u548c\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2601.11632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11632", "abs": "https://arxiv.org/abs/2601.11632", "authors": ["Zhiyang Li", "Ao Ke", "Yukun Cao", "Xike Xie"], "title": "KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering", "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.", "AI": {"tldr": "KG-ViP\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u6765\u89e3\u51b3MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347VQA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u53cc\u91cd\u9650\u5236\uff1a\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u7684\u534f\u540c\u6f5c\u529b\u3002", "method": "\u63d0\u51faKG-ViP\u7edf\u4e00\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u68c0\u7d22-\u878d\u5408\u7ba1\u9053\uff0c\u5229\u7528\u67e5\u8be2\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\u9010\u6b65\u6574\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\uff0c\u5408\u6210\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u4ee5\u4fc3\u8fdb\u53ef\u9760\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728FVQA 2.0+\u548cMVQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cKG-ViP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VQA\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u7684\u4e92\u8865\u4f18\u52bf\uff0cKG-ViP\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2601.12612", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12612", "abs": "https://arxiv.org/abs/2601.12612", "authors": ["Piyush Sao"], "title": "What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes", "comment": null, "summary": "Computing $\\log\\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \\tr(A^k)$, natural when matrix powers are available.\n  Classical moment-based approximations Taylor-expand $\\log(\u03bb)$ around the arithmetic mean. This requires $|\u03bb- \\AM| < \\AM$ and diverges when $\u03ba> 4$. We work instead with the moment-generating function $M(t) = \\E[X^t]$ for normalized eigenvalues $X = \u03bb/\\AM$. Since $M'(0) = \\E[\\log X]$, the log-determinant becomes $\\log\\det(A) = n(\\log \\AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \\log M(t)$ compresses this range. Normalization by $\\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.\n  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \\E[\\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\\det A)^{1/n}$. Given a spectral floor $r \\leq \u03bb_{\\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\\log\\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \\in \\{4, \\ldots, 8\\}$, this is effectively constant time.", "AI": {"tldr": "\u57fa\u4e8e\u77e9\u9635\u8ff9\u5e42 $p_k = \\tr(A^k)$ \u4f30\u8ba1\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u5bf9\u6570\u884c\u5217\u5f0f $\\log\\det(A)$ \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u751f\u6210\u51fd\u6570\u53d8\u6362\u548c\u63d2\u503c\u6280\u672f\uff0c\u63d0\u4f9b\u70b9\u4f30\u8ba1\u548c\u53ef\u8bc1\u660e\u7684\u4e0a\u4e0b\u754c", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\u548c\u8d1d\u53f6\u65af\u6a21\u578b\u6bd4\u8f83\u4e2d\u9700\u8981\u8ba1\u7b97\u5927\u89c4\u6a21\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u3002\u4f20\u7edf\u65b9\u6cd5\u7ed3\u5408\u77e9\u9635-\u5411\u91cf\u4e58\u79ef\u548c\u591a\u9879\u5f0f\u903c\u8fd1\uff0c\u672c\u6587\u7814\u7a76\u5f53\u77e9\u9635\u5e42\u8fd0\u7b97\u53ef\u7528\u65f6\uff0c\u5229\u7528\u8ff9\u5e42 $p_k = \\tr(A^k)$ \u8fd9\u4e00\u4e0d\u540c\u6a21\u578b", "method": "\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u4f30\u8ba1\u77e9\u751f\u6210\u51fd\u6570 $M(t) = \\E[X^t]$ \u5728 $t=0$ \u5904\u7684\u5bfc\u6570\uff0c\u5176\u4e2d $X = \u03bb/\\AM$ \u4e3a\u5f52\u4e00\u5316\u7279\u5f81\u503c\u3002\u901a\u8fc7\u53d8\u6362 $K(t) = \\log M(t)$ \u538b\u7f29\u8303\u56f4\uff0c\u5229\u7528 $K(0) = K(1) = 0$ \u4f5c\u4e3a\u951a\u70b9\uff0c\u5728 $m+1$ \u4e2a\u8fde\u7eed\u6574\u6570\u70b9\u63d2\u503c $K$ \u5e76\u6c42\u5bfc\u4f30\u8ba1 $K'(0)$\u3002\u540c\u65f6\u4ece\u76f8\u540c\u8ff9\u4fe1\u606f\u63a8\u5bfc\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u4e0a\u4e0b\u754c", "result": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u6709\u9650\u6b63\u77e9\u7684\u8fde\u7eed\u4f30\u8ba1\u5668\u65e0\u6cd5\u5728\u65e0\u754c\u6761\u4ef6\u6570\u4e0b\u4fdd\u6301\u5747\u5300\u7cbe\u5ea6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8c31\u4e0b\u754c $r \\leq \u03bb_{\\min}$ \u7684\u77e9\u7ea6\u675f\u4e0b\u754c\uff0c\u5f62\u6210 $\\log\\det(A)$ \u7684\u53ef\u8bc1\u660e\u533a\u95f4\u3002\u6240\u6709\u4f30\u8ba1\u5668\u548c\u8fb9\u754c\u8ba1\u7b97\u6210\u672c\u4e3a $O(m)$\uff0c\u5bf9\u4e8e $m \\in \\{4, \\ldots, 8\\}$ \u51e0\u4e4e\u662f\u5e38\u6570\u65f6\u95f4", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u8ff9\u5e42\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5305\u542b\u70b9\u4f30\u8ba1\u548c\u53ef\u8bc1\u660e\u8fb9\u754c\uff0c\u901a\u8fc7\u95f4\u9699\u8bca\u65ad\u6307\u793a\u4f55\u65f6\u4fe1\u4efb\u70b9\u4f30\u8ba1\u3001\u4f55\u65f6\u62a5\u544a\u8fb9\u754c\uff0c\u5728\u77e9\u9635\u5e42\u8fd0\u7b97\u53ef\u7528\u65f6\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u4f30\u8ba1"}}
{"id": "2601.14054", "categories": ["cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14054", "abs": "https://arxiv.org/abs/2601.14054", "authors": ["Zhihao Dou", "Dongfei Cui", "Weida Wang", "Anjun Gao", "Yueyang Quan", "Mengyao Ma", "Viet Vo", "Guangdong Bai", "Zhuqing Liu", "Minghong Fang"], "title": "SecureSplit: Mitigating Backdoor Attacks in Split Learning", "comment": "To appear in The Web Conference 2026", "summary": "Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.", "AI": {"tldr": "SecureSplit\u662f\u4e00\u79cd\u9488\u5bf9\u5206\u5272\u5b66\u4e60\u4e2d\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u7ef4\u5ea6\u53d8\u6362\u589e\u5f3a\u826f\u6027\u5d4c\u5165\u4e0e\u4e2d\u6bd2\u5d4c\u5165\u7684\u5dee\u5f02\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\u79fb\u9664\u6c61\u67d3\u5d4c\u5165", "motivation": "\u5206\u5272\u5b66\u4e60\u867d\u7136\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\uff0c\u6076\u610f\u5ba2\u6237\u7aef\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u5d4c\u5165\u6765\u690d\u5165\u9690\u85cf\u89e6\u53d1\u5668\uff0c\u4ece\u800c\u7834\u574f\u6700\u7ec8\u8bad\u7ec3\u6a21\u578b\u7684\u5b89\u5168\u6027", "method": "SecureSplit\u91c7\u7528\u7ef4\u5ea6\u53d8\u6362\u7b56\u7565\u6765\u653e\u5927\u826f\u6027\u5d4c\u5165\u4e0e\u4e2d\u6bd2\u5d4c\u5165\u4e4b\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u7136\u540e\u57fa\u4e8e\u591a\u6570\u6295\u7968\u7684\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\u79fb\u9664\u6c61\u67d3\u5d4c\u5165\uff0c\u540c\u65f6\u4fdd\u7559\u5e72\u51c0\u7684\u5d4c\u5165", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08CIFAR-10\u3001MNIST\u3001CINIC-10\u3001ImageNette\uff09\u3001\u4e94\u79cd\u540e\u95e8\u653b\u51fb\u573a\u666f\u548c\u4e03\u79cd\u66ff\u4ee3\u9632\u5fa1\u65b9\u6cd5\u7684\u4e25\u683c\u5b9e\u9a8c\u4e2d\uff0cSecureSplit\u5728\u5404\u79cd\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027", "conclusion": "SecureSplit\u4e3a\u5206\u5272\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u589e\u5f3a\u5d4c\u5165\u5dee\u5f02\u548c\u81ea\u9002\u5e94\u8fc7\u6ee4\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u5b89\u5168\u6027"}}
{"id": "2601.11635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11635", "abs": "https://arxiv.org/abs/2601.11635", "authors": ["Anil Egin", "Andrea Tangherloni", "Antitza Dantcheva"], "title": "Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos", "comment": null, "summary": "Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.", "AI": {"tldr": "Anon-NET\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u4eba\u8138\u89c6\u9891\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c5e\u6027\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u8868\u60c5\u8fc1\u79fb\u5b9e\u73b0\u8eab\u4efd\u6df7\u6dc6\uff0c\u540c\u65f6\u4fdd\u7559\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u3001\u59ff\u6001\u548c\u8868\u60c5\u7b49\u5c5e\u6027", "motivation": "\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5141\u8bb8\u89c6\u9891\u5728\u8868\u60c5\u8bc6\u522b\u3001\u4eba\u5458\u8ddf\u8e2a\u3001\u52a8\u4f5c\u8bc6\u522b\u7b49\u4e0b\u6e38\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8fdb\u884c\u5206\u6790\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u53bb\u8eab\u4efd\u5316\u53c8\u80fd\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u91cd\u8981\u5c5e\u6027\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faAnon-NET\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4eba\u8138\u4fee\u590d\uff0c\u901a\u8fc7\u9ad8\u7ea7\u5c5e\u6027\u8bc6\u522b\u548c\u8fd0\u52a8\u611f\u77e5\u8868\u60c5\u8fc1\u79fb\u5f15\u5bfc\uff1b2\uff09\u901a\u8fc7\u89c6\u9891\u9a71\u52a8\u52a8\u753b\u5bf9\u53bb\u8eab\u4efd\u5316\u4eba\u8138\u8fdb\u884c\u52a8\u753b\u5316\uff0c\u63a5\u53d7\u53bb\u8eab\u4efd\u5316\u4eba\u8138\u548c\u539f\u59cb\u89c6\u9891\u4f5c\u4e3a\u8f93\u5165", "result": "\u5728VoxCeleb2\u3001CelebV-HQ\u548cHDTF\u7b49\u5305\u542b\u591a\u6837\u5316\u9762\u90e8\u52a8\u6001\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eAnon-NET\u5728\u6df7\u6dc6\u8eab\u4efd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "conclusion": "Anon-NET\u662f\u4e00\u4e2a\u6709\u6548\u7684\u4eba\u8138\u89c6\u9891\u533f\u540d\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03"}}
{"id": "2601.12654", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12654", "abs": "https://arxiv.org/abs/2601.12654", "authors": ["Hyunseung Hwang", "Seungeun Lee", "Lucas Rosenblatt", "Julia Stoyanovich", "Steven Euijong Whang"], "title": "Explanation Multiplicity in SHAP: Characterization and Assessment", "comment": null, "summary": "Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.", "AI": {"tldr": "SHAP\u89e3\u91ca\u5b58\u5728\u591a\u91cd\u6027\uff1a\u5373\u4f7f\u8f93\u5165\u3001\u4efb\u52a1\u548c\u8bad\u7ec3\u6a21\u578b\u56fa\u5b9a\uff0c\u591a\u6b21\u8fd0\u884c\u4e5f\u4f1a\u4ea7\u751f\u663e\u8457\u4e0d\u540c\u7684\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\uff0c\u8fd9\u79cd\u5185\u90e8\u6709\u6548\u4f46\u5b9e\u8d28\u4e0a\u4e0d\u540c\u7684\u89e3\u91ca\u73b0\u8c61\u88ab\u79f0\u4e3a\"\u89e3\u91ca\u591a\u91cd\u6027\"\u3002", "motivation": "SHAP\u4f5c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u81ea\u52a8\u5316\u51b3\u7b56\u89e3\u91ca\u7684\u5e38\u7528\u5de5\u5177\uff0c\u5e38\u88ab\u89c6\u4e3a\u53ef\u9760\u7684\u7279\u5f81\u9a71\u52a8\u5206\u6790\u3002\u7136\u800c\uff0c\u5373\u4f7f\u8f93\u5165\u3001\u4efb\u52a1\u548c\u8bad\u7ec3\u6a21\u578b\u56fa\u5b9a\uff0cSHAP\u89e3\u91ca\u5728\u4e0d\u540c\u8fd0\u884c\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u79cd\"\u89e3\u91ca\u591a\u91cd\u6027\"\u73b0\u8c61\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u548c\u91cf\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8868\u5f81\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\u591a\u91cd\u6027\u7684\u65b9\u6cd5\u8bba\uff0c\u533a\u5206\u6a21\u578b\u8bad\u7ec3/\u9009\u62e9\u6765\u6e90\u4e0e\u89e3\u91ca\u7ba1\u9053\u5185\u5728\u968f\u673a\u6027\u3002\u4f7f\u7528\u5e45\u5ea6\u57fa\u8ddd\u79bb\u548c\u6392\u5e8f\u57fa\u5ea6\u91cf\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u968f\u673a\u57fa\u7ebf\u503c\u4f5c\u4e3a\u53c2\u8003\u57fa\u51c6\u3002", "result": "\u53d1\u73b0\u89e3\u91ca\u591a\u91cd\u6027\u666e\u904d\u5b58\u5728\u4e14\u6301\u7eed\u5b58\u5728\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4e2d\u3002\u5e45\u5ea6\u57fa\u8ddd\u79bb\u53ef\u80fd\u63a5\u8fd1\u96f6\uff0c\u800c\u6392\u5e8f\u57fa\u5ea6\u91cf\u63ed\u793atop\u7279\u5f81\u8eab\u4efd\u548c\u6392\u5e8f\u7684\u663e\u8457\u53d8\u5316\uff0c\u8868\u660e\u7a33\u5b9a\u6027\u5ea6\u91cf\u9700\u4e0e\u89e3\u91ca\u4f7f\u7528\u610f\u56fe\u5339\u914d\u3002", "conclusion": "SHAP\u89e3\u91ca\u7684\u591a\u91cd\u6027\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u5f00\u53d1\u4e0e\u89e3\u91ca\u9884\u671f\u7528\u9014\u76f8\u5339\u914d\u7684\u5ea6\u91cf\u548c\u57fa\u7ebf\uff0c\u4ee5\u786e\u4fdd\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u573a\u666f\u4e2d\u3002"}}
{"id": "2601.11640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11640", "abs": "https://arxiv.org/abs/2601.11640", "authors": ["Yingda Yu", "Jiaqi Xuan", "Shuhui Shi", "Xuanyu Teng", "Shuyang Xu", "Guanchao Tong"], "title": "Confident Learning for Object Detection under Model Constraints", "comment": "Submitted to ICPR 2026, currently under review", "summary": "Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.", "AI": {"tldr": "\u63d0\u51faMDDC\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u4f18\u5316\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\u4e0b\u5b9e\u73b05-25%\u7684mAP\u63d0\u5347", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u9762\u4e34\u6a21\u578b\u5bb9\u91cf\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u5b9e\u65f6\u63a8\u7406\u5ef6\u8fdf\u7684\u4e25\u683c\u7ea6\u675f\uff0c\u65e0\u6cd5\u901a\u8fc7\u6a21\u578b\u7f29\u653e\u6216\u96c6\u6210\u6765\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981\u6570\u636e\u4e2d\u5fc3\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u6a21\u578b\u9a71\u52a8\u6570\u636e\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u5c06\u68c0\u6d4b\u5931\u8d25\u5206\u4e3a\u56db\u7c7b\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u8bad\u7ec3-\u4fee\u590d-\u518d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u914d\u5408\u7248\u672c\u63a7\u5236\u6570\u636e\u7ba1\u7406", "result": "\u5728\u591a\u4e2a\u6742\u8349\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\u5b9e\u73b05-25%\u7684mAP@0.5\u63d0\u5347\uff0c\u8bc1\u660e\u6570\u636e\u8d28\u91cf\u4f18\u5316\u80fd\u6709\u6548\u7f13\u89e3\u56fa\u5b9a\u6a21\u578b\u5bb9\u91cf\u4e0b\u7684\u6027\u80fd\u74f6\u9888", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u8d28\u91cf\u4f18\u5316\u662f\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\uff0c\u5728\u56fa\u5b9a\u6a21\u578b\u7ea6\u675f\u4e0b\u901a\u8fc7\u6570\u636e\u6821\u6b63\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u6539\u8fdb"}}
{"id": "2601.12680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12680", "abs": "https://arxiv.org/abs/2601.12680", "authors": ["Zheng Fang", "Wolfgang Mayer", "Zeyu Zhang", "Jian Wang", "Hong-Yu Zhang", "Wanli Li", "Zaiwen Feng"], "title": "MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning", "comment": null, "summary": "Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.", "AI": {"tldr": "\u63d0\u51faMetaToolAgent\uff08MTA\uff09\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8155\u4e2a\u5de5\u5177\u30017\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u5de5\u5177\u9009\u62e9\u65b9\u6cd5\u5c40\u9650\u4e8e\u6709\u9650\u5de5\u5177\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u5b9e\u9645\u90e8\u7f72\u4e2d\u9047\u5230\u7684\u65b0\u5de5\u5177\uff0c\u9700\u8981\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u5de5\u5177\u534f\u8c03\u4e2d\u7684\u8de8\u5de5\u5177\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faMetaToolAgent\uff08MTA\uff09\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b7\u4e2a\u9886\u57df\u3001155\u4e2a\u5de5\u5177\u30019,377\u4e2a\u95ee\u7b54\u5bf9\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u6a21\u62df\u771f\u5b9e\u96c6\u6210\u573a\u666f", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMTA\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6784\u5efa\u7075\u6d3b\u53ef\u6269\u5c55\u7cfb\u7edf\u65b9\u9762\u7684\u6f5c\u529b", "conclusion": "MTA\u901a\u8fc7\u5143\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u5de5\u5177\u534f\u8c03\u4e2d\u7684\u8de8\u5de5\u5177\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.14033", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.14033", "abs": "https://arxiv.org/abs/2601.14033", "authors": ["Xiaochen Zhu", "Mayuri Sridhar", "Srinivas Devadas"], "title": "PAC-Private Responses with Adversarial Composition", "comment": "16 pages, 3 figures", "summary": "Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.\n  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.\n  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePAC\u9690\u79c1\u7684API\u6a21\u578b\u8f93\u51fa\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u4e92\u4fe1\u606f\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u9690\u79c1\uff0c\u652f\u6301\u5bf9\u6297\u6027\u81ea\u9002\u5e94\u67e5\u8be2\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u5728\u6781\u5c0f\u7684\u6bcf\u67e5\u8be2\u9690\u79c1\u9884\u7b97\u4e0b\u4fdd\u6301\u9ad8\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u901a\u8fc7API\u90e8\u7f72\uff0c\u4f20\u7edf\u7684\u6743\u91cd\u9690\u79c1\u65b9\u6cd5\uff08\u5982DP-SGD\uff09\u5728API\u573a\u666f\u4e0b\u4f1a\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u566a\u58f0\u5e76\u964d\u4f4e\u6548\u7528\u3002\u6a21\u578b\u6743\u91cd\u5bf9\u8bad\u7ec3\u6570\u636e\u654f\u611f\uff0c\u4f46\u6a21\u578b\u5bf9\u7279\u5b9a\u8f93\u5165\u7684\u8f93\u51fa\u7ef4\u5ea6\u66f4\u4f4e\u4e14\u66f4\u7a33\u5b9a\uff0c\u56e0\u6b64\u76f4\u63a5\u5728\u6a21\u578b\u8f93\u51fa\u4e0a\u5b9e\u65bd\u9690\u79c1\u4fdd\u62a4\u66f4\u4e3a\u6709\u6548\u3002", "method": "\u91c7\u7528PAC\u9690\u79c1\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u4e92\u4fe1\u606f\u4e3a\u4efb\u610f\u9ed1\u76d2\u51fd\u6570\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u9690\u79c1\u4fdd\u8bc1\u3002\u63d0\u51fa\u65b0\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6821\u51c6\u5b9e\u73b0\u5bf9\u6297\u6027\u7ec4\u5408\uff0c\u8bc1\u660e\u4e92\u4fe1\u606f\u4fdd\u8bc1\u5728\u81ea\u9002\u5e94\u548c\u5bf9\u6297\u6027\u67e5\u8be2\u4e0b\u7ebf\u6027\u7d2f\u79ef\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u5956\u52b1\u8f93\u51fa\u7a33\u5b9a\u6027\u4ee5\u51cf\u5c11\u566a\u58f0\u6c34\u5e73\u3002", "result": "\u5728\u8868\u683c\u3001\u89c6\u89c9\u548cNLP\u4efb\u52a1\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u5c0f\u7684\u6bcf\u67e5\u8be2\u9690\u79c1\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7528\u3002\u5728CIFAR-10\u4e0a\u8fbe\u523087.79%\u51c6\u786e\u7387\uff0c\u6bcf\u6b65\u4e92\u4fe1\u606f\u9884\u7b97\u4ec52^{-32}\u3002\u670d\u52a1100\u4e07\u67e5\u8be2\u65f6\uff0c\u6210\u5458\u63a8\u7406\u653b\u51fb\u6210\u529f\u7387\u4e0a\u9650\u4e3a51.08%\uff0c\u76f8\u5f53\u4e8e(0.04, 10^{-5})-DP\u4fdd\u8bc1\u3002\u901a\u8fc7\u79c1\u6709\u54cd\u5e94\u6807\u6ce8\u516c\u5171\u6570\u636e\u84b8\u998f\u51fa\u53ef\u53d1\u5e03\u7684\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\uff0c\u5728ImageNet\u5b50\u96c6\u4e0a\u4f7f\u7528210,000\u4e2a\u54cd\u5e94\u84b8\u998f\u7684\u6a21\u578b\u5728CIFAR-10\u4e0a\u8fbe\u523091.86%\u51c6\u786e\u7387\uff0cMIA\u6210\u529f\u7387\u4e0a\u965050.49%\uff0c\u76f8\u5f53\u4e8e(0.02,10^{-5})-DP\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728API\u90e8\u7f72\u573a\u666f\u4e0b\u6709\u6548\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\uff0c\u901a\u8fc7\u8f93\u51fa\u7ea7\u9690\u79c1\u4fdd\u62a4\u907f\u514d\u4f20\u7edf\u6743\u91cd\u9690\u79c1\u65b9\u6cd5\u7684\u566a\u58f0\u95ee\u9898\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5bf9\u6297\u6027\u67e5\u8be2\u7ec4\u5408\uff0c\u5e76\u80fd\u84b8\u998f\u51fa\u9ad8\u6027\u80fd\u7684\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11641", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11641", "abs": "https://arxiv.org/abs/2601.11641", "authors": ["Yuxi Liu", "Yipeng Hu", "Zekun Zhang", "Kunze Jiang", "Kun Yuan"], "title": "Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers", "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \\underline{\\textbf{M}}ixtrue-\\underline{\\textbf{O}}f-\\underline{\\textbf{D}}istribution \\textbf{DiT} (\\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.", "AI": {"tldr": "MOD-DiT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91c7\u6837\u7684\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u51c6\u786e\u5efa\u6a21\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u89c6\u9891\u751f\u6210\u4e2d\u9762\u4e34\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8fc7\u4e8e\u7b80\u5316\u7684\u9759\u6001\u6a21\u5f0f\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u91c7\u6837\u64cd\u4f5c\u6765\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u5f0f\u9884\u6d4b\u4e0d\u51c6\u786e\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5206\u5e03DiT\uff08MOD-DiT\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1\uff09\u5229\u7528\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6df7\u5408\u65b9\u6cd5\u5efa\u6a21\u9ad8\u6548\u7684\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\uff0c\u9884\u6d4b\u7279\u5b9a\u53bb\u566a\u533a\u95f4\u7684\u63a9\u7801\u6a21\u5f0f\uff1b2\uff09\u5728\u7ebf\u5757\u63a9\u7801\u7b56\u7565\u52a8\u6001\u5e94\u7528\u9884\u6d4b\u7684\u63a9\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u5386\u53f2\u7a00\u758f\u4fe1\u606f\uff0c\u65e0\u9700\u91cd\u590d\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u52a0\u901f\u548c\u8d28\u91cf\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86MOD-DiT\u5728\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "MOD-DiT\u901a\u8fc7\u91c7\u6837\u81ea\u7531\u7684\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u51c6\u786e\u5efa\u6a21\u6f14\u5316\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65e2\u9ad8\u6548\u53c8\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11642", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.11642", "abs": "https://arxiv.org/abs/2601.11642", "authors": ["Abbas Alzubaidi", "Ali Al-Bayaty"], "title": "PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models", "comment": "16 pages, 6 figures", "summary": "Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like \"0\" vs. \"2\") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\u751f\u6210\u53ef\u63a7\u819d\u5173\u8282X\u5149\u7247\uff0c\u7528\u4e8e\u9aa8\u5173\u8282\u708e\u8bc4\u4f30\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u7684\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b8b\u539f\u56e0\uff0c\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u7684Kellgren-Lawrence\u5206\u7ea7\u8bc4\u4f30\u3002AI\u548c\u5f71\u50cf\u7ec4\u5b66\u9700\u8981\u5927\u91cf\u6807\u6ce8\u826f\u597d\u7684X\u5149\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4f46\u7531\u4e8e\u9690\u79c1\u3001\u6cbb\u7406\u548c\u8d44\u6e90\u9650\u5236\uff0c\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\uff0c\u4ece\u53c2\u6570\u5316\u89e3\u5256\u6a21\u578b\u751f\u6210\u524d\u540e\u4f4d\u819d\u5173\u8282X\u5149\u7247\u3002\u521b\u5efa180\u540d\u53d7\u8bd5\u8005(260\u4e2a\u819d\u76d6)\u7684\u865a\u62df\u961f\u5217\uff0c\u6bcf\u4e2a\u819d\u76d6\u5728\u4e09\u79cd\u534f\u8bae\u4e0b\u6210\u50cf(\u53c2\u8003\u3001\u4f4e\u5242\u91cf\u3001\u51e0\u4f55\u504f\u79fb)\u3002\u81ea\u52a8\u5b9a\u4f4d\u5185\u4fa7\u5173\u8282\u533a\u57df\uff0c\u4f7f\u7528IBSI\u6807\u51c6\u8fdb\u884c\u9884\u5904\u7406\u548c\u7279\u5f81\u63d0\u53d6\u3002\u91c7\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e8c\u5143(\u7c7b\u4f3cKL\u5206\u7ea7\"0\" vs. \"2\")\u548c\u4e09\u5206\u7c7b(0-2)\u9884\u6d4b\u3002", "result": "\u5728IBSI\u534f\u8bae\u5185\u3001\u8de8\u534f\u8bae\u548c\u591a\u534f\u8bae\u573a\u666f\u4e0b\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u7c7b\u5185\u76f8\u5173\u7cfb\u6570\u8bc4\u4f30\u7279\u5f81\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "PSSF\u6846\u67b6\u80fd\u591f\u751f\u6210\u53ef\u63a7\u7684\u5408\u6210X\u5149\u7247\uff0c\u907f\u514d\u4e86\u60a3\u8005\u9690\u79c1\u548c\u673a\u6784\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u7684\u5b9a\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2601.11644", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11644", "abs": "https://arxiv.org/abs/2601.11644", "authors": ["Muhammad Imran", "Yugyung Lee"], "title": "Predicting When to Trust Vision-Language Models for Spatial Reasoning", "comment": "9 pages, 5 figures, 6 tables", "summary": "Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u51e0\u4f55\u9a8c\u8bc1\u6765\u9884\u6d4b\u4f55\u65f6\u4fe1\u4efbVLM\u7684\u7a7a\u95f4\u9884\u6d4b\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u8bc4\u4f30\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff08\u51c6\u786e\u7387\u4ec549%-54%\uff09\u3002\u4e3a\u786e\u4fdd\u5728\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u9700\u8981\u9884\u6d4b\u4f55\u65f6\u4fe1\u4efbVLM\u7684\u7a7a\u95f4\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u76f2\u76ee\u63a5\u53d7\u6240\u6709\u8f93\u51fa", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u51e0\u4f55\u9a8c\u8bc1\u4f7f\u7528\u7269\u4f53\u68c0\u6d4b\u6765\u9a8c\u8bc1VLM\u9884\u6d4b\u3002\u878d\u5408\u56db\u4e2a\u4fe1\u53f7\uff1aVLM\u58f0\u660e\u4e0e\u5750\u6807\u7684\u51e0\u4f55\u5bf9\u9f50\u3001\u91cd\u53e0\u5f15\u8d77\u7684\u7a7a\u95f4\u6a21\u7cca\u6027\u3001\u68c0\u6d4b\u8d28\u91cf\u3001VLM\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u8fdb\u884c\u878d\u5408", "result": "\u5728BLIP-2\u4e0a\u8fbe\u52300.674 AUROC\uff08\u6bd4\u6587\u672c\u57fa\u7ebf\u63d0\u534734.0%\uff09\uff0c\u5728CLIP\u4e0a\u8fbe\u52300.583 AUROC\uff08\u63d0\u534716.1%\uff09\u3002\u572860%\u76ee\u6807\u51c6\u786e\u7387\u4e0b\uff0c\u8986\u76d6\u7387\u8fbe\u523061.9%\uff08\u57fa\u7ebf27.6%\uff0c\u63d0\u53472.2\u500d\uff09\u3002\u7279\u5f81\u5206\u6790\u663e\u793a\u89c6\u89c9\u4fe1\u53f7\u8d21\u732e87.4%\u91cd\u8981\u6027\uff0cVLM\u7f6e\u4fe1\u5ea6\u4ec512.7%", "conclusion": "\u5916\u90e8\u51e0\u4f55\u9a8c\u8bc1\u4f18\u4e8e\u81ea\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u53ef\u9760\u5730\u8fdb\u884c\u573a\u666f\u56fe\u6784\u5efa\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u526a\u679d\u5c06\u7cbe\u5ea6\u4ece52.1%\u63d0\u5347\u523078.3%\uff0c\u540c\u65f6\u4fdd\u755968.2%\u7684\u8fb9\u3002\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86VLM\u7a7a\u95f4\u63a8\u7406\u7684\u53ef\u9760\u6027"}}
{"id": "2601.11645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11645", "abs": "https://arxiv.org/abs/2601.11645", "authors": ["Ujjwal Jain", "Oshin Misra", "Roshni Chakraborty", "Mahua Bhattacharya"], "title": "IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation", "comment": null, "summary": "Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.", "AI": {"tldr": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u548c\u6df7\u5408\u635f\u5931\u4f18\u5316\uff0c\u89e3\u51b3\u8367\u5149\u663e\u5fae\u955c\u795e\u7ecf\u5143\u5206\u5272\u4e2d\u7ec6\u80de\u5bc6\u5ea6\u4e0d\u5747\u3001\u5f62\u6001\u590d\u6742\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\u3002", "motivation": "\u8367\u5149\u663e\u5fae\u955c\u795e\u7ecf\u5143\u5206\u5272\u9762\u4e34\u5bc6\u96c6\u4e0e\u7a00\u758f\u7ec6\u80de\u5171\u5b58\u3001\u590d\u6742\u91cd\u53e0\u5f62\u6001\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u4fdd\u6301\u7cbe\u7ec6\u62d3\u6251\u7ec6\u8282\u548c\u51c6\u786e\u8fb9\u754c\u5212\u5206\u3002", "method": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u5bc6\u96c6\u5757(MSDBs)\u6355\u83b7\u4e0d\u540c\u611f\u53d7\u91ce\u7279\u5f81\uff0c\u5206\u5c42\u6ce8\u610f\u529b(HA)\u673a\u5236\u805a\u7126\u663e\u8457\u5f62\u6001\u7279\u5f81\uff0c\u4ee5\u53ca\u7ed3\u5408Tversky\u635f\u5931\u3001Focal\u635f\u5931\u3001\u62d3\u6251\u611f\u77e5\u4e2d\u5fc3\u7ebfDice\u635f\u5931\u548c\u8f6e\u5ed3\u52a0\u6743\u8fb9\u754c\u635f\u5931\u7684\u6df7\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u516c\u5f00FNC\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u7ec6\u80de\u60c5\u51b5\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u523081.4%\u7cbe\u786e\u7387\u300182.7%\u5b8fF1\u5206\u6570\u300183.3%\u5faeF1\u5206\u6570\u548c99.5%\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u53ef\u6cdb\u5316\u7684\u5206\u5272\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u751f\u7269\u533b\u5b66\u6210\u50cf\u6a21\u6001\uff0c\u63a8\u52a8AI\u8f85\u52a9\u5206\u6790\u5411\u9ad8\u901a\u91cf\u795e\u7ecf\u751f\u7269\u5b66\u6d41\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2601.11651", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11651", "abs": "https://arxiv.org/abs/2601.11651", "authors": ["Miriam Doh", "Aditya Gulati", "Corina Canali", "Nuria Oliver"], "title": "Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification", "comment": "22 pages, 15 figures", "summary": "This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.\n  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210AI\u548c\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\"\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\"\u504f\u89c1\uff0c\u5373\u57fa\u4e8e\u5916\u8c8c\u7684\u504f\u597d\u6027\u5bf9\u5f85\uff0c\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u79ef\u6781\u5c5e\u6027\u5173\u8054\uff0c\u4ee5\u53ca\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u4e2d\u7684\u663e\u8457\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8c03\u67e5\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210AI\u548c\u4e0b\u6e38\u6027\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\"\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\"\u95ee\u9898\uff0c\u5373\u57fa\u4e8e\u7269\u7406\u5916\u8c8c\u7684\u7cfb\u7edf\u6027\u504f\u597d\u5bf9\u5f85\u3002\u8fd9\u79cd\u504f\u89c1\u53cd\u6620\u4e86\u793e\u4f1a\u6784\u5efa\u7684\u504f\u89c1\u800c\u975e\u57fa\u4e8e\u8bc1\u636e\u7684\u5173\u8054\uff0c\u53ef\u80fd\u52a0\u5267\u73b0\u6709\u4e0d\u5e73\u7b49\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528Stable Diffusion 2.1\u548c3.5 Medium\u751f\u621026,400\u5f20\u5408\u6210\u4eba\u8138\uff0c\u5206\u6790\u751f\u6210AI\u6a21\u578b\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u79ef\u6781\u5c5e\u6027\u5173\u8054\u3002\u540c\u65f6\u8bc4\u4f30\u4e09\u79cd\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u5728\u4e0d\u540c\u5c5e\u6027\u8f93\u5165\u4eba\u8138\u4e0a\u7684\u8868\u73b0\uff0c\u68c0\u6d4b\u6027\u522b\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u5371\u5bb3\uff1a(1)T2I\u6a21\u578b\u4e2d\u7cfb\u7edf\u7f16\u7801\u4e86\u5438\u5f15\u529b-\u79ef\u6781\u5c5e\u6027\u5173\u8054\uff1b(2)\u6027\u522b\u5206\u7c7b\u7cfb\u7edf\u4e2d\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff0c\u5973\u6027\u9762\u5b54\uff08\u7279\u522b\u662f\u5e26\u6709\u8d1f\u9762\u5c5e\u6027\u7684\uff09\u8bef\u5206\u7c7b\u7387\u663e\u8457\u9ad8\u4e8e\u7537\u6027\uff1b(3)\u65b0\u6a21\u578b\u901a\u8fc7\u5e74\u9f84\u540c\u8d28\u5316\u3001\u6027\u522b\u5316\u66b4\u9732\u6a21\u5f0f\u548c\u5730\u7406\u7b80\u5316\u52a0\u5267\u4e86\u5ba1\u7f8e\u7ea6\u675f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\u662f\u8de8AI\u89c6\u89c9\u7cfb\u7edf\u8fd0\u4f5c\u7684\u7cfb\u7edf\u6027\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u8868\u5f81\u548c\u8bc6\u522b\u4e24\u65b9\u9762\u52a0\u5267\u73b0\u6709\u4e0d\u5e73\u7b49\u3002\u8fd9\u4e9b\u6536\u655b\u6a21\u5f0f\u63ed\u793a\u4e86\u751f\u6210AI\u548c\u5206\u7c7b\u7b97\u6cd5\u4e2d\u5d4c\u5165\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5e72\u9884\u6765\u89e3\u51b3\u8fd9\u4e9b\u504f\u89c1\u3002"}}
{"id": "2601.12707", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12707", "abs": "https://arxiv.org/abs/2601.12707", "authors": ["Junyi Liao", "Zihan Zhu", "Ethan Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization", "comment": "Extended journal version of ICML 2025 paper. Submitted to Operations Research", "summary": "Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u7528\u4e8e\u5728\u4e24\u4eba\u96f6\u548c\u77e9\u9635\u535a\u5f08\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u6062\u590d\u672a\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u5229\u7528\u71b5\u6b63\u5219\u5316\u548c\u91cf\u5316\u54cd\u5e94\u5747\u8861\u89e3\u51b3\u9006\u95ee\u9898\u7684\u6a21\u7cca\u6027\u548c\u6570\u636e\u8986\u76d6\u6709\u9650\u95ee\u9898\u3002", "motivation": "\u4f30\u8ba1\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u672a\u77e5\u5956\u52b1\u51fd\u6570\u662f\u9006\u5f3a\u5316\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7684\u6838\u5fc3\u95ee\u9898\u3002\u8be5\u4efb\u52a1\u9762\u4e34\u9006\u95ee\u9898\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3001\u53ef\u884c\u5956\u52b1\u7684\u975e\u552f\u4e00\u6027\u4ee5\u53ca\u89c2\u6d4b\u6570\u636e\u8986\u76d6\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u91cf\u5316\u54cd\u5e94\u5747\u8861\u7684\u5956\u52b1\u51fd\u6570\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4ece\u89c2\u6d4b\u52a8\u4f5c\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u65b0\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u8bbe\u7f6e\uff0c\u53ef\u7ed3\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7b49\u65b9\u6cd5\u3002", "result": "\u4e3a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u548c\u6837\u672c\u6548\u7387\u7684\u5f3a\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6570\u503c\u7814\u7a76\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u6709\u6548\u6027\uff0c\u4e3a\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u7528\u4e8e\u5728\u71b5\u6b63\u5219\u5316\u4e24\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u6062\u590d\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u9006\u95ee\u9898\u7684\u6311\u6218\uff0c\u4e3a\u7406\u89e3\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2601.11654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11654", "abs": "https://arxiv.org/abs/2601.11654", "authors": ["Kaustubh Shivshankar Shejole", "Gaurav Mishra"], "title": "PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation", "comment": null, "summary": "Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.", "code_url": "https://github.com/KaustubhShejole/PSSI-MaxST", "code_stars": 2, "code_last_update": "2026-01-19", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u50cf\u7d20\u6bb5\u76f8\u4f3c\u6027\u6307\u6570(PSSI)\u548c\u6700\u5927\u751f\u6210\u6811(MaxST)\u7684\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c10\u6ce2\u5747\u503c\u7ed3\u5408\u50cf\u7d20\u5f3a\u5ea6\u548c\u7a7a\u95f4\u5e73\u6ed1\u7279\u5f81\uff0c\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5bf9\u7528\u6237\u4ea4\u4e92\u654f\u611f\u3001\u524d\u666f\u80cc\u666f\u989c\u8272\u5206\u5e03\u76f8\u4f3c\u65f6\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\u3002\u56fe\u5206\u5272\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u662f\u56fe\u4e2d\u8fb9\u6743\u91cd\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002", "method": "\u63d0\u51fa\u50cf\u7d20\u6bb5\u76f8\u4f3c\u6027\u6307\u6570(PSSI)\uff0c\u5229\u7528\u8c10\u6ce2\u5747\u503c\u7ed3\u5408\u50cf\u7d20\u5f3a\u5ea6\u548c\u7a7a\u95f4\u5e73\u6ed1\u7279\u5f81\uff1b\u4f7f\u7528MeanShift\u8fdb\u884c\u4f4e\u5c42\u5206\u5272\u6355\u83b7\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\uff1b\u57fa\u4e8e\u50cf\u7d20\u6bb5\u6784\u5efa\u56fe\uff0c\u7528PSSI\u786e\u5b9a\u8fb9\u6743\u91cd\uff1b\u91c7\u7528\u6700\u5927\u751f\u6210\u6811(MaxST)\u8fdb\u884c\u5206\u5272\uff0c\u6355\u83b7\u5f3a\u8fde\u63a5\u7684\u5c40\u90e8\u90bb\u57df\u3002", "result": "\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u8d28\u91cf\uff08Jaccard\u6307\u6570\u3001F1\u5206\u6570\uff09\u3001\u6267\u884c\u65f6\u95f4\u548c\u5e73\u5747\u8bef\u5dee\u65b9\u9762\u5747\u4f18\u4e8eAMOE\u3001OneCut\u3001SSNCut\u7b49\u5f53\u524d\u56fe\u57fa\u4ea4\u4e92\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684PSSI-MaxST\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u989c\u8272\u76f8\u4f3c\u6027\u3001\u5e73\u6ed1\u6027\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u548c\u5f3a\u5c40\u90e8\u8fde\u901a\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2601.12730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12730", "abs": "https://arxiv.org/abs/2601.12730", "authors": ["Zhaochun Li", "Chen Wang", "Jionghao Bai", "Shisheng Cui", "Ge Lan", "Zhou Zhao", "Yue Wang"], "title": "Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off", "comment": null, "summary": "The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \\textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the \"luck\" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \\textbf{distribution-centric} perspective for RL, in which exploration is always guided by a \"better\" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.", "code_url": "https://github.com/597358816/DCPO", "code_stars": 0, "code_last_update": "2026-01-19", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u4e2d\u5fc3\u7b56\u7565\u4f18\u5316(DCPO)\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u63a7\u5236\u71b5\u8870\u51cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u6837\u672c\u4e2d\u5fc3\u65b9\u6cd5\u63d0\u5347\u7ea620%\u6027\u80fd", "motivation": "\u73b0\u6709GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u5229\u7528\u503e\u5411\uff1a\u71b5\u5355\u8c03\u4e0b\u964d\u3001\u6837\u672c\u6536\u655b\u3001\u63a2\u7d22\u6d88\u5931\u3002\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\u591a\u4e3a\u6837\u672c\u4e2d\u5fc3\u5f0f\uff0c\u4f9d\u8d56\u7a00\u6709\u6837\u672c\u7684\"\u8fd0\u6c14\"\uff0c\u7f3a\u4e4f\u5bf9\u7b56\u7565\u7684\u539f\u5219\u6027\u63a7\u5236\uff0c\u6548\u679c\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a", "method": "\u63d0\u51fa\u5206\u5e03\u4e2d\u5fc3\u89c6\u89d2\uff0c\u5c06\u63a2\u7d22\u89c6\u4e3a\u7531\"\u66f4\u597d\"\u76ee\u6807\u5206\u5e03\u5f15\u5bfc\uff0c\u5c06\u71b5\u8c03\u63a7\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u3002DCPO\u5b8c\u5168\u5728\u7b56\u7565\u5185\u5b9e\u73b0\u53ef\u63a7\u71b5\uff0c\u65e0\u9700\u4ece\u5916\u90e8\u5206\u5e03\u91c7\u6837\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u5347\u7ea620%\u3002\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u548c\u7075\u6d3b\u6846\u67b6\uff0c\u5b9e\u73b0\u53ef\u63a7\u63a2\u7d22\u548c\u66f4\u5f3a\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861", "conclusion": "DCPO\u7528\u5206\u5e03\u7ea7\u539f\u5219\u66ff\u4ee3\u6837\u672c\u7ea7\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u53ef\u63a7\u63a2\u7d22\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6846\u67b6\uff0c\u663e\u8457\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898"}}
{"id": "2601.12751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12751", "abs": "https://arxiv.org/abs/2601.12751", "authors": ["Manjish Pal"], "title": "A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining", "comment": null, "summary": "We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \\textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u7684GNN\u8868\u8fbe\u80fd\u529b\u65b0\u6846\u67b6\uff0c\u5f15\u5165\u5b50\u7fa4\u4f53\u5e03\u5c14\u540c\u6784\u6982\u5ff5\uff0c\u8d85\u8d8a\u73b0\u6709\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff0c\u8bc6\u522b\u8868\u8fbe\u80fd\u529b\u5173\u952e\u969c\u788d\uff0c\u8bbe\u8ba1\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u5e03\u5c14\u51fd\u6570\u5b50\u7fa4\u4f53\u7684\u516c\u5e73\u7b97\u6cd5", "motivation": "\u73b0\u6709GNN\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff08\u5982WL\u3001\u53cc\u8fde\u901a\u6027\u3001\u540c\u6001\u6846\u67b6\uff09\u65e0\u6cd5\u7cbe\u7ec6\u5206\u6790GNN\u6355\u6349\u590d\u6742\u5b50\u7fa4\u4f53\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u516c\u5e73\u6027\u573a\u666f\u4e2d\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u7684\u5b50\u7fa4\u4f53\u65f6\u5b58\u5728\u5c40\u9650", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u7684\u8868\u8fbe\u80fd\u529b\u6846\u67b6\uff0c\u5f15\u5165\u5b50\u7fa4\u4f53\u5e03\u5c14\u540c\u6784\u6982\u5ff5\uff1b\u8bc6\u522b\u5085\u91cc\u53f6\u5ea6\u3001\u7535\u8def\u7c7b\uff08AC\u2070\u3001NC\u00b9\uff09\u548c\u5f71\u54cd\u529b\u4f5c\u4e3a\u8868\u8fbe\u80fd\u529b\u5173\u952e\u969c\u788d\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u7535\u8def\u904d\u5386\u7684\u516c\u5e73\u7b97\u6cd5\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u7684\u5b50\u7fa4\u4f53", "result": "\u7406\u8bba\u8bc1\u660eSBI\u4e25\u683c\u5305\u542b\u73b0\u6709\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff1b\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u56fe\u4e0a\u80fd\u5b9e\u73b0\u8de8\u4ea4\u96c6\u7fa4\u4f53\u7684\u4f4e\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5931\u8d25\uff1b\u9996\u6b21\u4e3a\u516c\u5e73\u6027\u91cf\u8eab\u5b9a\u5236GNN\u8868\u8fbe\u80fd\u529b\u7684\u539f\u7406\u6027\u5904\u7406", "conclusion": "\u63d0\u51fa\u7684\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u6846\u67b6\u4e3aGNN\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5206\u6790\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u516c\u5e73\u6027\u573a\u666f\uff1bSBI\u6982\u5ff5\u8d85\u8d8a\u73b0\u6709\u5ea6\u91cf\uff0c\u57fa\u4e8e\u7535\u8def\u904d\u5386\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u5b50\u7fa4\u4f53\uff0c\u4e3a\u516c\u5e73\u6027GNN\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2601.11665", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11665", "abs": "https://arxiv.org/abs/2601.11665", "authors": ["Amir Farzin Nikkhah", "Dong Chen", "Bradford Campbell", "Somayeh Asadi", "Arsalan Heydarian"], "title": "UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM", "comment": "Accepted for publication at the International Conference on Construction Engineering and Management (I3CE 2025)", "summary": "Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u65e0\u4eba\u673a\u5728AEC+FM\u9886\u57df\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u6570\u636e\u91c7\u96c6\u3001\u5efa\u6a21\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u51b3\u7b56\u652f\u6301\uff0c\u63d0\u51fa\u4e86\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u7684\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u4eba\u673a\u6b63\u5728\u6539\u53d8\u5efa\u7b51\u3001\u5de5\u7a0b\u3001\u65bd\u5de5\u548c\u8bbe\u65bd\u7ba1\u7406\u9886\u57df\u7684\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u65b9\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u5904\u7406\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u6574\u5408\u5148\u8fdb\u6280\u672f\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5206\u6790150\u591a\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210RGB\u56fe\u50cf\u3001LiDAR\u548c\u70ed\u4f20\u611f\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u5de5\u4f5c\u6d41\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u8def\u5f84\u89c4\u5212\u9002\u5e94\u590d\u6742\u73af\u5883\uff0c\u5f62\u6210\u5206\u6b65\u6307\u5bfc\u65b9\u6848\u3002", "result": "\u65e0\u4eba\u673a\u5df2\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u3001\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3001\u80fd\u6e90\u6548\u7387\u8bc4\u4f30\u548c\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7b49\u9886\u57df\u8bc1\u660e\u4ef7\u503c\uff0c\u4f7f\u7528YOLO\u3001Faster R-CNN\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\uff0c\u8def\u5f84\u4f18\u5316\u548c\u70ed\u96c6\u6210\u7b49\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u5c3d\u7ba1\u65e0\u4eba\u673a\u68c0\u6d4b\u6280\u672f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u5b9e\u65f6\u5904\u7406\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u6cdb\u5316\u6027\u7b49\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8f7b\u91cf\u7ea7AI\u6a21\u578b\u3001\u81ea\u9002\u5e94\u98de\u884c\u89c4\u5212\u3001\u5408\u6210\u6570\u636e\u96c6\u548c\u66f4\u4e30\u5bcc\u7684\u6a21\u6001\u878d\u5408\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u73b0\u4ee3\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u6d41\u7a0b\u3002"}}
{"id": "2601.11666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11666", "abs": "https://arxiv.org/abs/2601.11666", "authors": ["Muhammad Imran", "Chi Lee", "Yugyung Lee"], "title": "MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models", "comment": "12 pages, 3 figures, 1 table", "summary": "We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.", "AI": {"tldr": "MATEX\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u5c42\u4e00\u81f4\u6027\u5206\u6790\uff0c\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4e0d\u7cbe\u786e\u3001\u7f3a\u4e4f\u89e3\u5256\u5b66\u57fa\u7840\u3001\u6ce8\u610f\u529b\u7c92\u5ea6\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5fe0\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u89e3\u91ca\u6846\u67b6\u6765\u589e\u5f3a\u653e\u5c04\u5b66AI\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u548c\u900f\u660e\u5ea6\u3002", "method": "MATEX\u6846\u67b6\u7ed3\u5408\u591a\u5c42\u6ce8\u610f\u529b\u5c55\u5f00\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u5c42\u4e00\u81f4\u6027\u5206\u6790\uff0c\u751f\u6210\u7cbe\u786e\u3001\u7a33\u5b9a\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u68af\u5ea6\u5f52\u56e0\u56fe\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u4fe1\u606f\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728MS-CXR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMATEX\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u53d1\u73b0\u7684\u5bf9\u9f50\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684M2IB\u65b9\u6cd5\u3002", "conclusion": "MATEX\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5fe0\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u89e3\u91ca\uff0c\u5177\u6709\u589e\u5f3a\u653e\u5c04\u5b66AI\u5e94\u7528\u4fe1\u4efb\u5ea6\u548c\u900f\u660e\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12785", "abs": "https://arxiv.org/abs/2601.12785", "authors": ["Yuqi Li", "Kuiye Ding", "Chuanguang Yang", "Szu-Yu Chen", "Yingli Tian"], "title": "Distilling Time Series Foundation Models for Efficient Forecasting", "comment": "Accepted by ICASSP-2026", "summary": "Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.", "code_url": "https://github.com/itsnotacie/DistilTS-ICASSP2026", "code_stars": 0, "code_last_update": "2025-09-17", "AI": {"tldr": "DistilTS\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u548c\u67b6\u6784\u5dee\u5f02\u4e24\u5927\u6311\u6218\uff0c\u5b9e\u73b0\u6a21\u578b\u538b\u7f29\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u867d\u7136\u9884\u6d4b\u6027\u80fd\u5f3a\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u73b0\u6709\u901a\u7528\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7279\u6b8a\u6027\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u84b8\u998f\u6846\u67b6\u3002", "method": "\u63d0\u51faDistilTS\u6846\u67b6\uff1a1) \u9488\u5bf9\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\uff0c\u5f15\u5165horizon-weighted objectives\u5e73\u8861\u4e0d\u540c\u9884\u6d4b\u65f6\u957f\u7684\u5b66\u4e60\uff1b2) \u9488\u5bf9\u67b6\u6784\u5dee\u5f02\uff0c\u8bbe\u8ba1temporal alignment strategy\u51cf\u5c11\u67b6\u6784\u4e0d\u5339\u914d\u3002\u901a\u8fc7\u8fd9\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\u5b9e\u73b0\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDistilTS\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u6574\u89c4\u6a21TSFMs\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u53c2\u6570\u51cf\u5c11\u9ad8\u8fbe1/150\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe6000\u500d\u3002", "conclusion": "DistilTS\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u7684\u84b8\u998f\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u548c\u67b6\u6784\u5dee\u5f02\u4e24\u5927\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u538b\u7f29\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.11675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11675", "abs": "https://arxiv.org/abs/2601.11675", "authors": ["Ritik Raina", "Abe Leite", "Alexandros Graikos", "Seoyoung Ahn", "Dimitris Samaras", "Gregory J. Zelinsky"], "title": "Generating metamers of human scene understanding", "comment": null, "summary": "Human vision combines low-resolution \"gist\" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. \"foveated\") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a \"same\" or \"different\" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.", "AI": {"tldr": "MetamerGen\u662f\u4e00\u4e2a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5468\u8fb9\u89c6\u89c9\u83b7\u5f97\u7684\u573a\u666f\u8981\u70b9\u4fe1\u606f\u548c\u6ce8\u89c6\u70b9\u83b7\u5f97\u7684\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u751f\u6210\u4e0e\u4eba\u7c7b\u573a\u666f\u7406\u89e3\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u7269\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u7ed3\u5408\u5468\u8fb9\u4f4e\u5206\u8fa8\u7387\"\u8981\u70b9\"\u4fe1\u606f\u548c\u6ce8\u89c6\u70b9\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u6765\u7406\u89e3\u573a\u666f\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u6f5c\u5728\u573a\u666f\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u573a\u666f\u7406\u89e3\u673a\u5236\u3002", "method": "\u63d0\u51faMetamerGen\u2014\u2014\u4e00\u79cd\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u6d41\u8868\u793a\u65b9\u6cd5\uff1a\u4f7f\u7528DINOv2\u4ee4\u724c\u878d\u5408\u6ce8\u89c6\u533a\u57df\u7684\u8be6\u7ec6\u7279\u5f81\u548c\u5468\u8fb9\u964d\u7ea7\u7279\u5f81\uff08\u6355\u6349\u573a\u666f\u4e0a\u4e0b\u6587\uff09\u3002\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\uff08\u76f8\u540c-\u4e0d\u540c\u5224\u65ad\u4efb\u52a1\uff09\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e0e\u4eba\u7c7b\u6f5c\u5728\u573a\u666f\u8868\u5f81\u7684\u611f\u77e5\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "MetamerGen\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u573a\u666f\u7406\u89e3\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u7269\u3002\u5f53\u751f\u6210\u573a\u666f\u57fa\u4e8e\u89c2\u770b\u8005\u81ea\u8eab\u7684\u6ce8\u89c6\u533a\u57df\u65f6\uff0c\u9ad8\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50\u6700\u80fd\u9884\u6d4b\u5143\u5339\u914d\u6027\u3002\u6982\u5ff5\u9a8c\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u591a\u4e2a\u89c6\u89c9\u5904\u7406\u5c42\u6b21\u4e0a\u5f71\u54cd\u4eba\u7c7b\u5224\u65ad\u7684\u5177\u4f53\u7279\u5f81\u3002", "conclusion": "MetamerGen\u662f\u7406\u89e3\u573a\u666f\u7406\u89e3\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u6f5c\u5728\u573a\u666f\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u7269\uff0c\u4e3a\u7814\u7a76\u4eba\u7c7b\u89c6\u89c9\u573a\u666f\u7406\u89e3\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.12807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12807", "abs": "https://arxiv.org/abs/2601.12807", "authors": ["Zixing Song", "Irwin King"], "title": "Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs", "comment": null, "summary": "The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.", "AI": {"tldr": "SIT-Graph\uff1a\u4e00\u79cd\u7528\u4e8e\u56fe\u5b66\u4e60\u7684\u534a\u76d1\u7763\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8bad\u7ec3\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u589e\u5f3aLLMs\u5728\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5e26\u6807\u7b7e\u8282\u70b9\u6784\u5efa(\u6307\u4ee4,\u8f93\u51fa)\u5bf9\uff0c\u8fd9\u5728\u793e\u4ea4\u7b49\u9886\u57df\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u83b7\u53d6\uff1b\u540c\u65f6\u672a\u80fd\u5145\u5206\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u4e2d\u56e0\u8fb9\u8fde\u63a5\u4ea7\u751f\u7684\u6f5c\u5728\u76f8\u5173\u6027", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684SIT-Graph\u6846\u67b6\uff1a1) \u521d\u59cb\u4ec5\u7528\u6807\u8bb0\u8282\u70b9\u6307\u4ee4\u5bf9\u5fae\u8c03\u6a21\u578b\uff1b2) \u4e3a\u672a\u6807\u8bb0\u8282\u70b9\u751f\u6210\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u4f2a\u54cd\u5e94\uff1b3) \u8fed\u4ee3\u81ea\u8bad\u7ec3\u9010\u6b65\u5bf9\u9f50LLM\u4e0e\u5e95\u5c42\u8282\u70b9\u76f8\u5173\u6027", "result": "\u5c06SIT-Graph\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u56fe\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4e2d\uff0c\u5728\u6587\u672c\u5c5e\u6027\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u4f4e\u6807\u7b7e\u6bd4\u4f8b\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u8d85\u8fc720%\u7684\u6539\u8fdb", "conclusion": "SIT-Graph\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6307\u4ee4\u8c03\u4f18\u4e2d\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u7684\u6f5c\u5728\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347LLMs\u5728\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u6a21\u578b\u65e0\u5173\u7684\u901a\u7528\u6027"}}
{"id": "2601.11679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11679", "abs": "https://arxiv.org/abs/2601.11679", "authors": ["Richard Hartley"], "title": "Conformal Point and the Calibrated Conic", "comment": null, "summary": "This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff0c\u8fd9\u4e9b\u6982\u5ff5\u6709\u52a9\u4e8e\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u56fe\u50cf\u4e2d\u89d2\u5ea6\u548c\u65b9\u5411\u7684\u76f4\u89c2\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u76f4\u89c2\u7684\u51e0\u4f55\u53ef\u89c6\u5316\u5de5\u5177\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316\u56fe\u50cf\u4e2d\u89d2\u5ea6\u548c\u65b9\u5411\u7b49\u51e0\u4f55\u5c5e\u6027\u7684\u8ba1\u7b97\uff0c\u63d0\u9ad8\u56fe\u50cf\u51e0\u4f55\u5206\u6790\u7684\u76f4\u89c2\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\uff0c\u5206\u6790\u5b83\u4eec\u4e4b\u95f4\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6982\u5ff5\u6784\u5efa\u56fe\u50cf\u51e0\u4f55\u7684\u53ef\u89c6\u5316\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u5171\u5f62\u70b9\u4e0e\u6821\u51c6\u5706\u9525\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u8fd9\u4e9b\u6982\u5ff5\u7684\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u89c2\u5730\u8ba1\u7b97\u56fe\u50cf\u4e2d\u7684\u89d2\u5ea6\u548c\u65b9\u5411\u3002", "conclusion": "\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u4e3a\u56fe\u50cf\u51e0\u4f55\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u89c6\u5316\u5de5\u5177\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u51e0\u4f55\u5c5e\u6027\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4e86\u56fe\u50cf\u51e0\u4f55\u7406\u89e3\u7684\u76f4\u89c2\u6027\u3002"}}
{"id": "2601.11724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11724", "abs": "https://arxiv.org/abs/2601.11724", "authors": ["Muditha Fernando", "Kajhanan Kailainathan", "Krishnakanth Nagaratnam", "Isuranga Udaravi Bandara Senavirathne", "Ranga Rodrigo"], "title": "SemAlign: Language Guided Semi-supervised Domain Generalization", "comment": "15 pages, 6 figures", "summary": "Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u91c7\u7528\u56fe\u50cf\u7ea7\u589e\u5f3a\u548c\u8f93\u51fa\u7ea7\u6b63\u5219\u5316\u7b56\u7565\u63d0\u9ad8\u6570\u636e\u5229\u7528\u7387\u548c\u9632\u6b62\u8fc7\u62df\u5408\u3002", "motivation": "\u73b0\u6709SSDG\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u800c\u5ffd\u89c6\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u6570\u636e\u5229\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u4f2a\u6807\u7b7e\u8d28\u91cf\u53c8\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u65b9\u6cd5\u6765\u6539\u5584\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u8bed\u4e49\u4e30\u5bcc\u4e14\u6cdb\u5316\u7684\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ee5\u4fc3\u8fdb\u57df\u4e0d\u53d8\u6027\uff1b2) \u91c7\u7528\u6709\u6548\u7684\u56fe\u50cf\u7ea7\u589e\u5f3a\u7b56\u7565\u63d0\u9ad8\u6570\u636e\u5229\u7528\u7387\uff1b3) \u4f7f\u7528\u8f93\u51fa\u7ea7\u6b63\u5219\u5316\u7b56\u7565\u6700\u5c0f\u5316\u8fc7\u62df\u5408\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e0e\u73b0\u6709SSDG\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb(SOTA)\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u7279\u5f81\u4e0eVLM\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3SSDG\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u7684\u8fc7\u5ea6\u5173\u6ce8\u3002"}}
{"id": "2601.12859", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.12859", "abs": "https://arxiv.org/abs/2601.12859", "authors": ["Luca Schaufelberger", "Aline Hartgers", "Kjell Jorner"], "title": "Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates", "comment": null, "summary": "Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.", "AI": {"tldr": "PuckerFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u73af\u72b6\u5206\u5b50\u7684\u6784\u8c61\u751f\u6210\uff0c\u901a\u8fc7Cremer-Pople\u7a7a\u95f4\u786e\u4fdd\u751f\u6210\u6709\u6548\u95ed\u73af\u7ed3\u6784\uff0c\u5728\u591a\u6837\u6027\u548c\u7cbe\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73af\u72b6\u5206\u5b50\u5728\u5316\u5b66\u548c\u751f\u7269\u5b66\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5176\u53d7\u9650\u7684\u6784\u8c61\u67d4\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u9884\u7ec4\u7ec7\u6027\uff0c\u8fd9\u5bf9\u836f\u7269\u53d1\u73b0\u548c\u50ac\u5316\u4e2d\u7684\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u53ef\u9760\u5730\u91c7\u6837\u73af\u7cfb\u7edf\u7684\u6784\u8c61\u96c6\u5408\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165PuckerFlow\u751f\u6210\u6a21\u578b\uff0c\u5728Cremer-Pople\u7a7a\u95f4\u4e0a\u6267\u884c\u6d41\u5339\u914d\u3002Cremer-Pople\u7a7a\u95f4\u662f\u4e00\u4e2a\u4f4e\u7ef4\u5185\u90e8\u5750\u6807\u7cfb\uff0c\u80fd\u591f\u6355\u6349\u73af\u7684\u76f8\u5173\u81ea\u7531\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u8bbe\u8ba1\u751f\u6210\u6709\u6548\u7684\u95ed\u73af\u7ed3\u6784\u3002", "result": "PuckerFlow\u5728\u51e0\u4e4e\u6240\u6709\u5b9a\u91cf\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u6784\u8c61\u751f\u6210\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u751f\u6210\u6784\u8c61\u7684\u591a\u6837\u6027\u548c\u7cbe\u786e\u6027\u65b9\u9762\u7684\u5f3a\u5927\u6027\u80fd\u3002\u7279\u522b\u5c55\u793a\u4e86PuckerFlow\u5728\u50ac\u5316\u3001\u836f\u7269\u53d1\u73b0\u7b49\u5316\u5b66\u5e94\u7528\u76f8\u5173\u73af\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5b9e\u73b0\u4e86\u73af\u72b6\u7ed3\u6784\u7684\u9ad8\u6548\u53ef\u9760\u6784\u8c61\u751f\u6210\uff0c\u4e3a\u5efa\u6a21\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u4ee5\u53ca\u8de8\u5316\u5b66\u548c\u751f\u7269\u5b66\u5e7f\u6cdb\u5e94\u7528\u7684\u5c5e\u6027\u5f15\u5bfc\u73af\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.11729", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11729", "abs": "https://arxiv.org/abs/2601.11729", "authors": ["Turhan Can Kargin", "Wojciech Jasi\u0144ski", "Adam Pardyl", "Bartosz Zieli\u0144ski", "Marcin Przewi\u0119\u017alikowski"], "title": "SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models", "comment": "Project page is available at https://sparrta.gmum.net/", "summary": "Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINO\u548cCLIP\uff09\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5177\u8eab\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u8fd1\u671f\u7814\u7a76\u5c1d\u8bd5\u5c063D\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\uff09\u878d\u5165VFM\u8bad\u7ec3\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u7a7a\u95f4\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5f15\u53d1\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u7a7a\u95f4\u610f\u8bc6\u8fd8\u662f\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a3D\u76ee\u6807\u7684\u7591\u95ee\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\uff08SpaRRTa\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u610f\u6570\u91cf\u7684\u903c\u771f\u56fe\u50cf\uff08\u5305\u542b\u591a\u6837\u5316\u573a\u666f\u548c\u5b8c\u5168\u53ef\u63a7\u7684\u5bf9\u8c61\u6392\u5217\uff09\u4ee5\u53ca\u53ef\u81ea\u7531\u8bbf\u95ee\u7684\u7a7a\u95f4\u6807\u6ce8\uff0c\u8bc4\u4f30VFM\u8bc6\u522b\u56fe\u50cf\u4e2d\u7269\u4f53\u76f8\u5bf9\u4f4d\u7f6e\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u4e0d\u540c\u4e8e\u4f20\u7edf3D\u76ee\u6807\uff08\u5982\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\uff09\u5173\u6ce8\u7cbe\u786e\u5ea6\u91cf\u9884\u6d4b\uff0c\u800c\u662f\u63a2\u7a76\u652f\u6491\u66f4\u9ad8\u7ea7\u4eba\u7c7b\u7a7a\u95f4\u7406\u89e3\u7684\u57fa\u672c\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002\u901a\u8fc7\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u5bf9\u73b0\u4ee3VFM\u4e2d\u652f\u6301\u6216\u963b\u788d\u7a7a\u95f4\u610f\u8bc6\u673a\u5236\u7684\u89c1\u89e3\u3002", "conclusion": "SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\u6709\u671b\u6210\u4e3a\u6307\u5bfc\u672a\u6765\u7a7a\u95f4\u611f\u77e5\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u7684\u6709\u7528\u5de5\u5177\u3002\u7814\u7a76\u8868\u660e\u73b0\u6709VFM\u5728\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.11769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11769", "abs": "https://arxiv.org/abs/2601.11769", "authors": ["Cheng Lyu", "Jingyue Zhang", "Ryan Maunu", "Mengwei Li", "Vinny DeGenova", "Yuanli Pei"], "title": "From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce", "comment": null, "summary": "Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u5206\u7c7b\u7684\u89c6\u89c9\u641c\u7d22\u67b6\u6784\uff0c\u4f7f\u7528\u65e0\u5206\u7c7b\u533a\u57df\u5efa\u8bae\u548c\u7edf\u4e00\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\uff0c\u5e76\u5f15\u5165LLM-as-a-Judge\u6846\u67b6\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u5728\u7535\u5546\u5bb6\u5c45\u5e73\u53f0\u5b9e\u73b0\u90e8\u7f72\u548c\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7535\u5546\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\u901a\u5e38\u5c06\u76ee\u6807\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u5206\u7c7b\u5b66\u7684\u5206\u7c7b\u8026\u5408\uff0c\u4f9d\u8d56\u6709\u566a\u58f0\u7684\u76ee\u5f55\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7279\u522b\u662f\u5728\u98ce\u683c\u9a71\u52a8\u7684\u9886\u57df\uff0c\u7528\u6237\u610f\u56fe\u4e3b\u89c2\u4e14\u5f00\u653e\u3002", "method": "1. \u63d0\u51fa\u5206\u7c7b\u5b66\u89e3\u8026\u67b6\u6784\uff1a\u4f7f\u7528\u65e0\u5206\u7c7b\u533a\u57df\u5efa\u8bae\u751f\u6210\u5019\u9009\u533a\u57df\uff0c\u901a\u8fc7\u7edf\u4e00\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u901a\u7528\u7684\u89c6\u89c9\u641c\u7d22\uff1b2. \u5f15\u5165LLM-as-a-Judge\u6846\u67b6\uff1a\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8bc4\u4f30\u67e5\u8be2-\u7ed3\u679c\u5bf9\u7684\u7ec6\u5fae\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u7c7b\u522b\u76f8\u5173\u6027\uff0c\u6446\u8131\u5bf9\u4eba\u5de5\u6807\u6ce8\u6216\u6709\u566a\u58f0\u76ee\u5f55\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5168\u7403\u5bb6\u5c45\u7535\u5546\u5e73\u53f0\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u7cfb\u7edf\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u5e26\u6765\u4e86\u53ef\u6d4b\u91cf\u7684\u5ba2\u6237\u53c2\u4e0e\u5ea6\u63d0\u5347\uff0c\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\u4e0e\u5b9e\u9645\u4e1a\u52a1\u7ed3\u679c\u5f3a\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u5b66\u89e3\u8026\u67b6\u6784\u548cLLM-as-a-Judge\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7535\u5546\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.12893", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12893", "abs": "https://arxiv.org/abs/2601.12893", "authors": ["Ting Dang", "Soumyajit Chatterjee", "Hong Jia", "Yu Wu", "Flora Salim", "Fahim Kawsar"], "title": "AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs", "comment": "Accepted by ICASSP 2026", "summary": "Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\\% and 28.4\\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.", "AI": {"tldr": "AdaNODEs\uff1a\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6e90\u65e0\u5173\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u5904\u7406\u5206\u5e03\u504f\u79fb\uff0c\u4ec5\u9700\u66f4\u65b0\u6709\u9650\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u72ec\u7acb\u6570\u636e\u8bbe\u8ba1\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7279\u6027\uff0c\u4e14\u5f88\u5c11\u5904\u7406\u9884\u6d4b\u4efb\u52a1\u3002\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u9700\u8981\u4e13\u95e8\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAdaNODEs\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u6784\u5efa\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u4e13\u95e8\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5206\u5e03\u504f\u79fb\u7279\u6027\u3002\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\u6765\u5904\u7406\u9884\u6d4b\u4efb\u52a1\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff0c\u4ec5\u9700\u66f4\u65b0\u6709\u9650\u6a21\u578b\u53c2\u6570\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5728\u4e00\u7ef4\u548c\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAdaNODEs\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e865.88%\u548c28.4%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728\u66f4\u9ad8\u4e25\u91cd\u7a0b\u5ea6\u7684\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AdaNODEs\u662f\u4e00\u79cd\u6709\u6548\u7684\u6e90\u65e0\u5173\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u8bbe\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002"}}
{"id": "2601.11779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11779", "abs": "https://arxiv.org/abs/2601.11779", "authors": ["Vinicius F. Arruda", "Rodrigo F. Berriel", "Thiago M. Paix\u00e3o", "Claudine Badue", "Alberto F. De Souza", "Nicu Sebe", "Thiago Oliveira-Santos"], "title": "Cross-Domain Object Detection Using Unsupervised Image Translation", "comment": null, "summary": "Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528CycleGAN\u548cAdaIN\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\uff0c\u65e0\u9700\u76ee\u6807\u57df\u6807\u6ce8\u6570\u636e\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4e2d\u95f4\u7279\u5f81\u5bf9\u9f50\u53d6\u5f97\u4e86\u4e0d\u9519\u6548\u679c\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u8bad\u7ec3\u7684\u4e0a\u754c\u4ecd\u6709\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\uff08CycleGAN\u548cAdaIN-based\u6a21\u578b\uff09\uff0c\u4ec5\u5229\u7528\u6e90\u57df\u6807\u6ce8\u6570\u636e\u548c\u76ee\u6807\u57df\u975e\u6807\u6ce8\u6570\u636e\uff0c\u751f\u6210\u76ee\u6807\u57df\u7684\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u4e0e\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u8bad\u7ec3\u7684\u4e0a\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u590d\u6742\u5ea6\u66f4\u4f4e\u3001\u66f4\u6613\u4e8e\u89e3\u91ca\uff0c\u800c\u4e14\u6548\u679c\u66f4\u597d\uff0c\u4e3a\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.12903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12903", "abs": "https://arxiv.org/abs/2601.12903", "authors": ["Meng Liu", "Ke Liang", "Siwei Wang", "Xingchen Hu", "Sihang Zhou", "Xinwang Liu"], "title": "Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets", "comment": null, "summary": "Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.", "code_url": "https://github.com/MGitHubL/BenchTGC", "code_stars": 1, "code_last_update": "2025-10-21", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBenchTGC\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u95f4\u56fe\u805a\u7c7b\uff08TGC\uff09\u4efb\u52a1\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u8bbe\u8ba1\u6846\u67b6\u3001\u6539\u8fdb\u73b0\u6709\u805a\u7c7b\u6280\u672f\u4ee5\u53ca\u521b\u5efa\u9002\u5408TGC\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u65f6\u95f4\u56fe\u805a\u7c7b\u662f\u4e00\u4e2a\u65b0\u5174\u4f46\u5173\u6ce8\u5ea6\u4f4e\u7684\u4efb\u52a1\uff0c\u76f8\u6bd4\u9759\u6001\u56fe\u805a\u7c7b\uff0c\u5b83\u80fd\u591f\u901a\u8fc7\u57fa\u4e8e\u4ea4\u4e92\u5e8f\u5217\u7684\u6279\u5904\u7406\u6a21\u5f0f\u5728\u65f6\u95f4\u8981\u6c42\u548c\u7a7a\u95f4\u8981\u6c42\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4e0d\u9002\u7528\u7684\u805a\u7c7b\u6280\u672f\u548c\u4e0d\u9002\u7528\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86TGC\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86BenchTGC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\uff1a1\uff09\u8bbe\u8ba1BenchTGC\u6846\u67b6\u6765\u8bf4\u660e\u65f6\u95f4\u56fe\u805a\u7c7b\u7684\u8303\u5f0f\uff1b2\uff09\u6539\u8fdb\u73b0\u6709\u805a\u7c7b\u6280\u672f\u4ee5\u9002\u5e94\u65f6\u95f4\u56fe\uff1b3\uff09\u8ba8\u8bba\u516c\u5171\u65f6\u95f4\u56fe\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u591a\u4e2a\u9002\u5408TGC\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff08BenchTGC\u6570\u636e\u96c6\uff09\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BenchTGC\u7684\u4f18\u52bf\uff0c\u5e76\u8bc1\u660e\u4e86\u65f6\u95f4\u56fe\u805a\u7c7b\u4efb\u52a1\u7684\u5fc5\u8981\u6027\u548c\u91cd\u8981\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u53d8\u5316\u548c\u590d\u6742\u7684\u573a\u666f\u662f\u65f6\u95f4\u56fe\u805a\u7c7b\u7684\u57fa\u7840\u3002", "conclusion": "BenchTGC\u4e3a\u65f6\u95f4\u56fe\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86TGC\u7684\u4f18\u52bf\uff0c\u8fd8\u5f3a\u8c03\u4e86\u5176\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u590d\u6742\u573a\u666f\u4e2d\u7684\u91cd\u8981\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.12917", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12917", "abs": "https://arxiv.org/abs/2601.12917", "authors": ["He Sun", "Jinrui Zhou", "Li Li", "Mingjun Xiao"], "title": "CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction", "comment": "14 pages, 9 figures, under review", "summary": "Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\\%$, accelerates convergence by $8.8 \\times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.", "AI": {"tldr": "CooperLLM\uff1a\u4e91\u8f85\u52a9\u7684\u8fb9\u7f18\u7aef\u534f\u540c\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u79fb\u52a8\u7aef\u7684\u96f6\u9636\u4f18\u5316\u548c\u4e91\u7aef\u7684\u68af\u5ea6\u4fee\u6b63\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u7cbe\u5ea6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5fae\u8c03\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5185\u5b58\u5bc6\u96c6\u578b\u53cd\u5411\u4f20\u64ad\uff0c\u8981\u4e48\u4f7f\u7528\u6536\u655b\u6162\u3001\u7cbe\u5ea6\u4f4e\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5", "method": "\u63d0\u51faCooperLLM\u6846\u67b6\uff1a\u79fb\u52a8\u5ba2\u6237\u7aef\u5728\u79c1\u6709\u6570\u636e\u4e0a\u6267\u884c\u8f7b\u91cf\u7ea7\u96f6\u9636\u4f18\u5316\u66f4\u65b0\uff0c\u4e91\u7aef\u5728\u8f85\u52a9\u516c\u5171\u6570\u636e\u4e0a\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u5fae\u8c03\u5e76\u6ce8\u5165\u5f15\u5bfc\u6270\u52a8\u6765\u4fee\u6b63\u672c\u5730\u66f4\u65b0\uff1b\u5f15\u5165\u6d41\u6c34\u7ebf\u8c03\u5ea6\u548c\u81ea\u9002\u5e94\u538b\u7f29\u6765\u91cd\u53e0\u8ba1\u7b97\u901a\u4fe1\u5e76\u51cf\u5c11\u5185\u5b58\u4f7f\u7528", "result": "\u5728\u591a\u4e2aTransformer\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCooperLLM\u5c06\u8bbe\u5907\u5185\u5b58\u964d\u4f4e\u9ad8\u8fbe86.4%\uff0c\u52a0\u901f\u6536\u655b8.8\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u9636\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe10\u4e2a\u767e\u5206\u70b9", "conclusion": "CooperLLM\u901a\u8fc7\u4e91\u8f85\u52a9\u7684\u534f\u540c\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u5fae\u8c03\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u7cbe\u5ea6"}}
{"id": "2601.11898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11898", "abs": "https://arxiv.org/abs/2601.11898", "authors": ["Yilmaz Korkmaz", "Vishal M. Patel"], "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection", "comment": null, "summary": "Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \\href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\\underline{here}}.", "code_url": "https://github.com/yilmazkorkmaz1/RemoteVAR", "code_stars": 2, "code_last_update": "2026-01-21", "AI": {"tldr": "RemoteVAR\uff1a\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u7279\u5f81\u878d\u5408\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6269\u6563\u548cTransformer\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u53ef\u63a7\u6027\u5f31\u3001\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u548c\u66dd\u5149\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u5e94\u7528\u4e8e\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51faRemoteVAR\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u81ea\u56de\u5f52\u9884\u6d4b\u6761\u4ef6\u5316\u4e8e\u591a\u5206\u8fa8\u7387\u878d\u5408\u7684\u53cc\u65f6\u76f8\u7279\u5f81\uff1b2\uff09\u91c7\u7528\u4e13\u95e8\u4e3a\u53d8\u5316\u56fe\u9884\u6d4b\u8bbe\u8ba1\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u53d8\u5316\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRemoteVAR\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u548cTransformer\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5efa\u7acb\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u81ea\u56de\u5f52\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RemoteVAR\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u878d\u5408\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.12928", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.12928", "abs": "https://arxiv.org/abs/2601.12928", "authors": ["Yaima Paz Soto", "Silena Herold Garcia", "Ximo Gual-Arnau", "Antoni Jaume-i-Cap\u00f3", "Manuel Gonz\u00e1lez-Hidalgo"], "title": "An efficient heuristic for geometric analysis of cell deformations", "comment": null, "summary": "Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f62\u72b6\u7a7a\u95f4\u548c\u6a21\u677f\u5bf9\u9f50\u7684\u9570\u72b6\u7ec6\u80de\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u5316\u548c\u6a21\u677f\u5bf9\u9f50\u7b80\u5316\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9570\u72b6\u7ec6\u80de\u75c5\u5bfc\u81f4\u7ea2\u7ec6\u80de\u53d8\u5f62\uff0c\u5f71\u54cd\u8840\u6db2\u6d41\u52a8\u548c\u6c27\u6c14\u8f93\u9001\uff0c\u5168\u7403\u60a3\u75c5\u7387\u9ad8\u4e14\u5bf9\u533b\u7597\u7cfb\u7edf\u8d1f\u62c5\u91cd\u3002\u81ea\u52a8\u5206\u7c7b\u9570\u72b6\u7ec6\u80de\u5bf9\u4e8e\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u3001\u907f\u514d\u91cf\u5316\u9519\u8bef\u548c\u8bc4\u4f30\u5371\u673a\u4e25\u91cd\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5f62\u72b6\u7a7a\u95f4\u65b9\u6cd5\u867d\u51c6\u786e\u7387\u9ad8\u4f46\u8ba1\u7b97\u590d\u6742\u3002", "method": "\u5c06\u7ea2\u7ec6\u80de\u5efa\u6a21\u4e3a\u5f62\u72b6\u7a7a\u95f4\u4e2d\u7684\u5c01\u95ed\u5e73\u9762\u66f2\u7ebf\uff0c\u4f7f\u7528\u5f39\u6027\u8ddd\u79bb\uff08\u5bf9\u65cb\u8f6c\u3001\u5e73\u79fb\u3001\u7f29\u653e\u548c\u91cd\u53c2\u6570\u5316\u4e0d\u53d8\uff09\u3002\u521b\u65b0\u70b9\u5305\u62ec\uff1a(1) \u57fa\u4e8e\u7ec6\u80de\u4e3b\u8f74\u4f7f\u7528\u56fa\u5b9a\u53c2\u6570\u5316\u8ba1\u7b97\u8ddd\u79bb\uff1b(2) \u5728\u8ba1\u7b97\u8ddd\u79bb\u524d\uff0c\u4f7f\u7528\u8be5\u53c2\u6570\u5316\u5c06\u6bcf\u4e2a\u7ec6\u80de\u4e0e\u4e24\u4e2a\u6a21\u677f\u5bf9\u9f50\u3002\u8fd9\u79cd\u6a21\u677f\u5bf9\u9f50\u7b56\u7565\u7b80\u5316\u4e86\u8ba1\u7b97\u3002", "result": "\u5728\u76d1\u7763\u5206\u7c7b\u548c\u65e0\u76d1\u7763\u805a\u7c7b\u4e2d\u5747\u8fbe\u523096.03%\u7684\u51c6\u786e\u7387\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u6539\u8fdb\u5f62\u72b6\u7a7a\u95f4\u6a21\u578b\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u56fa\u5b9a\u53c2\u6570\u5316\u548c\u6a21\u677f\u5bf9\u9f50\u7684\u5f62\u72b6\u7a7a\u95f4\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7ea2\u7ec6\u80de\u5206\u7c7b\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u9570\u72b6\u7ec6\u80de\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11907", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11907", "abs": "https://arxiv.org/abs/2601.11907", "authors": ["Prosenjit Chatterjee", "ANK Zaman"], "title": "Towards Airborne Object Detection: A Deep Learning Analysis", "comment": null, "summary": "The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.", "AI": {"tldr": "\u57fa\u4e8eEfficientNetB4\u7684\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u8fdb\u884c\u7a7a\u4e2d\u76ee\u6807\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\uff0c\u5728\u81ea\u5efaAODTA\u6570\u636e\u96c6\u4e0a\u53d6\u5f9796%\u5206\u7c7b\u51c6\u786e\u7387\u548c90%\u5a01\u80c1\u9884\u6d4b\u51c6\u786e\u7387", "motivation": "\u7a7a\u4e2d\u5e73\u53f0\uff08\u5546\u7528\u98de\u673a\u3001\u65e0\u4eba\u673a\u3001UAV\uff09\u5feb\u901f\u589e\u591a\uff0c\u9700\u8981\u5b9e\u65f6\u81ea\u52a8\u5316\u5a01\u80c1\u8bc4\u4f30\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76d1\u63a7\uff0c\u53ef\u6269\u5c55\u6027\u6709\u9650\u4e14\u6548\u7387\u4f4e\u4e0b", "method": "\u91c7\u7528EfficientNetB4\u6784\u5efa\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u6267\u884c\u7a7a\u4e2d\u76ee\u6807\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u805a\u5408\u591a\u4e2a\u516c\u5f00\u6e90\u6784\u5efaAODTA\u6570\u636e\u96c6\u3002\u5728AVD\u6570\u636e\u96c6\u548cAODTA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0eResNet-50\u57fa\u7ebf\u5bf9\u6bd4", "result": "EfficientNetB4\u6a21\u578b\u5728\u76ee\u6807\u5206\u7c7b\u4e0a\u8fbe\u523096%\u51c6\u786e\u7387\uff0c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\u8fbe\u523090%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eResNet-50\u57fa\u7ebf\u3002\u6a21\u578b\u5728AVD\u548cAODTA\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u826f\u597d", "conclusion": "\u63d0\u51fa\u7684\u53cc\u4efb\u52a1EfficientNetB4\u6a21\u578b\u5728\u81ea\u52a8\u5a01\u80c1\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u76d1\u89c6\u3001\u56fd\u9632\u548c\u7a7a\u57df\u7ba1\u7406\u5e94\u7528\u3002\u867d\u7136\u6807\u9898\u63d0\u53ca\u68c0\u6d4b\uff0c\u4f46\u7814\u7a76\u4e13\u6ce8\u4e8e\u4f7f\u7528\u9884\u5b9a\u4f4d\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u548c\u5a01\u80c1\u63a8\u65ad"}}
{"id": "2601.11909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11909", "abs": "https://arxiv.org/abs/2601.11909", "authors": ["Io Yamada", "Hirotsugu Okuno"], "title": "Effects of the retina-inspired light intensity encoding on color discrimination performance", "comment": "8 pages, 14 figures, 4 tables", "summary": "Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u4e2d\u4e0d\u540c\u5149\u5f3a\u7f16\u7801\u51fd\u6570\uff08\u5bf9\u6570\u51fd\u6570 vs Naka-Rushton\u51fd\u6570\uff09\u5bf9\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0N-R\u51fd\u6570\u7ed3\u5408\u53cc\u5bf9\u7acb\u8272\u5e73\u9762\u8868\u793a\u63d0\u4f9b\u6700\u4f73\u989c\u8272\u8fa8\u522b\u6027\u80fd\u3002", "motivation": "\u989c\u8272\u662f\u7269\u4f53\u8bc6\u522b\u7b49\u89c6\u89c9\u529f\u80fd\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u53d7\u5149\u7167\u989c\u8272\u5f71\u54cd\u5f88\u5927\u3002\u989c\u8272\u6052\u5e38\u6027\uff08CC\uff09\u662f\u89c6\u89c9\u7cfb\u7edf\u7684\u91cd\u8981\u7279\u6027\uff0c\u80fd\u591f\u72ec\u7acb\u4e8e\u5149\u7167\u989c\u8272\u611f\u77e5\u76ee\u6807\u989c\u8272\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5149\u5f3a\u7f16\u7801\u51fd\u6570\u5bf9\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u5149\u5f3a\u7f16\u7801\u51fd\u6570\uff1a\u539f\u59cbC/S Retinex\u6a21\u578b\u7684\u5bf9\u6570\u51fd\u6570\u548c\u89c6\u7f51\u819c\u5149\u611f\u53d7\u5668\u54cd\u5e94\u6a21\u578bNaka-Rushton\u51fd\u6570\u3002\u91c7\u7528\u989c\u8272\u53ef\u53d8LED\u4ee5\u4e0d\u540c\u5149\u7167\u989c\u8272\u7167\u5c04\u89c6\u89c9\u76ee\u6807\uff0c\u8bc4\u4f30\u5404\u6a21\u578b\u8ba1\u7b97\u7684\u989c\u8272\u4fe1\u606f\u5728\u4e0d\u540c\u5149\u7167\u4e0b\u5bf9\u76ee\u6807\u989c\u8272\u7684\u8fa8\u522b\u80fd\u529b\u3002\u989c\u8272\u4fe1\u606f\u4f7f\u7528HSV\u8272\u5f69\u7a7a\u95f4\u548c\u57fa\u4e8e\u7ecf\u5178\u5bf9\u7acb\u8272\u7406\u8bba\u7684\u8272\u5e73\u9762\u8868\u793a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cNaka-Rushton\u51fd\u6570\u4e0e\u53cc\u5bf9\u7acb\u8272\u5e73\u9762\u8868\u793a\u7684\u7ec4\u5408\u63d0\u4f9b\u4e86\u6700\u4f18\u7684\u989c\u8272\u8fa8\u522b\u6027\u80fd\u3002", "conclusion": "Naka-Rushton\u51fd\u6570\u4f5c\u4e3a\u5149\u5f3a\u7f16\u7801\u51fd\u6570\uff0c\u7ed3\u5408\u53cc\u5bf9\u7acb\u8272\u5e73\u9762\u8868\u793a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4e2d\u5fc3/\u5468\u8fb9Retinex\u6a21\u578b\u7684\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\uff0c\u4e3a\u4f7f\u7528\u989c\u8272\u4fe1\u606f\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u989c\u8272\u8fa8\u522b\u80fd\u529b\u3002"}}
{"id": "2601.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11910", "abs": "https://arxiv.org/abs/2601.11910", "authors": ["Guiying Zhu", "Bowen Yang", "Yin Zhuang", "Tong Zhang", "Guanqun Wang", "Zhihao Che", "He Chen", "Lianlin Li"], "title": "A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection", "comment": null, "summary": "Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of \"guess what\". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684GW-VLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a\uff0c\u5229\u7528\u9884\u8bad\u7ec3VLM\u548cLLM\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u81ea\u7136\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u57fa\u4e8e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u901a\u7528\u5bf9\u8c61\u8ba4\u77e5\u7406\u89e3\u7684\u5fc5\u8981\u6027\u3002\u867d\u7136\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u901a\u7528\u7406\u89e3\u8303\u5f0f\u3002", "method": "\u63d0\u51faGW-VLM\u6846\u67b6\uff1a1\uff09\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22\uff08MS-VLS\uff09\u5229\u7528VLM\u8fdb\u884c\u7c7b\u65e0\u5173\u68c0\u6d4b\u7ed3\u679c\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u8f6f\u5bf9\u9f50\u751f\u6210\u7247\u6bb5\uff1b2\uff09\u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a\uff08CCP\uff09\u57fa\u4e8eMS-VLS\u5f62\u6210\u6982\u5ff5\u6d41\uff0c\u4f7fLLM\u7406\u89e3\u7247\u6bb5\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3002", "result": "\u5728COCO val\u3001Pascal VOC\u3001DIOR\u548cNWPU-10\u7b49\u81ea\u7136\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cGW-VLM\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u6b65\u9aa4\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "GW-VLM\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3VLM\u548cLLM\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u901a\u7528\u7406\u89e3\u8303\u5f0f\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.11915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11915", "abs": "https://arxiv.org/abs/2601.11915", "authors": ["Chi Wang", "Xinjue Hu", "Boyu Wang", "Ziwen He", "Zhangjie Fu"], "title": "From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection", "comment": null, "summary": "The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path\" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e72\u9884\u6d88\u9664\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u65b9\u6cd5\uff0c\u4ec5\u97000.43M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u6e90\u4e8e\u865a\u5047\u76f8\u5173\u6027\u56e0\u7d20\uff08\u4f2a\u9020\u65e0\u5173\u4fe1\u606f\uff09\u901a\u8fc7\"\u540e\u95e8\u8def\u5f84\"\u5f71\u54cd\u6807\u7b7e\u9884\u6d4b\uff0c\u5bfc\u81f4\u6709\u504f\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bc6\u522b\u5177\u4f53\u865a\u5047\u76f8\u5173\u6027\u5e76\u9488\u5bf9\u6027\u89e3\u51b3\uff0c\u4f46\u865a\u5047\u76f8\u5173\u6027\u6e90\u4e8e\u4e0d\u53ef\u89c2\u6d4b\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u96be\u4ee5\u9010\u4e00\u8bc6\u522b\u548c\u5904\u7406\u3002", "method": "\u63d0\u51fa\u8868\u793a\u7a7a\u95f4\u5e72\u9884\u8303\u5f0f\uff1a\u5c06\u5b9e\u4f8b\u7ea7\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u5e72\u9884\u3002\u901a\u8fc7\u6b63\u4ea4\u4f4e\u79e9\u6295\u5f71\u5c06\u865a\u5047\u76f8\u5173\u7279\u5f81\u5206\u89e3\u5230\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u4ece\u539f\u59cb\u8868\u793a\u4e2d\u79fb\u9664\u8be5\u5b50\u7a7a\u95f4\uff0c\u8bad\u7ec3\u5176\u6b63\u4ea4\u8865\u7a7a\u95f4\u6765\u6355\u83b7\u4f2a\u9020\u76f8\u5173\u7279\u5f81\u3002\u4f4e\u79e9\u6295\u5f71\u79fb\u9664\u6709\u6548\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u56e0\u7d20\u3002", "result": "\u4ec5\u4f7f\u75280.43M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u5e72\u9884\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u9010\u4e00\u8bc6\u522b\u5177\u4f53\u865a\u5047\u76f8\u5173\u6027\u7684\u4e0d\u5207\u5b9e\u9645\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u4f2a\u9020\u68c0\u6d4b\u3002"}}
{"id": "2601.13020", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13020", "abs": "https://arxiv.org/abs/2601.13020", "authors": ["Zhiyan Hou", "Haiyun Guo", "Haokai Ma", "Yandu Sun", "Yonghui Yang", "Jinqiao Wang"], "title": "PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning", "comment": null, "summary": "Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faPASs\uff08\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff09\u65b9\u6cd5\u89e3\u51b3\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u4e13\u5bb6\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u8def\u5f84\u6fc0\u6d3b\u4fe1\u53f7\u6821\u51c6\u8def\u7531\u5e76\u7a33\u5b9a\u91cd\u8981\u79e9\u65b9\u5411\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u7684MoE\u65b9\u6cd5\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\uff0c\u8def\u7531\u5668\u548c\u4e13\u5bb6\u4f1a\u5171\u540c\u6f02\u79fb\uff0c\u5bfc\u81f4\u65e9\u671f\u8f93\u5165-\u4e13\u5bb6\u4e13\u4e1a\u5316\u9010\u6e10\u504f\u79bb\uff0c\u5f62\u6210\"\u9519\u4f4d\u5171\u6f02\u79fb\"\u73b0\u8c61\uff0c\u6a21\u7cca\u4e13\u5bb6\u804c\u8d23\u5e76\u52a0\u5267\u9057\u5fd8\u95ee\u9898", "method": "\u63d0\u51fa\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff08PASs\uff09\u4f5c\u4e3a\u80fd\u529b\u5bf9\u9f50\u7684\u5750\u6807\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1PASs-MoE-LoRA\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1\uff09PAS\u5f15\u5bfc\u91cd\u52a0\u6743\uff1a\u5229\u7528\u4e13\u5bb6\u8def\u5f84\u6fc0\u6d3b\u4fe1\u53f7\u6821\u51c6\u8def\u7531\uff1b2\uff09PAS\u611f\u77e5\u79e9\u7a33\u5b9a\u5316\uff1a\u9009\u62e9\u6027\u7a33\u5b9a\u5bf9\u5148\u524d\u4efb\u52a1\u91cd\u8981\u7684\u79e9\u65b9\u5411", "result": "\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u548c\u591a\u79cdMoE-LoRA\u53d8\u4f53\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u53c2\u6570", "conclusion": "PASs\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8def\u7531\u5668\u548c\u4e13\u5bb6\u7684\u66f4\u65b0\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9519\u4f4d\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u4fdd\u6301\u673a\u5236"}}
{"id": "2601.11930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11930", "abs": "https://arxiv.org/abs/2601.11930", "authors": ["Xulei Shi", "Maoyu Wang", "Yuning Peng", "Guanbo Wang", "Xin Wang", "Qi Chen", "Pengjie Tao"], "title": "SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM", "comment": null, "summary": "Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.", "AI": {"tldr": "SupScene\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eSfM\u56fe\u50cf\u68c0\u7d22\u7684\u65b0\u578b\u5168\u5c40\u63cf\u8ff0\u7b26\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u56fe\u8bad\u7ec3\u7b56\u7565\u548cDiVLAD\u805a\u5408\u5668\uff0c\u4e13\u6ce8\u4e8e\u5bfb\u627e\u5177\u6709\u76f8\u4f3c\u51e0\u4f55\u7279\u5f81\u7684\u91cd\u53e0\u56fe\u50cf\u5bf9\u800c\u975e\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u6279\u91cf\u4e8c\u5143\u5206\u7c7b\uff08\u91cd\u53e0vs\u975e\u91cd\u53e0\uff09\uff0c\u4f46\u672a\u80fd\u5145\u5206\u6355\u6349SfM\u4e2d\u51e0\u4f55\u5339\u914d\u6027\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u4f20\u7edf\u65b9\u6cd5\u66f4\u6ce8\u91cd\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u800c\u975e\u51e0\u4f55\u6027\u8d28\u76f8\u4f3c\u7684\u91cd\u53e0\u56fe\u50cf\u5bf9\uff0c\u8fd9\u9650\u5236\u4e86SfM\u4e2d\u56fe\u50cf\u5339\u914d\u7684\u6548\u7387\u3002", "method": "1. \u91c7\u7528\u57fa\u4e8e\u5b50\u56fe\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u8d85\u8d8a\u540c\u7b49\u91cd\u8981\u7684\u5b64\u7acb\u56fe\u50cf\u5bf9\uff0c\u5229\u7528\u771f\u5b9e\u51e0\u4f55\u91cd\u53e0\u5173\u7cfb\u53ca\u5176\u4e0d\u540c\u6743\u91cd\uff0c\u901a\u8fc7\u8f6f\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\u30022. \u63d0\u51faDiVLAD\u805a\u5408\u5668\uff0c\u5229\u7528ViT\u6700\u540e\u4e00\u5c42\u7684\u591a\u5934\u90e8\u6ce8\u610f\u529b\u56fe\u30023. \u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u7ed3\u5408\u8bed\u4e49\u663e\u8457\u7ebf\u7d22\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u751f\u6210\u66f4\u5177\u533a\u5206\u6027\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\u3002", "result": "\u5728GL3D\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eNetVLAD\uff0c\u540c\u65f6\u4ec5\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4e0d\u540c\u805a\u5408\u6280\u672f\u4e2d\u90fd\u5e26\u6765\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SupScene\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u805a\u5408\u5668\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86SfM\u4e2d\u56fe\u50cf\u68c0\u7d22\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u4e13\u6ce8\u4e8e\u51e0\u4f55\u5339\u914d\u6027\u800c\u975e\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u65e0\u7ea6\u675fSfM\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u56fe\u50cf\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13021", "abs": "https://arxiv.org/abs/2601.13021", "authors": ["Nata\u0161a Petrovi\u0107", "Gabriel Moy\u00e0-Alcover", "Antoni Jaume-i-Cap\u00f3", "Jose Maria Buades Rubio"], "title": "Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis", "comment": null, "summary": "This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\\% and SDS-score 89.51\\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u9570\u72b6\u7ec6\u80de\u75c5\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u5728\u9570\u72b6\u7ec6\u80de\u75c5\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u5206\u7c7b\u4e0a\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u9570\u72b6\u7ec6\u80de\u75c5\u63d0\u4f9b\u57fa\u4e8e\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\uff0c\u91cd\u70b9\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\u4ee5\u53ca\u7279\u5f81\u5197\u4f59\u5bfc\u81f4\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "method": "1) \u5bf9\u663e\u5fae\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\u548c\u5206\u5272\u4ee5\u786e\u4fdd\u9ad8\u8d28\u91cf\u7279\u5f81\u63d0\u53d6\uff1b2) \u91c7\u7528\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u548c\u6781\u7aef\u968f\u673a\u6811\uff09\u8fdb\u884c\u7ec6\u80de\u5f62\u6001\u5206\u7c7b\uff1b3) \u8bbe\u8ba1\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u65b9\u6cd5\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u3001\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff1b4) \u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u968f\u673a\u68ee\u6797\u548c\u6781\u7aef\u968f\u673a\u6811\u7684\u96c6\u6210\u5206\u7c7b\u5668\u83b7\u5f97F1\u5206\u657090.71%\u548cSDS\u5206\u657093.33%\uff0c\u663e\u8457\u4f18\u4e8e\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\u768487.32%\u548c89.51%\u3002\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u76f8\u5173\u4ee3\u7801\u3001\u53c2\u6570\u548c\u6df7\u6dc6\u77e9\u9635\u6570\u636e\u5df2\u516c\u5f00\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u9570\u72b6\u7ec6\u80de\u75c5\u8bca\u65ad\u652f\u6301\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5206\u7c7b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u6709\u6548\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11931", "abs": "https://arxiv.org/abs/2601.11931", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Shenao Jiang", "Hangrui Xu", "Zirui Liao", "Luyuan Zhang", "Huaqiu Li", "Peng Jiao", "Haoqian Wang"], "title": "Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition", "comment": null, "summary": "Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.", "AI": {"tldr": "LMGait\u6846\u67b6\u4f7f\u7528\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u8fdb\u884c\u6b65\u6001\u8bc6\u522b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u63d0\u793a\u6765\u6355\u6349\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u566a\u58f0\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u76f4\u63a5\u4ece\u56fe\u50cf\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6c60\u5316\u64cd\u4f5c\u83b7\u5f97\u5e8f\u5217\u7ea7\u8868\u793a\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u566a\u58f0\uff08\u5982\u8863\u7269\uff09\uff0c\u540c\u65f6\u65e0\u6cd5\u6709\u6548\u6355\u6349\u52a8\u6001\u8fd0\u52a8\u533a\u57df", "method": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u6b65\u6001\u8bc6\u522b\u6846\u67b6LMGait\uff0c\u5229\u7528\u8bbe\u8ba1\u7684\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u63d0\u793a\u6765\u6355\u6349\u6b65\u6001\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u8fd0\u52a8\u7279\u5f81", "result": "\u4ece\u6458\u8981\u4e2d\u65e0\u6cd5\u83b7\u53d6\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "conclusion": "LMGait\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u6539\u5584\u6b65\u6001\u8bc6\u522b\u4e2d\u5bf9\u52a8\u6001\u8fd0\u52a8\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u9759\u6001\u566a\u58f0\u7684\u8fc7\u5ea6\u62df\u5408"}}
{"id": "2601.13054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13054", "abs": "https://arxiv.org/abs/2601.13054", "authors": ["Kamogelo Taueatsoala", "Caitlyn Daniels", "Angelina J. Ramsunar", "Petrus Bronkhorst", "Absalom E. Ezugwu"], "title": "TinyML-Enabled IoT for Sustainable Precision Irrigation", "comment": null, "summary": "Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5c0f\u89c4\u6a21\u519c\u4e1a\u7684\u8fb9\u7f18\u4f18\u5148\u7269\u8054\u7f51\u6846\u67b6\uff0c\u96c6\u6210TinyML\u5b9e\u73b0\u79bb\u7ebf\u7cbe\u51c6\u704c\u6e89\uff0c\u663e\u8457\u964d\u4f4e\u6c34\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u5c0f\u89c4\u6a21\u519c\u4e1a\u793e\u533a\u9762\u4e34\u6c34\u8d44\u6e90\u77ed\u7f3a\u3001\u6c14\u5019\u6a21\u5f0f\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u7f3a\u4e4f\u5148\u8fdb\u3001\u7ecf\u6d4e\u5b9e\u60e0\u7684\u519c\u4e1a\u6280\u672f\u7b49\u95ee\u9898\u3002\u4f20\u7edf\u704c\u6e89\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u4f9d\u8d56\u4e91\u8ba1\u7b97\u7684\u89e3\u51b3\u65b9\u6848\u5728\u4e92\u8054\u7f51\u8fde\u63a5\u6709\u9650\u7684\u519c\u6751\u5730\u533a\u4e0d\u53ef\u884c\u3002", "method": "\u91c7\u7528\u56db\u5c42\u67b6\u6784\uff1a1) \u4f20\u611f\u5668\u5c42\uff08\u571f\u58e4\u6e7f\u5ea6\u3001\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001pH\u3001\u73af\u5883\u5149\uff09\uff1b2) ESP32\u5fae\u63a7\u5236\u5668\u4f5c\u4e3a\u8fb9\u7f18\u63a8\u7406\u8282\u70b9\u8fd0\u884cTinyML\u6a21\u578b\uff1b3) Raspberry Pi\u4f5c\u4e3a\u672c\u5730\u8fb9\u7f18\u670d\u52a1\u5668\uff1b4) MQTT\u5c40\u57df\u7f51\u534f\u8bae\u5b9e\u73b0\u672c\u5730\u901a\u4fe1\u3002\u901a\u8fc7\u6bd4\u8f83\u96c6\u6210\u6a21\u578b\uff0c\u9009\u62e9\u68af\u5ea6\u63d0\u5347\u4f5c\u4e3a\u6700\u4f18\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u8f7b\u91cf\u7ea7TinyML\u63a8\u7406\u5f15\u64ce\u90e8\u7f72\u5728ESP32\u4e0a\u3002", "result": "\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cR\u00b2\u5f97\u52060.9973\uff0cMAPE\u4e3a0.99%\uff0c\u4f18\u4e8e\u968f\u673a\u68ee\u6797\u6a21\u578b\uff08R\u00b2=0.9916\uff0cMAPE=1.81%\uff09\u3002\u90e8\u7f72\u540e\u7cfb\u7edf\u9884\u6d4b\u704c\u6e89\u9700\u6c42\u7684MAPE<1%\u3002\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u9a8c\u8bc1\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u7528\u6c34\u91cf\u3002\u7cfb\u7edf\u4f4e\u529f\u8017\u8bbe\u8ba1\u548c\u79bb\u7ebf\u529f\u80fd\u8bc1\u5b9e\u5176\u5728\u8d44\u6e90\u53d7\u9650\u519c\u6751\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u8fb9\u7f18\u4f18\u5148\u7269\u8054\u7f51\u6846\u67b6\u4e3a\u5c0f\u89c4\u6a21\u519c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bbe\u5907\u7aef\u4eba\u5de5\u667a\u80fd\u63d0\u5347\u6c34\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u5f25\u5408\u519c\u4e1a\u6280\u672f\u9e3f\u6c9f\uff0c\u9002\u7528\u4e8e\u4e92\u8054\u7f51\u8fde\u63a5\u6709\u9650\u7684\u519c\u6751\u5730\u533a\u3002"}}
{"id": "2601.11952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11952", "abs": "https://arxiv.org/abs/2601.11952", "authors": ["Haonan An", "Guang Hua", "Wei Du", "Hangcheng Cao", "Yihang Tao", "Guowen Xu", "Susanto Rahardja", "Yuguang Fang"], "title": "Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal", "comment": null, "summary": "Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.", "AI": {"tldr": "\u63d0\u51faDecoder Gradient Shields (DGS)\u9632\u5fa1\u673a\u5236\uff0c\u4fdd\u62a4\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7684\u89e3\u7801\u5668\u514d\u53d7\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u800c\u89e3\u7801\u5668\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u6c34\u5370\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002\u672c\u6587\u8bc6\u522b\u4e86\u4e00\u79cd\u9488\u5bf9\u89e3\u7801\u5668\u7684\u653b\u51fb\uff0c\u653b\u51fb\u8005\u5229\u7528\u67e5\u8be2\u54cd\u5e94\u83b7\u53d6\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u6765\u8bad\u7ec3\u6c34\u5370\u79fb\u9664\u5668\u3002", "method": "\u63d0\u51faDecoder Gradient Shields (DGS)\u9632\u5fa1\u673a\u5236\u5bb6\u65cf\uff0c\u5305\u62ec\u8f93\u51fa\u5c42DGS-O\u3001\u8f93\u5165\u5c42DGS-I\u548c\u5c42\u95f4DGS-L\u3002DGS-O\u6709\u95ed\u5f0f\u89e3\uff0c\u6240\u6709DGS\u90fd\u6709\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u3002\u901a\u8fc7\u91cd\u65b0\u5b9a\u5411\u548c\u91cd\u65b0\u7f29\u653e\u6765\u81ea\u6c34\u5370\u901a\u9053\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u68af\u5ea6\uff0c\u9632\u6b62\u6c34\u5370\u79fb\u9664\u5668\u8fbe\u5230\u8bad\u7ec3\u6536\u655b\u3002", "result": "\u5728\u53bb\u96e8\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u65e0\u76d2\u6c34\u5370\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\uff0cDGS\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86100%\u7684\u9632\u5fa1\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u5668\u8f93\u51fa\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "DGS\u673a\u5236\u6709\u6548\u4fdd\u62a4\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7684\u89e3\u7801\u5668\u514d\u53d7\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u7684\u653b\u51fb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6c34\u5370\u4fdd\u62a4\u7814\u7a76\u4e2d\u5bf9\u89e3\u7801\u5668\u5b89\u5168\u6027\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.13075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13075", "abs": "https://arxiv.org/abs/2601.13075", "authors": ["Abhinav Rajeev Kumar", "Dhruv Trehan", "Paras Chopra"], "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions", "comment": "12 pages, 5 figures, 4 tables", "summary": "Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.", "AI": {"tldr": "METIS\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u3001\u9636\u6bb5\u611f\u77e5\u7684AI\u5bfc\u5e08\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u672c\u79d1\u751f\u4ece\u7814\u7a76\u60f3\u6cd5\u53d1\u5c55\u5230\u5b8c\u6574\u8bba\u6587\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u4f18\u4e8eGPT-5\u548cClaude Sonnet 4.5\u3002", "motivation": "\u8bb8\u591a\u5b66\u751f\u7f3a\u4e4f\u4e13\u5bb6\u7814\u7a76\u6307\u5bfc\uff0c\u9700\u8981AI\u5bfc\u5e08\u5e2e\u52a9\u4ed6\u4eec\u4ece\u7814\u7a76\u60f3\u6cd5\u53d1\u5c55\u5230\u5b8c\u6574\u8bba\u6587\uff0c\u89e3\u51b3\u7814\u7a76\u6307\u5bfc\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaMETIS\u7cfb\u7edf\uff0c\u5305\u542b\u6587\u732e\u641c\u7d22\u3001\u7cbe\u9009\u6307\u5357\u3001\u65b9\u6cd5\u5b66\u68c0\u67e5\u548c\u8bb0\u5fc6\u529f\u80fd\uff0c\u91c7\u7528\u5de5\u5177\u589e\u5f3a\u548c\u9636\u6bb5\u611f\u77e5\u67b6\u6784\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\uff1aLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6210\u5bf9\u504f\u597d\u6bd4\u8f83\u3001\u5b66\u751f\u89d2\u8272\u8bc4\u5206\u6807\u51c6\u3001\u77ed\u591a\u8f6e\u8f85\u5bfc\u5bf9\u8bdd\u3001\u8bc1\u636e/\u5408\u89c4\u6027\u68c0\u67e5\uff0c\u8986\u76d6\u516d\u4e2a\u5199\u4f5c\u9636\u6bb5\u3002", "result": "\u572890\u4e2a\u5355\u8f6e\u63d0\u793a\u4e2d\uff0cLLM\u8bc4\u5224\u8005\u504f\u597dMETIS\u8d85\u8fc7Claude Sonnet 4.5\uff0871%\uff09\u548cGPT-5\uff0854%\uff09\u3002\u5b66\u751f\u8bc4\u5206\uff08\u6e05\u6670\u5ea6/\u53ef\u64cd\u4f5c\u6027/\u7ea6\u675f\u9002\u5e94\u6027\uff09\u5728\u6240\u6709\u9636\u6bb5\u5747\u66f4\u9ad8\u3002\u5728\u591a\u8f6e\u4f1a\u8bdd\u4e2d\uff0cMETIS\u4ea7\u751f\u7684\u6700\u7ec8\u8d28\u91cf\u7565\u9ad8\u4e8eGPT-5\u3002\u4f18\u52bf\u96c6\u4e2d\u5728\u6587\u6863\u57fa\u7840\u9636\u6bb5\uff08D-F\uff09\uff0c\u4e0e\u9636\u6bb5\u611f\u77e5\u8def\u7531\u548c\u57fa\u7840\u529f\u80fd\u4e00\u81f4\u3002", "conclusion": "METIS\u4f5c\u4e3aAI\u7814\u7a76\u5bfc\u5e08\u5728\u5e2e\u52a9\u5b66\u751f\u4ece\u60f3\u6cd5\u53d1\u5c55\u5230\u8bba\u6587\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u57fa\u7840\u9636\u6bb5\u3002\u5931\u8d25\u6a21\u5f0f\u5305\u62ec\u8fc7\u65e9\u5de5\u5177\u8def\u7531\u3001\u6d45\u5c42\u57fa\u7840\u548c\u5076\u5c14\u7684\u9636\u6bb5\u5206\u7c7b\u9519\u8bef\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.11970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11970", "abs": "https://arxiv.org/abs/2601.11970", "authors": ["S. M. Khalid Bin Zahid", "Md. Rakibul Hasan Nishat", "Abdul Hasib", "Md. Rakibul Hasan", "Md. Ashiqussalehin", "Md. Sahadat Hossen Sajib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms", "comment": null, "summary": "Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6811\u8393\u6d3e5\u8fb9\u7f18\u5e73\u53f0\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u60c5\u611f\u68c0\u6d4b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\u5c06\u8ba1\u7b97\u8d1f\u8f7d\u964d\u4f4e65%\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u901a\u5e38\u72ec\u7acb\u5904\u7406\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u3001\u81ea\u9002\u5e94\u7684\u8fd0\u884c\u65f6\u8c03\u5ea6\u5668\u6765\u6839\u636e\u4e0a\u4e0b\u6587\u89e6\u53d1\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6574\u4f53\u7406\u89e3\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u5c06\u76ee\u6807\u68c0\u6d4b\uff08YOLOv8n\uff09\u3001\u6240\u6709\u8005\u7279\u5b9a\u4eba\u8138\u8bc6\u522b\uff08\u57fa\u4e8eFaceNet\u7684\u81ea\u5b9a\u4e49\u5d4c\u5165\u7cfb\u7edf\uff09\u548c\u60c5\u611f\u68c0\u6d4b\uff08DeepFace\u7684CNN\uff09\u96c6\u6210\u5230\u7edf\u4e00\u6d41\u6c34\u7ebf\u4e2d\u3002\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u6a21\u5757\u6765\u51cf\u5c11\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\u5e73\u5747\u7cbe\u5ea6\uff08AP\uff09\u8fbe\u52300.861\uff0c\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u7387\u4e3a88%\uff0c\u60c5\u611f\u68c0\u6d4b\u5bf9\u7279\u5b9a\u60c5\u7eea\u7684AUC\u9ad8\u8fbe0.97\uff0c\u7cfb\u7edf\u4ee55.6\u5e27/\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u76f8\u6bd4\u8fde\u7eed\u5904\u7406\u964d\u4f4e\u4e8665%\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u662f\u5b9e\u73b0\u590d\u6742\u591a\u6a21\u6001AI\u5728\u6210\u672c\u6548\u76ca\u8fb9\u7f18\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u5173\u952e\uff0c\u4f7f\u667a\u80fd\u611f\u77e5\u66f4\u52a0\u53ef\u8bbf\u95ee\u4e14\u4fdd\u62a4\u9690\u79c1\u3002"}}
{"id": "2601.13100", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13100", "abs": "https://arxiv.org/abs/2601.13100", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement", "comment": null, "summary": "Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.\n  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.\n  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.", "AI": {"tldr": "\u63d0\u51fa\u9012\u5f52\u5143\u84b8\u998f\u7684\u7b97\u5b50\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u9012\u5f52\u84b8\u998f\u4f1a\u8bf1\u5bfcKL\u6563\u5ea6\u6536\u7f29\uff0c\u51e0\u4f55\u6536\u655b\u5230\u57fa\u7840\u6559\u5e08\u5206\u5e03\uff0c\u5b58\u5728\u552f\u4e00\u5168\u5c40\u5438\u5f15\u4e0d\u52a8\u70b9\u3002", "motivation": "\u5f53\u524d\u6982\u7387\u57df\u77e5\u8bc6\u84b8\u998f\u7814\u7a76\u5df2\u5efa\u7acb\u6e29\u5ea6\u7f29\u653e\u3001\u591a\u6559\u5e08\u805a\u5408\u548c\u5355\u9636\u6bb5\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u4f46\u9012\u5f52\u6216\u591a\u4ee3\u84b8\u998f\u7684\u6570\u5b66\u884c\u4e3a\u7406\u89e3\u4e0d\u8db3\uff0c\u5148\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u542f\u53d1\u5f0f\u3002", "method": "\u5f15\u5165\u9012\u5f52\u5143\u84b8\u998f\u7684\u516c\u7406\u5316\u548c\u7b97\u5b50\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8fed\u4ee3\u77e5\u8bc6\u84b8\u998f\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u5206\u5e03\u7b97\u5b50\u5e8f\u5217\uff0c\u5e76\u660e\u786e\u951a\u5b9a\u5230\u57fa\u7840\u6559\u5e08\u3002\u5b9a\u4e49\u6709\u6548\u5143\u6559\u5e08\u6784\u5efa\u7684\u7ed3\u6784\u516c\u7406\uff0c\u8bc1\u660e\u6ee1\u8db3\u8fd9\u4e9b\u516c\u7406\u7684\u975e\u5e73\u51e1\u7b97\u5b50\u65cf\u5b58\u5728\u6027\u3002", "result": "\u5728\u6e29\u548c\u53ef\u5b9e\u73b0\u6027\u548c\u51f8\u6027\u5047\u8bbe\u4e0b\uff0c\u951a\u5b9a\u9012\u5f52\u84b8\u998f\u8bf1\u5bfcKL\u6563\u5ea6\u6536\u7f29\uff0c\u4ea7\u751f\u51e0\u4f55\u6536\u655b\u5230\u57fa\u7840\u6559\u5e08\u5206\u5e03\uff0c\u5b58\u5728\u552f\u4e00\u5168\u5c40\u5438\u5f15\u4e0d\u52a8\u70b9\u3002\u6846\u67b6\u72ec\u7acb\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u7ec6\u8282\u6216\u5177\u4f53\u7b97\u5b50\u5b9e\u4f8b\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u57fa\u7840\u6027\u800c\u975e\u7b97\u6cd5\u6027\u7684\u8d21\u732e\uff0c\u523b\u753b\u4e86\u9012\u5f52\u84b8\u998f\u4f55\u65f6\u6570\u5b66\u9002\u5b9a\u4e14\u6536\u655b\u800c\u975e\u8bef\u5dee\u7d2f\u79ef\uff0c\u4e3a\u7406\u89e3\u5bb9\u91cf\u7ea6\u675f\u4e0b\u8fed\u4ee3\u548c\u591a\u6559\u5e08\u84b8\u998f\u7684\u7a33\u5b9a\u6027\u3001\u504f\u5dee-\u65b9\u5dee\u884c\u4e3a\u548c\u5931\u6548\u6a21\u5f0f\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.11976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11976", "abs": "https://arxiv.org/abs/2601.11976", "authors": ["Zongmin Li", "Yachuan Li", "Lei Kang", "Dimosthenis Karatzas", "Wenkang Ma"], "title": "AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering", "comment": "7 pages, 3 figures", "summary": "Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.", "code_url": "https://github.com/Li-yachuan/AVIR", "AI": {"tldr": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u5185\u68c0\u7d22\uff0c\u663e\u8457\u51cf\u5c11\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u6240\u9700\u7684\u9875\u9762\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u957f\u6587\u6863\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u524a\u5f31\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u6587\u6863\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u5185\u68c0\u7d22(AVIR)\u6846\u67b6\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u68c0\u7d22\u6a21\u578b\u5bf9\u6bcf\u9875\u8fdb\u884c\u95ee\u9898\u76f8\u5173\u6027\u8bc4\u5206\uff1b2) \u6839\u636e\u8bc4\u5206\u5206\u5e03\u8fdb\u884c\u9875\u9762\u805a\u7c7b\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u5185\u5bb9\uff1b3) \u5bf9\u805a\u7c7b\u9875\u9762\u8fdb\u884cTop-K\u7b5b\u9009\u4fdd\u6301\u4e0a\u4e0b\u6587\u7d27\u51d1\uff1b4) \u5bf9\u77ed\u6587\u6863\u4f7f\u7528\u76f8\u5173\u6027\u6982\u7387\u9608\u503c\u9009\u62e9\u9875\u9762\uff1b5) \u4ec5\u5c06\u9009\u4e2d\u7684\u9875\u9762\u8f93\u5165\u51bb\u7ed3\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728MP-DocVQA\u6570\u636e\u96c6\u4e0a\uff0cAVIR\u5c06\u95ee\u7b54\u6240\u9700\u7684\u5e73\u5747\u9875\u9762\u6570\u91cf\u51cf\u5c1170%\uff0c\u540c\u65f6\u8fbe\u523084.58%\u7684ANLS\u5206\u6570\uff0c\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002\u5728SlideVQA\u548cDUDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u68c0\u7d22\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6ce8\u610f\u529b\u673a\u5236\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u4e3a\u957f\u6587\u6863\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13143", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13143", "abs": "https://arxiv.org/abs/2601.13143", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Seungwoo Lee", "Joon Son Chung"], "title": "FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference", "comment": null, "summary": "In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.", "AI": {"tldr": "FastAV\u662f\u9996\u4e2a\u4e3a\u97f3\u9891-\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u526a\u679d\u7b56\u7565\u51cf\u5c1140%\u4ee5\u4e0aFLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u4ee4\u724c\u526a\u679d\u5728\u6807\u51c6LLMs\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u6709\u63a2\u7d22\uff0c\u4f46\u5728\u97f3\u9891-\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u53d7\u5230\u5173\u6ce8\u3002\u591a\u6a21\u6001\u6574\u5408\u663e\u8457\u589e\u52a0\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4ee4\u724c\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u526a\u679d\u65b9\u6cd5\u6765\u89e3\u51b3\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u526a\u679d\u7b56\u7565\uff1a1\uff09\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u88ab\u5f3a\u8c03\u7684\u4ee4\u724c\u5e76\u8bc4\u4f30\u5176\u91cd\u8981\u6027\uff1b2\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u526a\u679d\uff1a\u4e2d\u95f4\u5c42\u8fdb\u884c\u5168\u5c40\u526a\u679d\u53bb\u9664\u5e7f\u6cdb\u5f71\u54cd\u529b\u8f83\u5c0f\u7684\u4ee4\u724c\uff0c\u540e\u671f\u5c42\u8fdb\u884c\u7cbe\u7ec6\u526a\u679d\u8003\u8651\u5bf9\u4e0b\u4e00\u4e2a\u4ee4\u724c\u751f\u6210\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5b8c\u6574\u6ce8\u610f\u529b\u56fe\uff0c\u4e0eFlashAttention\u7b49\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u5b8c\u5168\u517c\u5bb9\u3002", "result": "\u5728\u4e24\u79cd\u4ee3\u8868\u6027AV-LLMs\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFastAV\u80fd\u591f\u51cf\u5c11\u8d85\u8fc740%\u7684FLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FastAV\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aAV-LLMs\u8bbe\u8ba1\u7684\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u526a\u679d\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u97f3\u9891-\u89c6\u89c9\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11981", "abs": "https://arxiv.org/abs/2601.11981", "authors": ["Jian Lang", "Rongpei Hong", "Ting Zhong", "Yong Wang", "Fan Zhou"], "title": "Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection", "comment": "13 pages. Accepted by KDD 2026 research track. Codes are released at https://github.com/Jian-Lang/RADAR", "summary": "Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.", "AI": {"tldr": "RADAR\uff1a\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u7684\u9002\u5e94\u8303\u5f0f\u89e3\u51b3\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u65b0\u95fb\u4e3b\u9898\u5206\u5e03\u4e00\u81f4\uff0c\u65e0\u6cd5\u68c0\u6d4b\u4e0e\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u76f8\u5173\u7684\u5047\u65b0\u95fb\u89c6\u9891\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\u3002", "method": "\u63d0\u51faRADAR\u6846\u67b6\uff0c\u91c7\u7528\u68c0\u7d22\u5f15\u5bfc\u7684\u9002\u5e94\u8303\u5f0f\uff1a1\uff09\u57fa\u4e8e\u71b5\u9009\u62e9\u7684\u68c0\u7d22\u673a\u5236\uff0c\u4e3a\u76ee\u6807\u57df\u63d0\u4f9b\u7a33\u5b9a\uff08\u4f4e\u71b5\uff09\u3001\u76f8\u5173\u7684\u53c2\u8003\u89c6\u9891\uff1b2\uff09\u7a33\u5b9a\u951a\u70b9\u5f15\u5bfc\u7684\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u5339\u914d\u5c06\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u8868\u793a\u4e0e\u6e90\u57df\u5bf9\u9f50\uff1b3\uff09\u76ee\u6807\u57df\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u8303\u5f0f\uff0c\u751f\u6210\u7531\u7a33\u5b9a\u53c2\u8003\u589e\u5f3a\u7684\u4fe1\u606f\u6027\u4f2a\u6807\u7b7e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRADAR\u5728\u6d4b\u8bd5\u65f6\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5bf9\u672a\u89c1\u5047\u65b0\u95fb\u89c6\u9891\u4e3b\u9898\u8fdb\u884c\u5f3a\u5927\u7684\u5b9e\u65f6\u9002\u5e94\u3002", "conclusion": "RADAR\u662f\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u68c0\u7d22\u5f15\u5bfc\u9002\u5e94\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u7684\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u6311\u6218\u3002"}}
{"id": "2601.13160", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13160", "abs": "https://arxiv.org/abs/2601.13160", "authors": ["Zhipeng Zhang", "Zhenjie Yao", "Kai Li", "Lei Yang"], "title": "Training instability in deep learning follows low-dimensional dynamical principles", "comment": null, "summary": "Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.\n  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.\n  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bad\u7ec3\u7a33\u5b9a\u6027\u4f5c\u4e3a\u5b66\u4e60\u7cfb\u7edf\u7684\u5185\u5728\u52a8\u6001\u7279\u6027\uff0c\u901a\u8fc7\u56db\u7ef4\u6846\u67b6\uff08\u4f18\u5316\u3001\u73af\u5883/\u6570\u636e\u3001\u53c2\u6570\u3001\u5b66\u4e60\u4fe1\u53f7\uff09\u548c\u53d7\u63a7\u6270\u52a8\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u63ed\u793a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u89c4\u5f8b\u6027\u6a21\u5f0f", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u867d\u7136\u53d6\u5f97\u663e\u8457\u7ecf\u9a8c\u6027\u80fd\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u672c\u8eab\u7684\u7a33\u5b9a\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u8bad\u7ec3\u4f5c\u4e3a\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\uff0c\u5bf9\u4f18\u5316\u3001\u6570\u636e\u3001\u53c2\u6570\u6216\u5b66\u4e60\u4fe1\u53f7\u7684\u5fae\u5c0f\u6270\u52a8\u53ef\u80fd\u5bfc\u81f4\u7a81\u7136\u4e14\u4e0d\u53ef\u9006\u7684\u5d29\u6e83\uff0c\u635f\u5bb3\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u52a8\u6001\u89c6\u89d2\uff0c\u5c06\u8bad\u7ec3\u7a33\u5b9a\u6027\u7ec4\u7ec7\u4e3a\u56db\u4e2a\u76f8\u4e92\u4f5c\u7528\u7ef4\u5ea6\uff1a\u4f18\u5316\u7a33\u5b9a\u6027\u3001\u73af\u5883/\u6570\u636e\u7a33\u5b9a\u6027\u3001\u53c2\u6570\u7a33\u5b9a\u6027\u3001\u5b66\u4e60\u4fe1\u53f7\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u53d7\u63a7\u6270\u52a8\u5ba1\u8ba1\u8bad\u7ec3\u8f68\u8ff9\u6765\u64cd\u4f5c\u5316\u8fd9\u4e00\u89c6\u89d2\uff0c\u5728\u4e0d\u4fee\u6539\u5b66\u4e60\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\u63a2\u6d4b\u5b66\u4e60\u52a8\u6001\u5bf9\u7ed3\u6784\u5316\u6270\u52a8\u7684\u54cd\u5e94", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u8bc6\u522b\u51fa\u4e09\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u89c4\u5f8b\uff1a1\uff09\u6700\u7ec8\u9ad8\u6027\u80fd\u5e38\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u8131\u94a9\uff1b2\uff09\u53d7\u63a7\u968f\u673a\u6027\u5728\u4e0d\u540c\u8303\u5f0f\u4e2d\u4e00\u81f4\u5730\u7f13\u51b2\u5b66\u4e60\u52a8\u6001\uff1b3\uff09\u4f4e\u7ef4\u6f5c\u5728\u5143\u72b6\u6001\u7684\u504f\u5dee\u7cfb\u7edf\u5730\u5148\u4e8e\u53ef\u89c2\u5bdf\u7684\u6027\u80fd\u5d29\u6e83", "conclusion": "\u8bad\u7ec3\u7a33\u5b9a\u6027\u662f\u5b66\u4e60\u7cfb\u7edf\u53ef\u6d4b\u91cf\u548c\u53ef\u6bd4\u8f83\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e3a\u8d85\u8d8a\u6700\u7ec8\u6027\u80fd\u7ed3\u679c\u7814\u7a76\u5b66\u4e60\u52a8\u6001\u63d0\u4f9b\u4e86\u63cf\u8ff0\u6027\u57fa\u7840"}}
{"id": "2601.11983", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11983", "abs": "https://arxiv.org/abs/2601.11983", "authors": ["Md. Asiful Islam", "Abdul Hasib", "Tousif Mahmud Emon", "Khandaker Tabin Hasan", "A. S. M. Ahsanul Sarkar Akib"], "title": "An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System", "comment": null, "summary": "The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\\% success rate, ultrasonic obstacle detection reached 94\\% accuracy, and YOLOv8-based object detection delivered 91.5\\% Precision, 90.2\\% Recall, and a 90.8\\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.", "AI": {"tldr": "\u57fa\u4e8eAI-IoT\u7684\u667a\u80fd\u8f6e\u6905\u7cfb\u7edf\uff0c\u96c6\u6210\u624b\u52bf\u63a7\u5236\u3001\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u548c\u5065\u5eb7\u76d1\u6d4b\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u591a\u6a21\u6001\u7684\u8f85\u52a9\u89e3\u51b3\u65b9\u6848", "motivation": "\u4f20\u7edf\u8f6e\u6905\u7f3a\u4e4f\u52a8\u6001\u529f\u80fd\uff0c\u73b0\u6709\u667a\u80fd\u8f6e\u6905\u6210\u672c\u9ad8\u3001\u529f\u80fd\u5355\u4e00\u4e14\u5065\u5eb7\u76d1\u6d4b\u96c6\u6210\u4e0d\u8db3\uff0c\u9700\u8981\u4e3a\u6b8b\u969c\u4eba\u58eb\u548c\u8001\u5e74\u4eba\u63d0\u4f9b\u5148\u8fdb\u3001\u4e2a\u6027\u5316\u4e14\u8d1f\u62c5\u5f97\u8d77\u7684\u8f85\u52a9\u6280\u672f", "method": "\u63d0\u51fa\u7efc\u5408AI-IoT\u667a\u80fd\u8f6e\u6905\u7cfb\u7edf\uff1a1) \u57fa\u4e8e\u624b\u5957\u7684\u624b\u52bf\u63a7\u5236\u5b9e\u73b0\u514d\u624b\u5bfc\u822a\uff1b2) YOLOv8\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u914d\u5408\u542c\u89c9\u53cd\u9988\u907f\u969c\uff1b3) \u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u5b9e\u73b0\u5373\u65f6\u78b0\u649e\u907f\u514d\uff1b4) \u6301\u7eed\u76d1\u6d4b\u5fc3\u7387\u3001\u8840\u6c27\u3001\u5fc3\u7535\u56fe\u3001\u4f53\u6e29\u7b49\u751f\u547d\u4f53\u5f81\uff0c\u4e0a\u4f20\u81f3ThingSpeak\u5e73\u53f0\u5e76\u89e6\u53d1\u90ae\u4ef6\u8b66\u62a5", "result": "\u624b\u52bf\u63a7\u5236\u6210\u529f\u738795.5%\uff0c\u8d85\u58f0\u6ce2\u969c\u788d\u68c0\u6d4b\u51c6\u786e\u738794%\uff0cYOLOv8\u7269\u4f53\u68c0\u6d4b\u7cbe\u5ea691.5%\u3001\u53ec\u56de\u738790.2%\u3001F1\u5206\u657090.8%\u3002\u7cfb\u7edf\u57fa\u4e8e\u6a21\u5757\u5316\u4f4e\u6210\u672c\u67b6\u6784\uff0c\u63d0\u4f9b\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8fd9\u79cd\u96c6\u6210\u591a\u6a21\u6001\u65b9\u6cd5\u5f25\u5408\u4e86\u521b\u65b0\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u589e\u5f3a\u7528\u6237\u81ea\u4e3b\u6027\u3001\u5b89\u5168\u6027\u548c\u72ec\u7acb\u6027\uff0c\u4e3a\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13190", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.13190", "abs": "https://arxiv.org/abs/2601.13190", "authors": ["Vittoria De Pellegrini", "Tariq Alkhalifah"], "title": "LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations", "comment": null, "summary": "Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.", "AI": {"tldr": "LAViG-FLOW\uff1a\u4e00\u79cd\u7528\u4e8e\u5730\u4e0b\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u5efa\u6a21\u7684\u6f5c\u5728\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u751f\u6210\u9971\u548c\u5ea6\u4e0e\u538b\u529b\u573a\u7684\u8026\u5408\u6f14\u5316\uff0c\u6bd4\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u9ad8\u4fdd\u771f\u591a\u76f8\u6a21\u62df\u5668\u5728\u9700\u8981\u5927\u91cf\u524d\u5411\u8fd0\u884c\u8fdb\u884c\u53cd\u6f14\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5730\u8d28CO2\u5c01\u5b58\u7b49\u5730\u5b66\u5e94\u7528\u4e2d\u7684\u5efa\u6a21\u4e0e\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51faLAViG-FLOW\u6846\u67b6\uff1a\u4f7f\u7528\u4e13\u75282D\u81ea\u7f16\u7801\u5668\u538b\u7f29\u6bcf\u4e2a\u72b6\u6001\u53d8\u91cf\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668(VDiT)\u5efa\u6a21\u5b83\u4eec\u5728\u65f6\u95f4\u4e0a\u7684\u8026\u5408\u5206\u5e03\uff0c\u5148\u5728\u7ed9\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u8bad\u7ec3\u5b66\u4e60\u8026\u5408\u5173\u7cfb\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u56de\u5f52\u5fae\u8c03\u5b9e\u73b0\u8d85\u51fa\u89c2\u6d4b\u65f6\u95f4\u7a97\u53e3\u7684\u5916\u63a8\u3002", "result": "\u5728\u5f00\u6e90CO2\u5c01\u5b58\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLAViG-FLOW\u751f\u6210\u7684\u9971\u548c\u5ea6\u4e0e\u538b\u529b\u573a\u5728\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LAViG-FLOW\u4e3a\u5730\u4e0b\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7269\u7406\u573a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5927\u91cf\u6a21\u62df\u7684\u5730\u5b66\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.13243", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13243", "abs": "https://arxiv.org/abs/2601.13243", "authors": ["Yapeng Li", "Jiakuo Yu", "Zhixin Liu", "Xinnan Liu", "Jing Yu", "Songze Li", "Tonghua Su"], "title": "A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u63a8\u7406\u8303\u5f0f\uff08\u76f4\u63a5\u751f\u6210\u3001CoT\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u7684\u6027\u80fd\u3001\u6210\u672c-\u7cbe\u5ea6\u6743\u8861\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6MIMeBench\u8bc4\u4f30\u8bed\u4e49\u62bd\u8c61\u548c\u5bf9\u6bd4\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u63a8\u7406\u7cfb\u7edf\u90e8\u7f72\u65f6\uff0c\u4e0d\u540c\u63a8\u7406\u8303\u5f0f\uff08\u5982CoT\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u7684\u76f8\u5bf9\u6709\u6548\u6027\u3001\u6210\u672c-\u7cbe\u5ea6\u6743\u8861\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u8bc4\u4f30\u8bed\u4e49\u80fd\u529b\u3002", "method": "1) \u7edf\u4e00\u8bc4\u4f30\u76f4\u63a5\u5355\u6a21\u578b\u751f\u6210\u3001CoT\u589e\u5f3a\u5355\u6a21\u578b\u63a8\u7406\u3001\u4ee3\u8868\u6027MAS\u5de5\u4f5c\u6d41\u5728\u5c01\u95ed\u5f0f\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff1b2) \u901a\u8fc7\u89d2\u8272\u9694\u79bb\u5206\u6790\u63a2\u7a76MAS\u4e2d\u89d2\u8272\u7279\u5b9a\u80fd\u529b\u9700\u6c42\uff1b3) \u5206\u6790\u6210\u672c-\u7cbe\u5ea6\u6743\u8861\uff1b4) \u5f15\u5165\u65b0\u57fa\u51c6MIMeBench\u8bc4\u4f30\u8bed\u4e49\u62bd\u8c61\u548c\u5bf9\u6bd4\u533a\u5206\u80fd\u529b\u3002", "result": "1) \u589e\u52a0\u7ed3\u6784\u590d\u6742\u6027\u5e76\u4e0d\u603b\u80fd\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5176\u6548\u76ca\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63a8\u7406\u8303\u5f0f\u672c\u8eab\u7684\u7279\u6027\u548c\u9002\u7528\u6027\uff1b2) \u8bc6\u522b\u4e86\u54ea\u4e9bMAS\u5de5\u4f5c\u6d41\u5728\u6210\u672c\u4e0e\u7cbe\u5ea6\u95f4\u8fbe\u5230\u826f\u597d\u5e73\u8861\uff0c\u54ea\u4e9b\u56e0\u8fb9\u9645\u6536\u76ca\u5fae\u5c0f\u800c\u6210\u672c\u8fc7\u9ad8\uff1b3) MIMeBench\u63d0\u4f9b\u4e86\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u6355\u6349\u7684\u8bed\u4e49\u80fd\u529b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "conclusion": "LLM\u63a8\u7406\u8303\u5f0f\u7684\u9009\u62e9\u5e94\u57fa\u4e8e\u4efb\u52a1\u7279\u6027\u548c\u6210\u672c\u7ea6\u675f\uff0c\u7ed3\u6784\u590d\u6742\u6027\u5e76\u975e\u603b\u662f\u6709\u76ca\uff1bMIMeBench\u4e3a\u8bc4\u4f30\u8bed\u4e49\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7ef4\u5ea6\uff1b\u9700\u8981\u66f4\u7ec6\u81f4\u5730\u7406\u89e3\u4e0d\u540c\u63a8\u7406\u8303\u5f0f\u7684\u9002\u7528\u573a\u666f\u548c\u6743\u8861\u3002"}}
{"id": "2601.12015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12015", "abs": "https://arxiv.org/abs/2601.12015", "authors": ["Pavan Kumar Yata", "Pediredla Pradeep", "Goli Himanish", "Swathi M"], "title": "SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture", "comment": "12 pages, 6 figures. Submitted to arXiv. Code and dataset details included in the paper", "summary": "Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.", "AI": {"tldr": "\u63d0\u51faDeepSegFusion\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eSAR\u56fe\u50cf\u4e2d\u7684\u6ea2\u6cb9\u5206\u5272\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\u5e76\u63d0\u9ad8\u8fb9\u754c\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u9608\u503c\u65b9\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u6ea2\u6cb9\u68c0\u6d4b\u4e2d\u56e0\u98ce\u6ed1\u3001\u8239\u8ff9\u7b49\u76f8\u4f3c\u73b0\u8c61\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5", "method": "\u7ed3\u5408SegNet\u548cDeepLabV3+\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u96c6\u6210\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u8fb9\u754c\u7cbe\u5ea6\u548c\u4e0a\u4e0b\u6587\u7406\u89e3", "result": "\u5728SAR\u6ea2\u6cb9\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.85%\u51c6\u786e\u7387\u30010.5685 IoU\u548c0.9330 ROC-AUC\uff0c\u8bef\u68c0\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u964d\u4f4e64.4%\uff0c\u51cf\u5c11\u4e09\u500d\u4ee5\u4e0a\u8bef\u68c0", "conclusion": "DeepSegFusion\u5728\u5404\u79cd\u6d77\u6d0b\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u53ef\u7528\u4e8e\u8fd1\u5b9e\u65f6\u6ea2\u6cb9\u76d1\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u4e00\u57fa\u7ebf\u6a21\u578b"}}
{"id": "2601.13272", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13272", "abs": "https://arxiv.org/abs/2601.13272", "authors": ["Aaron Pim", "Tristan Pryer"], "title": "Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification", "comment": "26 pages, 11 figures", "summary": "We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ea7\u8499\u7279\u5361\u6d1b\uff08MLMC\uff09\u7684\u8499\u7279\u5361\u6d1bdropout\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528dropout\u63a9\u7801\u6784\u5efa\u8026\u5408\u7684\u7c97-\u7ec6\u4f30\u8ba1\u5668\uff0c\u964d\u4f4e\u65b9\u5dee\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8499\u7279\u5361\u6d1bdropout\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5927\u91cf\u968f\u673a\u524d\u5411\u4f20\u64ad\u6765\u4f30\u8ba1\u9884\u6d4b\u77e9\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u6765\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u5c06dropout\u63a9\u7801\u89c6\u4e3a\u8ba4\u77e5\u968f\u673a\u6e90\uff0c\u901a\u8fc7\u968f\u673a\u524d\u5411\u4f20\u64ad\u6b21\u6570\u5b9a\u4e49\u4fdd\u771f\u5ea6\u5c42\u6b21\u7ed3\u6784\u3002\u91cd\u7528dropout\u63a9\u7801\u6784\u5efa\u8026\u5408\u7684\u7c97-\u7ec6\u4f30\u8ba1\u5668\uff0c\u5f62\u6210\u7528\u4e8e\u9884\u6d4b\u5747\u503c\u548c\u9884\u6d4b\u65b9\u5dee\u7684\u53ef\u4f38\u7f29MLMC\u4f30\u8ba1\u5668\u3002\u63a8\u5bfc\u4e86\u663e\u5f0f\u504f\u5dee\u3001\u65b9\u5dee\u548c\u6709\u6548\u6210\u672c\u8868\u8fbe\u5f0f\uff0c\u4ee5\u53ca\u8de8\u5c42\u7ea7\u7684\u6837\u672c\u5206\u914d\u89c4\u5219\u3002", "result": "\u5728\u6b63\u5411\u548c\u9006\u5411PINNs-Uzawa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u9884\u6d4b\u7684\u65b9\u5dee\u7387\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u76f8\u540c\u8ba1\u7b97\u6210\u672c\u4e0b\u76f8\u6bd4\u5355\u7ea7MC-dropout\u7684\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684MLMC\u6846\u67b6\u4e3a\u8499\u7279\u5361\u6d1bdropout\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528dropout\u63a9\u7801\u6784\u5efa\u8026\u5408\u4f30\u8ba1\u5668\uff0c\u5728\u4fdd\u6301\u65e0\u504f\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u65b9\u5dee\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12020", "abs": "https://arxiv.org/abs/2601.12020", "authors": ["Guillermo Figueroa-Araneda", "Iris Diana Jimenez", "Florian Hofherr", "Manny Ko", "Hector Andrade-Loarca", "Daniel Cremers"], "title": "DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering", "comment": null, "summary": "Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).\n  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.\n  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.", "AI": {"tldr": "DIAMOND-SSS\uff1a\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6781\u7a00\u758f\u76d1\u7763\uff08\u5c11\u81f310\u5f20\u56fe\u50cf\uff09\u5b9e\u73b0\u9ad8\u4fdd\u771f\u534a\u900f\u660e\u91cd\u5efa\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u65b0\u89c6\u89d2\u548c\u91cd\u5149\u7167\uff0c\u51cf\u5c11\u771f\u5b9e\u91c7\u96c6\u9700\u6c42\u8fbe90%", "motivation": "\u89e3\u51b3\u795e\u7ecf\u6e32\u67d3\u4e2d\u534a\u900f\u660e\u6750\u8d28\uff08\u5982\u8721\u3001\u7389\u77f3\u3001\u5927\u7406\u77f3\u3001\u76ae\u80a4\uff09\u5efa\u6a21\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6750\u8d28\u9700\u8981\u590d\u6742\u7684\u6b21\u8868\u9762\u6563\u5c04\u5149\u4f20\u8f93\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u7684\u591a\u89c6\u89d2\u591a\u5149\u7167\u6570\u636e\u96c6\uff08\u901a\u5e38\u8d85\u8fc7100\u4e2a\u89c6\u89d2\u548c112\u4e2aOLAT\uff09\uff0c\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u6602", "method": "1. \u5fae\u8c03\u6269\u6563\u6a21\u578b\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\u548c\u91cd\u5149\u7167\uff0c\u57fa\u4e8e\u4f30\u8ba1\u7684\u51e0\u4f55\u6761\u4ef6\uff0c\u5728\u4e0d\u52307%\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff1b2. \u5f15\u5165\u5149\u7167\u65e0\u5173\u7684\u51e0\u4f55\u5148\u9a8c\uff1a\u591a\u89c6\u89d2\u8f6e\u5ed3\u4e00\u81f4\u6027\u635f\u5931\u548c\u591a\u89c6\u89d2\u6df1\u5ea6\u4e00\u81f4\u6027\u635f\u5931\uff0c\u4ee5\u7a33\u5b9a\u7a00\u758f\u6216\u5408\u6210\u76d1\u7763\u4e0b\u7684\u91cd\u5efa\uff1b3. \u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u8fdb\u884c\u53ef\u91cd\u5149\u7167\u7684\u6e32\u67d3", "result": "DIAMOND-SSS\u5728\u6240\u6709\u7a00\u758f\u5ea6\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u8d28\u91cf\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u589e\u5f3a\u6570\u636e\u66ff\u4ee3\u9ad8\u8fbe95%\u7684\u7f3a\u5931\u91c7\u96c6\uff0c\u76f8\u6bd4SSS-3DGS\u51cf\u5c11\u4e8690%\u7684\u771f\u5b9e\u91c7\u96c6\u9700\u6c42", "conclusion": "DIAMOND-SSS\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u534a\u900f\u660e\u6750\u8d28\u91cd\u5efa\u7684\u6570\u636e\u9700\u6c42\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u548c\u51e0\u4f55\u5148\u9a8c\u5b9e\u73b0\u4e86\u4ece\u6781\u7a00\u758f\u76d1\u7763\uff08\u5c11\u81f310\u5f20\u56fe\u50cf\uff09\u7684\u9ad8\u8d28\u91cf\u53ef\u91cd\u5149\u7167\u91cd\u5efa\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u795e\u7ecf\u6e32\u67d3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13284", "abs": "https://arxiv.org/abs/2601.13284", "authors": ["Duygu Nur Yaldiz", "Evangelia Spiliopoulou", "Zheng Qi", "Siddharth Varia", "Srikanth Doss", "Nikolaos Pappas"], "title": "Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86LLM\u5728\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RLVR)\u4e24\u79cd\u5fae\u8c03\u8303\u5f0f\u4e0b\u7684\u6821\u51c6\u95ee\u9898\uff0c\u53d1\u73b0RLVR\u867d\u7136\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u4f46\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u6a21\u578b\uff0c\u800cSFT\u6821\u51c6\u66f4\u597d\u4f46\u6027\u80fd\u589e\u76ca\u8f83\u5c0f\u3002\u4f5c\u8005\u63d0\u51fa\u6821\u51c6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301RLVR\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8fc7\u5ea6\u81ea\u4fe1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u51b3\u7b56\u4efb\u52a1\uff0c\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002\u826f\u597d\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4f7f\u4e0b\u6e38\u7cfb\u7edf\u80fd\u591f\u51b3\u5b9a\u4f55\u65f6\u4fe1\u4efb\u6a21\u578b\u3001\u4f55\u65f6\u4f9d\u8d56\u5907\u7528\u673a\u5236\u3002\u7136\u800c\uff0c\u5f53\u524d\u5bf9\u4e24\u79cd\u4e3b\u6d41\u5fae\u8c03\u8303\u5f0f\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u7684\u6821\u51c6\u7279\u6027\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "1. \u7cfb\u7edf\u7814\u7a76SFT\u548cRLVR\u4e24\u79cd\u5fae\u8c03\u8303\u5f0f\u7684\u6821\u51c6\u7279\u6027\uff1b2. \u901a\u8fc7\u9488\u5bf9\u6027\u5b9e\u9a8c\u8bca\u65adRLVR\u5931\u8d25\u539f\u56e0\uff0c\u53d1\u73b0\u51b3\u7b56\u4ee4\u724c\u5728\u63a8\u7406\u8f68\u8ff9\u4e2d\u4ec5\u4f5c\u4e3a\u51b3\u7b56\u63d0\u53d6\u6b65\u9aa4\uff0c\u4e0d\u643a\u5e26\u7f6e\u4fe1\u5ea6\u4fe1\u606f\uff1b3. \u63d0\u51fa\u6821\u51c6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u516c\u5f0f\uff0c\u76f4\u63a5\u8c03\u6574\u51b3\u7b56\u4ee4\u724c\u6982\u7387\u3002", "result": "1. RLVR\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u4f46\u4ea7\u751f\u6781\u5ea6\u8fc7\u5ea6\u81ea\u4fe1\u6a21\u578b\uff1b2. SFT\u4ea7\u751f\u663e\u8457\u66f4\u597d\u7684\u6821\u51c6\uff0c\u5373\u4f7f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4e5f\u5982\u6b64\uff0c\u4f46\u6027\u80fd\u589e\u76ca\u8f83\u5c0f\uff1b3. \u63d0\u51fa\u7684\u6821\u51c6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4fdd\u6301RLVR\u51c6\u786e\u6027\u6c34\u5e73\u7684\u540c\u65f6\u51cf\u8f7b\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5c06ECE\u5206\u6570\u964d\u4f4e\u9ad8\u8fbe9\u4e2a\u70b9\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u867d\u7136\u80fd\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u5219\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6821\u51c6\u7279\u6027\u3002\u901a\u8fc7\u7406\u89e3\u51b3\u7b56\u4ee4\u724c\u5728\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u673a\u5236\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u6821\u51c6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u6539\u5584\u6821\u51c6\u8d28\u91cf\u3002"}}
{"id": "2601.12049", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12049", "abs": "https://arxiv.org/abs/2601.12049", "authors": ["Chenchen Zhao", "Muxi Chen", "Qiang Xu"], "title": "\\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions", "comment": "12 pages, 13 figures", "summary": "Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.", "AI": {"tldr": "FocaLogic\uff1a\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u548c\u91cf\u5316\u89c6\u89c9\u6a21\u578b\u51b3\u7b56\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u89c6\u89c9\u533a\u57df\u5e76\u8f6c\u5316\u4e3a\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u5b9a\u91cf\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u8981\u4e48\u4f9d\u8d56\u767d\u76d2\u6a21\u578b\u8bbf\u95ee\u6743\u9650\uff0c\u8981\u4e48\u7f3a\u4e4f\u8db3\u591f\u7684\u5b9a\u91cf\u4e25\u8c28\u6027\u3002\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faFocaLogic\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u3002\u5b83\u8bc6\u522b\u5bf9\u6a21\u578b\u9884\u6d4b\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u7684\u6700\u5c0f\u53ef\u89e3\u91ca\u89c6\u89c9\u533a\u57df\u5b50\u96c6\uff08\u79f0\u4e3a\"\u89c6\u89c9\u7126\u70b9\"\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u89c6\u89c9\u7126\u70b9\u8f6c\u5316\u4e3a\u7cbe\u786e\u7d27\u51d1\u7684\u903b\u8f91\u8868\u8fbe\u5f0f\u3002\u540c\u65f6\u63d0\u51fa\u4e00\u5957\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u7126\u70b9\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u5206\u6b67\u5ea6\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u8868\u660eFocaLogic\u80fd\u591f\u63ed\u793a\u5173\u952e\u6d1e\u5bdf\uff1a\u8bad\u7ec3\u5f15\u8d77\u7684\u6ce8\u610f\u529b\u96c6\u4e2d\u3001\u901a\u8fc7\u6cdb\u5316\u63d0\u9ad8\u7126\u70b9\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5728\u504f\u89c1\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u5f02\u5e38\u7126\u70b9\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u5b9a\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FocaLogic\u4e3a\u89e3\u91ca\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b9a\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u903b\u8f91\u8868\u793a\u5b9e\u73b0\u900f\u660e\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u5e76\u80fd\u591f\u5ba2\u89c2\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u3002"}}
{"id": "2601.12051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12051", "abs": "https://arxiv.org/abs/2601.12051", "authors": ["Weixin Ye", "Wei Wang", "Yahui Liu", "Yue Song", "Bin Ren", "Wei Bi", "Rita Cucchiara", "Nicu Sebe"], "title": "A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models", "comment": "9 figures, 12 tables", "summary": "In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \\textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack", "code_url": "https://github.com/ywxsuperstar/transformerattack", "code_stars": 0, "code_last_update": "2025-10-26", "AI": {"tldr": "\u63d0\u51faMJP\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6253\u4e71token\u987a\u5e8f\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u6765\u63a9\u76d6\u4f4d\u7f6e\u4fe1\u606f\uff0c\u589e\u5f3aTransformer\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2dTransformer\u9762\u4e34\u68af\u5ea6\u653b\u51fb\u5a01\u80c1\uff0c\u7814\u7a76\u53d1\u73b0\u4f4d\u7f6e\u5d4c\u5165\u7684\u68af\u5ea6\u5305\u542b\u8db3\u591f\u4fe1\u606f\u53ef\u7528\u4e8e\u91cd\u6784\u8f93\u5165\u6570\u636e\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5b89\u5168\u9690\u60a3", "method": "\u63d0\u51faMasked Jigsaw Puzzle\u6846\u67b6\uff1a1) \u968f\u673a\u6253\u4e71token\u987a\u5e8f\u7834\u574ftoken\u987a\u5e8f\uff1b2) \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u63a9\u76d6\u6253\u4e71token\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u7834\u574f\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\u7f16\u7801", "result": "MJP\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u6a21\u578b\u5bf9\u68af\u5ea6\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u80fd\u63d0\u5347\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u6587\u672c\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u662f\u9002\u7528\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6", "conclusion": "MJP\u901a\u8fc7\u7834\u574f\u4f4d\u7f6e\u5d4c\u5165\u4e2d\u7684\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u66f4\u5c11\u4f9d\u8d56\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\u7684\u7279\u5f81\u8868\u793a\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u9700\u6c42"}}
{"id": "2601.12052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12052", "abs": "https://arxiv.org/abs/2601.12052", "authors": ["Zaiyan Zhang", "Jie Li", "Shaowei Shi", "Qiangqiang Yuan"], "title": "Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation", "comment": "Submitted to IGARSS 2026 Conference", "summary": "Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\\% of the parameters, and achieves a 1.4\\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.", "AI": {"tldr": "TDP-CR\u662f\u4e00\u4e2a\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u4e91\u53bb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u7f16\u7801\u4e91\u5c42\u539a\u5ea6\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408SAR\u6570\u636e\u81ea\u9002\u5e94\u4fee\u590d\u5149\u5b66\u9065\u611f\u56fe\u50cf\uff0c\u540c\u65f6\u8fdb\u884c\u4e91\u53bb\u9664\u548c\u571f\u5730\u8986\u76d6\u5206\u5272\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e91\u53bb\u9664\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u4f4e\u5c42\u4fdd\u771f\u5ea6\uff0c\u5bb9\u6613\u8fc7\u5ea6\u5e73\u6ed1\u7eb9\u7406\u548c\u8fb9\u754c\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u4fee\u590d\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002\u5149\u5b66\u9065\u611f\u56fe\u50cf\u53d7\u4e91\u906e\u6321\u9650\u5236\uff0c\u9700\u8981\u65e2\u80fd\u6709\u6548\u53bb\u9664\u4e91\u5c42\u53c8\u80fd\u4fdd\u6301\u5206\u6790\u5c31\u7eea\u6570\u636e\uff08ARD\uff09\u8bed\u4e49\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTDP-CR\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u63d0\u793a\u5f15\u5bfc\u878d\u5408\uff08PGF\uff09\u673a\u5236\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u7f16\u7801\u4e91\u5c42\u539a\u5ea6\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u901a\u9053\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u63d0\u793a\u6761\u4ef6\u7a7a\u95f4\u504f\u7f6e\uff0c\u81ea\u9002\u5e94\u5730\u4ec5\u5728\u5149\u5b66\u6570\u636e\u635f\u574f\u5904\u878d\u5408SAR\u4fe1\u606f\u3002\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u8026\u91cd\u5efa\u548c\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728LuojiaSET-OSFCR\u6570\u636e\u96c6\u4e0a\uff0cTDP-CR\u5728PSNR\u4e0a\u8d85\u8fc7\u6700\u5148\u8fdb\u57fa\u7ebf0.18 dB\uff0c\u4ec5\u4f7f\u752815%\u7684\u53c2\u6570\uff1b\u5728\u591a\u4efb\u52a1\u7ade\u4e89\u8005\u4e2dmIoU\u6301\u7eed\u63d0\u53471.4%\uff0c\u6709\u6548\u63d0\u4f9b\u5206\u6790\u5c31\u7eea\u6570\u636e\u3002", "conclusion": "TDP-CR\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e91\u53bb\u9664\u4e2d\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u7684PGF\u673a\u5236\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u6790\u5c31\u7eea\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13350", "abs": "https://arxiv.org/abs/2601.13350", "authors": ["Abdel Djalil Sad Saoud", "Fred Maurice Ngol\u00e8 Mboula", "Hanane Slimani"], "title": "Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans", "comment": "5 pages, 2 figures", "summary": "Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8c31\u5d4c\u5165\u7684\u57df\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5e73\u6ed1\u4f20\u8f93\u8ba1\u5212\u89e3\u91ca\u4e3a\u4e8c\u5206\u56fe\u90bb\u63a5\u77e9\u9635\uff0c\u7528\u4e8e\u89e3\u51b3\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4f7f\u7528\u4f20\u8f93\u8ba1\u5212\u8fd1\u4f3cMonge\u6620\u5c04\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u4f20\u8f93\u95ee\u9898\u6b63\u5219\u5316\u7b56\u7565\u548c\u8d85\u53c2\u6570\u654f\u611f\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u504f\u7684\u57df\u5bf9\u9f50\u3002", "method": "\u5c06\u5e73\u6ed1\u4f20\u8f93\u8ba1\u5212\u89e3\u91ca\u4e3a\u8fde\u63a5\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u4e8c\u5206\u56fe\u7684\u90bb\u63a5\u77e9\u9635\uff0c\u901a\u8fc7\u8c31\u5d4c\u5165\u63a8\u5bfc\u57df\u4e0d\u53d8\u6837\u672c\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u76f4\u63a5\u8fd1\u4f3cMonge\u6620\u5c04\u7684\u654f\u611f\u6027\u95ee\u9898\u3002", "result": "\u5728\u97f3\u4e50\u6d41\u6d3e\u8bc6\u522b\u3001\u97f3\u4e50-\u8bed\u97f3\u533a\u5206\u4ee5\u53ca\u4e0d\u540c\u8bca\u65ad\u8bbe\u7f6e\u4e0b\u4f7f\u7528\u65f6\u57df\u53cd\u5c04\u7684\u7535\u7f06\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u6574\u4f53\u5f3a\u52b2\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u8c31\u5d4c\u5165\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u4f20\u8f93\u8ba1\u5212\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\u5e76\u63d0\u53d6\u8c31\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u57df\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2601.12066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12066", "abs": "https://arxiv.org/abs/2601.12066", "authors": ["Zijie Lou", "Xiangwei Feng", "Jiaxin Wang", "Xiaochao Qu", "Luoqi Liu", "Ting Liu"], "title": "Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation", "comment": null, "summary": "Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u6865\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\uff0c\u5229\u7528\u6e90\u89c6\u9891\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u4ece\u9ad8\u65af\u566a\u58f0\u5f00\u59cb\u751f\u6210\uff0c\u4e22\u5f03\u4e86\u539f\u59cb\u89c6\u9891\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u5148\u9a8c\uff0c\u5bfc\u81f4\u5bf9\u8c61\u64e6\u9664\u4e0d\u5b8c\u6574\u6216\u751f\u6210\u5185\u5bb9\u4e0e\u573a\u666f\u7269\u7406\u903b\u8f91\u51b2\u7a81\u3002", "method": "\u5c06\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u968f\u673a\u6865\u6a21\u578b\u5b9e\u73b0\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u4efb\u52a1\uff0c\u5efa\u7acb\u4ece\u6e90\u89c6\u9891\uff08\u542b\u5bf9\u8c61\uff09\u5230\u76ee\u6807\u89c6\u9891\uff08\u5bf9\u8c61\u79fb\u9664\uff09\u7684\u76f4\u63a5\u968f\u673a\u8def\u5f84\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u7b56\u7565\uff0c\u6839\u636e\u63a9\u7801\u7279\u5f81\u52a8\u6001\u8c03\u8282\u8f93\u5165\u5d4c\u5165\uff0c\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6865\u5f0f\u5efa\u6a21\u6846\u67b6\u548c\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u8f93\u5165\u89c6\u9891\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5bf9\u8c61\u79fb\u9664\uff0c\u540c\u65f6\u786e\u4fdd\u586b\u5145\u533a\u57df\u4e0e\u5468\u56f4\u73af\u5883\u903b\u8f91\u4e00\u81f4\u3002"}}
{"id": "2601.13398", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13398", "abs": "https://arxiv.org/abs/2601.13398", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "title": "Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility", "comment": "32 pages (preprint)", "summary": "LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.", "AI": {"tldr": "RTCE\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801LLM\u5f80\u8fd4\u6267\u884c\u4e00\u81f4\u6027\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u56db\u4e2a\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u5728\u6b63\u5411\u548c\u53cd\u5411\u6267\u884c\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u6620\u5c04\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5f80\u8fd4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f80\u8fd4\u4ee3\u7801\u6267\u884c\u4e2d\u663e\u793a\u51fa\u5728\u6b63\u5411\u548c\u53cd\u5411\u6267\u884c\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u5728\u7f16\u7801\u548c\u89e3\u7801\u64cd\u4f5c\u95f4\u4fdd\u6301\u4e00\u81f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "method": "\u63d0\u51faRoundTripCodeEval\uff08RTCE\uff09\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u4e0d\u540c\u7684\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u91c7\u7528\u514d\u6267\u884c\u7684\u7cbe\u786e\u5339\u914d\u8bc4\u4f30\u65b9\u6cd5\u6d4b\u8bd5\u53cc\u5c04\u4fdd\u771f\u5ea6\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u6267\u884c\u8f68\u8ff9\u548c\u81ea\u53cd\u601d\u673a\u5236\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u4ee3\u7801LLM\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u53cd\u601d\u673a\u5236\uff09\u90fd\u53ea\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff0c\u4f46\u90fd\u65e0\u6cd5\u5f25\u8865\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524dLLM\u5728\u771f\u6b63\u7684\u5f80\u8fd4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u53ef\u4fe1\u4ee3\u7801\u63a8\u7406\u6240\u9700\u7684\u5185\u5728\u4e00\u81f4\u6027\u3002", "conclusion": "RTCE\u63ed\u793a\u4e86\u73b0\u6709I/O\u9884\u6d4b\u3001\u6267\u884c\u63a8\u7406\u6216\u5f80\u8fd4\u81ea\u7136\u8bed\u8a00\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u7684\u65b0\u89c1\u89e3\uff0c\u8868\u660e\u5f53\u524dLLM\u7f3a\u4e4f\u53ef\u4fe1\u4ee3\u7801\u63a8\u7406\u6240\u9700\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u5f80\u8fd4\u4e00\u81f4\u6027\u662f\u8bc4\u4f30\u4ee3\u7801LLM\u53ef\u9760\u6027\u7684\u91cd\u8981\u7ef4\u5ea6\u3002"}}
{"id": "2601.12076", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12076", "abs": "https://arxiv.org/abs/2601.12076", "authors": ["H. Jiang", "Y. Sun", "Z. Dong", "T. Liu", "Y. Gu"], "title": "CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation", "comment": null, "summary": "Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5bf9\u8c61\u5206\u5272\uff08RS-RVOS\uff09\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6RS-RVOS Bench\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bb0\u5fc6\u8d28\u91cf\u63a7\u5236\u7684\u5728\u7ebf\u5206\u5272\u6846\u67b6MQC-SAM\uff0c\u901a\u8fc7\u521d\u59cb\u8bb0\u5fc6\u6821\u51c6\u548c\u52a8\u6001\u8d28\u91cf\u8bc4\u4f30\u89e3\u51b3\u76ee\u6807\u663e\u8457\u6027\u5f31\u3001\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u7b49\u95ee\u9898\u3002", "motivation": "RS-RVOS\u9762\u4e34\u76ee\u6807\u663e\u8457\u6027\u5f31\u3001\u52a8\u6001\u573a\u666f\u4e2d\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u4e25\u91cd\u7b49\u6311\u6218\uff0c\u96be\u4ee5\u5728\u5206\u5272\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5224\u522b\u6027\u76ee\u6807\u8868\u793a\u3002\u8be5\u9886\u57df\u8fdb\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e13\u7528\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6a21\u578b\u5b58\u5728\u521d\u59cb\u8bb0\u5fc6\u6784\u5efa\u504f\u5dee\u5f71\u54cd\u590d\u6742\u573a\u666f\u4e2d\u7684\u5b9e\u4f8b\u5b9a\u4f4d\uff0c\u4ee5\u53ca\u65e0\u5dee\u522b\u8bb0\u5fc6\u79ef\u7d2f\u5bfc\u81f4\u906e\u6321\u6216\u8bef\u5206\u7c7b\u566a\u58f0\u7f16\u7801\u548c\u6301\u7eed\u9519\u8bef\u4f20\u64ad\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u8d21\u732e\uff1a1\uff09\u6784\u5efaRS-RVOS Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b111\u4e2a\u89c6\u9891\u5e8f\u5217\u3001\u7ea625,000\u5e27\u548c213,000\u4e2a\u65f6\u5e8f\u6307\u4ee3\u6807\u6ce8\uff0c\u91c7\u7528\u4e25\u683c\u7684\u56e0\u679c\u611f\u77e5\u6807\u6ce8\u7b56\u7565\uff1b2\uff09\u63d0\u51fa\u8bb0\u5fc6\u8d28\u91cf\u611f\u77e5\u7684\u5728\u7ebf\u6307\u4ee3\u5206\u5272\u6846\u67b6MQC-SAM\uff0c\u5305\u542b\u65f6\u95f4\u8fd0\u52a8\u4e00\u81f4\u6027\u6a21\u5757\u8fdb\u884c\u521d\u59cb\u8bb0\u5fc6\u6821\u51c6\uff0c\u5229\u7528\u77ed\u671f\u8fd0\u52a8\u8f68\u8ff9\u5148\u9a8c\u6821\u6b63\u7ed3\u6784\u504f\u5dee\u5e76\u5efa\u7acb\u51c6\u786e\u8bb0\u5fc6\u951a\u5b9a\uff0c\u4ee5\u53ca\u89e3\u8026\u6ce8\u610f\u529b\u8bb0\u5fc6\u96c6\u6210\u673a\u5236\u914d\u5408\u52a8\u6001\u8d28\u91cf\u8bc4\u4f30\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u9ad8\u7f6e\u4fe1\u5ea6\u8bed\u4e49\u7279\u5f81\u5e76\u8fc7\u6ee4\u4e0d\u53ef\u9760\u4fe1\u606f\u3002", "result": "\u5728RS-RVOS Bench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMQC-SAM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21RS-RVOS\u57fa\u51c6\u6570\u636e\u96c6\u548c\u63d0\u51fa\u8bb0\u5fc6\u8d28\u91cf\u63a7\u5236\u7684\u5728\u7ebf\u5206\u5272\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u89c6\u9891\u6307\u4ee3\u5bf9\u8c61\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u521d\u59cb\u8bb0\u5fc6\u504f\u5dee\u3001\u9519\u8bef\u4f20\u64ad\u7b49\u95ee\u9898\uff0c\u4e3a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2601.13435", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13435", "abs": "https://arxiv.org/abs/2601.13435", "authors": ["Shuozhe Li", "Du Cheng", "Leqi Liu"], "title": "A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization", "comment": null, "summary": "Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \\emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \\pm 0.045$ and a Sharpe ratio of $2.157 \\pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.", "AI": {"tldr": "WaveLSFormer\uff1a\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u957f\u77ed\u671fTransformer\uff0c\u7528\u4e8e\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u65e5\u5185\u4ea4\u6613\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u5c0f\u6ce2\u5206\u89e3\u548c\u591a\u5c3a\u5ea6\u4fe1\u606f\u878d\u5408\uff0c\u5728\u591a\u4e2a\u884c\u4e1a\u7ec4\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u7684\u6536\u76ca\u548c\u98ce\u9669\u8c03\u6574\u56de\u62a5\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u65e5\u5185\u4ea4\u6613\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1\uff09\u566a\u58f0\u4e25\u91cd\uff1b2\uff09\u975e\u5e73\u7a33\u6027\uff1b3\uff09\u76f8\u5173\u8d44\u4ea7\u95f4\u7684\u5f3a\u6a2a\u622a\u9762\u4f9d\u8d56\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8054\u5408\u6267\u884c\u591a\u5c3a\u5ea6\u5206\u89e3\u548c\u6536\u76ca\u5bfc\u5411\u51b3\u7b56\u5b66\u4e60\u7684\u65b0\u578b\u67b6\u6784\u3002", "method": "\u63d0\u51faWaveLSFormer\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53ef\u5b66\u4e60\u5c0f\u6ce2\u524d\u7aef\uff1a\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u7ec4\u751f\u6210\u4f4e/\u9ad8\u9891\u5206\u91cf\uff0c\u4f7f\u7528\u9891\u8c31\u6b63\u5219\u5316\u5668\u786e\u4fdd\u7a33\u5b9a\u4e14\u5206\u79bb\u826f\u597d\u7684\u9891\u5e26\uff1b2\uff09\u4f4e\u9891\u5f15\u5bfc\u9ad8\u9891\u6ce8\u5165\uff08LGHI\uff09\u6a21\u5757\uff1a\u7528\u9ad8\u9891\u7ebf\u7d22\u7cbe\u70bc\u4f4e\u9891\u8868\u793a\uff0c\u540c\u65f6\u63a7\u5236\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b3\uff09Transformer\u4e3b\u5e72\uff1a\u5904\u7406\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u8f93\u51fa\u591a\u7a7a\u5934\u5bf8\u7ec4\u5408\uff0c\u901a\u8fc7\u98ce\u9669\u9884\u7b97\u91cd\u7f29\u653e\uff0c\u76f4\u63a5\u4f7f\u7528\u4ea4\u6613\u76ee\u6807\u548c\u98ce\u9669\u611f\u77e5\u6b63\u5219\u5316\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u57285\u5e74\u5c0f\u65f6\u6570\u636e\u30016\u4e2a\u884c\u4e1a\u7ec4\u300110\u4e2a\u968f\u673a\u79cd\u5b50\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cWaveLSFormer\u59cb\u7ec8\u4f18\u4e8eMLP\u3001LSTM\u548cTransformer\u57fa\u51c6\u6a21\u578b\uff08\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u79bb\u6563\u5c0f\u6ce2\u524d\u7aef\uff09\u3002\u5728\u6240\u6709\u884c\u4e1a\u5e73\u5747\u4e2d\uff0cWaveLSFormer\u5b9e\u73b0\u4e860.607\u00b10.045\u7684\u7d2f\u8ba1\u603b\u7b56\u7565\u6536\u76ca\u548c2.157\u00b10.166\u7684\u590f\u666e\u6bd4\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76c8\u5229\u80fd\u529b\u548c\u98ce\u9669\u8c03\u6574\u56de\u62a5\u3002", "conclusion": "WaveLSFormer\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u5206\u89e3\u548c\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u4ea4\u6613\u4e2d\u7684\u566a\u58f0\u3001\u975e\u5e73\u7a33\u6027\u548c\u6a2a\u622a\u9762\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u65e5\u5185\u4ea4\u6613\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5728\u6536\u76ca\u548c\u98ce\u9669\u8c03\u6574\u8868\u73b0\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.12080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12080", "abs": "https://arxiv.org/abs/2601.12080", "authors": ["Haipeng Zhou", "Zhaohu Xing", "Hongqiu Wang", "Jun Ma", "Ping Li", "Lei Zhu"], "title": "Toward Real-World High-Precision Image Matting and Segmentation", "comment": "Accepted by AAAI2026, Poster", "summary": "High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.", "AI": {"tldr": "FCLM\u6a21\u578b\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u3001\u57df\u4e0d\u53d8\u5b66\u4e60\u548c\u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u89e3\u51b3\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u62a0\u56fe\u548c\u4e8c\u5206\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u663e\u8457\u5355\u524d\u666f\u5bf9\u8c61\uff0c\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u7c7b\u522b\u4e0d\u53ef\u77e5\u9650\u5236\u4e86\u8de8\u7c7b\u522b\u6cdb\u5316\uff0c\u9ad8\u8d28\u91cf\u6807\u6ce8\u7a00\u7f3a\u5bfc\u81f4\u4f9d\u8d56\u4e0d\u548c\u8c10\u7684\u5408\u6210\u6570\u636e\uff0c\u5bf9\u771f\u5b9e\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u524d\u666f\u4e00\u81f4\u6027\u5b66\u4e60\u6a21\u578bFCLM\uff1a1) \u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u7b56\u7565\uff0c\u8f6c\u79fb\u6df1\u5ea6\u76f8\u5173\u77e5\u8bc6\u4ee5\u6539\u5584\u524d\u666f\u8868\u793a\uff1b2) \u5c06\u5408\u6210\u6570\u636e\u5904\u7406\u89c6\u4e3a\u57df\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u57df\u4e0d\u53d8\u5b66\u4e60\u7b56\u7565\u4e13\u6ce8\u4e8e\u524d\u666f\u5b66\u4e60\uff1b3) \u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u53ef\u63a5\u6536\u89c6\u89c9\u548c\u8bed\u8a00\u63d0\u793a\u6765\u9884\u6d4b\u53c2\u8003\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FCLM\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u3001\u57df\u4e0d\u53d8\u5b66\u4e60\u548c\u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8de8\u7c7b\u522b\u6cdb\u5316\u548c\u771f\u5b9e\u573a\u666f\u9002\u5e94\u6027\u3002"}}
{"id": "2601.13448", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13448", "abs": "https://arxiv.org/abs/2601.13448", "authors": ["Sofiane Tanji", "Samuel Vaiter", "Yassine Laguel"], "title": "Fairness-informed Pareto Optimization : An Efficient Bilevel Framework", "comment": null, "summary": "Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.", "AI": {"tldr": "BADR\u662f\u4e00\u4e2a\u53cc\u5c42\u81ea\u9002\u5e94\u91cd\u6807\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u4efb\u4f55\u516c\u5e73\u6027\u6307\u6807\u6062\u590d\u6700\u4f18\u5e15\u7d2f\u6258\u6548\u7387\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u516c\u5e73\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e38\u4ea7\u751f\u5e15\u7d2f\u6258\u65e0\u6548\u6a21\u578b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f20\u7edf\u5904\u7406\u65b9\u6cd5\uff08\u5982\u516c\u5e73\u6b63\u5219\u5316\uff09\u5e38\u4ea7\u751f\u5e15\u7d2f\u6258\u65e0\u6548\u6a21\u578b\uff0c\u5373\u67d0\u4e9b\u7fa4\u4f53\u6027\u80fd\u53ef\u5728\u4e0d\u635f\u5bb3\u5176\u4ed6\u7fa4\u4f53\u60c5\u51b5\u4e0b\u63d0\u5347\uff1b2\uff09\u73b0\u6709\u5e15\u7d2f\u6258\u6548\u7387\u65b9\u6cd5\u504f\u5411\u7279\u5b9a\u516c\u5e73\u89c6\u89d2\uff0c\u65e0\u6cd5\u9002\u5e94\u6587\u732e\u4e2d\u5e7f\u6cdb\u7684\u516c\u5e73\u6027\u6307\u6807\u3002", "method": "\u63d0\u51faBADR\uff08Bilevel Adaptive Rescalarisation\uff09\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5c42\u4f18\u5316\uff1a\u4e0b\u5c42\u662f\u52a0\u6743\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4efb\u52a1\uff0c\u6743\u91cd\u4e3a\u5404\u7fa4\u4f53\u7684\u51f8\u7ec4\u5408\uff1b\u4e0a\u5c42\u4f18\u5316\u6240\u9009\u516c\u5e73\u6027\u76ee\u6807\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u5927\u89c4\u6a21\u5355\u5faa\u73af\u7b97\u6cd5BADR-GD\u548cBADR-SGD\uff0c\u5e76\u5efa\u7acb\u4e86\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u53d1\u5e03\u4e86badr\u5f00\u6e90Python\u5de5\u5177\u7bb1\uff0c\u652f\u6301\u591a\u79cd\u5b66\u4e60\u4efb\u52a1\u548c\u516c\u5e73\u6027\u6307\u6807\u3002\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8868\u660eBADR\u4f18\u4e8e\u73b0\u6709\u5e15\u7d2f\u6258\u6548\u7387\u516c\u5e73\u65b9\u6cd5\u3002", "conclusion": "BADR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6846\u67b6\uff0c\u53ef\u4e3a\u4efb\u4f55\u516c\u5e73\u6027\u6307\u6807\u6062\u590d\u6700\u4f18\u5e15\u7d2f\u6258\u6548\u7387\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u6536\u655b\u4fdd\u8bc1\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u7528\u5316\u3002"}}
{"id": "2601.12082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12082", "abs": "https://arxiv.org/abs/2601.12082", "authors": ["Tiffanie Godelaine", "Maxime Zanella", "Karim El Khoury", "Sa\u00efd Mahmoudi", "Beno\u00eet Macq", "Christophe De Vleeschouwer"], "title": "Conditional Random Fields for Interactive Refinement of Histopathological Predictions", "comment": null, "summary": "Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.", "code_url": "https://github.com/tgodelaine/HistoCRF", "code_stars": 0, "code_last_update": "2025-12-11", "AI": {"tldr": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u6539\u8fdb\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5229\u7528\u6807\u6ce8\u4fe1\u606f\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u5bf9\u764c\u75c7\u68c0\u6d4b\u548c\u5206\u671f\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u4ef7\u503c\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u4ecd\u4e0d\u5b8c\u7f8e\uff0c\u9700\u8981\u6539\u8fdb\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u5c06\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u9002\u914d\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u5e94\u7528\uff0c\u8bbe\u8ba1\u65b0\u9896\u7684\u6210\u5bf9\u52bf\u51fd\u6570\u4ee5\u4fc3\u8fdb\u6807\u7b7e\u591a\u6837\u6027\u5e76\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u3002\u8003\u8651\u4e09\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff1a\u65e0\u6807\u6ce8\u3001\u6709\u4e13\u5bb6\u6807\u6ce8\u3001\u4ee5\u53ca\u8fed\u4ee3\u5f0f\u4eba\u673a\u534f\u540c\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u6db5\u76d6\u4e0d\u540c\u5668\u5b98\u548c\u75be\u75c5\u7684patch\u7ea7\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\uff1a\u65e0\u6807\u6ce8\u65f616.0%\uff0c\u4ec5100\u4e2a\u6807\u6ce8\u65f627.5%\u3002\u4eba\u673a\u534f\u540c\u6807\u6ce8\u8fdb\u4e00\u6b65\u8fbe\u523032.6%\u7684\u589e\u76ca\u3002", "conclusion": "HistoCRF\u80fd\u6709\u6548\u6539\u8fdb\u7ec4\u7ec7\u75c5\u7406\u5b66VLM\u7684\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7CRF\u6846\u67b6\u548c\u6807\u6ce8\u4fe1\u606f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2601.13456", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13456", "abs": "https://arxiv.org/abs/2601.13456", "authors": ["Sahasra Kokkula", "Daniel David", "Aaditya Baruah"], "title": "Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay", "comment": "8 pages, 5 figures. Course project for Neural Networks & Deep Learning COMSW4776 course at Columbia University", "summary": "Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5e94\u5bf9\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\uff1a\u901a\u8fc7\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u5bfc\u81f4\u6807\u51c6FedAvg\u5728\u5b63\u8282\u6027\u6f02\u79fb\u573a\u666f\u4e2d\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u4e00\u4e2a\u5c0f\u7684\u5386\u53f2\u6837\u672c\u7f13\u51b2\u533a\uff0c\u5728\u672c\u5730\u8bad\u7ec3\u65f6\u5c06\u8fc7\u53bb\u6837\u672c\u4e0e\u5f53\u524d\u6570\u636e\u6df7\u5408\uff0c\u65e0\u9700\u6539\u53d8\u670d\u52a1\u5668\u805a\u5408\u673a\u5236", "result": "\u5728Fashion-MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u6807\u51c6FedAvg\u51c6\u786e\u7387\u4ece74%\u964d\u81f328%\uff1b\u4f7f\u7528\u6bcf\u7c7b50\u4e2a\u6837\u672c\u7684\u7f13\u51b2\u533a\u540e\uff0c\u6027\u80fd\u6062\u590d\u81f378-82%\uff0c\u6709\u6548\u9632\u6b62\u9057\u5fd8\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u5185\u5b58\u4e0e\u51c6\u786e\u7387\u5b58\u5728\u6743\u8861\u5173\u7cfb", "conclusion": "\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6297\u9057\u5fd8\u65b9\u6cd5\uff0c\u65e0\u9700\u4fee\u6539\u670d\u52a1\u5668\u7aef\uff0c\u901a\u8fc7\u7ef4\u62a4\u5c0f\u89c4\u6a21\u5386\u53f2\u6837\u672c\u7f13\u51b2\u533a\u5373\u53ef\u663e\u8457\u7f13\u89e3\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u5e26\u6765\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898"}}
{"id": "2601.13474", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13474", "abs": "https://arxiv.org/abs/2601.13474", "authors": ["Jianhao Ma", "Yu Huang", "Yuejie Chi", "Yuxin Chen"], "title": "Preconditioning Benefits of Spectral Orthogonalization in Muon", "comment": null, "summary": "The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u548c\u7ebf\u6027Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e24\u4e2a\u6848\u4f8b\uff0c\u5206\u6790\u4e86\u7b80\u5316\u7248Muon\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5177\u6709\u4e0e\u6761\u4ef6\u6570\u65e0\u5173\u7684\u7ebf\u6027\u6536\u655b\u901f\u5ea6\uff0c\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u548cAdam\u7b97\u6cd5\u3002", "motivation": "Muon\u4f18\u5316\u5668\u4f5c\u4e3a\u5229\u7528\u68af\u5ea6\u8c31\u6b63\u4ea4\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u91cc\u7a0b\u7891\u7b97\u6cd5\uff0c\u5176\u5e95\u5c42\u673a\u5236\u7279\u522b\u662f\u68af\u5ea6\u6b63\u4ea4\u5316\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u76ee\u524d\u5f88\u5c11\u6709\u5de5\u4f5c\u63d0\u4f9b\u7aef\u5230\u7aef\u5206\u6790\u6765\u4e25\u683c\u89e3\u91ca\u5176\u5728\u5177\u4f53\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u7b80\u5316\u7248Muon\u53d8\u4f53\u5728\u4e24\u4e2a\u5177\u4f53\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff1a\u77e9\u9635\u5206\u89e3\u548c\u7ebf\u6027Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u5bf9\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u7b80\u5316Muon\u5177\u6709\u4e0e\u76f8\u5173\u6761\u4ef6\u6570\u65e0\u5173\u7684\u7ebf\u6027\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u7b80\u5316Muon\u5728\u7ebf\u6027Transformer\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u77e9\u9635\u5206\u89e3\u95ee\u9898\u4e0a\u5747\u5177\u6709\u4e0e\u6761\u4ef6\u6570\u65e0\u5173\u7684\u7ebf\u6027\u6536\u655b\u901f\u5ea6\uff0c\u5728\u8fed\u4ee3\u590d\u6742\u5ea6\u4e0a\u4e25\u683c\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u548cAdam\u7b97\u6cd5\u3002\u5206\u6790\u63ed\u793a\u4e86Muon\u52a8\u529b\u5b66\u5728\u8c31\u57df\u89e3\u8026\u4e3a\u72ec\u7acb\u6807\u91cf\u5e8f\u5217\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u8c31\u6b63\u4ea4\u5316\u8bf1\u5bfc\u7684\u9884\u5904\u7406\u6548\u5e94\uff0c\u4e3a\u7406\u89e3Muon\u5728\u8fd9\u4e9b\u77e9\u9635\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u53ef\u80fd\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.12119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12119", "abs": "https://arxiv.org/abs/2601.12119", "authors": ["Xiaotong Zhou", "Zhenhui Yuan", "Yi Han", "Tianhua Xu", "Laurence T. Yang"], "title": "CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction", "comment": null, "summary": "Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.", "code_url": "https://github.com/Rebecca689/CARLA-Round", "code_stars": 0, "code_last_update": "2026-01-12", "AI": {"tldr": "CARLA-Round\u662f\u4e00\u4e2a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u4eff\u771f\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u73af\u5c9b\u573a\u666f\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u901a\u8fc7\u63a7\u5236\u5929\u6c14\u6761\u4ef6\u548c\u4ea4\u901a\u5bc6\u5ea6\u521b\u5efa25\u4e2a\u7ed3\u6784\u5316\u573a\u666f\uff0c\u91cf\u5316\u4e86\u4e0d\u540c\u56e0\u7d20\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73af\u5c9b\u573a\u666f\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u5bf9\u51cf\u5c11\u4ea4\u901a\u4e8b\u6545\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u5706\u5f62\u51e0\u4f55\u7ed3\u6784\u3001\u8fde\u7eed\u7684\u6c47\u5165\u548c\u8ba9\u884c\u4ea4\u4e92\u4ee5\u53ca\u7f3a\u4e4f\u4ea4\u901a\u4fe1\u53f7\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u5b58\u5728\u89c2\u6d4b\u4e0d\u5b8c\u6574\u548c\u6df7\u6742\u56e0\u7d20\u96be\u4ee5\u5206\u79bb\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528CARLA\u4eff\u771f\u5e73\u53f0\u7cfb\u7edf\u8bbe\u8ba1\u6570\u636e\u96c6\uff0c\u7ed3\u6784\u5316\u63a7\u5236\u5929\u6c14\u6761\u4ef6\uff085\u79cd\u7c7b\u578b\uff09\u548c\u4ea4\u901a\u5bc6\u5ea6\u6c34\u5e73\uff08\u670d\u52a1\u6c34\u5e73A-E\u7ea7\uff09\uff0c\u521b\u5efa25\u4e2a\u53ef\u63a7\u573a\u666f\u3002\u6bcf\u4e2a\u573a\u666f\u5305\u542b\u771f\u5b9e\u7684\u9a7e\u9a76\u884c\u4e3a\u6df7\u5408\uff0c\u5e76\u63d0\u4f9b\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u663e\u5f0f\u6807\u6ce8\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u4f7f\u7528\u6807\u51c6\u57fa\u7ebf\u6a21\u578b\uff08LSTM\u3001GCN\u3001GRU+GCN\uff09\u663e\u793a\uff1a\u4ea4\u901a\u5bc6\u5ea6\u5bf9\u9884\u6d4b\u96be\u5ea6\u5177\u6709\u4e3b\u5bfc\u6027\u7684\u5355\u8c03\u6548\u5e94\uff0c\u800c\u5929\u6c14\u6761\u4ef6\u5448\u73b0\u975e\u7ebf\u6027\u5f71\u54cd\u3002\u6700\u4f73\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754crounD\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.312m ADE\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u3002", "conclusion": "CARLA-Round\u6570\u636e\u96c6\u901a\u8fc7\u7ed3\u6784\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5bf9\u73af\u5c9b\u8f68\u8ff9\u9884\u6d4b\u4e2d\u4e0d\u540c\u56e0\u7d20\u5f71\u54cd\u7684\u7cbe\u786e\u91cf\u5316\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u6df7\u6742\u56e0\u7d20\u96be\u4ee5\u5206\u79bb\u7684\u95ee\u9898\uff0c\u4e3a\u73af\u5c9b\u573a\u666f\u7684\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2601.13522", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.13522", "abs": "https://arxiv.org/abs/2601.13522", "authors": ["Shuang Li"], "title": "StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing", "comment": null, "summary": "Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTucker\u5206\u89e3\u7684\u968f\u673a\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f4eTucker\u79e9\u5f20\u91cf\u611f\u77e5\u95ee\u9898\uff0c\u907f\u514d\u91cd\u590d\u5f20\u91cf\u6295\u5f71\uff0c\u5b9e\u73b0\u4f4e\u7ef4\u5f20\u91cf\u56e0\u5b50\u7684\u9ad8\u6548\u5c0f\u6279\u91cf\u66f4\u65b0\u3002", "motivation": "\u4f4eTucker\u79e9\u5f20\u91cf\u80fd\u6709\u6548\u6355\u6349\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u591a\u6a21\u6001\u5b50\u7a7a\u95f4\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u6062\u590d\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u5f20\u91cf\u6295\u5f71\uff0c\u8981\u4e48\u4f9d\u8d56\u5168\u68af\u5ea6\u8ba1\u7b97\uff0c\u800c\u5927\u591a\u6570\u968f\u673a\u56e0\u5b50\u5316\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5f20\u91cf\u5206\u89e3\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u968f\u673a\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728Tucker\u5206\u89e3\u4e0b\u7684\u6838\u5fc3\u5f20\u91cf\u548c\u56e0\u5b50\u77e9\u9635\u4e0a\u64cd\u4f5c\uff0c\u907f\u514d\u91cd\u590d\u5f20\u91cf\u6295\u5f71\uff0c\u5b9e\u73b0\u5bf9\u4f4e\u7ef4\u5f20\u91cf\u56e0\u5b50\u7684\u9ad8\u6548\u5c0f\u6279\u91cf\u66f4\u65b0\u3002", "result": "\u5728\u5408\u6210\u5f20\u91cf\u611f\u77e5\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u76f8\u6bd4\u4ee3\u8868\u6027\u968f\u673a\u5f20\u91cf\u6062\u590d\u57fa\u7ebf\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6536\u655b\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4eTucker\u79e9\u5f20\u91cf\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u66f4\u597d\u7684\u65f6\u95f4\u6548\u7387\u3002"}}
{"id": "2601.12147", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12147", "abs": "https://arxiv.org/abs/2601.12147", "authors": ["Zezhong Fan", "Xiaohan Li", "Topojoy Biswas", "Kaushiki Nag", "Kannan Achan"], "title": "Segment and Matte Anything in a Unified Model", "comment": "AAAI 2026", "summary": "Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.", "AI": {"tldr": "SAMA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6269\u5c55\u7684SAM\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5206\u5272\u548c\u62a0\u56fe\uff0c\u901a\u8fc7MVLE\u7f16\u7801\u5668\u548cLocal-Adapter\u63d0\u5347\u8fb9\u754c\u7ec6\u8282\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1SAM\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u7075\u6d3b\u63d0\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a9\u7801\u9884\u6d4b\u7cbe\u5ea6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u663e\u4e0d\u8db3\u3002\u73b0\u6709\u7ec6\u5316\u6a21\u5757\u96be\u4ee5\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5bf9\u8c61\u5206\u5272\uff0c\u4e14\u4ea4\u4e92\u5f0f\u56fe\u50cf\u62a0\u56fe\u5728SAM\u80cc\u666f\u4e0b\u5c1a\u672a\u63a2\u7d22\u3002\u7814\u7a76\u8868\u660e\u5206\u5272\u4e0e\u62a0\u56fe\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8fd9\u4e3a\u7edf\u4e00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faSAMA\u4f5c\u4e3aSAM\u7684\u8f7b\u91cf\u7ea7\u6269\u5c55\uff0c\u5305\u542b\uff1a1) Multi-View Localization Encoder (MVLE)\u4ece\u5c40\u90e8\u89c6\u56fe\u6355\u83b7\u8be6\u7ec6\u7279\u5f81\uff1b2) Localization Adapter (Local-Adapter)\u901a\u8fc7\u6062\u590d\u7ec6\u5fae\u8fb9\u754c\u7ec6\u8282\u6765\u7ec6\u5316\u63a9\u7801\u8f93\u51fa\uff1b3) \u4e3a\u6bcf\u4e2a\u4efb\u52a1\u96c6\u6210\u4e24\u4e2a\u9884\u6d4b\u5934\uff0c\u540c\u65f6\u751f\u6210\u5206\u5272\u548c\u62a0\u56fe\u63a9\u7801\u3002\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "SAMA\u5728\u591a\u4e2a\u5206\u5272\u548c\u62a0\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u6a21\u578b\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u548c\u62a0\u56fe\u7ed3\u679c\u3002", "conclusion": "SAMA\u6210\u529f\u5730\u5c06\u5206\u5272\u548c\u62a0\u56fe\u4efb\u52a1\u7edf\u4e00\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7MVLE\u548cLocal-Adapter\u6709\u6548\u63d0\u5347\u4e86\u8fb9\u754c\u7ec6\u8282\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5206\u5272\u548c\u62a0\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13534", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13534", "abs": "https://arxiv.org/abs/2601.13534", "authors": ["Xu Zhang", "Junwei Deng", "Chang Xu", "Hao Li", "Jiang Bian"], "title": "MN-TSG:Continuous Time Series Generation with Irregular Observations", "comment": "34 pages", "summary": "Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.\n  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.\n  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.\n  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.", "AI": {"tldr": "MN-TSG\uff1a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7684\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u8fde\u7eed\u751f\u6210\u6846\u67b6", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u89c4\u5219\u91c7\u6837\u548c\u56fa\u5b9a\u8f93\u51fa\u5206\u8fa8\u7387\uff0c\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u7a00\u758f\u89c2\u6d4b\u7684\u6570\u636e\u4e0d\u5339\u914d\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u76d1\u6d4b\u7b49\u9700\u8981\u8fde\u7eed\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u5e8f\u5217\u7684\u5e94\u7528\u4e2d\u5c24\u4e3a\u7a81\u51fa\u3002\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u867d\u80fd\u5efa\u6a21\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\uff0c\u4f46\u5728\u6355\u6349\u590d\u6742\u52a8\u6001\u65f6\u95f4\u6a21\u5f0f\u548c\u652f\u6491\u8fde\u7eed\u751f\u6210\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u63d0\u51faMN-TSG\u6846\u67b6\uff0c\u6838\u5fc3\u662fMoE-NCDE\u67b6\u6784\uff1a1\uff09\u91c7\u7528\u52a8\u6001\u53c2\u6570\u5316\u7684\u4e13\u5bb6\u51fd\u6570\uff1b2\uff09\u89e3\u8026\u8bbe\u8ba1\u4ee5\u4f18\u5316MoE\u52a8\u6001\uff1b3\uff09\u5229\u7528\u73b0\u6709TSG\u6a21\u578b\u5b66\u4e60\u4e13\u5bb6\u6df7\u5408\u4e0e\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u8054\u5408\u5206\u5e03\uff0c\u4f7f\u6846\u67b6\u4e0d\u4ec5\u80fd\u751f\u6210\u65b0\u6837\u672c\uff0c\u8fd8\u80fd\u4e3a\u6bcf\u4e2a\u6837\u672c\u751f\u6210\u9002\u5f53\u7684\u4e13\u5bb6\u914d\u7f6e\u3002", "result": "\u572810\u4e2a\u516c\u5171\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMN-TSG\u5728\"\u4e0d\u89c4\u5219\u5230\u89c4\u5219\"\u548c\"\u4e0d\u89c4\u5219\u5230\u8fde\u7eed\"\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u4e00\u81f4\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MN-TSG\u901a\u8fc7\u6574\u5408MoE-NCDE\u67b6\u6784\u4e0e\u73b0\u6709TSG\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u76d1\u6d4b\u7b49\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12150", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12150", "abs": "https://arxiv.org/abs/2601.12150", "authors": ["Mengxuan Hu", "Zihan Guan", "John Kang", "Sheng Li", "Zhongliang Zhou"], "title": "Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models", "comment": "8 pages", "summary": "Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7a7a\u95f4\u548c\u65f6\u95f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u7684\u90bb\u8fd1\u5757\u7a00\u758f\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u5168\u5c40\u6ce8\u610f\u529b\u5206\u6570\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027token\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u9ad8\u5206\u8fa8\u7387\u5168\u5207\u7247\u56fe\u50cf\u63a8\u7406\u65f6\u7684GPU\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u901a\u5e38\u53d7\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u5c3a\u5bf8\uff08\u5982224\u00d7224\uff09\uff0c\u5728\u5904\u7406\u8de8\u8d8a\u6570\u5343\u5206\u8fa8\u7387\u7ea7\u522b\u7684\u5168\u5207\u7247\u56fe\u50cf\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u7b80\u5355\u653e\u5927\u8f93\u5165\u4f1a\u5bfc\u81f4GPU\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u800c\u964d\u4f4e\u91c7\u6837\u4f1a\u6539\u53d8\u5fae\u7c73\u6bcf\u50cf\u7d20\u5206\u8fa8\u7387\u5e76\u6a21\u7cca\u5173\u952e\u5f62\u6001\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u548c\u65f6\u95f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\uff1a1\uff09\u4f7f\u7528\u7a7a\u95f4\u611f\u77e5\u7684\u90bb\u8fd1\u5757\u7a00\u758f\u5316\u6ce8\u610f\u529b\u673a\u5236\uff1b2\uff09\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5206\u6570\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027token\u3002\u8be5\u8bbe\u8ba1\u5728\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u63a8\u7406\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11GPU\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728ROI\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.67%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u83b7\u5f97\u517c\u5bb9\u7684\u7ed3\u679c\u3002\u80fd\u591f\u5728\u76f8\u540cGPU\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5168\u5207\u7247\u56fe\u50cf\u65f6\u7684\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u7a00\u758f\u5316\u548ctoken\u8fc7\u6ee4\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u5316\u4e86\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u3002"}}
{"id": "2601.13563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13563", "abs": "https://arxiv.org/abs/2601.13563", "authors": ["Aryan Karmore"], "title": "ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits", "comment": null, "summary": "Linear memory scaling stores $N$ independent expert weight matrices requiring $\\mathcal{O}(N \\cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\\mathcal{O}(d^2 + N \\cdot d \\log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.", "AI": {"tldr": "ButterflyMoE\u901a\u8fc7\u5c06\u4e13\u5bb6\u89c6\u4e3a\u5171\u4eab\u91cf\u5316\u57fa\u8d28\u7684\u51e0\u4f55\u91cd\u5b9a\u5411\uff0c\u800c\u975e\u72ec\u7acb\u6743\u91cd\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u4e13\u5bb6\u6570\u91cf\u7684\u4e9a\u7ebf\u6027\u5185\u5b58\u589e\u957f\uff0c\u5728256\u4e2a\u4e13\u5bb6\u65f6\u8fbe\u5230150\u500d\u5185\u5b58\u538b\u7f29\u3002", "motivation": "\u4f20\u7edfMoE\u65b9\u6cd5\u4e2d\uff0cN\u4e2a\u72ec\u7acb\u4e13\u5bb6\u6743\u91cd\u77e9\u9635\u9700\u8981O(N\u00b7d\u00b2)\u5185\u5b58\uff0c\u8fd9\u8d85\u51fa\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u5b58\u50a8\u9884\u7b97\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff08\u91cf\u5316\u3001\u526a\u679d\u3001\u4f4e\u79e9\u5206\u89e3\uff09\u53ea\u80fd\u51cf\u5c11\u5e38\u6570\u56e0\u5b50\uff0c\u65e0\u6cd5\u89e3\u51b3\u7ebf\u6027\u6269\u5c55\u74f6\u9888\u3002", "method": "\u5c06\u4e13\u5bb6\u89c6\u4e3a\u5171\u4eab\u4e09\u5143\u539f\u578b\u7684\u51e0\u4f55\u91cd\u5b9a\u5411\uff0c\u800c\u975e\u72ec\u7acb\u77e9\u9635\u3002\u901a\u8fc7\u5bf9\u5171\u4eab\u91cf\u5316\u57fa\u8d28\u5e94\u7528\u5b66\u4e60\u5230\u7684\u65cb\u8f6c\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u5b9e\u73b0O(d\u00b2 + N\u00b7d log d)\u5185\u5b58\u3002\u8bad\u7ec3\u8fd9\u4e9b\u65cb\u8f6c\u4e0e\u91cf\u5316\u7ed3\u5408\uff0c\u51cf\u5c11\u4e86\u6fc0\u6d3b\u5f02\u5e38\u503c\u5e76\u7a33\u5b9a\u4e86\u6781\u7aef\u4f4e\u4f4d\u8bad\u7ec3\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cButterflyMoE\u5728256\u4e2a\u4e13\u5bb6\u65f6\u5b9e\u73b0\u4e86150\u500d\u5185\u5b58\u538b\u7f29\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002\u8fd9\u4f7f\u5f9764\u4e2a\u4e13\u5bb6\u53ef\u4ee5\u90e8\u7f72\u57284GB\u8bbe\u5907\u4e0a\uff0c\u800c\u6807\u51c6MoE\u53ea\u80fd\u5bb9\u7eb38\u4e2a\u4e13\u5bb6\u3002", "conclusion": "\u51e0\u4f55\u53c2\u6570\u5316\u6253\u7834\u4e86MoE\u7684\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u74f6\u9888\uff0c\u901a\u8fc7\u5c06\u4e13\u5bb6\u591a\u6837\u6027\u89c6\u4e3a\u5171\u4eab\u5bb9\u91cf\u7684\u4e0d\u540c\u89c6\u89d2\uff0c\u800c\u975e\u5197\u4f59\u5b58\u50a8\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7ebf\u6027\u5185\u5b58\u589e\u957f\uff0c\u4f7f\u5927\u89c4\u6a21\u4e13\u5bb6\u6a21\u578b\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2601.12193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12193", "abs": "https://arxiv.org/abs/2601.12193", "authors": ["Shaunak Halbe", "Bhagyashree Puranik", "Jayakrishnan Unnikrishnan", "Kushan Thakkar", "Vimal Bhat", "Toufiq Parag"], "title": "VIRTUE: Versatile Video Retrieval Through Unified Embeddings", "comment": null, "summary": "Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.", "AI": {"tldr": "VIRTUE\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u96c6\u6210\u8bed\u6599\u5e93\u548c\u65f6\u523b\u7ea7\u68c0\u7d22\u80fd\u529b\uff0c\u652f\u6301\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u548cLoRA\u9ad8\u6548\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u3001\u65f6\u523b\u68c0\u7d22\u548c\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e13\u7528\u67b6\u6784\u867d\u7136\u68c0\u7d22\u6027\u80fd\u5f3a\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\uff1b\u800c\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\u652f\u6301\u4e30\u5bcc\u591a\u6a21\u6001\u641c\u7d22\uff0c\u4f46\u68c0\u7d22\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4e13\u7528\u7cfb\u7edf\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u5904\u7406\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u53c8\u80fd\u8fbe\u5230\u4e13\u7528\u7cfb\u7edf\u68c0\u7d22\u6027\u80fd\u7684\u901a\u7528\u89c6\u9891\u68c0\u7d22\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5171\u4eabMLLM\u4e3b\u5e72\u751f\u6210\u89c6\u89c9\u548c\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u4fc3\u8fdb\u9ad8\u6548\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u5019\u9009\u641c\u7d22\u3002\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u572870\u4e07\u5bf9\u89c6\u89c9-\u6587\u672c\u6570\u636e\u6837\u672c\u4e0a\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002\u901a\u8fc7\u91cd\u6392\u5e8f\u8fdb\u4e00\u6b65\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u96f6\u6837\u672c\u65f6\u523b\u68c0\u7d22\u3002", "result": "\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5176\u4ed6MLLM\u65b9\u6cd5\uff1b\u96f6\u6837\u672c\u65f6\u523b\u68c0\u7d22\u8fbe\u5230\u7ade\u4e89\u6027\u7ed3\u679c\uff1b\u96f6\u6837\u672c\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u901a\u8fc7\u91cd\u6392\u5e8f\u540e\uff0c\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\u73b0\u6709MLLM\u68c0\u7d22\u7cfb\u7edf\uff0c\u4e0e\u5728\u66f4\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "VIRTUE\u5c55\u793a\u4e86MLLM\u5728\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u5355\u4e00\u67b6\u6784\u5b9e\u73b0\u4e86\u8bed\u6599\u5e93\u68c0\u7d22\u3001\u65f6\u523b\u5b9a\u4f4d\u548c\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\u7684\u96c6\u6210\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8bad\u7ec3\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u4e0e\u4e13\u7528\u7cfb\u7edf\u76f8\u5f53\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u901a\u7528\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13566", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13566", "abs": "https://arxiv.org/abs/2601.13566", "authors": ["Tianyi Qiu", "Ahmed Hani Ismail", "Zhonghao He", "Shi Feng"], "title": "Self-Improvement as Coherence Optimization: A Theoretical Account", "comment": "39 pages", "summary": "Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bed\u8a00\u6a21\u578b\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u81ea\u6211\u63d0\u5347\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8fa9\u8bba\u3001\u5f15\u5bfc\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\u7b49\u65b9\u6cd5\u7edf\u4e00\u4e3a\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u5e76\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u63cf\u8ff0\u957f\u5ea6\u6b63\u5219\u5316", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8fa9\u8bba\u3001\u5f15\u5bfc\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\u7b49\u65b9\u6cd5\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u51c6\u786e\u6027\uff0c\u751a\u81f3\u8fbe\u5230\u6709\u76d1\u7763\u5fae\u8c03\u7684\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u4f55\u6709\u6548\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca", "method": "\u63d0\u51fa\u4e00\u81f4\u6027\u4f18\u5316\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u65e0\u76d1\u7763\u81ea\u6211\u63d0\u5347\u65b9\u6cd5\u7edf\u4e00\u4e3a\u5bfb\u627e\u6700\u53ef\u538b\u7f29\u548c\u8054\u5408\u53ef\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u5230\u884c\u4e3a\u6620\u5c04\uff0c\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u63cf\u8ff0\u957f\u5ea6\u6b63\u5219\u5316\uff0c\u5e76\u63a8\u5bfc\u51fa\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u57fa\u7840\u4e0a\u7684\u6700\u4f18\u6b63\u5219\u5316\u65b9\u6848", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u4e00\u81f4\u6027\u4f18\u5316\u662f\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u6700\u4f18\u7684\u6b63\u5219\u5316\u65b9\u6848\uff0c\u521d\u6b65\u5b9e\u9a8c\u652f\u6301\u7406\u8bba\u9884\u6d4b\uff0c\u89e3\u91ca\u4e86\u65e0\u53cd\u9988\u81ea\u6211\u63d0\u5347\u4e3a\u4f55\u6709\u6548\uff0c\u5e76\u9884\u6d4b\u4e86\u5176\u6210\u529f\u548c\u5931\u8d25\u7684\u6761\u4ef6", "conclusion": "\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u76d1\u7763\u81ea\u6211\u63d0\u5347\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u5185\u5728\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u8fd9\u7c7b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2601.12224", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12224", "abs": "https://arxiv.org/abs/2601.12224", "authors": ["Meng Wei", "Kun Yuan", "Shi Li", "Yue Zhou", "Long Bai", "Nassir Navab", "Hongliang Ren", "Hong Joo Lee", "Tom Vercauteren", "Nicolas Padoy"], "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion", "comment": null, "summary": "Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.", "AI": {"tldr": "SurgRef\uff1a\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ddf\u8e2a\u624b\u672f\u5668\u68b0\u7684\u8fd0\u52a8\u8f68\u8ff9\u800c\u975e\u9759\u6001\u5916\u89c2\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5bf9\u624b\u672f\u5668\u68b0\u7684\u7cbe\u51c6\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u3001\u6a21\u7cca\u548c\u672f\u8bed\u4e0d\u719f\u6089\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u573a\u666f\u4e2d\u7684\u6307\u4ee3\u5206\u5272\u4efb\u52a1\uff08\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5b9a\u4f4d\u624b\u672f\u5668\u68b0\uff09\u5b58\u5728\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7279\u5f81\u548c\u9884\u5b9a\u4e49\u5668\u68b0\u540d\u79f0\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u624b\u672f\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u3001\u6a21\u7cca\u6216\u4f7f\u7528\u975e\u6807\u51c6\u672f\u8bed\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSurgRef\u6846\u67b6\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u5668\u68b0\u7684\u8fd0\u52a8\u8f68\u8ff9\u800c\u975e\u5916\u89c2\u7279\u5f81\u8fdb\u884c\u8bed\u8a00\u63a5\u5730\u3002\u901a\u8fc7\u5206\u6790\u5de5\u5177\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u79fb\u52a8\u548c\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5bf9\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u8868\u8fbe\u7684\u7406\u89e3\u548c\u5668\u68b0\u5206\u5272\u3002\u540c\u65f6\u6784\u5efa\u4e86Ref-IMotion\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u673a\u6784\u624b\u672f\u89c6\u9891\u3001\u5bc6\u96c6\u65f6\u7a7a\u63a9\u7801\u548c\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "SurgRef\u5728\u591a\u79cd\u624b\u672f\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9c81\u68d2\u7684\u8bed\u8a00\u9a71\u52a8\u624b\u672f\u89c6\u9891\u5206\u5272\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u5373\u4f7f\u5728\u5668\u68b0\u88ab\u906e\u6321\u3001\u63cf\u8ff0\u6a21\u7cca\u6216\u4f7f\u7528\u4e0d\u719f\u6089\u672f\u8bed\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u51c6\u786e\u5206\u5272\u76ee\u6807\u5668\u68b0\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bed\u8a00\u8868\u8fbe\u4e0e\u5668\u68b0\u8fd0\u52a8\u6a21\u5f0f\u800c\u975e\u9759\u6001\u89c6\u89c9\u7279\u5f81\u76f8\u7ed3\u5408\uff0cSurgRef\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u4e2d\u8bed\u8a00\u9a71\u52a8\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u624b\u672f\u5ba4\u548c\u81ea\u4e3b\u624b\u672f\u673a\u5668\u4eba\u8f85\u52a9\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2601.12233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12233", "abs": "https://arxiv.org/abs/2601.12233", "authors": ["Zhenzhen Wang", "Zhongliang Zhou", "Zhuoyu Wen", "Jeong Hwan Kook", "John B Wojcik", "John Kang"], "title": "DiffusionQC: Artifact Detection in Histopathology via Diffusion Model", "comment": "7 pages", "summary": "Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.", "AI": {"tldr": "\u63d0\u51faDiffusionQC\u65b9\u6cd5\uff0c\u4ec5\u9700\u5e72\u51c0\u56fe\u50cf\u8bad\u7ec3\u5373\u53ef\u68c0\u6d4b\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u6216\u9884\u5b9a\u4e49\u4f2a\u5f71\u7c7b\u578b", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u56fe\u50cf\u5e38\u5305\u542b\u5236\u5907\u548c\u6570\u5b57\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u4f2a\u5f71\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u53ef\u9760\u6027\u3002\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u4f2a\u5f71\u7c7b\u578b\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u901a\u7528\u6027\u5dee", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5c06\u4f2a\u5f71\u68c0\u6d4b\u4e3a\u5e72\u51c0\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u4ec5\u9700\u5e72\u51c0\u56fe\u50cf\u8bad\u7ec3\u96c6\u3002\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u663e\u5f0f\u6269\u5927\u4f2a\u5f71\u4e0e\u5e72\u51c0\u56fe\u50cf\u5206\u5e03\u5dee\u5f02\uff0c\u5f62\u6210\u589e\u5f3a\u7248\u672c", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5177\u6709\u8de8\u67d3\u8272\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u6240\u9700\u6570\u636e\u548c\u6807\u6ce8\u663e\u8457\u51cf\u5c11", "conclusion": "DiffusionQC\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u4f2a\u5f71\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u9700\u5e72\u51c0\u56fe\u50cf\u5373\u53ef\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u6807\u6ce8\u8d1f\u62c5\u548c\u6cdb\u5316\u9650\u5236\u95ee\u9898"}}
{"id": "2601.13572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13572", "abs": "https://arxiv.org/abs/2601.13572", "authors": ["Xiangchi Yuan", "Dachuan Shi", "Chunhui Zhang", "Zheyuan Liu", "Shenglong Yao", "Soroush Vosoughi", "Wenke Lee"], "title": "Behavior Knowledge Merge in Reinforced Agentic Models", "comment": null, "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.", "AI": {"tldr": "RAM\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aRL\u8bad\u7ec3\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u5206\u5e03\u611f\u77e5\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u66f4\u65b0\uff0c\u89e3\u51b3RL\u4e0eSFT\u4efb\u52a1\u5411\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u534f\u540c\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u5408\u5e76\u65b9\u6cd5\u4e13\u4e3a\u76d1\u7763\u5fae\u8c03\u8bbe\u8ba1\uff0c\u4e0d\u9002\u7528\u4e8eRL\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u56e0\u4e3aRL\u4ea7\u751f\u7684\u4efb\u52a1\u5411\u91cf\u7a00\u758f\u4e14\u5f02\u8d28\uff0c\u800cSFT\u5408\u5e76\u5047\u8bbe\u5bc6\u96c6\u4e14\u5168\u5c40\u53ef\u6bd4\uff0c\u5bfc\u81f4\u6807\u51c6\u5168\u5c40\u5e73\u5747\u4f1a\u7a00\u91ca\u5173\u952e\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u667a\u80fd\u4f53\u5408\u5e76(RAM)\u6846\u67b6\uff0c\u89e3\u8026\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u72ec\u7279\u53c2\u6570\u66f4\u65b0\uff0c\u5e73\u5747\u5171\u4eab\u7ec4\u4ef6\uff0c\u540c\u65f6\u9009\u62e9\u6027\u4fdd\u7559\u548c\u91cd\u65b0\u7f29\u653e\u72ec\u7279\u7ec4\u4ef6\u4ee5\u62b5\u6d88\u53c2\u6570\u66f4\u65b0\u7a00\u91ca\u3002", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u9886\u57df\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAM\u4e0d\u4ec5\u8d85\u8d8a\u5408\u5e76\u57fa\u7ebf\uff0c\u8fd8\u80fd\u89e3\u9501\u667a\u80fd\u4f53\u95f4\u7684\u534f\u540c\u6f5c\u529b\uff0c\u5728\u5404\u81ea\u9886\u57df\u5b9e\u73b0\u4f18\u4e8e\u4e13\u7528\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "RAM\u89e3\u51b3\u4e86RL\u4e0eSFT\u4efb\u52a1\u5411\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3aRL\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u6709\u6548\u5408\u5e76\u63d0\u4f9b\u4e86\u4e13\u95e8\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u80fd\u529b\u7684\u534f\u540c\u589e\u5f3a\u3002"}}
{"id": "2601.13578", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13578", "abs": "https://arxiv.org/abs/2601.13578", "authors": ["Qian Feng", "JiaHang Tu", "Mintong Kang", "Hanbin Zhao", "Chao Zhang", "Hui Qian"], "title": "FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning", "comment": "This paper has been accepted by ICCV 2025. code: \\url{https://github.com/RAIAN08/FG-OrIU}", "summary": "Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \\textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\\textbf{F}eature-\\textbf{G}radient \\textbf{Or}thogonality for \\textbf{I}ncremental \\textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "FG-OrIU\u63d0\u51fa\u9996\u4e2a\u5728\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7edf\u4e00\u6b63\u4ea4\u7ea6\u675f\u7684\u589e\u91cf\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7SVD\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\u5e76\u65bd\u52a0\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e0d\u53ef\u9006\u7684\u6df1\u5ea6\u9057\u5fd8", "motivation": "\u73b0\u6709\u589e\u91cf\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u5728\u53c2\u6570\u5c42\u9762\u6291\u5236\u6216\u6df7\u6dc6\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7684\u663e\u5f0f\u7ea6\u675f\uff0c\u5bfc\u81f4\"\u8868\u9762\u9057\u5fd8\"\uff08\u6b8b\u7559\u4fe1\u606f\u53ef\u6062\u590d\uff09\uff0c\u8fd9\u79cd\u4e0d\u5b8c\u5168\u9057\u5fd8\u5b58\u5728\u5b89\u5168\u98ce\u9669\u5e76\u7834\u574f\u4fdd\u7559\u5e73\u8861", "method": "FG-OrIU\u901a\u8fc7SVD\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\uff0c\u5c06\u9057\u5fd8\u7c7b\u548c\u4fdd\u7559\u7c7b\u7279\u5f81\u5206\u79bb\u5230\u4e0d\u540c\u5b50\u7a7a\u95f4\uff1b\u65bd\u52a0\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\uff1a\u7279\u5f81\u6b63\u4ea4\u6295\u5f71\uff08\u5bf9\u9057\u5fd8\u548c\u4fdd\u7559\u7c7b\uff09\u548c\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\uff08\u9632\u6b62\u9057\u5fd8\u77e5\u8bc6\u91cd\u65b0\u5f15\u5165\uff09\uff1b\u52a8\u6001\u5b50\u7a7a\u95f4\u9002\u5e94\u5408\u5e76\u65b0\u9057\u5fd8\u5b50\u7a7a\u95f4\u5e76\u6536\u7f29\u4fdd\u7559\u5b50\u7a7a\u95f4", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0d\u53ef\u9006\u7684\u6df1\u5ea6\u9057\u5fd8\uff0c\u5728\u8fde\u7eed\u9057\u5fd8\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u7684\u79fb\u9664-\u4fdd\u7559\u5e73\u8861", "conclusion": "FG-OrIU\u662f\u9996\u4e2a\u5728\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7edf\u4e00\u6b63\u4ea4\u7ea6\u675f\u7684\u589e\u91cf\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\u548c\u52a8\u6001\u5b50\u7a7a\u95f4\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u9762\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u53ef\u9760\u7684\u6df1\u5ea6\u9057\u5fd8"}}
{"id": "2601.13580", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13580", "abs": "https://arxiv.org/abs/2601.13580", "authors": ["Ahmad Al-Zuraiqi"], "title": "Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models", "comment": "27 pages, 8 figures, 16 tables. Decoder-only transformers (124M-20B parameters). Complete experimental results and reproducibility details in appendices. Code and checkpoints: https://github.com/zuraiqi/neural-organ-transplant", "summary": "We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets (\"donor organs\") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.", "AI": {"tldr": "Neural Organ Transplantation (NOT) \u662f\u4e00\u79cd\u6a21\u5757\u5316\u9002\u914d\u6846\u67b6\uff0c\u53ef\u5c06\u8bad\u7ec3\u597d\u7684Transformer\u5c42\u4f5c\u4e3a\u53ef\u91cd\u7528\u3001\u53ef\u79fb\u690d\u7684\u68c0\u67e5\u70b9\u7528\u4e8e\u9886\u57df\u9002\u914d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5c06\u8bad\u7ec3\u53c2\u6570\u4e0e\u7279\u5b9a\u6a21\u578b\u5b9e\u4f8b\u548c\u8bad\u7ec3\u6570\u636e\u7d27\u5bc6\u8026\u5408\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u53d6\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u8fde\u7eed\u5c42\u5b50\u96c6\u4f5c\u4e3a\u72ec\u7acb\u68c0\u67e5\u70b9\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u4e13\u4e1a\u77e5\u8bc6\u5171\u4eab\u65b9\u6cd5\u3002", "method": "\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u63d0\u53d6\u8fde\u7eed\u5c42\u5b50\u96c6\uff08\"\u4f9b\u4f53\u5668\u5b98\"\uff09\uff0c\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u72ec\u7acb\u8bad\u7ec3\uff0c\u4fdd\u5b58\u4e3a\u72ec\u7acb\u68c0\u67e5\u70b9\u6587\u4ef6\uff0c\u7136\u540e\u79fb\u690d\u5230\u517c\u5bb9\u7684\u53d7\u4f53\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u4ec5\u89e3\u7801\u5668Transformer\u67b6\u6784\uff08GPT-2\u3001TinyLlama\u3001GPT-OSS\uff0c124M\u523020B\u53c2\u6570\uff09\u4e0a\uff0c\u4f9b\u4f53\u79fb\u690d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9002\u914d\u65b9\u6cd5\uff0c\u56f0\u60d1\u5ea6\u6bd4LoRA\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002\u65b9\u6cd5\u5177\u6709\u4f4d\u7f6e\u4f9d\u8d56\u6027\uff0c\u65e9\u671f\u63d2\u5165\u4f4d\u7f6e\u6548\u679c\u6700\u4f73\u3002\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u8de8\u9886\u57df\u8f6c\u79fb\u663e\u793a\u51fa\u610f\u5916\u7684\u6b63\u5219\u5316\u6548\u76ca\u3002", "conclusion": "Transformer\u4e2d\u95f4\u5c42\u53ef\u4ee5\u652f\u6301\u4ec5\u89e3\u7801\u5668\u67b6\u6784\u7684\u9ad8\u6548\u6a21\u5757\u5316\u8f6c\u79fb\uff0c\u901a\u8fc7\u68c0\u67e5\u70b9\u5206\u53d1\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u4e13\u4e1a\u77e5\u8bc6\u5171\u4eab\u3002\u8be5\u65b9\u6cd5\u76ee\u524d\u4ec5\u9650\u4e8e\u4ec5\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5728\u7f16\u7801\u5668\u67b6\u6784\u4e0a\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2601.13599", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13599", "abs": "https://arxiv.org/abs/2601.13599", "authors": ["Linrui Ma", "Yufei Cui", "Kai Han", "Yunhe Wang"], "title": "Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models", "comment": "Work In Progress", "summary": "Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.", "AI": {"tldr": "\u63d0\u51faDiffusion in Diffusion\u6846\u67b6\uff0c\u901a\u8fc7\"\u8349\u7a3f-\u7cbe\u4fee\"\u4e24\u9636\u6bb5\u89e3\u51b3\u5757\u6269\u6563\u6a21\u578b\u7684\u53ef\u9006\u6027\u548c\u77ed\u89c6\u95ee\u9898\uff0c\u5728OpenWebText\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u4e25\u683c\u7684\u5355\u5411\u5757\u4f9d\u8d56\u5bfc\u81f4\u4e0d\u53ef\u9006\u6027\u548c\u727a\u7272\u4e86\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u89c4\u5212\u80fd\u529b", "method": "\u91c7\u7528\u8349\u7a3f-\u7cbe\u4fee\u6846\u67b6\uff1a1) \u7528\u5c0f\u5757\u8fdb\u884c\u5757\u6269\u6563\u751f\u6210\u5feb\u901f\u8349\u7a3f\uff1b2) \u7528\u66f4\u5927\u53cc\u5411\u611f\u53d7\u91ce\u8fdb\u884c\u5168\u5c40\u53cc\u5411\u6269\u6563\u7cbe\u4fee\uff1b\u4f7f\u7528\u5feb\u7167\u7f6e\u4fe1\u5ea6\u91cd\u63a9\u7801\u8bc6\u522b\u9700\u8981\u4fee\u6539\u7684\u5173\u952etoken\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u5c3a\u5ea6\u8bad\u7ec3\u6269\u5c55\u5757\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u80fd\u529b", "result": "\u5728OpenWebText\u6570\u636e\u96c6\u4e0a\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff1a\u4ec5\u4f7f\u7528\u57fa\u7ebf\u6a21\u578b26%\u7684\u5fae\u8c03\u9884\u7b97\uff0c\u5c06\u751f\u6210\u56f0\u60d1\u5ea6\u4ece25.7\u964d\u81f321.9\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd", "conclusion": "Diffusion in Diffusion\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5757\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u9006\u6027\u548c\u77ed\u89c6\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf"}}
{"id": "2601.12282", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12282", "abs": "https://arxiv.org/abs/2601.12282", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training", "comment": null, "summary": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.", "AI": {"tldr": "CytoCLIP\uff1a\u57fa\u4e8e\u9884\u8bad\u7ec3CLIP\u6846\u67b6\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u5927\u8111\u7ec6\u80de\u6784\u7b51\u7684\u8054\u5408\u89c6\u89c9-\u6587\u672c\u8868\u793a\uff0c\u5b9e\u73b0\u5927\u8111\u533a\u57df\u81ea\u52a8\u8bc6\u522b", "motivation": "\u5927\u8111\u4e0d\u540c\u533a\u57df\u7684\u529f\u80fd\u4e0e\u5176\u72ec\u7279\u7684\u7ec6\u80de\u6784\u7b51\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u624b\u52a8\u5728\u8111\u7ec4\u7ec7\u5207\u7247\u4e2d\u63cf\u7ed8\u8fd9\u4e9b\u533a\u57df\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf", "method": "\u63d0\u51faCytoCLIP\u5957\u4ef6\uff0c\u5305\u542b\u4e24\u79cd\u6a21\u578b\u53d8\u4f53\uff1a1) \u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u5168\u533a\u57df\u56fe\u50cf\u8bad\u7ec3\u4ee5\u7406\u89e3\u533a\u57df\u6574\u4f53\u7ec6\u80de\u6784\u7b51\u6a21\u5f0f\uff1b2) \u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5757\u8bad\u7ec3\u4ee5\u83b7\u5f97\u7ec6\u80de\u7ea7\u8be6\u7ec6\u8868\u793a\u3002\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u4e0d\u540c\u5b55\u5468\u80ce\u513f\u5927\u8111\u7684NISSL\u67d3\u8272\u7ec4\u7ec7\u5207\u7247\uff0c\u5305\u542b86\u4e2a\u533a\u57df\uff08\u4f4e\u5206\u8fa8\u7387\uff09\u548c384\u4e2a\u533a\u57df\uff08\u9ad8\u5206\u8fa8\u7387\uff09", "result": "CytoCLIP\u5728\u533a\u57df\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5168\u533a\u57df\u5206\u7c7bF1\u5206\u6570\u8fbe0.87\uff0c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5757\u5206\u7c7bF1\u5206\u6570\u8fbe0.91\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u7ec6\u80de\u6784\u7b51\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "CytoCLIP\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6709\u6548\u5b66\u4e60\u5927\u8111\u7ec6\u80de\u6784\u7b51\u7684\u8054\u5408\u8868\u793a\uff0c\u4e3a\u5927\u8111\u533a\u57df\u81ea\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u4e13\u5bb6\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387"}}
{"id": "2601.13608", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13608", "abs": "https://arxiv.org/abs/2601.13608", "authors": ["Zhipeng Chang", "Ting He", "Wenrui Hao"], "title": "Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data", "comment": null, "summary": "Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.", "AI": {"tldr": "FIPA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u4fe1\u606f\u7684\u53c2\u6570\u7ea7\u805a\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u7279\u5b9a\u7684\u6743\u91cd\u66ff\u4ee3\u5ba2\u6237\u7aef\u7ea7\u6807\u91cf\u6743\u91cd\uff0c\u63d0\u9ad8\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u975eIID\u6570\u636e\u5206\u5e03\u5bfc\u81f4\u5ba2\u6237\u7aef\u66f4\u65b0\u5b58\u5728\u5f3a\u70c8\u9519\u4f4d\uff0c\u4f20\u7edf\u7684FedAvg\u7b49\u65b9\u6cd5\u5bf9\u6240\u6709\u53c2\u6570\u4f7f\u7528\u76f8\u540c\u7684\u6807\u91cf\u6743\u91cd\uff0c\u8fd9\u79cd\u5747\u5300\u52a0\u6743\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u5e76\u964d\u4f4e\u5168\u5c40\u6a21\u578b\u8d28\u91cf\u3002", "method": "\u63d0\u51faFisher\u4fe1\u606f\u53c2\u6570\u7ea7\u805a\u5408(FIPA)\uff0c\u4f7f\u7528Fisher\u4fe1\u606f\u77e9\u9635(FIM)\u6743\u91cd\u66ff\u4ee3\u5ba2\u6237\u7aef\u7ea7\u6807\u91cf\u6743\u91cd\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u53c2\u6570\u7ea7\u7f29\u653e\u3002\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u4fdd\u6301\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u6355\u6349\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6570\u636e\u5bf9\u4e0d\u540c\u53c2\u6570\u7684\u72ec\u7279\u5f71\u54cd\u3002", "result": "\u5728\u975e\u7ebf\u6027\u51fd\u6570\u56de\u5f52\u3001PDE\u5b66\u4e60\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFIPA\u76f8\u6bd4\u57fa\u4e8e\u5e73\u5747\u7684\u805a\u5408\u65b9\u6cd5\u6301\u7eed\u6539\u8fdb\u6027\u80fd\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5ba2\u6237\u7aef\u4f18\u5316\u7b97\u6cd5\u7ed3\u5408\u65f6\uff0c\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "FIPA\u4e3a\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u53c2\u6570\u7279\u5b9a\u7684Fisher\u4fe1\u606f\u6743\u91cd\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u805a\u5408\uff0c\u663e\u8457\u6539\u5584\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.12283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12283", "abs": "https://arxiv.org/abs/2601.12283", "authors": ["Bowen Lin", "Fanjiang Ye", "Yihua Liu", "Zhenghui Guo", "Boyuan Zhang", "Weijian Zheng", "Yufan Xu", "Tiancheng Xing", "Yuke Wang", "Chengming Zhang"], "title": "SDiT: Semantic Region-Adaptive for Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.", "AI": {"tldr": "SDiT\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bed\u4e49\u533a\u57df\u81ea\u9002\u5e94\u6269\u6563Transformer\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u805a\u7c7b\u3001\u590d\u6742\u5ea6\u9a71\u52a8\u533a\u57df\u8c03\u5ea6\u548c\u8fb9\u754c\u611f\u77e5\u7ec6\u5316\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u548c\u8bed\u4e49\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe3.0\u500d\u52a0\u901f", "motivation": "\u6269\u6563Transformer\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u53bb\u566a\u8fc7\u7a0b\u7684\u8fed\u4ee3\u6027\u548c\u5168\u5c40\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\uff0c\u8ba1\u7b97\u5f00\u9500\u4ecd\u7136\u5f88\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u53bb\u566a\u52a8\u6001\u5728\u7a7a\u95f4\u4e0a\u4e0d\u5747\u5300\uff1a\u80cc\u666f\u533a\u57df\u5feb\u901f\u6536\u655b\uff0c\u800c\u8fb9\u7f18\u548c\u7eb9\u7406\u533a\u57df\u6f14\u5316\u66f4\u6d3b\u8dc3\u3002", "method": "\u63d0\u51faSDiT\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u5feb\u901fQuickshift\u5206\u5272\u8fdb\u884c\u8bed\u4e49\u611f\u77e5\u805a\u7c7b\uff1b2\uff09\u590d\u6742\u5ea6\u9a71\u52a8\u533a\u57df\u8c03\u5ea6\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u4fe1\u606f\u4e30\u5bcc\u7684\u533a\u57df\uff1b3\uff09\u8fb9\u754c\u611f\u77e5\u7ec6\u5316\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u8d2f\u6027\u3002\u8be5\u6846\u67b6\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u3002", "result": "SDiT\u5728\u4fdd\u6301\u4e0e\u5168\u6ce8\u610f\u529b\u63a8\u7406\u51e0\u4e4e\u76f8\u540c\u7684\u611f\u77e5\u548c\u8bed\u4e49\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.0\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u53bb\u566a\u52a8\u6001\u7684\u7a7a\u95f4\u4e0d\u5747\u5300\u6027\uff0cSDiT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563Transformer\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13645", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13645", "abs": "https://arxiv.org/abs/2601.13645", "authors": ["Euijin You", "Hyang-Won Lee"], "title": "Quadratic Upper Bound for Boosting Robustness", "comment": "Accepted at ICML 2025. Published in PMLR 267:72656-72676", "summary": "Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e8c\u6b21\u4e0a\u754c\u635f\u5931\u51fd\u6570\u6765\u6539\u5584\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u6ed1\u635f\u5931\u666f\u89c2\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u867d\u7136\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u4f46\u7531\u4e8e\u5bf9\u6297\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\uff0c\u5f80\u5f80\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7f13\u89e3FAT\u4e2d\u7684\u9c81\u68d2\u6027\u9000\u5316\u95ee\u9898\u3002", "method": "\u63a8\u5bfc\u4e86\u5bf9\u6297\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u4e8c\u6b21\u4e0a\u754c\uff0c\u5e76\u5c06\u8be5\u4e0a\u754c\u4e0e\u73b0\u6709FAT\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\uff0c\u5f62\u6210QUB\u635f\u5931\u51fd\u6570\u3002", "result": "\u5c06QUB\u635f\u5931\u5e94\u7528\u4e8e\u73b0\u6709\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8bc1\u660e\u8fd9\u79cd\u6539\u8fdb\u53ef\u80fd\u6e90\u4e8e\u6240\u5f97\u6a21\u578b\u7684\u5e73\u6ed1\u635f\u5931\u666f\u89c2\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e8c\u6b21\u4e0a\u754c\u635f\u5931\u51fd\u6570\u6709\u6548\u6539\u5584\u4e86\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u6ed1\u635f\u5931\u666f\u89c2\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2601.12285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12285", "abs": "https://arxiv.org/abs/2601.12285", "authors": ["Safa C. Medin", "Gengyan Li", "Ziqian Bai", "Ruofei Du", "Leonhard Helminger", "Yinda Zhang", "Stephan J. Garbin", "Philip L. Davidson", "Gregory W. Wornell", "Thabo Beeler", "Abhimitra Meka"], "title": "LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines", "comment": null, "summary": "We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578b\u951a\u5b9a\u8f90\u5c04\u573a\u7684\u65b0\u578b3D\u4eba\u8138\u5316\u8eab\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u590d\u6742\u9762\u90e8\u7279\u5f81\u4f53\u79ef\u6e32\u67d3\uff0c\u652f\u6301\u5728\u7ebf\u6d41\u5f0f\u4f20\u8f93\u5e76\u5728\u4f20\u7edf\u56fe\u5f62\u5e73\u53f0\u4e0a\u4f7f\u7528\u7ecf\u5178\u7f51\u683c\u548c\u7740\u8272\u5668\u6e32\u67d3\u3002", "motivation": "\u73b0\u67093D\u4eba\u8138\u5316\u8eab\u6e32\u67d3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u5b9a\u5236\u5de5\u7a0b\u548c\u96c6\u6210\uff0c\u96be\u4ee5\u5728\u4f20\u7edf\u56fe\u5f62\u5e73\u53f0\u4e0a\u9ad8\u6548\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\uff0c\u53c8\u80fd\u5728\u6807\u51c6\u56fe\u5f62\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578b\u951a\u5b9a\u8f90\u5c04\u573a\uff0c\u5b66\u4e603D\u7a7a\u95f4\u4e2d\u7684\u8f90\u5c04\u6d41\u5f62\uff0c\u63d0\u53d6\u663e\u5f0f\u5206\u5c42\u7f51\u683c\u4ee5\u53ca\u5916\u89c2\u548c\u53d8\u5f62\u7eb9\u7406\u3002\u901a\u8fc7\u7b80\u5355\u7684\u7ebf\u6027\u6df7\u5408\u548calpha\u5408\u6210\u5728\u9759\u6001\u7f51\u683c\u4e0a\u63a7\u5236\u52a8\u753b\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u9762\u90e8\u7279\u5f81\uff08\u5934\u53d1\u3001\u76ae\u80a4\u3001\u773c\u775b\uff09\u7684\u53ef\u63a7\u4f53\u79ef\u6e32\u67d3\uff0c\u751f\u6210\u7684\u5316\u8eab\u53ef\u4ee5\u9ad8\u6548\u5728\u7ebf\u6d41\u5f0f\u4f20\u8f93\uff0c\u5e76\u5728\u4f20\u7edf\u56fe\u5f62\u5e73\u53f0\u4e0a\u4f7f\u7528\u7ecf\u5178\u7f51\u683c\u548c\u7740\u8272\u5668\u6e32\u67d3\uff0c\u65e0\u9700\u5b9a\u5236\u5de5\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u4e14\u6613\u4e8e\u90e8\u7f72\u76843D\u4eba\u8138\u5316\u8eab\u8868\u793a\u65b9\u6848\uff0c\u5c06\u5148\u8fdb\u7684\u8f90\u5c04\u573a\u6280\u672f\u4e0e\u4f20\u7edf\u56fe\u5f62\u6e32\u67d3\u7ba1\u7ebf\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u903c\u771f\u6e32\u67d3\u4e0e\u9ad8\u6548\u90e8\u7f72\u7684\u5e73\u8861\u3002"}}
{"id": "2601.12303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12303", "abs": "https://arxiv.org/abs/2601.12303", "authors": ["Shizhan Gong", "Xiaofan Zhang", "Qi Dou"], "title": "Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations", "comment": "AAAI 2026", "summary": "Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.", "AI": {"tldr": "PCBM-ReD\u662f\u4e00\u79cd\u540e\u5904\u7406\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u901a\u8fc7\u8868\u793a\u5206\u89e3\u5c06\u53ef\u89e3\u91ca\u6027\u6ce8\u5165\u9884\u8bad\u7ec3\u9ed1\u76d2\u6a21\u578b\uff0c\u81ea\u52a8\u63d0\u53d6\u89c6\u89c9\u6982\u5ff5\u5e76\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u3002\u73b0\u6709\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u6982\u5ff5\u76f8\u5173\u6027\u4e0d\u53ef\u9760\u3001\u6982\u5ff5\u5b9a\u4e49\u975e\u89c6\u89c9\u5316\u6216\u52b3\u52a8\u5bc6\u96c6\u3001\u6a21\u578b\u6216\u6570\u636e\u65e0\u5173\u5047\u8bbe\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PCBM-ReD\u901a\u8fc7\u4e09\u4e2a\u6b65\u9aa4\u5b9e\u73b0\uff1a1)\u4ece\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e2d\u81ea\u52a8\u63d0\u53d6\u89c6\u89c9\u6982\u5ff5\uff1b2)\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u89c6\u89c9\u53ef\u8bc6\u522b\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u6807\u6ce8\u548c\u7b5b\u9009\u6982\u5ff5\uff1b3)\u901a\u8fc7\u91cd\u5efa\u5f15\u5bfc\u7684\u4f18\u5316\u9009\u62e9\u72ec\u7acb\u6982\u5ff5\u5b50\u96c6\u3002\u5229\u7528CLIP\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u5c06\u56fe\u50cf\u8868\u793a\u5206\u89e3\u4e3a\u6982\u5ff5\u5d4c\u5165\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u9002\u914d\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u6846\u67b6\u3002", "result": "\u572811\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPCBM-ReD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u7f29\u5c0f\u4e86\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "PCBM-ReD\u6210\u529f\u5730\u5c06\u53ef\u89e3\u91ca\u6027\u6ce8\u5165\u9884\u8bad\u7ec3\u9ed1\u76d2\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u6982\u5ff5\u63d0\u53d6\u548c\u7b5b\u9009\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6982\u5ff5\u89e3\u91ca\uff0c\u4e3a\u5173\u952e\u9886\u57df\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12304", "abs": "https://arxiv.org/abs/2601.12304", "authors": ["Wutao Chen", "Huaqin Zou", "Chen Wan", "Lifeng Huang"], "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models", "comment": "Accepted to ICASSP 2026", "summary": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.", "AI": {"tldr": "\u63d0\u51fa2S-GDA\uff1a\u4e00\u79cd\u4e24\u9636\u6bb5\u5168\u5c40\u591a\u6837\u6027\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u53cc\u91cd\u6270\u52a8\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u653b\u51fb\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u6270\u52a8\u591a\u6837\u6027\u6709\u9650\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6548\u679c\u53d7\u9650", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5168\u5c40\u591a\u6837\u6027\u653b\u51fb\u6846\u67b6\uff1a1) \u6587\u672c\u6270\u52a8\u9636\u6bb5\u901a\u8fc7\u5019\u9009\u6587\u672c\u6269\u5c55\u548c\u5168\u5c40\u611f\u77e5\u66ff\u6362\u5b9e\u73b0\u5168\u5c40\u591a\u6837\u6027\uff1b2) \u89c6\u89c9\u6270\u52a8\u9636\u6bb5\u4f7f\u7528\u591a\u5c3a\u5ea6\u8c03\u6574\u548c\u5757\u72b6\u65cb\u8f6c\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027", "result": "\u5728VLP\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c2S-GDA\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\uff0c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u6700\u9ad8\u63d0\u534711.17%\uff0c\u4e14\u6846\u67b6\u6a21\u5757\u5316\u6613\u4e8e\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408", "conclusion": "2S-GDA\u901a\u8fc7\u589e\u5f3a\u6270\u52a8\u591a\u6837\u6027\u6709\u6548\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u7684\u8f6c\u79fb\u6027\uff0c\u4e3a\u9ed1\u76d2\u573a\u666f\u4e0b\u7684\u591a\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13710", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13710", "abs": "https://arxiv.org/abs/2601.13710", "authors": ["Sayeed Shafayet Chowdhury", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Vijay R. Ramakrishnan"], "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction", "comment": null, "summary": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u76d1\u7763\u673a\u5668\u5b66\u4e60\u4e0e\u751f\u6210\u5f0fAI\u5728\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u624b\u672f\u9884\u540e\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0MLP\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u4e34\u5e8a\u51b3\u7b56\u6548\u76ca\u65b9\u9762\u4f18\u4e8e\u751f\u6210\u5f0fAI\uff0c\u5efa\u8bae\u91c7\u7528ML\u4e3a\u4e3b\u3001GenAI\u4e3a\u8f85\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u8fdb\u884c\u524d\u77bb\u6027\u51b3\u7b56\u652f\u6301\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u672f\u524d\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u60a3\u8005\u672f\u540e\u4e34\u5e8a\u6539\u5584\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u8bc6\u522b\u90a3\u4e9b\u53ef\u80fd\u4ece\u624b\u672f\u4e2d\u83b7\u76ca\u6709\u9650\u7684\u60a3\u8005\uff0c\u4ece\u800c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u524d\u77bb\u6027\u6536\u96c6\u7684\u961f\u5217\u6570\u636e\uff0c\u6240\u6709\u60a3\u8005\u5747\u63a5\u53d7\u624b\u672f\u3002\u5b9a\u4e49\u624b\u672f\u6210\u529f\u4e3aSNOT-22\u8bc4\u5206\u57286\u4e2a\u6708\u540e\u964d\u4f4e\u8d85\u8fc78.9\u5206\uff08\u6700\u5c0f\u4e34\u5e8a\u91cd\u8981\u5dee\u5f02\uff09\u3002\u6bd4\u8f83\u4e86\u76d1\u7763\u673a\u5668\u5b66\u4e60\uff08\u903b\u8f91\u56de\u5f52\u3001\u6811\u96c6\u6210\u65b9\u6cd5\u548c\u81ea\u7814MLP\uff09\u4e0e\u751f\u6210\u5f0fAI\uff08ChatGPT\u3001Claude\u3001Gemini\u3001Perplexity\uff09\u7684\u8868\u73b0\uff0c\u6240\u6709\u6a21\u578b\u63a5\u6536\u76f8\u540c\u7684\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u5e76\u7ea6\u675f\u8f93\u51fa\u4e3a\u4e8c\u5143\u63a8\u8350\u53ca\u7f6e\u4fe1\u5ea6\u3002\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u8868\u683c\u6570\u636e\u5230\u751f\u6210\u5f0fAI\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u8fdb\u884c\u4e86\u4e9a\u7ec4\u5206\u6790\u3002", "result": "\u6700\u4f73ML\u6a21\u578b\uff08MLP\uff09\u8fbe\u523085%\u7684\u51c6\u786e\u7387\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u6821\u51c6\u7279\u6027\u548c\u51b3\u7b56\u66f2\u7ebf\u51c0\u6548\u76ca\u3002\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u533a\u5206\u5ea6\u548c\u6821\u51c6\u8868\u73b0\u5747\u8f83\u5dee\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u751f\u6210\u5f0fAI\u7684\u51b3\u7b56\u7406\u7531\u4e0e\u4e34\u5e8a\u533b\u751f\u7684\u542f\u53d1\u5f0f\u601d\u7ef4\u548cMLP\u7684\u7279\u5f81\u91cd\u8981\u6027\u4e00\u81f4\uff0c\u53cd\u590d\u5f3a\u8c03\u57fa\u7ebfSNOT-22\u8bc4\u5206\u3001CT/\u5185\u955c\u4e25\u91cd\u7a0b\u5ea6\u3001\u606f\u8089\u8868\u578b\u4ee5\u53ca\u5fc3\u7406/\u75bc\u75db\u5171\u75c5\u7b49\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u91c7\u7528ML\u4e3a\u4e3b\u3001GenAI\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a\u90e8\u7f72\u6821\u51c6\u826f\u597d\u7684ML\u6a21\u578b\u8fdb\u884c\u624b\u672f\u5019\u9009\u8005\u7684\u521d\u6b65\u7b5b\u9009\uff0c\u4f7f\u7528\u751f\u6210\u5f0fAI\u4f5c\u4e3a\u89e3\u91ca\u5de5\u5177\u6765\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u5171\u4eab\u51b3\u7b56\u5236\u5b9a\u3002\u8fd9\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u65bd\u6846\u67b6\u3002"}}
{"id": "2601.12312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12312", "abs": "https://arxiv.org/abs/2601.12312", "authors": ["Yongjun Jeon", "Jongmin Shin", "Kanggil Park", "Seonmin Park", "Soyoung Lim", "Jung Yong Kim", "Jinsoo Rhu", "Jongman Kim", "Gyu-Seong Choi", "Namkee Oh", "Kyu-Hwan Jung"], "title": "CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding", "comment": null, "summary": "Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.", "AI": {"tldr": "\u63d0\u51faCurConMix+\u6846\u67b6\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u591a\u5206\u8fa8\u7387\u65f6\u5e8fTransformer\uff0c\u7528\u4e8e\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6LLS48\u4e0a\u9a8c\u8bc1\u6027\u80fd", "motivation": "\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u5bf9\u5de5\u4f5c\u6d41\u5206\u6790\u548c\u6280\u80fd\u8bc4\u4f30\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u89c6\u89c9\u53d8\u5316\u7ec6\u5fae\u3001\u8bed\u4e49\u76f8\u4e92\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u8054\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u57fa\u4e8eCurConMix\u7a7a\u95f4\u8868\u793a\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u96be\u4f8b\u91c7\u6837\u548c\u7279\u5f81\u7ea7\u6df7\u5408\uff1b\u5176\u65f6\u5e8f\u6269\u5c55CurConMix+\u96c6\u6210\u591a\u5206\u8fa8\u7387\u65f6\u5e8fTransformer\uff0c\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c3a\u5ea6\u65f6\u5e8f\u7279\u5f81\u5e76\u52a8\u6001\u5e73\u8861\u65f6\u7a7a\u7ebf\u7d22", "result": "\u5728CholecT45\u548cLLS48\u6570\u636e\u96c6\u4e0a\uff0cCurConMix+\u5728\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u5c42\u7ea7\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u7ec6\u7c92\u5ea6\u7279\u5f81\u53ef\u6709\u6548\u8fc1\u79fb\u5230\u66f4\u9ad8\u5c42\u7ea7\u7684\u9636\u6bb5\u548c\u6b65\u9aa4\u8bc6\u522b\u4efb\u52a1", "conclusion": "\u8be5\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u5c42\u6b21\u611f\u77e5\u3001\u53ef\u590d\u73b0\u3001\u53ef\u89e3\u91ca\u7684\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728GitHub\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2601.13748", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13748", "abs": "https://arxiv.org/abs/2601.13748", "authors": ["Tien-Dat Pham", "Xuan-The Tran"], "title": "EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory", "comment": null, "summary": "Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation", "AI": {"tldr": "EEG-Titans\uff1a\u4e00\u79cd\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u5faa\u73af\u8bb0\u5fc6\u7684\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7528\u4e8e\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.46%\u7684\u5e73\u5747\u6bb5\u7ea7\u7075\u654f\u5ea6", "motivation": "\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u53d1\u4f5c\u524d\u52a8\u6001\u53ef\u80fd\u8de8\u8d8a\u957f\u65f6\u95f4\u8303\u56f4\uff0c\u800c\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u53ef\u80fd\u5fae\u5999\u4e14\u77ed\u6682\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d85\u957f\u5e8f\u5217\u5904\u7406\u65f6\uff0c\u9700\u8981\u5728\u6355\u83b7\u5c40\u90e8\u65f6\u7a7a\u6a21\u5f0f\u4e0e\u4fdd\u6301\u4fe1\u606f\u4e30\u5bcc\u7684\u957f\u7a0b\u4e0a\u4e0b\u6587\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u63d0\u51faEEG-Titans\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u73b0\u4ee3\u795e\u7ecf\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002\u6a21\u578b\u5305\u542b\uff1a1\uff09\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5206\u652f\u6355\u83b7\u77ed\u671f\u5f02\u5e38\uff1b2\uff09\u5faa\u73af\u8bb0\u5fc6\u8def\u5f84\u603b\u7ed3\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7f13\u6162\u6e10\u8fdb\u8d8b\u52bf\u3002\u8fd8\u91c7\u7528\u5206\u5c42\u4e0a\u4e0b\u6587\u7b56\u7565\uff0c\u4e3a\u9ad8\u566a\u58f0\u53d7\u8bd5\u8005\u6269\u5c55\u611f\u53d7\u91ce\u3002", "result": "\u5728CHB-MIT\u5934\u76aeEEG\u6570\u636e\u96c6\u4e0a\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u4fdd\u7559\u534f\u8bae\u8bc4\u4f30\uff0cEEG-Titans\u572818\u540d\u53d7\u8bd5\u8005\u4e2d\u8fbe\u523099.46%\u7684\u5e73\u5747\u6bb5\u7ea7\u7075\u654f\u5ea6\u3002\u5206\u6790\u4f2a\u5f71\u6613\u53d1\u8bb0\u5f55\u7684\u5b89\u5168\u4f18\u5148\u64cd\u4f5c\u70b9\u663e\u793a\uff0c\u5206\u5c42\u4e0a\u4e0b\u6587\u7b56\u7565\u53ef\u663e\u8457\u51cf\u5c11\u8bef\u62a5\uff08\u6781\u7aef\u5f02\u5e38\u503c\u964d\u81f30.00 FPR/h\uff09\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u7075\u654f\u5ea6\u3002", "conclusion": "\u8bb0\u5fc6\u589e\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u591f\u5728\u4e34\u5e8a\u7ea6\u675f\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u7a33\u5065\u7684\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u5c40\u90e8\u7279\u5f81\u6355\u83b7\u4e0e\u957f\u671f\u4e0a\u4e0b\u6587\u4fdd\u6301\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.12313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12313", "abs": "https://arxiv.org/abs/2601.12313", "authors": ["Xiangyu Hu", "Yicheng Hong", "Hongchuang Zheng", "Wenjun Zeng", "Bingyao Liu"], "title": "S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection", "comment": "27pages 9figures", "summary": "The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.", "AI": {"tldr": "S\u00b2F-Net\uff1a\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u5dee\u5f02\u7684\u8de8\u6a21\u578b\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u5224\u522b\u6027\u9891\u5e26\uff0c\u572817\u7c7b\u751f\u6210\u6a21\u578b\u4e0a\u5b9e\u73b090.49%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u68c0\u6d4b\u65b9\u6848\u63d0\u51fa\u4e86\u8feb\u5207\u9700\u6c42\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u6e90\u6a21\u578b\uff0c\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u67b6\u6784\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u578b\u68c0\u6d4b\u6846\u67b6S\u00b2F-Net\uff0c\u6838\u5fc3\u5728\u4e8e\u63a2\u7d22\u548c\u5229\u7528\u771f\u5b9e\u4e0e\u5408\u6210\u7eb9\u7406\u4e4b\u95f4\u7684\u56fa\u6709\u9891\u8c31\u5dee\u5f02\u3002\u5f15\u5165\u53ef\u5b66\u4e60\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u534f\u540c\u7a7a\u95f4\u7eb9\u7406\u5206\u6790\u548c\u9891\u8c31\u4f9d\u8d56\u5173\u7cfb\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u548c\u589e\u5f3a\u5224\u522b\u6027\u9891\u5e26\u3002", "result": "\u5728\u5305\u542b17\u7c7b\u751f\u6210\u6a21\u578b\u7684AIGCDetectBenchmark\u4e0a\uff0cS\u00b2F-Net\u8fbe\u523090.49%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u8de8\u57df\u68c0\u6d4b\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u5404\u79cd\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u4e0a\u91c7\u6837\u64cd\u4f5c\u5728\u9891\u57df\u7559\u4e0b\u7684\u72ec\u7279\u6307\u7eb9\uff0c\u4ece\u6839\u672c\u5c42\u9762\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u751f\u6210\u6a21\u578b\u68c0\u6d4b\u7684\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.13768", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13768", "abs": "https://arxiv.org/abs/2601.13768", "authors": ["Wenzhen Yue", "Ruohao Guo", "Ji Shi", "Zihan Hao", "Shiyu Hu", "Xianghua Ying"], "title": "vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting", "comment": null, "summary": "In this paper, we present \\textbf{vLinear}, an effective yet efficient \\textbf{linear}-based multivariate time series forecaster featuring two components: the \\textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \\textbf{velocity-oriented} flow matching objectives, we demonstrate that a \\textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.", "AI": {"tldr": "vLinear\uff1a\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u8fd0\u7b97\u7684\u9ad8\u6548\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\uff0c\u5305\u542bvecTrans\u6a21\u5757\u548cWFMLoss\u76ee\u6807\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\u901a\u5e38\u9762\u4e34O(N\u00b2)\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5176\u4e2dN\u662f\u53d8\u91cf\u6570\u91cf\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5904\u7406\u9ad8\u7ef4\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u65f6\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51favLinear\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) vecTrans\u6a21\u5757\uff1a\u4f7f\u7528\u53ef\u5b66\u4e60\u5411\u91cf\u5efa\u6a21\u591a\u5143\u76f8\u5173\u6027\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(N\u00b2)\u964d\u4f4e\u5230O(N)\uff1b2) WFMLoss\u76ee\u6807\u51fd\u6570\uff1a\u91c7\u7528\u6700\u7ec8\u5e8f\u5217\u5bfc\u5411\u7684\u6d41\u5339\u914d\u635f\u5931\uff0c\u7ed3\u5408\u8def\u5f84\u548c\u65f6\u57df\u52a0\u6743\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u66f4\u53ef\u9760\u7684\u8def\u5f84\u548c\u9884\u6d4b\u65f6\u57df\u3002", "result": "\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c124\u4e2a\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1bvecTrans\u6a21\u5757\u53ef\u65e0\u7f1d\u96c6\u6210\u5230Transformer\u9884\u6d4b\u5668\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff1bWFMLoss\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u76ee\u6807\u51fd\u6570\uff0c\u80fd\u6301\u7eed\u6539\u8fdb\u73b0\u6709\u9884\u6d4b\u5668\u3002", "conclusion": "vLinear\u901a\u8fc7vecTrans\u6a21\u5757\u548cWFMLoss\u76ee\u6807\u51fd\u6570\uff0c\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u9ad8\u7ef4\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ebf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12316", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12316", "abs": "https://arxiv.org/abs/2601.12316", "authors": ["Xinyuan Zhao", "Xianrui Chen", "Ahmad Chaddad"], "title": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer", "comment": "accepted at ICASSP 2026", "summary": "We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49\u00b0, 3.22\u00b0, 10.16\u00b0, and 1.44\u00b0, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u8c03\u5236\u591a\u5c3a\u5ea6Transformer\u7528\u4e8e3D\u89c6\u7ebf\u4f30\u8ba1\uff0c\u901a\u8fc7\u539f\u578b\u5e93\u8c03\u8282CLIP\u5168\u5c40\u7279\u5f81\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4f7f\u7528MoE\u589e\u5f3a\u6761\u4ef6\u5bb9\u91cf\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u53d8\u5316\uff08\u5982\u5149\u7167\u3001\u5934\u90e8\u59ff\u6001\u3001\u80cc\u666f\u7b49\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u80fd\u529b\u6765\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "1. \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u539f\u578b\u5e93\uff08\u5149\u7167\u3001\u5934\u90e8\u59ff\u6001\u3001\u80cc\u666f\u3001\u65b9\u5411\uff09\u8c03\u8282CLIP\u5168\u5c40\u7279\u5f81\uff1b2. \u5728\u7edf\u4e00\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u878d\u5408\u539f\u578b\u589e\u5f3a\u7684\u5168\u5c40\u5411\u91cf\u3001CLIP\u8865\u4e01token\u548c\u9ad8\u5206\u8fa8\u7387CNN token\uff1b3. \u7528\u8def\u7531/\u5171\u4eab\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u66ff\u6362\u591a\u4e2aFFN\u5757\u4ee5\u589e\u52a0\u6761\u4ef6\u5bb9\u91cf\u3002", "result": "\u5728MPIIFaceGaze\u3001EYEDIAP\u3001Gaze360\u548cETH-XGaze\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52302.49\u00b0\u30013.22\u00b0\u300110.16\u00b0\u548c1.44\u00b0\u7684\u89d2\u8bef\u5dee\uff0c\u76f8\u6bd4\u5148\u524d\u7ed3\u679c\u76f8\u5bf9\u63d0\u5347\u6700\u9ad8\u8fbe64%\uff0c\u521b\u4e0b\u65b0\u7684SOTA\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u8c03\u5236\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u548cMoE\u67b6\u6784\u7684\u534f\u540c\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u7ebf\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u539f\u578b\u8c03\u8282\u3001\u8de8\u5c3a\u5ea6\u878d\u5408\u548cMoE\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.13780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13780", "abs": "https://arxiv.org/abs/2601.13780", "authors": ["Antoine Siraudin", "Christopher Morris"], "title": "Principled Latent Diffusion for Graphs via Laplacian Autoencoders", "comment": "Preprint, under review", "summary": "Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\\times$ speed-up.", "AI": {"tldr": "LG-Flow\uff1a\u4e00\u79cd\u6f5c\u5728\u56fe\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u6362\u7b49\u53d8\u81ea\u7f16\u7801\u5668\u5c06\u56fe\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6269\u6563\uff0c\u5b9e\u73b0\u9ad8\u6548\u56fe\u751f\u6210\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe1000\u500d", "motivation": "\u73b0\u6709\u56fe\u6269\u6563\u6a21\u578b\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e14\u5927\u90e8\u5206\u5bb9\u91cf\u6d6a\u8d39\u5728\u7a00\u758f\u56fe\u7684\u975e\u8fb9\u5efa\u6a21\u4e0a\u3002\u867d\u7136\u5176\u4ed6\u6a21\u6001\u7684\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u63d0\u4f9b\u4e86\u542f\u53d1\uff0c\u4f46\u56fe\u751f\u6210\u9700\u8981\u8fd1\u4e4e\u65e0\u635f\u7684\u91cd\u5efa\uff0c\u56e0\u4e3a\u90bb\u63a5\u77e9\u9635\u7684\u5355\u4e2a\u9519\u8bef\u5c31\u53ef\u80fd\u5bfc\u81f4\u6574\u4e2a\u6837\u672c\u65e0\u6548\uff0c\u8fd9\u4e00\u6311\u6218\u5c1a\u672a\u89e3\u51b3", "method": "\u63d0\u51faLG-Flow\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u7f6e\u6362\u7b49\u53d8\u81ea\u7f16\u7801\u5668\u5c06\u6bcf\u4e2a\u8282\u70b9\u6620\u5c04\u5230\u56fa\u5b9a\u7ef4\u5d4c\u5165\uff0c\u4ece\u8be5\u5d4c\u5165\u4e2d\u53ef\u4ee5\u8bc1\u660e\u5b8c\u5168\u6062\u590d\u5b8c\u6574\u7684\u90bb\u63a5\u77e9\u9635\uff1b2\uff09\u6f5c\u5728\u8868\u793a\u7684\u7ef4\u5ea6\u4e0e\u8282\u70b9\u6570\u6210\u7ebf\u6027\u5173\u7cfb\uff0c\u6d88\u9664\u4e86\u4e8c\u6b21\u74f6\u9888\uff1b3\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u5e26\u6709\u6d41\u5339\u914d\u7684\u6269\u6563\u53d8\u6362\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u56fe\u751f\u6210", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u5411\u56fe\u548cDAG\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u91cd\u5efa\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u56fe\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u83b7\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8fbe1000\u500d\u7684\u52a0\u901f", "conclusion": "LG-Flow\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u6f5c\u5728\u6269\u6563\u4e2d\u7684\u91cd\u5efa\u6311\u6218\uff0c\u901a\u8fc7\u7ebf\u6027\u7f29\u653e\u7684\u6f5c\u5728\u8868\u793a\u6d88\u9664\u4e86\u4e8c\u6b21\u590d\u6742\u5ea6\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12326", "abs": "https://arxiv.org/abs/2601.12326", "authors": ["Jing Zhang", "Bingjie Fan"], "title": "EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation", "comment": "11pages,10figures", "summary": "Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.", "AI": {"tldr": "EmoKGEdit\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5173\u8054\u77e5\u8bc6\u56fe\u8c31\u7684\u65e0\u8bad\u7ec3\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u60c5\u611f\u4e0e\u5185\u5bb9\u8868\u793a\u5b9e\u73b0\u7cbe\u786e\u4e14\u7ed3\u6784\u4fdd\u6301\u7684\u60c5\u611f\u7f16\u8f91", "motivation": "\u73b0\u6709\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u4ece\u6f5c\u5728\u5185\u5bb9\u8868\u793a\u4e2d\u89e3\u8026\u60c5\u611f\u7ebf\u7d22\uff0c\u5bfc\u81f4\u60c5\u611f\u8868\u8fbe\u5f31\u4e14\u89c6\u89c9\u7ed3\u6784\u626d\u66f2\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u7cbe\u786e\u4fdd\u6301\u7ed3\u6784\u7684\u60c5\u611f\u7f16\u8f91\u65b9\u6cd5", "method": "\u6784\u5efa\u591a\u6a21\u6001\u60c5\u611f\u5173\u8054\u77e5\u8bc6\u56fe\u8c31\uff08MSA-KG\uff09\u6765\u89e3\u8026\u5bf9\u8c61\u3001\u573a\u666f\u3001\u5c5e\u6027\u3001\u89c6\u89c9\u7ebf\u7d22\u548c\u60c5\u611f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff1b\u8bbe\u8ba1\u89e3\u8026\u7ed3\u6784-\u60c5\u611f\u7f16\u8f91\u6a21\u5757\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u660e\u786e\u5206\u79bb\u60c5\u611f\u5c5e\u6027\u548c\u5e03\u5c40\u7279\u5f81", "result": "EmoKGEdit\u5728\u60c5\u611f\u4fdd\u771f\u5ea6\u548c\u5185\u5bb9\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u7ed3\u6784\u4fdd\u6301\u7684\u56fe\u50cf\u60c5\u611f\u7f16\u8f91", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u89e3\u8026\u65b9\u6cd5\uff0cEmoKGEdit\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u4e2d\u7684\u60c5\u611f-\u5185\u5bb9\u7ea0\u7f20\u95ee\u9898\uff0c\u4e3a\u7cbe\u786e\u4e14\u7ed3\u6784\u4fdd\u6301\u7684\u60c5\u611f\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2601.13793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13793", "abs": "https://arxiv.org/abs/2601.13793", "authors": ["ByeoungDo Kim", "JunYeop Na", "Kyungwook Tak", "JunTae Kim", "DongHyeon Kim", "Duckky Kim"], "title": "PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles", "comment": "7 pages, 3 figures, ITSC 2025, to be published", "summary": "In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5386\u53f2\u9053\u8def\u901f\u5ea6\u6a21\u5f0f\u63d0\u5347\u5230\u8fbe\u65f6\u95f4\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u7684\u540c\u65f6\u6709\u6548\u6355\u6349\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u666e\u53ca\uff0c\u51c6\u786e\u53ef\u9760\u7684ETA\u4f30\u8ba1\u5bf9\u5bfc\u822a\u3001\u51fa\u884c\u89c4\u5212\u548c\u4ea4\u901a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5b9e\u65f6\u548c\u5386\u53f2\u4ea4\u901a\u6570\u636e\u7ed3\u5408\u65b9\u5f0f\u7b80\u5355\uff0c\u6216\u4f9d\u8d56\u590d\u6742\u89c4\u5219\u8ba1\u7b97\uff1b\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u80fd\u6709\u6548\u6355\u6349ETA\u9884\u6d4b\u6240\u9700\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u6a21\u578b\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u548c\u5229\u7528\u8def\u7ebf\u6bcf\u4e2a\u65f6\u7a7a\u70b9\u7d2f\u79ef\u7684\u65f6\u95f4\u7279\u5f81\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5386\u53f2\u9053\u8def\u901f\u5ea6\u6a21\u5f0f\uff0c\u4ee5\u4efb\u52a1\u611f\u77e5\u65b9\u5f0f\u6574\u5408\u9053\u8def\u7279\u5f81\u3001\u5b9e\u65f6\u4ea4\u901a\u72b6\u51b5\u548c\u5386\u53f2\u901f\u5ea6\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8f7b\u91cf\u5316\u548c\u53ef\u6269\u5c55\u7684\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5728\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u6709\u6548\u6574\u5408\u9053\u8def\u7279\u5f81\u3001\u5b9e\u65f6\u4ea4\u901a\u6761\u4ef6\u548c\u5386\u53f2\u901f\u5ea6\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684ETA\u4f30\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u6349\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u548c\u53ef\u6269\u5c55\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5230\u8fbe\u65f6\u95f4\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12329", "abs": "https://arxiv.org/abs/2601.12329", "authors": ["Mithlesh Singla", "Seema Kumari", "Shanmuganathan Raman"], "title": "FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching", "comment": null, "summary": "Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.", "AI": {"tldr": "\u63d0\u51faFlowIID\uff0c\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u8f7b\u91cf\u7ea7\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u91cf\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u548c\u5b9e\u65f6\u5e94\u7528", "motivation": "\u73b0\u6709\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u6a21\u578b\u867d\u7136\u6548\u679c\u597d\u4f46\u53c2\u6570\u91cf\u5927\uff0c\u96be\u4ee5\u4e0e\u5176\u4ed6\u6a21\u578b\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u53c2\u6570\u9ad8\u6548\u4e14\u6027\u80fd\u826f\u597d\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faFlowIID\u67b6\u6784\uff0c\u57fa\u4e8e\u6f5c\u5728\u6d41\u5339\u914d\uff0c\u7ed3\u5408VAE\u5f15\u5bfc\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u6d41\u5339\u914d\u6a21\u5757\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u53cd\u7167\u7387\u548c\u7740\u8272\u5206\u89e3", "result": "FlowIID\u4e0d\u4ec5\u53c2\u6570\u9ad8\u6548\uff0c\u8fd8\u80fd\u5355\u6b65\u63a8\u7406\u4ea7\u751f\u7ed3\u679c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd", "conclusion": "FlowIID\u662f\u4e00\u79cd\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u548c\u5b9e\u65f6\u89c6\u89c9\u5e94\u7528\u90e8\u7f72"}}
{"id": "2601.13851", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13851", "abs": "https://arxiv.org/abs/2601.13851", "authors": ["Alessandro Londei", "Matteo Benati", "Denise Lanzieri", "Vittorio Loreto"], "title": "Inverting Self-Organizing Maps: A Unified Activation-Based Framework", "comment": null, "summary": "Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOM\uff09\u7684\u7cbe\u786e\u8f93\u5165\u6062\u590d\u65b9\u6cd5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86MUSIC\u66f4\u65b0\u89c4\u5219\uff0c\u7528\u4e8e\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53ef\u63a7\u7684\u8bed\u4e49\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOM\uff09\u867d\u7136\u5e7f\u6cdb\u7528\u4e8e\u53ef\u89c6\u5316\u3001\u805a\u7c7b\u548c\u5411\u91cf\u91cf\u5316\uff0c\u4f46\u5176\u6fc0\u6d3b\u6a21\u5f0f\uff08\u5230\u539f\u578b\u7684\u5e73\u65b9\u8ddd\u79bb\uff09\u7684\u53cd\u5411\u6062\u590d\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u8bba\u6587\u65e8\u5728\u8bc1\u660e\u5728\u6e29\u548c\u7684\u51e0\u4f55\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7SOM\u6fc0\u6d3b\u6a21\u5f0f\u7cbe\u786e\u6062\u590d\u539f\u59cb\u8f93\u5165\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u53ef\u63a7\u7684\u6f5c\u5728\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u51e0\u4f55\u5b66\u539f\u7406\uff1aD\u7ef4\u7a7a\u95f4\u4e2d\u7684\u70b9\u7531\u5176\u5230D+1\u4e2a\u4eff\u5c04\u72ec\u7acb\u53c2\u8003\u70b9\u7684\u8ddd\u79bb\u552f\u4e00\u786e\u5b9a\u3002\u8bba\u6587\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684\u7ebf\u6027\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u4e86\u53cd\u6f14\u7684\u826f\u597d\u6761\u4ef6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86MUSIC\u66f4\u65b0\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u901a\u8fc7\u4fee\u6539\u9009\u5b9a\u539f\u578b\u7684\u5e73\u65b9\u8ddd\u79bb\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8ddd\u79bb\uff0c\u4ea7\u751f\u4e0eSOM\u5206\u6bb5\u7ebf\u6027\u7ed3\u6784\u5bf9\u9f50\u7684\u786e\u5b9a\u6027\u51e0\u4f55\u6d41\u3002\u4f7f\u7528Tikhonov\u6b63\u5219\u5316\u6765\u7a33\u5b9a\u66f4\u65b0\u89c4\u5219\u5e76\u786e\u4fdd\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u6ed1\u8fd0\u52a8\u3002", "result": "\u5728\u5408\u6210\u9ad8\u65af\u6df7\u5408\u3001MNIST\u548cFaces in the Wild\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cMUSIC\u80fd\u591f\u4ea7\u751f\u5e73\u6ed1\u3001\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u6d41\u5f62\u7684\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002\u5f53\u4e0d\u65bd\u52a0\u6270\u52a8\u65f6\uff0c\u53cd\u6f14\u80fd\u591f\u7cbe\u786e\u6062\u590d\u8f93\u5165\uff1b\u5f53\u6307\u5b9a\u76ee\u6807\u805a\u7c7b\u6216\u539f\u578b\u65f6\uff0cMUSIC\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u7684\u8bed\u4e49\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u57fa\u4e8eSOM\u6fc0\u6d3b\u6a21\u5f0f\u53cd\u6f14\u7684\u65b0\u89c6\u89d2\uff0c\u4e3a\u6570\u636e\u589e\u5f3a\u548c\u53ef\u63a7\u6f5c\u5728\u63a2\u7d22\u63d0\u4f9b\u4e86\u4ec5\u4f9d\u8d56\u539f\u578b\u51e0\u4f55\u7684\u65b9\u6cd5\u3002\u4e0e\u53d8\u5206\u6216\u6982\u7387\u751f\u6210\u6a21\u578b\u4e0d\u540c\uff0cMUSIC\u4e0d\u4f9d\u8d56\u91c7\u6837\u3001\u6f5c\u5728\u5148\u9a8c\u6216\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u800c\u662f\u57fa\u4e8e\u786e\u5b9a\u6027\u51e0\u4f55\u539f\u7406\uff0c\u4e3aSOM\u63d0\u4f9b\u4e86\u8d85\u8d8a\u65e0\u76d1\u7763\u805a\u7c7b\u7684\u65b0\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2601.12357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12357", "abs": "https://arxiv.org/abs/2601.12357", "authors": ["Hailing Jin", "Huiying Li"], "title": "SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence", "comment": null, "summary": "Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.", "code_url": "https://github.com/hailong23-jin/SimpleMatch", "code_stars": 1, "code_last_update": "2026-01-19", "AI": {"tldr": "SimpleMatch\uff1a\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u8bed\u4e49\u5bf9\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e0a\u91c7\u6837\u89e3\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u76d1\u7763\u635f\u5931\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1151%\u7684\u8bad\u7ec3\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u6df1\u5ea6\u4e0b\u91c7\u6837\u64cd\u4f5c\u4f1a\u5bfc\u81f4\u76f8\u90bb\u5173\u952e\u70b9\u7279\u5f81\u4e0d\u53ef\u9006\u5730\u878d\u5408\uff0c\u5f53\u8bed\u4e49\u4e0d\u540c\u7684\u5173\u952e\u70b9\u843d\u5165\u540c\u4e00\u964d\u91c7\u6837\u611f\u53d7\u91ce\u65f6\uff08\u598216x16\u5757\uff09\uff0c\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51faSimpleMatch\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u4e0a\u91c7\u6837\u89e3\u7801\u5668\uff0c\u5c06\u6df1\u5c42\u7279\u5f81\u9010\u6b65\u4e0a\u91c7\u6837\u52301/4\u5206\u8fa8\u7387\u4ee5\u6062\u590d\u7a7a\u95f4\u7ec6\u8282\uff1b2\uff09\u591a\u5c3a\u5ea6\u76d1\u7763\u635f\u5931\uff0c\u786e\u4fdd\u4e0a\u91c7\u6837\u7279\u5f81\u5728\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u5224\u522b\u6027\u7279\u5f81\uff1b3\uff09\u7a00\u758f\u5339\u914d\u548c\u57fa\u4e8e\u7a97\u53e3\u7684\u5b9a\u4f4d\uff0c\u4f18\u5316\u8bad\u7ec3\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5728252x252\u5206\u8fa8\u7387\uff08\u6bd4\u5f53\u524dSOTA\u65b9\u6cd5\u5c0f3.3\u500d\uff09\u4e0b\uff0cSimpleMatch\u5728SPair-71k\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u523084.1% PCK@0.1\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u5185\u5b58\u4f7f\u7528\u51cf\u5c1151%\u3002", "conclusion": "SimpleMatch\u4e3a\u8bed\u4e49\u5bf9\u5e94\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u5f3a\u5927\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u5bf9\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u4f9d\u8d56\u95ee\u9898\u3002"}}
{"id": "2601.12238", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12238", "abs": "https://arxiv.org/abs/2601.12238", "authors": ["Sharan Sahu", "Cameron J. Hogan", "Martin T. Wells"], "title": "On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization", "comment": "70 pages, 4 figures, 2 tables", "summary": "While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.", "AI": {"tldr": "\u52a8\u91cf\u4f18\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u5b58\u5728\u6027\u80fd\u6743\u8861\uff1a\u52a8\u91cf\u80fd\u6291\u5236\u68af\u5ea6\u566a\u58f0\u4f46\u4f1a\u653e\u5927\u6f02\u79fb\u5f15\u8d77\u7684\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5f53\u52a8\u91cf\u53c2\u6570\u8d8b\u8fd11\u65f6\uff0c\u8ddf\u8e2a\u8bef\u5dee\u53ef\u80fd\u65e0\u9650\u653e\u5927", "motivation": "\u52a8\u91cf\u52a0\u901f\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u548c\u6700\u4f18\u53c2\u6570\u968f\u65f6\u95f4\u6f02\u79fb\u7684\u975e\u5e73\u7a33\u73af\u5883\u4e2d\uff0c\u5176\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5206\u6790SGD\u53ca\u5176\u52a8\u91cf\u53d8\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8ddf\u8e2a\u6027\u80fd", "method": "\u5728\u5747\u5300\u5f3a\u51f8\u6027\u548c\u5149\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0c\u5206\u6790SGD\u53ca\u5176\u52a8\u91cf\u53d8\u4f53\uff08Polyak\u91cd\u7403\u6cd5\u548cNesterov\u6cd5\uff09\u5728\u4e0d\u540c\u6b65\u957f\u673a\u5236\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u63a8\u5bfc\u8ddf\u8e2a\u8bef\u5dee\u7684\u6709\u9650\u65f6\u95f4\u8fb9\u754c\uff08\u671f\u671b\u548c\u9ad8\u6982\u7387\uff09\uff0c\u5efa\u7acb\u5305\u542b\u521d\u59cb\u5316\u77ac\u6001\u9879\u3001\u566a\u58f0\u8bf1\u5bfc\u65b9\u5dee\u9879\u548c\u6f02\u79fb\u8bf1\u5bfc\u8ddf\u8e2a\u6ede\u540e\u9879\u7684\u4e09\u5206\u91cf\u5206\u89e3", "result": "\u63ed\u793a\u4e86\u52a8\u91cf\u4f18\u5316\u7684\u57fa\u672c\u6743\u8861\uff1a\u52a8\u91cf\u80fd\u6291\u5236\u68af\u5ea6\u566a\u58f0\uff0c\u4f46\u4f1a\u663e\u8457\u653e\u5927\u6f02\u79fb\u5f15\u8d77\u7684\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5f53\u52a8\u91cf\u53c2\u6570\u8d8b\u8fd11\u65f6\uff0c\u653e\u5927\u6548\u5e94\u53ef\u80fd\u65e0\u9650\u589e\u5927\u3002\u5efa\u7acb\u4e86\u68af\u5ea6\u53d8\u5316\u7ea6\u675f\u4e0b\u52a8\u6001\u9057\u61be\u7684\u6700\u5c0f\u6700\u5927\u4e0b\u754c\uff0c\u8bc1\u660e\u60ef\u6027\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\u4e0d\u662f\u5206\u6790\u5047\u8c61\uff0c\u800c\u662f\u4fe1\u606f\u7406\u8bba\u969c\u788d", "conclusion": "\u52a8\u91cf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56fa\u6709\u7684\"\u60ef\u6027\u7a97\u53e3\"\uff0c\u4f1a\u4ece\u6839\u672c\u4e0a\u964d\u4f4e\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u52a8\u91cf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7ecf\u9a8c\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5212\u5b9a\u4e86SGD\u53ef\u8bc1\u660e\u4f18\u4e8e\u5176\u52a0\u901f\u5bf9\u5e94\u65b9\u6cd5\u7684\u7cbe\u786e\u673a\u5236\u8fb9\u754c"}}
{"id": "2601.13892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13892", "abs": "https://arxiv.org/abs/2601.13892", "authors": ["Andrej Schwanke", "Lyubomir Ivanov", "David Salinas", "Frank Hutter", "Arber Zela"], "title": "Multi-Objective Hierarchical Optimization with Large Language Models", "comment": "23 pages, 21 figures, 9 tables", "summary": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u548c\u5019\u9009\u91c7\u6837\u5668\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u5206\u5c42\u641c\u7d22\u7b56\u7565\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u533a\u8f93\u5165\u7a7a\u95f4\u5e76\u9650\u5236LLM\u5728\u7279\u5b9a\u9ad8\u6f5c\u529b\u5b50\u7a7a\u95f4\u751f\u6210\uff0c\u5b9e\u73b0\u6536\u655b\u5230\u771f\u5b9e\u5e15\u7d2f\u6258\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u6210\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u7684\u73b0\u6210\u9009\u62e9\u3002\u4f20\u7edf\u65b9\u6cd5\u56e0\u5176\u5904\u7406\u6570\u503c\u8f93\u5165\u3001\u5e73\u8861\u63a2\u7d22\u4e0e\u5e15\u7d2f\u6258\u524d\u6cbf\u5229\u7528\u4ee5\u53ca\u5904\u7406\u591a\u51b2\u7a81\u76ee\u6807\u7684\u80fd\u529b\u800c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5229\u7528LLM\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u548c\u5019\u9009\u91c7\u6837\u5668\uff0c\u5d4c\u5165\u7ed3\u6784\u5316\u5206\u5c42\u641c\u7d22\u7b56\u7565\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5c06\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u7684\u8d85\u77e9\u5f62\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u590d\u5408\u8bc4\u5206\u51fd\u6570\u5bf9\u8fd9\u4e9b\u533a\u57df\u8fdb\u884c\u6392\u5e8f\uff0c\u5c06LLM\u7684\u751f\u6210\u8fc7\u7a0b\u9650\u5236\u5728\u7279\u5b9a\u9ad8\u6f5c\u529b\u5b50\u7a7a\u95f4\uff0c\u4f7fLLM\u53ea\u9700\u8fdb\u884c\u5c40\u90e8\u63a8\u7406\u800c\u975e\u5168\u5c40\u63a8\u7406\u3002", "result": "\u5728\u6807\u51c6\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\uff0c\u7b97\u6cd5\u751f\u6210\u7684\u5019\u9009\u89e3\u5728Hausdorff\u8ddd\u79bb\u4e0a\u6536\u655b\u5230\u771f\u5b9e\u5e15\u7d2f\u6258\u96c6\u3002\u5b9e\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u5168\u5c40LLM\u7684\u591a\u76ee\u6807\u4f18\u5316\u5668\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6807\u51c6\u8fdb\u5316\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u4e0e\u7ed3\u6784\u5316\u5206\u5c42\u641c\u7d22\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7fLLM\u80fd\u591f\u4e13\u6ce8\u4e8e\u5c40\u90e8\u63a8\u7406\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u7406\u8bba\u6536\u655b\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002"}}
{"id": "2601.12366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12366", "abs": "https://arxiv.org/abs/2601.12366", "authors": ["Jiafei Zhang", "Songliang Cao", "Binghui Xu", "Yanan Li", "Weiwei Jia", "Tingting Wu", "Hao Lu", "Weijuan Hu", "Zhiguo Han"], "title": "DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data", "comment": "13 pages, 15 figures and 7 tables", "summary": "DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.", "AI": {"tldr": "DepthCropSeg++\u662f\u4e00\u4e2a\u7528\u4e8e\u4f5c\u7269\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5f00\u653e\u7530\u95f4\u73af\u5883\u4e0b\u5206\u5272\u4e0d\u540c\u4f5c\u7269\u7269\u79cd\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u4f5c\u7269\u5206\u5272\u6a21\u578b\u5927\u591a\u56e0\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u800c\u53ea\u80fd\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u901a\u5e38\u4ec5\u5728\u7279\u5b9a\u4f5c\u7269\u7c7b\u578b\u6216\u53d7\u63a7\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u7269\u79cd\u3001\u8de8\u573a\u666f\u6cdb\u5316\u7684\u4f5c\u7269\u5206\u5272\u57fa\u7840\u6a21\u578b\u3002", "method": "1) \u6269\u5c55\u8de8\u7269\u79cd\u8de8\u573a\u666f\u4f5c\u7269\u5206\u5272\u6570\u636e\u96c6\uff0828,406\u5f20\u56fe\u50cf\uff0c30+\u7269\u79cd\uff0c15\u79cd\u73af\u5883\u6761\u4ef6\uff09\uff1b2) \u57fa\u4e8eViT-Adapter\u67b6\u6784\uff0c\u589e\u5f3a\u52a8\u6001\u4e0a\u91c7\u6837\u4ee5\u63d0\u5347\u7ec6\u8282\u611f\u77e5\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u8bad\u7ec3\u6d41\u6c34\u7ebf\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u7efc\u5408\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523093.11% mIoU\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\uff08+0.36%\uff09\u548c\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5982SAM\uff08+48.57%\uff09\u3002\u5728\u591c\u95f4\u73af\u5883\uff0886.90% mIoU\uff09\u3001\u9ad8\u5bc6\u5ea6\u51a0\u5c42\uff0890.09% mIoU\uff09\u548c\u672a\u89c1\u4f5c\u7269\u54c1\u79cd\uff0890.09% mIoU\uff09\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DepthCropSeg++\u4ee3\u8868\u4e86\u4f5c\u7269\u5206\u5272\u7684\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u519c\u4e1a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f00\u653e\u7530\u95f4\u73af\u5883\u4e0b\u7684\u4f5c\u7269\u5206\u5272\u95ee\u9898\u3002"}}
{"id": "2601.13191", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13191", "abs": "https://arxiv.org/abs/2601.13191", "authors": ["Francisco Daunas", "I\u00f1aki Esnaola", "Samir M. Perlaza", "H. Vincent Poor"], "title": "Empirical Risk Minimization with $f$-Divergence Regularization", "comment": "Submitted to IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2502.14544, arXiv:2508.03314", "summary": "In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4e0ef-\u6563\u5ea6\u6b63\u5219\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u6b63\u5219\u5316\u95ee\u9898\u4e0e\u7ea6\u675f\u95ee\u9898\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u5f15\u5165\u5f52\u4e00\u5316\u51fd\u6570\u6982\u5ff5\u5e76\u7ed9\u51fa\u6570\u503c\u7b97\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540cf-\u6563\u5ea6\u4e4b\u95f4\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\u3002", "motivation": "\u89e3\u51b3\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4e0ef-\u6563\u5ea6\u6b63\u5219\u5316\u95ee\u9898\uff0c\u6269\u5c55\u9002\u7528f-\u6563\u5ea6\u7c7b\u522b\uff0c\u5efa\u7acb\u6b63\u5219\u5316\u4e0e\u7ea6\u675f\u95ee\u9898\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u5b9e\u9645\u8ba1\u7b97\u63d0\u4f9b\u6709\u6548\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u5f52\u4e00\u5316\u51fd\u6570\u6982\u5ff5\uff0c\u5efa\u7acb\u5176\u975e\u7ebf\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\u8868\u793a\uff0c\u8bc1\u660e\u5173\u952e\u6027\u8d28\u5e76\u6784\u9020\u6570\u503c\u8fd1\u4f3c\u7b97\u6cd5\uff1b\u5206\u6790\u4e0d\u540cf-\u6563\u5ea6\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u53d8\u6362\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\u3002", "result": "\u5efa\u7acb\u4e86ERM-fDR\u89e3\u4e0e\u7ea6\u675f\u95ee\u9898\u89e3\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u6269\u5c55\u4e86\u9002\u7528f-\u6563\u5ea6\u7c7b\u522b\uff1b\u7ed9\u51fa\u4e86\u5f52\u4e00\u5316\u51fd\u6570\u7684\u6570\u503c\u7b97\u6cd5\uff1b\u8bc1\u660e\u4e86\u4e0d\u540cf-\u6563\u5ea6\u95ee\u9898\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\uff1b\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e0d\u540cf\u51fd\u6570\u9009\u62e9\u5bf9\u8bad\u7ec3\u548c\u6d4b\u8bd5\u98ce\u9669\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u89e3\u51b3\u4e86ERM-fDR\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u5de5\u5177\uff0c\u5f52\u4e00\u5316\u51fd\u6570\u662f\u8fde\u63a5\u5bf9\u5076\u5f62\u5f0f\u548c\u5b9e\u9645\u8ba1\u7b97\u7684\u5173\u952e\uff0c\u4e0d\u540cf-\u6563\u5ea6\u9009\u62e9\u5177\u6709\u5b9e\u9645\u5f71\u54cd\uff0c\u4e3af-\u6563\u5ea6\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12379", "abs": "https://arxiv.org/abs/2601.12379", "authors": ["Jiahui Sheng", "Yidan Shi", "Shu Xiang", "Xiaorun Li", "Shuhan Chen"], "title": "Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection", "comment": null, "summary": "Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.", "code_url": "https://github.com/jiahuisheng/ScoreAD", "code_stars": 0, "code_last_update": "2025-09-30", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b(ScoreAD)\u7684\u9ad8\u5149\u8c31\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u6570\u636e\u5206\u5e03\u7684\u65f6\u53d8\u68af\u5ea6\u573a(\u5206\u6570)\u6765\u533a\u5206\u80cc\u666f\u548c\u5f02\u5e38\u5149\u8c31", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u7684\u5149\u8c31\u7531\u5c11\u6570\u56e0\u7d20(\u5982\u5316\u5b66\u6210\u5206\u3001\u5149\u7167)\u51b3\u5b9a\uff0c\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\u3002\u80cc\u666f\u5149\u8c31\u4f4d\u4e8e\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u800c\u5f02\u5e38\u5149\u8c31\u56e0\u72ec\u7279\u5149\u8c31\u7279\u5f81\u88ab\u89c6\u4e3a\u4e0d\u9075\u5faa\u80cc\u666f\u6d41\u5f62\u7684\u79bb\u7fa4\u70b9", "method": "\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b(SGM)\u5b66\u4e60\u6570\u636e\u5206\u5e03\u7684\u68af\u5ea6\u573a(\u5206\u6570)\u3002\u9996\u5148\u5728\u6574\u4e2a\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5149\u8c31\u96c6\u4e0a\u8bad\u7ec3SGM\uff0c\u6d4b\u8bd5\u65f6\u6bcf\u4e2a\u5149\u8c31\u901a\u8fc7\u6270\u52a8\u6838\u5904\u7406\uff0c\u5c06\u6270\u52a8\u540e\u7684\u5149\u8c31\u8f93\u5165\u8bad\u7ec3\u597d\u7684SGM\u83b7\u53d6\u4f30\u8ba1\u5206\u6570\uff0c\u5229\u7528\u80cc\u666f\u548c\u5f02\u5e38\u5149\u8c31\u5728\u6d41\u5f62\u5206\u5e03\u4e0a\u7684\u5dee\u5f02\u8fdb\u884c\u68c0\u6d4b", "result": "\u5728\u56db\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "ScoreAD\u65b9\u6cd5\u5229\u7528\u5206\u6570\u751f\u6210\u6a21\u578b\u6355\u6349\u9ad8\u5149\u8c31\u6570\u636e\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6d41\u5f62\u5047\u8bbe\u7684\u5f02\u5e38\u68c0\u6d4b"}}
{"id": "2601.12382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12382", "abs": "https://arxiv.org/abs/2601.12382", "authors": ["Furkan Yuceyalcin", "Abdurrahim Yilmaz", "Burak Temelkuran"], "title": "A Hierarchical Benchmark of Foundation Models for Dermatology", "comment": null, "summary": "Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a \"granularity gap\" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8610\u79cd\u57fa\u7840\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5c42\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u7528\u533b\u5b66\u5f71\u50cf\u6a21\u578b\u5728\u9ad8\u5c42\u6b21\u7b5b\u67e5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u76ae\u80a4\u75c5\u4e13\u7528\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4e9a\u578b\u533a\u5206\u65b9\u9762\u66f4\u4f18\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u7684\"\u7c92\u5ea6\u5dee\u8ddd\"\u3002", "motivation": "\u5f53\u524d\u76ae\u80a4\u75c5\u5b66\u57fa\u51c6\u901a\u5e38\u5c06\u590d\u6742\u7684\u8bca\u65ad\u5206\u7c7b\u7b80\u5316\u4e3a\u6241\u5e73\u5316\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u533a\u5206\u9ed1\u8272\u7d20\u7624\u4e0e\u826f\u6027\u75e3\uff09\uff0c\u8fd9\u79cd\u8fc7\u5ea6\u7b80\u5316\u63a9\u76d6\u4e86\u6a21\u578b\u6267\u884c\u7ec6\u7c92\u5ea6\u9274\u522b\u8bca\u65ad\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u5bf9\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528DERM12345\u6570\u636e\u96c6\uff08\u5305\u542b40\u4e2a\u75c5\u53d8\u4e9a\u7c7b\uff09\uff0c\u8ba1\u7b9710\u79cd\u57fa\u7840\u6a21\u578b\uff08\u6db5\u76d6\u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u901a\u7528\u533b\u5b66\u5f71\u50cf\u548c\u76ae\u80a4\u75c5\u4e13\u7528\u9886\u57df\uff09\u7684\u51bb\u7ed3\u5d4c\u5165\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u578b\uff0c\u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002\u5f15\u5165\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u56db\u4e2a\u4e34\u5e8a\u7c92\u5ea6\u7ea7\u522b\u8bc4\u4f30\u6027\u80fd\uff1a40\u4e2a\u4e9a\u7c7b\u300115\u4e2a\u4e3b\u7c7b\u30012\u4e2a\u548c4\u4e2a\u8d85\u7c7b\u4ee5\u53ca\u4e8c\u5143\u6076\u6027\u5206\u7c7b\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u7684\"\u7c92\u5ea6\u5dee\u8ddd\"\uff1aMedImageInsights\u5728\u4e8c\u5143\u6076\u6027\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08\u52a0\u6743F1\u5206\u657097.52%\uff09\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea640\u7c7b\u4e9a\u578b\u5206\u7c7b\u4e2d\u4e0b\u964d\u81f365.50%\u3002\u76f8\u53cd\uff0cMedSigLip\uff0869.79%\uff09\u548c\u76ae\u80a4\u75c5\u4e13\u7528\u6a21\u578b\uff08Derm Foundation\u548cMONET\uff09\u5728\u7ec6\u7c92\u5ea640\u7c7b\u4e9a\u578b\u533a\u5206\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u66f4\u5e7f\u6cdb\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u6574\u4f53\u6027\u80fd\u4f4e\u4e8eMedImageInsights\u3002", "conclusion": "\u867d\u7136\u901a\u7528\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5728\u9ad8\u5c42\u6b21\u7b5b\u67e5\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4f46\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u533a\u5206\u9700\u8981\u4e13\u95e8\u7684\u5efa\u6a21\u7b56\u7565\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6839\u636e\u4e34\u5e8a\u9700\u6c42\u9009\u62e9\u9002\u5f53\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u5de5\u4f5c\u5e94\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u5c42\u6b21\u8bca\u65ad\u4efb\u52a1\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2601.12391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12391", "abs": "https://arxiv.org/abs/2601.12391", "authors": ["Dasith de Silva Edirimuni", "Ajmal Saeed Mian"], "title": "Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation", "comment": "Accepted to AAAI 2026, Main Technical Track", "summary": "Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\\textit{codebook collapse}$, we propose a $\\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.", "AI": {"tldr": "\u63d0\u51faCPVQ-VAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u522b\u5206\u533a\u7801\u672c\u548c\u7c7b\u611f\u77e5\u66f4\u65b0\u673a\u5236\uff0c\u76f4\u63a5\u4ece\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u7279\u5f81\u89e3\u7801\u4e3a\u70b9\u4e91\u5bf9\u8c61\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u5e93\u68c0\u7d22\uff0c\u5b9e\u73b0\u7eaf\u70b9\u4e91\u573a\u666f\u751f\u6210\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5927\u591a\u53ea\u80fd\u751f\u6210\u7269\u4f53\u8fb9\u754c\u6846\u53c2\u6570\uff0c\u8f83\u65b0\u7684\u6269\u6563\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u7c7b\u522b\u6807\u7b7e\u548c\u6f5c\u5728\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u81ea\u7f16\u7801\u5668\u65e0\u6cd5\u6709\u6548\u89e3\u7801\u8fd9\u4e9b\u7279\u5f81\u4e3a\u4e0e\u76ee\u6807\u7c7b\u522b\u4e00\u81f4\u7684\u6b63\u786e\u70b9\u4e91\u5bf9\u8c61\u3002\u5bf9\u4e8e\u5305\u542b\u591a\u6837\u5316\u3001\u591a\u7c7b\u522b\u7269\u4f53\u7684\u590d\u6742\u573a\u666f\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u5728\u7279\u5f81\u65e0\u6cd5\u88ab\u5f53\u524d\u81ea\u7f16\u7801\u5668\u6b63\u786e\u89e3\u7801\u3002", "method": "1. \u63d0\u51fa\u7c7b\u522b\u5206\u533a\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(CPVQ-VAE)\uff0c\u91c7\u7528\u521b\u65b0\u7684\u7c7b\u522b\u5206\u533a\u7801\u672c\uff0c\u5176\u4e2d\u7801\u5411\u91cf\u6309\u7c7b\u522b\u6807\u8bb0\uff1b2. \u4e3a\u89e3\u51b3\u7801\u672c\u574d\u7f29\u95ee\u9898\uff0c\u63d0\u51fa\u7c7b\u611f\u77e5\u8fd0\u884c\u5e73\u5747\u66f4\u65b0\u673a\u5236\uff0c\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u91cd\u65b0\u521d\u59cb\u5316\u6b7b\u7801\u5411\u91cf\uff1b3. \u63a8\u7406\u65f6\uff0c\u7531\u4e13\u95e8\u4e3a\u573a\u666f\u751f\u6210\u8bbe\u8ba1\u7684\u6f5c\u5728\u7a7a\u95f4\u6d41\u5339\u914d\u6a21\u578b(LFMM)\u751f\u6210\u7269\u4f53\u7279\u5f81\u548c\u7c7b\u522b\u6807\u7b7e\uff0cCPVQ-VAE\u901a\u8fc7\u7c7b\u611f\u77e5\u9006\u67e5\u627e\u5c06\u751f\u6210\u7684\u6f5c\u5728\u7279\u5f81\u6620\u5c04\u5230\u7801\u672c\u6761\u76ee\uff0c\u89e3\u7801\u4e3a\u7c7b\u522b\u7279\u5b9a\u7684\u70b9\u4e91\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u6062\u590d\u5408\u7406\u7684\u70b9\u4e91\u573a\u666f\uff0c\u5728\u590d\u6742\u5ba2\u5385\u573a\u666f\u4e0a\uff0cChamfer\u8bef\u5dee\u548cPoint2Mesh\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8670.4%\u548c72.3%\u3002", "conclusion": "CPVQ-VAE\u65b9\u6cd5\u901a\u8fc7\u7c7b\u522b\u5206\u533a\u7801\u672c\u548c\u7c7b\u611f\u77e5\u66f4\u65b0\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7279\u5f81\u89e3\u7801\u4e3a\u7c7b\u522b\u4e00\u81f4\u70b9\u4e91\u5bf9\u8c61\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u8c61\u6570\u636e\u5e93\u7684\u7eaf\u70b9\u4e91\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2601.13519", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.13519", "abs": "https://arxiv.org/abs/2601.13519", "authors": ["Wenzhi Gao", "Chang He", "Madeleine Udell"], "title": "Small Gradient Norm Regret for Online Convex Optimization", "comment": null, "summary": "This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\\sum_{t=1}^T \\|\\nabla \\ell(x^\\star)\\|^2$. We show that the $G^\\star$ regret strictly refines the existing $L^\\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u51f8\u4f18\u5316\u95ee\u9898\u4f9d\u8d56\u540e\u6094\u5ea6\u91cfG*\u540e\u6094\uff0c\u5b83\u57fa\u4e8e\u7d2f\u79ef\u5e73\u65b9\u68af\u5ea6\u8303\u6570\uff0c\u6bd4\u73b0\u6709\u7684L*\uff08\u5c0f\u635f\u5931\uff09\u540e\u6094\u66f4\u7cbe\u7ec6\uff0c\u5728\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\u663e\u8457\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u51f8\u4f18\u5316\u4e2d\u7684\u95ee\u9898\u4f9d\u8d56\u540e\u6094\u5ea6\u91cf\uff08\u5982L*\u540e\u6094\uff09\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u591f\u7cbe\u7ec6\uff0c\u7279\u522b\u662f\u5f53\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u540e\u6094\u5ea6\u91cf\u6765\u66f4\u597d\u5730\u8861\u91cf\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u5f15\u5165\u65b0\u7684G*\u540e\u6094\u5ea6\u91cf\uff0c\u5b9a\u4e49\u4e3a\u2211\u2016\u2207\u2113(x*)\u2016\u00b2\uff0c\u5373\u7d2f\u79ef\u5e73\u65b9\u68af\u5ea6\u8303\u6570\u5728\u4e8b\u540e\u6700\u4f18\u89e3\u5904\u7684\u8bc4\u4f30\u3002\u5efa\u7acbG*\u540e\u6094\u7684\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u52a8\u6001\u540e\u6094\u548c\u8001\u864e\u673a\u8bbe\u7f6e\u3002", "result": "\u8bc1\u660eG*\u540e\u6094\u4e25\u683c\u7ec6\u5316\u4e86\u73b0\u6709\u7684L*\u540e\u6094\uff0c\u5f53\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\uff0cG*\u540e\u6094\u53ef\u4ee5\u4efb\u610f\u66f4\u5c16\u9510\u3002\u5efa\u7acb\u4e86G*\u540e\u6094\u7684\u7406\u8bba\u754c\u9650\uff0c\u5e76\u6539\u8fdb\u4e86\u63d2\u503c\u673a\u5236\u4e0b\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u7684\u6536\u655b\u5206\u6790\u3002", "conclusion": "G*\u540e\u6094\u4e3a\u5728\u7ebf\u51f8\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u95ee\u9898\u4f9d\u8d56\u540e\u6094\u5ea6\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u7684\u60c5\u51b5\uff0c\u4e3a\u7b97\u6cd5\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2601.13642", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13642", "abs": "https://arxiv.org/abs/2601.13642", "authors": ["Yuchen Jiao", "Jiin Woo", "Gen Li", "Gauri Joshi", "Yuejie Chi"], "title": "Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning", "comment": null, "summary": "Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{\\varepsilon^3}\\right)$, where $\\|h^{\\star}\\|_{\\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}^2}{\\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{M\\varepsilon^3}\\right)$, with only $\\widetilde{O}\\left(\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}}{\\varepsilon}\\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5e73\u5747\u5956\u52b1MDP\u7684Q\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u5355\u667a\u80fd\u4f53\u548c\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u5747\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u6837\u672c\u590d\u6742\u5ea6\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u8054\u90a6Q\u5b66\u4e60\u3002", "motivation": "\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e3a\u957f\u671f\u51b3\u7b56\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u4f46Q\u5b66\u4e60\u5728\u5e73\u5747\u5956\u52b1MDP\u4e2d\u7684\u7406\u8bba\u4fdd\u8bc1\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5e73\u5747\u5956\u52b1MDP\u7684Q\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684Q\u5b66\u4e60\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u9650\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f31\u901a\u4fe1\u5e73\u5747\u5956\u52b1MDP\u3002\u5728\u5355\u667a\u80fd\u4f53\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u53c2\u6570\u5b9e\u73b0\u4f18\u5316\uff1b\u5728\u8054\u90a6\u573a\u666f\u4e0b\uff0c\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7b97\u6cd5\uff0c\u51cf\u5c11\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u6837\u672c\u9700\u6c42\u3002", "result": "\u5355\u667a\u80fd\u4f53\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{\\varepsilon^3}\\right)$\uff0c\u76f8\u6bd4\u5148\u524d\u7ed3\u679c\u81f3\u5c11\u6539\u8fdb$\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}^2}{\\varepsilon^2}$\u500d\u3002\u8054\u90a6\u573a\u666f\u4e0b\u6bcf\u4e2a\u667a\u80fd\u4f53\u6837\u672c\u590d\u6742\u5ea6\u964d\u81f3$\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{M\\varepsilon^3}\\right)$\uff0c\u4ec5\u9700$\\widetilde{O}\\left(\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}}{\\varepsilon}\\right)$\u8f6e\u901a\u4fe1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5efa\u7acb\u4e86\u5e73\u5747\u5956\u52b1MDP\u7684\u8054\u90a6Q\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u6837\u672c\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u590d\u6742\u5ea6\u65b9\u9762\u5747\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6548\u7387\uff0c\u4e3a\u957f\u671f\u51b3\u7b56\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.12423", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12423", "abs": "https://arxiv.org/abs/2601.12423", "authors": ["Antonin Clerc", "Michael Quellmalz", "Moritz Piening", "Philipp Flotho", "Gregor Kornhardt", "Gabriele Steidl"], "title": "HOT-POT: Optimal Transport for Sparse Stereo Matching", "comment": "18 pages, 10 figures, 6 tables", "summary": "Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u975e\u76d1\u7763\u7a00\u758f\u7279\u5f81\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u673a\u51e0\u4f55\u7684\u7ebf\u7ea6\u675f\u6765\u89e3\u51b3\u7acb\u4f53\u89c6\u89c9\u4e2d\u7684\u5339\u914d\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9762\u90e8\u6807\u5fd7\u70b9\u5339\u914d\u7b49\u5e94\u7528\u3002", "motivation": "\u7acb\u4f53\u89c6\u89c9\u5728\u56fe\u50cf\u95f4\u9762\u4e34\u906e\u6321\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u7578\u53d8\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u7a00\u758f\u7279\u5f81\uff08\u5982\u9762\u90e8\u6807\u5fd7\u70b9\uff09\u8fdb\u884c\u7acb\u4f53\u5339\u914d\u65f6\uff0c\u7531\u4e8e\u53c2\u6570\u654f\u611f\u6027\u5bfc\u81f4\u95ee\u9898\u66f4\u52a0\u590d\u6742\u3002\u9700\u8981\u514b\u670d\u8fd9\u79cd\u4e0d\u9002\u5b9a\u6027\u5e76\u5b9e\u73b0\u975e\u76d1\u7763\u7a00\u758f\u5339\u914d\u3002", "method": "\u4ece\u6700\u4f18\u4f20\u8f93\u89c6\u89d2\u8003\u8651\u76f8\u673a\u51e0\u4f55\u7684\u7ebf\u7ea6\u675f\uff0c\u5c06\u76f8\u673a\u6295\u5f71\u70b9\u5efa\u6a21\u4e3a\uff08\u534a\uff09\u7ebf\uff0c\u63d0\u51fa\u4f7f\u7528\u7ecf\u5178\u6781\u7ebf\u8ddd\u79bb\u548c3D\u5c04\u7ebf\u8ddd\u79bb\u6765\u91cf\u5316\u5339\u914d\u8d28\u91cf\u3002\u5c06\u8fd9\u4e9b\u8ddd\u79bb\u4f5c\u4e3a\uff08\u90e8\u5206\uff09\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u6210\u672c\u51fd\u6570\uff0c\u5f97\u5230\u53ef\u9ad8\u6548\u6c42\u89e3\u7684\u5206\u914d\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u76d1\u7763\u5bf9\u8c61\u5339\u914d\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u5206\u5c42\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u8fdb\u884c\u7279\u5f81\u548c\u5bf9\u8c61\u5339\u914d\uff0c\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u7279\u522b\u5173\u6ce8\u9762\u90e8\u5206\u6790\u5e94\u7528\uff0c\u65e8\u5728\u5339\u914d\u4e0d\u540c\u7684\u6807\u5fd7\u70b9\u6807\u6ce8\u89c4\u8303\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u6846\u67b6\uff0c\u5229\u7528\u76f8\u673a\u51e0\u4f55\u7ea6\u675f\u89e3\u51b3\u4e86\u7a00\u758f\u7279\u5f81\u5339\u914d\u7684\u6311\u6218\uff0c\u4e3a\u9762\u90e8\u5206\u6790\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u975e\u76d1\u7763\u5339\u914d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13874", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13874", "abs": "https://arxiv.org/abs/2601.13874", "authors": ["Shijie Zhong", "Jiangfeng Fu", "Yikun Yang"], "title": "Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses", "comment": null, "summary": "The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\\mathcal O(n^2)$ to $\\mathcal O(n \\log n)$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7edf\u8ba1\u91cf\u7684\u65b9\u5dee\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u6709\u754c\u6837\u672c\u65b9\u5dee\u8868\u5f81\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u62c9\u666e\u62c9\u65af\u6838\u4e0b\u7684\u7cbe\u786e\u52a0\u901f\u7b97\u6cd5\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(n log n)\u3002", "motivation": "MMD\u662f\u57fa\u4e8e\u6838\u7684\u975e\u53c2\u6570\u53cc\u6837\u672c\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5176\u63a8\u65ad\u51c6\u786e\u6027\u4e25\u91cd\u4f9d\u8d56\u4e8e\u65b9\u5dee\u8868\u5f81\u3002\u73b0\u6709\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u79cdMMD\u65b9\u5dee\u7684\u6709\u9650\u6837\u672c\u4f30\u8ba1\u5668\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u96f6\u5047\u8bbe\u548c\u5907\u62e9\u5047\u8bbe\u4e0b\u3001\u4ee5\u53ca\u5728\u5e73\u8861\u6216\u4e0d\u5e73\u8861\u91c7\u6837\u65b9\u6848\u4e2d\u5f80\u5f80\u5b58\u5728\u5dee\u5f02\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7MMD\u7edf\u8ba1\u91cf\u7684U\u7edf\u8ba1\u91cf\u8868\u793a\u548cHoeffding\u5206\u89e3\u6765\u7814\u7a76\u5176\u65b9\u5dee\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u8986\u76d6\u4e0d\u540c\u5047\u8bbe\u548c\u6837\u672c\u914d\u7f6e\u7684\u7edf\u4e00\u6709\u9650\u6837\u672c\u8868\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u9488\u5bf9\u5355\u53d8\u91cf\u60c5\u51b5\u548c\u62c9\u666e\u62c9\u65af\u6838\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u52a0\u901f\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684MMD\u65b9\u5dee\u6709\u9650\u6837\u672c\u8868\u5f81\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5047\u8bbe\u548c\u6837\u672c\u914d\u7f6e\u3002\u63d0\u51fa\u7684\u52a0\u901f\u7b97\u6cd5\u5c06\u5355\u53d8\u91cf\u62c9\u666e\u62c9\u65af\u6838\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u663e\u8457\u964d\u4f4e\u5230O(n log n)\u3002", "conclusion": "\u672c\u6587\u4e3aMMD\u65b9\u5dee\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u7cbe\u786e\u52a0\u901f\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u53cc\u6837\u672c\u68c0\u9a8c\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u7b97\u6cd5\u4f18\u5316\u3002"}}
{"id": "2601.14053", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14053", "abs": "https://arxiv.org/abs/2601.14053", "authors": ["Badri N. Patro", "Vijay S. Agneeswaran"], "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems", "comment": null, "summary": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.", "AI": {"tldr": "LLMOrbit\u63d0\u51fa\u4e00\u4e2a2019-2025\u5e74\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u5206\u679050\u591a\u4e2a\u6a21\u578b\uff0c\u8bc6\u522b\u4e09\u5927\u5371\u673a\uff08\u6570\u636e\u7a00\u7f3a\u3001\u6210\u672c\u6307\u6570\u589e\u957f\u3001\u80fd\u8017\u4e0d\u53ef\u6301\u7eed\uff09\u548c\u516d\u5927\u7a81\u7834\u8303\u5f0f\uff0c\u63ed\u793a\u4e09\u5927\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740AI\u4ece\u57fa\u7840Transformer\u67b6\u6784\u53d1\u5c55\u5230\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u8109\u7edc\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5168\u9762\u7684\u5206\u7c7b\u6cd5\u5bfc\u822aLLM\u9886\u57df\uff0c\u8bc6\u522b\u9650\u5236\u66b4\u529b\u6269\u5c55\u7684\"\u6269\u5c55\u5899\"\u5371\u673a\uff0c\u5e76\u63a2\u7d22\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u7684\u521b\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51faLLMOrbit\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u516b\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8f68\u9053\u7ef4\u5ea6\u5206\u67902019-2025\u5e74\u95f4\u8d85\u8fc750\u4e2a\u6a21\u578b\u548c15\u4e2a\u7ec4\u7ec7\u3002\u91c7\u7528\u591a\u7ef4\u5ea6\u5206\u6790\u65b9\u6cd5\uff0c\u6db5\u76d6\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u6548\u7387\u6a21\u5f0f\u7b49\uff0c\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u73b0\u4ee3LLM\u3001\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u5927\u5173\u952e\u5371\u673a\uff1a1) \u6570\u636e\u7a00\u7f3a\uff082026-2028\u5e74\u5c06\u8017\u5c3d9-27T tokens\uff09\uff1b2) \u6210\u672c\u6307\u6570\u589e\u957f\uff085\u5e74\u5185\u4ece300\u4e07\u7f8e\u5143\u589e\u81f33\u4ebf\u7f8e\u5143\u4ee5\u4e0a\uff09\uff1b3) \u4e0d\u53ef\u6301\u7eed\u7684\u80fd\u8017\uff08\u589e\u957f22\u500d\uff09\u3002\u53d1\u73b0\u516d\u5927\u7a81\u7834\u8303\u5f0f\uff1a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u91cf\u5316\u3001\u5206\u5e03\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u3001\u6a21\u578b\u878d\u5408\u3001\u9ad8\u6548\u8bad\u7ec3\u3001\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u3002\u63ed\u793a\u4e09\u5927\u8303\u5f0f\u8f6c\u53d8\uff1a\u540e\u8bad\u7ec3\u589e\u76ca\u3001\u6548\u7387\u9769\u547d\u3001\u6c11\u4e3b\u5316\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u9886\u57df\u6b63\u9762\u4e34\u6269\u5c55\u5899\u7684\u4e25\u5cfb\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u6548\u7387\u4f18\u5316\u548c\u5f00\u6e90\u6c11\u4e3b\u5316\u7b49\u521b\u65b0\u8303\u5f0f\uff0c\u53ef\u4ee5\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002\u672a\u6765\u7684\u53d1\u5c55\u5c06\u4ece\u88ab\u52a8\u751f\u6210\u8f6c\u5411\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\uff0c\u540e\u8bad\u7ec3\u6280\u672f\u548c\u6548\u7387\u9769\u547d\u5c06\u6210\u4e3a\u5173\u952e\u9a71\u52a8\u529b\uff0c\u5f00\u6e90\u6a21\u578b\u5df2\u5f00\u59cb\u8d85\u8d8a\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.12432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.12432", "abs": "https://arxiv.org/abs/2601.12432", "authors": ["Shunyu Huang", "Yunjiao Zhou", "Jianfei Yang"], "title": "SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition", "comment": "Published in IEEE Internet of Things Journal", "summary": "Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.", "code_url": "https://github.com/Huang0035/Skef", "AI": {"tldr": "SkeFi\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u4e30\u5bcc\u7684RGB\u6a21\u6001\u6765\u589e\u5f3a\u65e0\u7ebf\u4f20\u611f\u5668\uff08LiDAR\u548cmmWave\uff09\u7684\u9aa8\u67b6\u4f30\u8ba1\u548c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u9ed1\u6697\u73af\u5883\u548c\u9690\u79c1\u9650\u5236\u4e0b\u7684\u52a8\u4f5c\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRGB\u6444\u50cf\u5934\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5728\u9ed1\u6697\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u4e14\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u667a\u80fd\u5bb6\u5c45\u548c\u533b\u9662\u7b49\u573a\u666f\u7684\u5e94\u7528\u3002\u65e0\u7ebf\u4f20\u611f\u5668\uff08LiDAR\u548cmmWave\uff09\u4f5c\u4e3a\u975e\u4fb5\u5165\u5f0f\u66ff\u4ee3\u65b9\u6848\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u65e0\u7ebf\u4f20\u611f\u5668\u6a21\u6001\u6570\u636e\u4e0d\u8db3\u96be\u4ee5\u8bad\u7ec3\u51c6\u786e\u7684\u9aa8\u67b6\u4f30\u8ba1\u6a21\u578b\uff1b2\uff09\u65e0\u7ebf\u4f20\u611f\u5668\u63d0\u53d6\u7684\u9aa8\u67b6\u5173\u952e\u70b9\u566a\u58f0\u66f4\u5927\uff0c\u5f71\u54cd\u540e\u7eed\u52a8\u4f5c\u8bc6\u522b\u3002", "method": "\u63d0\u51faSkeFi\u6846\u67b6\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u4ece\u6570\u636e\u4e30\u5bcc\u7684RGB\u6a21\u6001\u83b7\u53d6\u77e5\u8bc6\u3002\u6838\u5fc3\u5305\u62ec\uff1a1\uff09\u589e\u5f3a\u7684\u65f6\u95f4\u76f8\u5173\u6027\u81ea\u9002\u5e94\u56fe\u5377\u79ef\uff08TC-AGC\uff09\u7ed3\u5408\u5e27\u4ea4\u4e92\u589e\u5f3a\uff0c\u5904\u7406\u7f3a\u5931\u6216\u4e0d\u8fde\u7eed\u5e27\u7684\u566a\u58f0\uff1b2\uff09\u53cc\u91cd\u65f6\u95f4\u5377\u79ef\u589e\u5f3a\u591a\u5c3a\u5ea6\u65f6\u95f4\u5efa\u6a21\uff1b3\uff09\u5c06TC-AGC\u4e0e\u65f6\u95f4\u5efa\u6a21\u96c6\u6210\u7528\u4e8e\u8de8\u6a21\u6001\u8f6c\u79fb\uff0c\u4ece\u566a\u58f0\u65e0\u7ebf\u4f20\u611f\u5668\u4e2d\u63d0\u53d6\u51c6\u786e\u59ff\u6001\u548c\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSkeFi\u5728mmWave\u548cLiDAR\u4f20\u611f\u5668\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u65e0\u7ebf\u4f20\u611f\u5668\u6a21\u6001\u4e0a\u63d0\u53d6\u51c6\u786e\u59ff\u6001\u548c\u52a8\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "SkeFi\u901a\u8fc7\u521b\u65b0\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u7ebf\u4f20\u611f\u5668\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u6570\u636e\u4e0d\u8db3\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u9ed1\u6697\u73af\u5883\u548c\u9690\u79c1\u654f\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u975e\u4fb5\u5165\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5728mmWave\u548cLiDAR\u4f20\u611f\u5668\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.12443", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12443", "abs": "https://arxiv.org/abs/2601.12443", "authors": ["Xiaowei Fu", "Lei Zhang"], "title": "Adversarial Defense in Vision-Language Models: An Overview", "comment": null, "summary": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u6297\u6027\u9632\u5fa1\u7684\u4e09\u5927\u8303\u5f0f\uff1a\u8bad\u7ec3\u65f6\u9632\u5fa1\u3001\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\u548c\u514d\u8bad\u7ec3\u9632\u5fa1\uff0c\u5206\u6790\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u53ca\u5f53\u524d\u6311\u6218\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5bf9\u6297\u6027\u653b\u51fb\u8106\u5f31\u6027\u5f15\u53d1\u4e86\u5b89\u5168\u62c5\u5fe7\u3002\u8fd9\u4e9b\u653b\u51fb\u53ef\u80fd\u635f\u5bb3\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u548c\u7cfb\u7edf\u5b89\u5168\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u672c\u6587\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u4e09\u79cd\u4e3b\u8981\u9632\u5fa1\u8303\u5f0f\uff1a1\uff09\u8bad\u7ec3\u65f6\u9632\u5fa1\uff08\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u589e\u5f3a\u9c81\u68d2\u6027\uff09\uff1b2\uff09\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\uff08\u5728\u63a8\u7406\u65f6\u66f4\u65b0\u53c2\u6570\u5904\u7406\u5bf9\u6297\u6837\u672c\uff09\uff1b3\uff09\u514d\u8bad\u7ec3\u9632\u5fa1\uff08\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u6216\u7279\u5f81\u5d4c\u5165\u6765\u7f13\u89e3\u653b\u51fb\u5f71\u54cd\uff09\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u5404\u7c7b\u9632\u5fa1\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff1a\u8bad\u7ec3\u65f6\u9632\u5fa1\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\u7075\u6d3b\u4f46\u589e\u52a0\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff1b\u514d\u8bad\u7ec3\u9632\u5fa1\u65e0\u9700\u4fee\u6539\u6a21\u578b\u4f46\u6548\u679c\u53ef\u80fd\u53d7\u9650\u3002\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u9632\u5fa1\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u4f46\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672a\u6765\u9700\u8981\u66f4\u9ad8\u6548\u3001\u901a\u7528\u4e14\u5b9e\u7528\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u5bf9\u6297\u6027\u653b\u51fb\u5a01\u80c1\u3002"}}
{"id": "2601.14099", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14099", "abs": "https://arxiv.org/abs/2601.14099", "authors": ["Shi-Shun Chen", "Xiao-Yang Li", "Enrico Zio"], "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping", "comment": null, "summary": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.", "code_url": "https://github.com/dirge1/TDPCM", "code_stars": 1, "code_last_update": "2026-01-19", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u7684\u56e0\u679c\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u89e3\u51b3\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u53d8\u91cf\u65f6\u6ede\u56e0\u679c\u548c\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u8f6f\u6d4b\u91cf\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u56e0\u679c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u5de5\u4e1a\u8fc7\u7a0b\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a1) \u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5b58\u5728\u65f6\u6ede\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u540c\u4e00\u65f6\u95f4\u7ef4\u5ea6\u5206\u6790\u56e0\u679c\u5173\u7cfb\uff1b2) \u5de5\u4e1a\u8fc7\u7a0b\u53d8\u91cf\u76f8\u4e92\u4f9d\u8d56\uff0c\u4e0e\u4f20\u7edf\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u7684\u53bb\u76f8\u5173\u5047\u8bbe\u76f8\u77db\u76fe\u3002\u8fd9\u5bfc\u81f4\u57fa\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8f6f\u6d4b\u91cf\u6a21\u578b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u7684\u56e0\u679c\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff1a1) \u4f7f\u7528\u65f6\u6ede\u6536\u655b\u4ea4\u53c9\u6620\u5c04(TDCCM)\u8fdb\u884c\u603b\u56e0\u679c\u63a8\u65ad\uff1b2) \u4f7f\u7528\u65f6\u6ede\u504f\u4ea4\u53c9\u6620\u5c04(TDPCM)\u8fdb\u884c\u76f4\u63a5\u56e0\u679c\u63a8\u65ad\uff1b3) \u63d0\u51fa\u5ba2\u89c2\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u9a8c\u8bc1\u96c6\u6a21\u578b\u6027\u80fd\u81ea\u52a8\u786e\u5b9a\u56e0\u679c\u9608\u503c\u5e76\u9009\u62e9\u56e0\u679c\u7279\u5f81\u3002", "result": "\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff1aTDCCM\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6027\u80fd\uff0c\u800cTDPCM\u5728\u6700\u5dee\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u8f6f\u6d4b\u91cf\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u5728GitHub\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u53d8\u91cf\u65f6\u6ede\u56e0\u679c\u548c\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u6d4b\u91cf\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5de5\u4e1a\u8fc7\u7a0b\u76d1\u63a7\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u56e0\u679c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002"}}
{"id": "2601.12464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12464", "abs": "https://arxiv.org/abs/2601.12464", "authors": ["Yanrui Lu", "Danyang Chen", "Haowen Xiao", "Jiarui Zhu", "Fukang Ge", "Binqian Zou", "Jiali Guan", "Jiayin Liang", "Yuting Wang", "Ziqian Guan", "Xiangcheng Bao", "Jinhao Bi", "Lin Gu", "Jun He", "Yingying Zhu"], "title": "Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild", "comment": null, "summary": "Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6765\u6e90\u7684\u7535\u5b50\u663e\u5fae\u955c\u591a\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u5f202D EM\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u7ec6\u80de\u7c7b\u578b\u548c\u4e94\u4e2a\u7ec6\u80de\u5668\u7c7b\u522b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754cEM\u6570\u636e\u5f02\u8d28\u6027\u548c\u5927\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5c0f\u578b\u3001\u7cbe\u9009\u6570\u636e\u96c6\u7684\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754cEM\u6570\u636e\u56fa\u6709\u7684\u5f02\u8d28\u6027\u548c\u5927\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u8fd9\u7ed9\u57fa\u4e8e\u5c40\u90e8\u8865\u4e01\u7684\u65b9\u6cd5\u5e26\u6765\u4e86\u6839\u672c\u6027\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6765\u6e90\u7684\u591a\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u5f202D EM\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u7ec6\u80de\u7c7b\u578b\u548c\u4e94\u4e2a\u7ec6\u80de\u5668\u7c7b\u522b\u3002\u4f7f\u7528\u8bbe\u8ba1\u7684\u8fde\u901a\u6027\u611f\u77e5\u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5\uff083D LPA\uff09\u751f\u6210\u6570\u636e\u96c6\u6807\u6ce8\uff0c\u5e76\u7ecf\u8fc7\u4e13\u5bb6\u7cbe\u4fee\u3002\u8fdb\u4e00\u6b65\u5bf9\u5305\u62ecU-Net\u3001SAM\u53d8\u4f53\u548cMask2Former\u5728\u5185\u7684\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5b58\u5728\u591a\u4e2a\u5c40\u9650\u6027\uff1a\u5728\u5f02\u8d28EM\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bf9\u5177\u6709\u5168\u5c40\u3001\u5206\u5e03\u5f62\u6001\u7684\u7ec6\u80de\u5668\uff08\u5982\u5185\u8d28\u7f51\uff09\u8868\u73b0\u8f83\u5dee\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5c40\u90e8\u4e0a\u4e0b\u6587\u6a21\u578b\u4e0e\u5728\u771f\u5b9e\u4e16\u754c\u53d8\u5f02\u6027\u5b58\u5728\u4e0b\u5efa\u6a21\u957f\u7a0b\u7ed3\u6784\u8fde\u7eed\u6027\u6311\u6218\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u957f\u7a0b\u7ed3\u6784\u8fde\u7eed\u6027\u548c\u771f\u5b9e\u4e16\u754c\u53d8\u5f02\u6027\u7684\u65b0\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u7684\u6839\u672c\u6311\u6218\u3002\u8be5\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6807\u6ce8\u5de5\u5177\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.12481", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.12481", "abs": "https://arxiv.org/abs/2601.12481", "authors": ["Vanessa Sklyarova", "Berna Kabadayi", "Anastasios Yiannakidis", "Giorgio Becherini", "Michael J. Black", "Justus Thies"], "title": "NeuralFur: Animal Fur Reconstruction From Multi-View Images", "comment": "For additional results and code, please refer to https://neuralfur.is.tue.mpg.de", "summary": "Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u52a8\u7269\u6bdb\u53d1\u4e09\u7ef4\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u6bdb\u53d1\u957f\u5ea6\u548c\u65b9\u5411\uff0c\u5b9e\u73b0\u591a\u79cd\u52a8\u7269\u6bdb\u53d1\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa", "motivation": "\u4ece\u56fe\u50cf\u91cd\u5efa\u903c\u771f\u52a8\u7269\u6bdb\u53d1\u51e0\u4f55\u9762\u4e34\u6311\u6218\uff1a\u6bdb\u53d1\u7ec6\u8282\u7cbe\u7ec6\u3001\u81ea\u906e\u6321\u3001\u89c6\u89d2\u4f9d\u8d56\u5916\u89c2\uff0c\u4e14\u7f3a\u4e4f\u53ef\u7528\u4e8e\u5b66\u4e60\u4e0d\u540c\u52a8\u7269\u6bdb\u53d1\u5148\u9a8c\u7684\u6570\u636e\u96c6", "method": "1) \u4f7f\u7528\u4f20\u7edf\u591a\u89c6\u89d2\u7acb\u4f53\u6280\u672f\u91cd\u5efa\u7c97\u7cd9\u8868\u9762\u51e0\u4f55\uff1b2) \u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u7d22\u8eab\u4f53\u5404\u90e8\u4f4d\u6bdb\u53d1\u957f\u5ea6\u7ed3\u6784\u4fe1\u606f\uff1b3) \u6784\u5efa\u65e0\u6bdb\u51e0\u4f55\u5e76\u5728\u5176\u4e0a\u751f\u957f\u6bdb\u53d1\uff1b4) \u4f7f\u7528\u51e0\u4f55\u548c\u5149\u5ea6\u635f\u5931\u76d1\u7763\u91cd\u5efa\uff1b5) \u5229\u7528VLM\u6307\u5bfc\u6bdb\u53d1\u751f\u957f\u65b9\u5411\u548c\u91cd\u529b\u5411\u91cf\u5173\u7cfb", "result": "\u5c55\u793a\u4e86\u5728\u591a\u79cd\u4e0d\u540c\u6bdb\u53d1\u7c7b\u578b\u52a8\u7269\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u4e09\u7ef4\u6bdb\u53d1\u91cd\u5efa", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u591a\u89c6\u89d2\u8f93\u5165\u4e09\u7ef4\u91cd\u5efa\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u8de8\u591a\u79cd\u52a8\u7269\u6bdb\u53d1\u7c7b\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6bdb\u53d1\u5efa\u6a21"}}
{"id": "2601.14175", "categories": ["cs.LG", "cs.AI", "cs.CL", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.14175", "abs": "https://arxiv.org/abs/2601.14175", "authors": ["Suvrat Raju", "Praneeth Netrapalli"], "title": "A model of errors in transformers", "comment": "8+17pages", "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u9700\u8981\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e0a\u7684\u9519\u8bef\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u8bef\u5dee\u79ef\u7d2f\u7684\u4e24\u53c2\u6570\u6a21\u578b\u6765\u91cf\u5316\u9884\u6d4b\u51c6\u786e\u7387\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76LLM\u5728\u9700\u8981\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e0a\u7684\u9519\u8bef\u7387\uff0c\u6311\u6218\u4e86\u73b0\u6709\u5173\u4e8eLLM\u5728\u957f\u91cd\u590d\u4efb\u52a1\u4e0a\u9519\u8bef\u6e90\u4e8e\"\u63a8\u7406\u5d29\u6e83\"\u6216\u65e0\u6cd5\u8868\u8fbe\"\u7ec4\u5408\u51fd\u6570\"\u7684\u89c2\u70b9\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8bef\u5dee\u5206\u6790\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\"\u6709\u6548\u573a\u8bba\"\u89c6\u89d2\uff0c\u5c06LLM\u7684\u4f17\u591a\u53c2\u6570\u91cd\u7ec4\u4e3a\u4e24\u4e2a\u5173\u952e\u53c2\u6570\uff1a\u57fa\u672c\u566a\u58f0\u7387\u548c\u53ef\u80fd\u9519\u8bef\u9884\u6d4b\u7684\u4ee4\u724c\u6570\u91cf\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5fae\u5c0f\u8bef\u5dee\u79ef\u7d2f\u8d85\u8fc7\u9608\u503c\u7684\u673a\u5236\uff0c\u63a8\u5bfc\u51fa\u51c6\u786e\u7387\u4e0e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u5b9a\u91cf\u5173\u7cfb\u3002\u4f7f\u7528Gemini 2.5 Flash\u3001Gemini 2.5 Pro\u548cDeepSeek R1\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u5b9e\u8bc1\u6d4b\u8bd5\u663e\u793a\uff0c\u5bf9\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u4e0e\u89c2\u6d4b\u51c6\u786e\u7387\u4e4b\u95f4\u5177\u6709\u6781\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e5f\u53d1\u73b0\u4e86\u504f\u5dee\u3002\u4e24\u53c2\u6570\u6a21\u578b\u80fd\u591f\u6709\u6548\u63cf\u8ff0LLM\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e0a\u7684\u9519\u8bef\u7387\u3002", "conclusion": "LLM\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e0a\u7684\u9519\u8bef\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8bef\u5dee\u79ef\u7d2f\u7684\u7b80\u5355\u4e24\u53c2\u6570\u6a21\u578b\u89e3\u91ca\uff0c\u8fd9\u4e3a\u7406\u89e3LLM\u9519\u8bef\u63d0\u4f9b\u4e86\u66ff\u4ee3\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u964d\u4f4e\u9519\u8bef\u7387\u3002"}}
{"id": "2601.12493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12493", "abs": "https://arxiv.org/abs/2601.12493", "authors": ["Mehrdad Noori", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Fereshteh Shakeri", "Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation", "comment": "Accepted to WACV 2026", "summary": "Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.", "code_url": "https://github.com/Mehrdad-Noori/Histopath-C", "code_stars": 1, "code_last_update": "2025-09-20", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Histopath-C\u57fa\u51c6\u6d4b\u8bd5\u548cLATTE\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u548c\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u7b49\u533b\u5b66\u5f71\u50cf\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5b58\u5728\u67d3\u8272\u3001\u6c61\u67d3\u3001\u6a21\u7cca\u548c\u566a\u58f0\u7b49\u4e25\u91cd\u9886\u57df\u504f\u79fb\uff0c\u8fd9\u4e9b\u504f\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u4e0b\u6e38\u6027\u80fd\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u8fd9\u4e9b\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\u548c\u9002\u5e94\u7b56\u7565\u3002", "method": "1. \u63d0\u51faHistopath-C\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6a21\u62df\u6570\u5b57\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u771f\u5b9e\u5206\u5e03\u504f\u79fb\u7684\u5408\u6210\u635f\u574f\uff1b2. \u5f00\u53d1\u52a8\u6001\u6846\u67b6\uff0c\u53ef\u5c06\u635f\u574f\u5e94\u7528\u4e8e\u4efb\u4f55\u53ef\u7528\u6570\u636e\u96c6\u5e76\u5b9e\u65f6\u8bc4\u4f30\u6d4b\u8bd5\u65f6\u9002\u5e94\u673a\u5236\uff1b3. \u63d0\u51faLATTE\u65b9\u6cd5\uff0c\u4e00\u79cd\u5229\u7528\u591a\u4e2a\u6587\u672c\u6a21\u677f\u7684\u8f6c\u5bfc\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\uff0c\u51cf\u8f7b\u7ec4\u7ec7\u75c5\u7406\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u6837\u5316\u6587\u672c\u8f93\u5165\u7684\u654f\u611f\u6027\u3002", "result": "LATTE\u65b9\u6cd5\u5728\u591a\u4e2a\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4e3a\u81ea\u7136\u56fe\u50cf\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u8bbe\u8ba1\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u9c81\u68d2\u9002\u5e94\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ec4\u7ec7\u75c5\u7406\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u51fa\u7684LATTE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u5347\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2601.14196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14196", "abs": "https://arxiv.org/abs/2601.14196", "authors": ["Albina Galiullina", "Wouter van Heeswijk", "Tom van Woensel"], "title": "Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery", "comment": null, "summary": "Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\uff08DPO\uff09\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u987e\u5ba2\u63a8\u8350\u5355\u4e00\u53d6\u8d27\u70b9\u800c\u975e\u63d0\u4f9b\u65e0\u9650\u5236\u9009\u62e9\uff0c\u6765\u8054\u5408\u51cf\u5c11\u914d\u9001\u5361\u8f66\u8def\u7ebf\u548c\u987e\u5ba2\u51fa\u884c\u7684\u78b3\u6392\u653e\u3002", "motivation": "\u53d6\u8d27\u70b9\u4f5c\u4e3a\u5bb6\u5ead\u914d\u9001\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u8ba2\u5355\u6574\u5408\u53ef\u4ee5\u7f29\u77ed\u914d\u9001\u8def\u7ebf\u5e76\u63d0\u9ad8\u9996\u6b21\u6295\u9012\u6210\u529f\u7387\u3002\u7136\u800c\uff0c\u5f53\u987e\u5ba2\u9a7e\u8f66\u53d6\u8d27\u65f6\uff0c\u8fd9\u4e9b\u73af\u5883\u6548\u76ca\u53ef\u80fd\u4f1a\u88ab\u62b5\u6d88\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7b56\u7565\u6765\u540c\u65f6\u51cf\u5c11\u914d\u9001\u5361\u8f66\u548c\u987e\u5ba2\u51fa\u884c\u7684\u78b3\u6392\u653e\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u8bbe\u8ba1DPO\u7b56\u7565\uff0c\u8003\u8651\u987e\u5ba2\u4e0e\u53d6\u8d27\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u53ca\u5176\u5bf9\u672a\u6765\u8def\u7ebf\u6574\u5408\u7684\u5f71\u54cd\u3002\u5728\u52a8\u6001\u968f\u673a\u73af\u5883\u4e2d\uff0c\u4e3a\u6bcf\u4e2a\u5230\u8fbe\u7684\u987e\u5ba2\u63d0\u4f9b\u5355\u4e00\u63a8\u8350\u53d6\u8d27\u70b9\uff0c\u540c\u65f6\u4fdd\u7559\u5bb6\u5ead\u914d\u9001\u9009\u9879\u3002", "result": "\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u603b\u78b3\u6392\u653e\u91cf\uff1a\u76f8\u5bf9\u4e8e\u7eaf\u5bb6\u5ead\u914d\u9001\u6700\u591a\u51cf\u5c119%\u6392\u653e\uff0c\u5e73\u5747\u6bd4\u66ff\u4ee3\u7b56\u7565\uff08\u5305\u62ec\u65e0\u9650\u5236\u53d6\u8d27\u70b9\u9009\u62e9\u548c\u6700\u8fd1\u53d6\u8d27\u70b9\u5206\u914d\uff09\u51cf\u5c112%\u3002\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u5f53\u53d6\u8d27\u70b9\u591a\u4e14\u8ddd\u79bb\u77ed\u65f6\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u603b\u78b3\u6392\u653e\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u3002\u5f53\u987e\u5ba2\u5bf9\u53d6\u8d27\u70b9\u914d\u9001\u7684\u63a5\u53d7\u5ea6\u8f83\u4f4e\u65f6\uff0c\u660e\u786e\u8003\u8651\u987e\u5ba2\u5230\u8fbe\u548c\u9009\u62e9\u7684\u52a8\u6001\u7279\u6027\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2601.12500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12500", "abs": "https://arxiv.org/abs/2601.12500", "authors": ["Yaowu Fan", "Jia Wan", "Tao Han", "Andy J. Ma", "Antoni B. Chan"], "title": "Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods", "comment": null, "summary": "Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u79fb\u52a8\u65e0\u4eba\u673a\u7684\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u8ba1\u6570\u4e0e\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u65b0\u6570\u636e\u96c6MovingDroneCrowd++\u548c\u4e24\u4e2a\u65b0\u65b9\u6cd5\uff1aGD3A\u7528\u4e8e\u89c6\u9891\u7ea7\u4eba\u7fa4\u8ba1\u6570\uff0cDVTrack\u7528\u4e8e\u884c\u4eba\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fa\u5b9a\u6444\u50cf\u5934\u6570\u636e\u96c6\uff0c\u7a7a\u95f4\u8986\u76d6\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u5206\u6790\u9700\u6c42\u3002\u79fb\u52a8\u65e0\u4eba\u673a\u80fd\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89c6\u89d2\u548c\u66f4\u5e7f\u7684\u8986\u76d6\u8303\u56f4\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1) \u63d0\u51faMovingDroneCrowd++\u6570\u636e\u96c6\uff0c\u5305\u542b\u79fb\u52a8\u65e0\u4eba\u673a\u5728\u4e0d\u540c\u9ad8\u5ea6\u3001\u89d2\u5ea6\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u6355\u83b7\u7684\u5bc6\u96c6\u4eba\u7fa4\u89c6\u9891\uff1b2) \u63d0\u51faGD3A\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u548c\u81ea\u9002\u5e94dustbin\u5206\u6570\u5efa\u7acb\u50cf\u7d20\u7ea7\u884c\u4eba\u63cf\u8ff0\u7b26\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c06\u5168\u5c40\u5bc6\u5ea6\u56fe\u5206\u89e3\u4e3a\u5171\u4eab\u3001\u6d41\u5165\u548c\u6d41\u51fa\u5206\u91cf\uff1b3) \u63d0\u51faDVTrack\u65b9\u6cd5\uff0c\u901a\u8fc7\u63cf\u8ff0\u7b26\u6295\u7968\u673a\u5236\u5c06\u63cf\u8ff0\u7b26\u7ea7\u5339\u914d\u8f6c\u6362\u4e3a\u5b9e\u4f8b\u7ea7\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5bc6\u96c6\u4eba\u7fa4\u548c\u590d\u6742\u8fd0\u52a8\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u6570\u8bef\u5dee\u964d\u4f4e47.4%\uff0c\u8ddf\u8e2a\u6027\u80fd\u63d0\u534739.2%\u3002", "conclusion": "\u79fb\u52a8\u65e0\u4eba\u673a\u4e3a\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u63d0GD3A\u548cDVTrack\u65b9\u6cd5\u5728\u79fb\u52a8\u65e0\u4eba\u673a\u6355\u83b7\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5bc6\u96c6\u4eba\u7fa4\u8ba1\u6570\u548c\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2601.14209", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14209", "abs": "https://arxiv.org/abs/2601.14209", "authors": ["Matthew Y. R. Yang", "Hao Bai", "Ian Wu", "Gene Yang", "Amrith Setlur", "Aviral Kumar"], "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning", "comment": null, "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.", "AI": {"tldr": "\u63d0\u51faIntervention Training (InT)\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u578b\u5bf9\u81ea\u8eab\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u8bc6\u522b\u9519\u8bef\u5e76\u63d0\u51fa\u5355\u6b65\u5e72\u9884\uff0c\u7136\u540e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u6700\u7ec8\u7b54\u6848\u5c42\u9762\u5206\u914d\u4fe1\u7528\uff0c\u5f53\u7ed3\u679c\u9519\u8bef\u65f6\u60e9\u7f5a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\uff0c\u7ed3\u679c\u6b63\u786e\u65f6\u5747\u5300\u5f3a\u5316\u6240\u6709\u6b65\u9aa4\u3002\u8fd9\u5bfc\u81f4\u6b63\u786e\u4e2d\u95f4\u6b65\u9aa4\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u88ab\u6291\u5236\uff0c\u800c\u865a\u5047\u6b65\u9aa4\u5728\u6210\u529f\u8f68\u8ff9\u4e2d\u88ab\u5f3a\u5316\uff0c\u5b58\u5728\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faIntervention Training (InT)\uff1a1) \u5229\u7528\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u901a\u5e38\u53ef\u7528\u7684\u53c2\u8003\u89e3\uff0c\u6a21\u578b\u8bc6\u522b\u81ea\u8eab\u63a8\u7406\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\uff1b2) \u63d0\u51fa\u5355\u6b65\u5e72\u9884\u5c06\u8f68\u8ff9\u8f6c\u5411\u6b63\u786e\u89e3\uff1b3) \u5bf9\u9519\u8bef\u70b9\u4e4b\u524d\u7684\u7b56\u7565\u5c55\u5f00\u4e0e\u5e72\u9884\u8fdb\u884c\u62fc\u63a5\uff0c\u7136\u540e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06\u9519\u8bef\u5b9a\u4f4d\u5230\u5bfc\u81f4\u5931\u8d25\u7684\u5177\u4f53\u6b65\u9aa4\u3002", "result": "\u57284B\u53c2\u6570\u57fa\u7840\u6a21\u578b\u4e0a\uff0c\u7ecf\u8fc7InT\u548c\u540e\u7eedRL\u5fae\u8c03\uff0c\u5728IMO-AnswerBench\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u8fd114%\uff0c\u8d85\u8fc7\u4e86gpt-oss-20b\u7b49\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "Intervention Training\u901a\u8fc7\u8ba9\u6a21\u578b\u5bf9\u81ea\u8eab\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u51c6RL\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u4e3aRL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.12512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12512", "abs": "https://arxiv.org/abs/2601.12512", "authors": ["Mohd Usama", "Belal Ahmad", "Faleh Menawer R Althiyabi"], "title": "Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images", "comment": "14 pages, 9 figures, 2 tables", "summary": "Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCycleGAN\u7684\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e0d\u540cMRI\u626b\u63cf\u4eea\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u53cc\u5411\u57df\u6620\u5c04", "motivation": "\u4e0d\u540c\u626b\u63cf\u4eea\u6216\u673a\u6784\u83b7\u53d6\u7684MRI\u56fe\u50cf\u5b58\u5728\u57df\u504f\u79fb\uff08\u786c\u4ef6\u3001\u534f\u8bae\u3001\u91c7\u96c6\u53c2\u6570\u5dee\u5f02\uff09\uff0c\u5bfc\u81f4\u5728\u6e90\u57df\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76ee\u6807\u57df\u6027\u80fd\u4e0b\u964d", "method": "\u57fa\u4e8eCycleGAN\u7684\u6a21\u578b\uff0c\u5229\u7528\u5faa\u73af\u4e00\u81f4\u6027\u5b66\u4e60\u6e90\u57df\u548c\u76ee\u6807\u57df\u95f4\u7684\u53cc\u5411\u6620\u5c04\uff0c\u7ed3\u5408\u5185\u5bb9\u548c\u5dee\u5f02\u635f\u5931\u4fdd\u6301\u56fe\u50cf\u89e3\u5256\u7ed3\u6784\u5b8c\u6574\u6027", "result": "\u5728\u591a\u4e2aMRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u65e0\u6807\u7b7e\u6570\u636e\u7684\u53cc\u5411\u57df\u9002\u5e94\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u57df\u76f8\u5173\u53d8\u5f02\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6539\u5584\u533b\u7597\u8bca\u65ad\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u4e00\u81f4\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790"}}
{"id": "2601.12527", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.12527", "abs": "https://arxiv.org/abs/2601.12527", "authors": ["Richard Liu", "Itai Lang", "Rana Hanocka"], "title": "Deep Feature Deformation Weights", "comment": null, "summary": "Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u6570\u636e\u9a71\u52a8\u8bed\u4e49\u5148\u9a8c\u4e0e\u4f20\u7edf\u7cbe\u786e\u63a7\u5236\u7684\u7f51\u683c\u53d8\u5f62\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u5b9e\u73b0\u5e73\u6ed1\u8bed\u4e49\u53d8\u5f62\u6743\u91cd\uff0c\u652f\u6301\u5b9e\u65f6\u8ba1\u7b97\u548c\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u5904\u7406\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u624b\u67c4\u7684\u7f51\u683c\u53d8\u5f62\u65b9\u6cd5\u9700\u8981\u7528\u6237\u9884\u5148\u77e5\u9053\u624b\u67c4\u7684\u7406\u60f3\u5206\u5e03\uff0c\u4e14\u624b\u67c4\u5230\u53d8\u5f62\u884c\u4e3a\u7684\u6620\u5c04\u4e0d\u76f4\u89c2\u3001\u975e\u8bed\u4e49\uff1b\u800c\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u7136\u80fd\u83b7\u5f97\u8bed\u4e49\u7f16\u8f91\uff0c\u4f46\u901f\u5ea6\u6162\u4e14\u4e0d\u7cbe\u786e\u3002\u9700\u8981\u4e00\u79cd\u878d\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u751f\u6210\u5e73\u6ed1\u8bed\u4e49\u53d8\u5f62\u6743\u91cd\uff0c\u65e0\u9700\u989d\u5916\u6b63\u5219\u5316\uff1b\u63d0\u51fa\u91cd\u5fc3\u7279\u5f81\u84b8\u998f\u7ba1\u9053\uff0c\u5229\u7528\u5f62\u72b6\u6e32\u67d3\u7684\u89c6\u89c9\u4fe1\u53f7\u6700\u5c0f\u5316\u84b8\u998f\u6210\u672c\uff1b\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u7ea6\u675f\u548c\u5c40\u90e8\u6027\u52a0\u6743\u4fdd\u7559\u4f20\u7edf\u65b9\u6cd5\u7279\u6027\uff1b\u652f\u6301\u8bed\u4e49\u5bf9\u79f0\u6027\u81ea\u52a8\u68c0\u6d4b\u548c\u4fdd\u6301\u3002", "result": "\u65b9\u6cd5\u80fd\u57281\u5206\u949f\u5185\u4e3a\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u8ba1\u7b97\u6743\u91cd\uff08\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\uff09\uff0c\u5728\u6d88\u8d39\u7ea7\u673a\u5668\u4e0a\u5b9e\u65f6\u5904\u7406\u767e\u4e07\u9762\u7f51\u683c\u53d8\u5f62\uff0c\u5b9e\u73b0\u8bed\u4e49\u90e8\u4ef6\u534f\u540c\u53d8\u5f62\u548c\u5bf9\u79f0\u4fdd\u6301\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u878d\u5408\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8bed\u4e49\u5148\u9a8c\u4e0e\u4f20\u7edf\u6846\u67b6\u7684\u7cbe\u786e\u63a7\u5236\u548c\u901f\u5ea6\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u5b9e\u73b0\u4e86\u7b80\u5355\u6709\u6548\u7684\u8bed\u4e49\u53d8\u5f62\u6743\u91cd\u8ba1\u7b97\uff0c\u4e3a\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u53d8\u5f62\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14234", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14234", "abs": "https://arxiv.org/abs/2601.14234", "authors": ["Qiyang Li", "Sergey Levine"], "title": "Q-learning with Adjoint Matching", "comment": "32 pages, 8 figures, 7 tables", "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.", "AI": {"tldr": "QAM\u662f\u4e00\u79cd\u65b0\u9896\u7684TD\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u89e3\u51b3\u4e86\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff1a\u9ad8\u6548\u4f18\u5316\u5177\u6709\u8868\u8fbe\u529b\u7684\u6269\u6563\u6216\u6d41\u5339\u914d\u7b56\u7565\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u53c2\u6570\u5316Q\u51fd\u6570\u7684\u4e00\u9636\u4fe1\u606f\u3002", "motivation": "\u8fde\u7eed\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5bf9\u5177\u6709\u8868\u8fbe\u529b\u7684\u6269\u6563\u6216\u6d41\u5339\u914d\u7b56\u7565\u8fdb\u884c\u9ad8\u6548\u4f18\u5316\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u4f7f\u7528\u4ef7\u503c\u4fe1\u606f\u800c\u4e22\u5f03\u68af\u5ea6\u4fe1\u606f\uff0c\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u65b9\u6cd5\u727a\u7272\u7b56\u7565\u8868\u8fbe\u6027\u6216\u5f15\u5165\u504f\u5dee\u3002\u76f4\u63a5\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5728\u6570\u503c\u4e0a\u4e0d\u7a33\u5b9a\u3002", "method": "QAM\u91c7\u7528\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u8fd1\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u5b83\u5c06\u8bc4\u8bba\u5bb6\u7684\u52a8\u4f5c\u68af\u5ea6\u8f6c\u6362\u4e3a\u9010\u6b65\u76ee\u6807\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u540c\u65f6\u5728\u6700\u4f18\u89e3\u5904\u63d0\u4f9b\u65e0\u504f\u4e14\u5177\u6709\u8868\u8fbe\u529b\u7684\u7b56\u7565\u3002\u7ed3\u5408\u65f6\u95f4\u5dee\u5206\u5907\u4efd\u8fdb\u884c\u8bc4\u8bba\u5bb6\u5b66\u4e60\u3002", "result": "QAM\u5728\u56f0\u96be\u3001\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u79bb\u7ebfRL\u8fd8\u662f\u79bb\u7ebf\u5230\u5728\u7ebfRL\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "QAM\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u53c8\u4fdd\u6301\u7b56\u7565\u8868\u8fbe\u6027\u7684\u7a33\u5b9a\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2601.12530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12530", "abs": "https://arxiv.org/abs/2601.12530", "authors": ["Jan Fabian Schmid", "Annika Hagemann"], "title": "XRefine: Attention-Guided Keypoint Match Refinement", "comment": null, "summary": "Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.", "code_url": "https://github.com/boschresearch/xrefine", "code_stars": 5, "code_last_update": "2026-01-21", "AI": {"tldr": "XRefine\uff1a\u4e00\u79cd\u4e0e\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u4e9a\u50cf\u7d20\u5173\u952e\u70b9\u7ec6\u5316\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4ee5\u5339\u914d\u5173\u952e\u70b9\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u5757\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u67b6\u6784\u9884\u6d4b\u7ec6\u5316\u5750\u6807\uff0c\u53ef\u63a8\u5e7f\u5230\u591a\u89c6\u89d2\u7279\u5f81\u8ddf\u8e2a", "motivation": "\u7a00\u758f\u5173\u952e\u70b9\u5339\u914d\u5bf93D\u89c6\u89c9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u5e38\u4ea7\u751f\u7a7a\u95f4\u4e0d\u51c6\u786e\u7684\u5339\u914d\u3002\u73b0\u6709\u7ec6\u5316\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5bf9\u9f50\u5339\u914d\u5173\u952e\u70b9\u4f4d\u7f6e\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u662f\u68c0\u6d4b\u5668\u7279\u5b9a\u7684\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u91cd\u65b0\u8bad\u7ec3", "method": "\u63d0\u51faXRefine\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0e\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u4e9a\u50cf\u7d20\u5173\u952e\u70b9\u7ec6\u5316\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u4ee5\u5339\u914d\u5173\u952e\u70b9\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u5757\u3002\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u7684\u67b6\u6784\u5b66\u4e60\u9884\u6d4b\u7ec6\u5316\u5173\u952e\u70b9\u5750\u6807\uff0c\u4e0d\u4f9d\u8d56\u68c0\u6d4b\u5668\u5185\u90e8\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u5904\u7406\u591a\u89c6\u89d2\u7279\u5f81\u8ddf\u8e2a", "result": "\u5728MegaDepth\u3001KITTI\u548cScanNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u63d0\u9ad8\u4e86\u51e0\u4f55\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u73b0\u6709\u7ec6\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fd0\u884c\u65f6\u6548\u7387", "conclusion": "XRefine\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e0e\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u5173\u952e\u70b9\u7ec6\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u5339\u914d\u7cbe\u5ea6\u5e76\u63a8\u5e7f\u5230\u4e0d\u540c\u68c0\u6d4b\u5668\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027"}}
{"id": "2601.14238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14238", "abs": "https://arxiv.org/abs/2601.14238", "authors": ["Shaurya Mathur", "Shreyas Bellary Manjunath", "Nitin Kulkarni", "Alina Vereshchaka"], "title": "Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression", "comment": "6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.", "AI": {"tldr": "FireCastRL\u662f\u4e00\u4e2a\u7ed3\u5408\u91ce\u706b\u9884\u6d4b\u4e0e\u667a\u80fd\u6251\u6551\u7684AI\u6846\u67b6\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u706b\u707e\u53d1\u751f\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u6a21\u62df\u4e2d\u6267\u884c\u5b9e\u65f6\u6251\u6551\u7b56\u7565\u3002", "motivation": "\u91ce\u706b\u9891\u7387\u548c\u5f3a\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u9020\u6210\u5de8\u5927\u751f\u6001\u548c\u7ecf\u6d4e\u635f\u5931\u3002\u4f20\u7edf\u706b\u707e\u7ba1\u7406\u4e3b\u8981\u662f\u88ab\u52a8\u54cd\u5e94\uff0c\u53ea\u5728\u706b\u707e\u53d1\u751f\u540e\u91c7\u53d6\u884c\u52a8\uff0c\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u9884\u9632\u548c\u5e94\u5bf9\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528\u6df1\u5ea6\u65f6\u7a7a\u6a21\u578b\u9884\u6d4b\u91ce\u706b\u53d1\u751f\u6982\u7387\uff1b2. \u5bf9\u9ad8\u98ce\u9669\u9884\u6d4b\uff0c\u90e8\u7f72\u9884\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5728\u7269\u7406\u4fe1\u606f3D\u6a21\u62df\u4e2d\u4f7f\u7528\u76f4\u5347\u673a\u90e8\u961f\u6267\u884c\u5b9e\u65f6\u6251\u6551\u6218\u672f\uff1b3. \u751f\u6210\u5a01\u80c1\u8bc4\u4f30\u62a5\u544a\u5e2e\u52a9\u5e94\u6025\u54cd\u5e94\u4eba\u5458\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5f00\u53d1\u4e86FireCastRL\u6846\u67b6\uff0c\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5305\u542b950\u4e07\u4e2a\u73af\u5883\u53d8\u91cf\u6837\u672c\u7684\u5927\u89c4\u6a21\u65f6\u7a7a\u6570\u636e\u96c6\u7528\u4e8e\u91ce\u706b\u9884\u6d4b\u3002\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5728\u91ce\u706b\u9884\u6d4b\u548c\u6218\u672f\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86AI\u6280\u672f\uff08\u6df1\u5ea6\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u91ce\u706b\u7ba1\u7406\u63d0\u4f9b\u4ece\u9884\u6d4b\u5230\u6218\u672f\u54cd\u5e94\u7684\u5168\u9762\u652f\u6301\uff0c\u4ee3\u8868\u4e86\u4ece\u88ab\u52a8\u54cd\u5e94\u5411\u4e3b\u52a8\u9884\u9632\u7684\u91cd\u8981\u8f6c\u53d8\u3002"}}
{"id": "2601.12533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12533", "abs": "https://arxiv.org/abs/2601.12533", "authors": ["Md. Ahanaf Arif Khan", "Ariful Islam", "Sangeeta Biswas", "Md. Iqbal Aziz Khan", "Subrata Pramanik", "Sanjoy Kumar Chakrabarty", "Bimal Kumar Pramanik"], "title": "BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images", "comment": null, "summary": "Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.", "AI": {"tldr": "\u4f5c\u8005\u521b\u5efa\u4e86BirdsEye-RU\u6570\u636e\u96c6\uff0c\u5305\u542b2,978\u5f20\u56fe\u50cf\u548c8,000\u591a\u4e2a\u6807\u6ce8\u7684\u4eba\u8138\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u9ad8\u7a7a\u56fe\u50cf\u4e2d\u7684\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\uff0c\u6570\u636e\u96c6\u5305\u542b\u65e0\u4eba\u673a\u548c\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u9ad8\u7a7a\u56fe\u50cf\u3002", "motivation": "\u9ad8\u7a7a\u56fe\u50cf\u4e2d\u7684\u4eba\u8138\u68c0\u6d4b\u9762\u4e34\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u548c\u73af\u5883\u6742\u6ce2\u7684\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86BirdsEye-RU\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b2,978\u5f20\u56fe\u50cf\u7684\u7efc\u5408\u96c6\u5408\uff0c\u6807\u6ce8\u4e86\u8d85\u8fc78,000\u4e2a\u4eba\u8138\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6355\u6349\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\uff0c\u5305\u542b\u65e0\u4eba\u673a\u548c\u9ad8\u7a7a\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u56fe\u50cf\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u9ad8\u7a7a\u4eba\u8138\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u73af\u5883\u573a\u666f\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u53ef\u901a\u8fc7Kaggle\u5e73\u53f0\u8bbf\u95ee\u3002", "conclusion": "BirdsEye-RU\u6570\u636e\u96c6\u4e3a\u89e3\u51b3\u9ad8\u7a7a\u56fe\u50cf\u4e2d\u5c0f\u5c3a\u5bf8\u4eba\u8138\u68c0\u6d4b\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.14243", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14243", "abs": "https://arxiv.org/abs/2601.14243", "authors": ["Haocheng Xi", "Charlie Ruan", "Peiyuan Liao", "Yujun Lin", "Han Cai", "Yilong Zhao", "Shuo Yang", "Kurt Keutzer", "Song Han", "Ligeng Zhu"], "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow", "comment": "11 pages, 6 figures, 4 tables", "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.", "AI": {"tldr": "Jet-RL\uff1a\u9996\u4e2a\u5168\u9762\u7684FP8\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u89e3\u51b3\u73b0\u6709BF16\u8bad\u7ec3+FP8 rollout\u7b56\u7565\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b033% rollout\u52a0\u901f\u548c16%\u7aef\u5230\u7aef\u52a0\u901f", "motivation": "\u73b0\u6709RL\u8bad\u7ec3\u7ba1\u9053\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0crollout\u9636\u6bb5\u5360\u8bad\u7ec3\u65f6\u95f470%\u4ee5\u4e0a\u3002\u91cf\u5316RL\u8bad\u7ec3\uff08\u7279\u522b\u662fFP8\u7cbe\u5ea6\uff09\u6709\u671b\u7f13\u89e3\u8fd9\u4e00\u74f6\u9888\uff0c\u4f46\u5e38\u7528\u7684BF16\u8bad\u7ec3+FP8 rollout\u7b56\u7565\u5728\u957f\u5e8f\u5217\u548c\u590d\u6742\u4efb\u52a1\u4e0b\u5b58\u5728\u4e25\u91cd\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u5d29\u6e83\u95ee\u9898", "method": "\u63d0\u51faJet-RL\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u540c\u65f6\u7528\u4e8e\u8bad\u7ec3\u548crollout\uff0c\u6700\u5c0f\u5316\u6570\u503c\u5dee\u5f02\uff0c\u6d88\u9664\u4f4e\u6548\u7684\u6b65\u95f4\u6821\u51c6\u9700\u6c42\uff0c\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027", "result": "Jet-RL\u5b9e\u73b033% rollout\u9636\u6bb5\u52a0\u901f\u300141%\u8bad\u7ec3\u9636\u6bb5\u52a0\u901f\u548c16%\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u662f\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u6570\u503c\u4e0d\u5339\u914d\u95ee\u9898\u7684\u5173\u952e\uff0cJet-RL\u6846\u67b6\u4e3a\u9ad8\u6548\u7a33\u5b9a\u7684\u91cf\u5316RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12534", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12534", "abs": "https://arxiv.org/abs/2601.12534", "authors": ["Marcus Ma", "Jordan Prescott", "Emily Zhou", "Tiantian Feng", "Kleanthis Avramidis", "Gabor Mihaly Toth", "Shrikanth Narayanan"], "title": "Encoding Emotion Through Self-Supervised Eye Movement Reconstruction", "comment": null, "summary": "The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u7684\u65b0\u6a21\u578b\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u9884\u6d4b\u60c5\u611f\u8868\u8fbe\uff0c\u5728Holocaust\u5e78\u5b58\u8005\u8bbf\u8c08\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u60c5\u611f\u9884\u6d4b\u7684\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u773c\u52a8-\u60c5\u611f\u5173\u7cfb\u7814\u7a76\u591a\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u773c\u52a8\u8ffd\u8e2a\u8bbe\u5907\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8303\u56f4\u548c\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4ece\u81ea\u7136\u3001\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u5229\u7528\u773c\u52a8\u9884\u6d4b\u60c5\u611f\u8868\u8fbe\u7684\u591a\u6a21\u6001\u6807\u8bb0", "method": "1) \u4f7f\u7528USC Shoah Foundation\u89c6\u89c9\u5386\u53f2\u6863\u6848\u4e2d\u7684Holocaust\u5e78\u5b58\u8005\u8bbf\u8c08\u89c6\u9891\uff1b2) \u53d7\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u542f\u53d1\uff0c\u5f00\u53d1\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u6a21\u578b\uff0c\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u89c6\u9891\uff1b3) \u4f7f\u7528\u6a21\u578b\u7f16\u7801\u5668\u5d4c\u5165\u5fae\u8c03\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff1a\u773c\u52a8\u4e0e\u8bed\u97f3\u60c5\u611f\u65b9\u5411\u4f30\u8ba1\u5bf9\u9f50\uff0c\u773c\u52a8\u9884\u6d4b\u4e09\u79cd\u77ac\u65f6\u60c5\u611f\u884c\u4e3a\uff08\u7b11\u3001\u54ed/\u62bd\u6ce3\u3001\u53f9\u6c14\uff09", "result": "\u65b0\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u60c5\u611f\u7ed3\u679c\uff0c\u89c2\u5bdf\u5230\u9884\u8bad\u7ec3\u6027\u80fd\u4e0e\u60c5\u611f\u5904\u7406\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u4e24\u4e2a\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9", "conclusion": "\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u662f\u7f16\u7801\u773c\u52a8\u6240\u643a\u5e26\u60c5\u611f\u4fe1\u53f7\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u4ece\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u63d0\u53d6\u60c5\u611f\u4fe1\u606f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2601.11559", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11559", "abs": "https://arxiv.org/abs/2601.11559", "authors": ["Zilal Eiz AlDin", "John Wu", "Jeffrey Paul Fung", "Jennifer King", "Mya Watts", "Lauren ONeill", "Adam Richard Cross", "Jimeng Sun"], "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?", "comment": "5 pages", "summary": "Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86MIMIC-RD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06\u4e34\u5e8a\u6587\u672c\u76f4\u63a5\u6620\u5c04\u5230Orphanet\u7f55\u89c1\u75c5\u6570\u636e\u5e93\u6765\u8bc4\u4f30LLM\u5728\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7f55\u89c1\u75c5\u5f71\u54cd\u5927\u91cf\u4eba\u7fa4\u4f46\u9274\u522b\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u8bc4\u4f30LLM\u5728\u7f55\u89c1\u75c5\u8bca\u65ad\u4e2d\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u4f9d\u8d56\u7406\u60f3\u5316\u7684\u4e34\u5e8a\u6848\u4f8b\u7814\u7a76\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u590d\u6742\u6027\uff1b2) \u4f7f\u7528ICD\u4ee3\u7801\u4f5c\u4e3a\u75be\u75c5\u6807\u7b7e\uff0c\u4f46\u8bb8\u591a\u7f55\u89c1\u75c5\u7f3a\u4e4f\u4e0eOrphanet\u7b49\u7efc\u5408\u7f55\u89c1\u75c5\u6570\u636e\u5e93\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u5bfc\u81f4\u4e25\u91cd\u4f4e\u4f30\u7f55\u89c1\u75c5\u6570\u91cf\u3002", "method": "\u5f00\u53d1MIMIC-RD\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u57fa\u51c6\uff1a1) \u901a\u8fc7LLM\u6316\u6398\u4e34\u5e8a\u6587\u672c\u5b9e\u4f53\uff1b2) \u7531\u56db\u4f4d\u533b\u5b66\u6807\u6ce8\u8005\u9a8c\u8bc1\u786e\u8ba4\u8bc6\u522b\u7684\u5b9e\u4f53\u662f\u771f\u6b63\u7684\u7f55\u89c1\u75c5\uff1b3) \u5c06\u4e34\u5e8a\u6587\u672c\u5b9e\u4f53\u76f4\u63a5\u6620\u5c04\u5230Orphanet\u6570\u636e\u5e93\uff1b4) \u5728145\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5404\u79cd\u6a21\u578b\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u51fa\u73b0\u6709\u80fd\u529b\u4e0e\u4e34\u5e8a\u9700\u6c42\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u7f55\u89c1\u75c5\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.12551", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.12551", "abs": "https://arxiv.org/abs/2601.12551", "authors": ["Tong Wu"], "title": "PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception", "comment": "4 pages, 4 figures, 3 tables. Submitted to IEICE Transactions", "summary": "We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.", "AI": {"tldr": "PISE\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u6df1\u5ea6\u9b3c\u6210\u50cf\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u611f\u77e5\uff0c\u901a\u8fc7\u7ed3\u5408\u4f34\u968f\u7b97\u5b50\u521d\u59cb\u5316\u548c\u8bed\u4e49\u6307\u5bfc\uff0c\u57285%\u91c7\u6837\u7387\u4e0b\u5c06\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53472.57%\uff0c\u65b9\u5dee\u964d\u4f4e9\u500d\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u9700\u8981\u4f4e\u5e26\u5bbd\u611f\u77e5\uff0c\u4f20\u7edf\u9b3c\u6210\u50cf\u65b9\u6cd5\u5728\u4f4e\u91c7\u6837\u7387\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u6df1\u5ea6\u5b66\u4e60\u6765\u63d0\u9ad8\u8fb9\u7f18\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faPISE\u6846\u67b6\uff0c\u7ed3\u5408\u4f34\u968f\u7b97\u5b50\u521d\u59cb\u5316\uff08\u5229\u7528\u7269\u7406\u6a21\u578b\u5148\u9a8c\uff09\u548c\u8bed\u4e49\u6307\u5bfc\uff08\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\uff09\uff0c\u5728\u4f4e\u91c7\u6837\u7387\u4e0b\u4f18\u5316\u9b3c\u6210\u50cf\u91cd\u5efa\u548c\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u57285%\u91c7\u6837\u7387\u4e0b\uff0cPISE\u5c06\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53472.57%\uff0c\u65b9\u5dee\u964d\u4f4e9\u500d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "PISE\u901a\u8fc7\u878d\u5408\u7269\u7406\u6a21\u578b\u5148\u9a8c\u548c\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u91c7\u6837\u7387\u9b3c\u6210\u50cf\u7684\u8fb9\u7f18\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12567", "abs": "https://arxiv.org/abs/2601.12567", "authors": ["W\u0142adys\u0142aw Skarbek", "Micha\u0142 Salomonowicz", "Micha\u0142 Kr\u00f3l"], "title": "Camera Pose Revisited", "comment": "30 pages, 9 figures, 9 tables", "summary": "Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \\texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \\texttt{SQPnP} and slightly higher than \\texttt{IPPE}, both prominent \\texttt{PnP-OpenCV} procedures. However, \\texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \\texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.", "AI": {"tldr": "\u63d0\u51faPnP-ProCay78\u7b97\u6cd5\u89e3\u51b3\u5e73\u9762\u900f\u89c6n\u70b9\u95ee\u9898\uff0c\u7ed3\u5408\u4e8c\u6b21\u91cd\u5efa\u8bef\u5dee\u516c\u5f0f\u3001Cayley\u65cb\u8f6c\u53c2\u6570\u5316\u548c\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u8d77\u59cb\u70b9\u9009\u62e9\u907f\u514d\u641c\u7d22\uff0c\u5728RGB\u548c\u70ed\u6210\u50cf\u76f8\u673a\u4e0a\u9a8c\u8bc1\u6027\u80fd\u63a5\u8fd1\u6700\u4f18SQPnP\u3002", "motivation": "\u89e3\u51b3\u76f8\u673a\u6807\u5b9a\u548c\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u2014\u2014\u76f8\u673a\u76f8\u5bf9\u4e8e\u89c2\u5bdf\u573a\u666f\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u7279\u522b\u5173\u6ce8\u6807\u5b9a\u7269\u4f53\u59ff\u6001\u7684\u521d\u59cb\u4f30\u8ba1\u3002\u73b0\u6709PnP\u6c42\u89e3\u5668\u5728\u51e0\u4f55\u900f\u660e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPnP-ProCay78\u7b97\u6cd5\uff1a1) \u7ed3\u5408\u7ecf\u5178\u4e8c\u6b21\u91cd\u5efa\u8bef\u5dee\u516c\u5f0f\u4e0eCayley\u65cb\u8f6c\u53c2\u6570\u5316\uff1b2) \u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff1b3) \u5173\u952e\u521b\u65b0\u662f\u901a\u8fc7\u5206\u6790\u4e24\u4e2a\u89c4\u8303\u5411\u91cf\u7684\u91cd\u5efa\u8bef\u5dee\u8fdb\u884c\u786e\u5b9a\u6027\u8d77\u59cb\u70b9\u9009\u62e9\uff0c\u907f\u514d\u6602\u8d35\u7684\u89e3\u7a7a\u95f4\u641c\u7d22\uff1b4) \u521b\u5efa\u6df7\u5408\u6210\u672c\u51fd\u6570\uff0c\u5c06\u6295\u5f71\u8bef\u5dee\u6700\u5c0f\u5316\u4e0e\u89e3\u6790\u6d88\u9664\u5e73\u79fb\u7684\u91cd\u5efa\u8bef\u5dee\u66ff\u4ee3\u9879\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387RGB\u76f8\u673a\u548c\u4f4e\u5206\u8fa8\u7387\u70ed\u6210\u50cf\u76f8\u673a\u7684\u96c6\u6210RGB-IR\u8bbe\u7f6e\u3002\u7ed3\u679c\u8868\u660e\uff1a1) \u6295\u5f71\u7cbe\u5ea6\u4e0e\u6700\u4f18SQPnP\u51e0\u4e4e\u76f8\u540c\uff0c\u7565\u9ad8\u4e8eIPPE\uff1b2) \u7b97\u6cd5\u7ed3\u6784\u663e\u8457\u66f4\u7b80\u5355\uff1b3) Cayley\u7a7a\u95f4\u4e2d\u7684\u4f18\u5316\u8f68\u8ff9\u5206\u6790\u63d0\u4f9b\u4e86\u6536\u655b\u8fc7\u7a0b\u7684\u76f4\u89c2\u7406\u89e3\uff1b4) \u6df7\u5408\u6210\u672c\u51fd\u6570\u517c\u5177\u51e0\u4f55\u900f\u660e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "PnP-ProCay78\u7b97\u6cd5\u5728\u4fdd\u6301\u4e0e\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u7684\u7b97\u6cd5\u7ed3\u6784\u548c\u51e0\u4f55\u900f\u660e\u6027\u3002Cayley\u53c2\u6570\u5316\u4f7f\u6536\u655b\u8fc7\u7a0b\u66f4\u76f4\u89c2\uff0c\u5177\u6709\u6559\u5b66\u4ef7\u503c\u3002\u8be5\u7b97\u6cd5\u4e3a\u5e73\u9762PnP\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11622", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11622", "abs": "https://arxiv.org/abs/2601.11622", "authors": ["Hassan Ugail", "Newton Howard"], "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models", "comment": null, "summary": "Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u65f6\u95f4\u6574\u5408\u4e0e\u4e9a\u7a33\u6001\u6982\u5ff5\u5e94\u7528\u4e8eTransformer\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u590d\u5408\u52a8\u529b\u5b66\u6307\u6807\u6765\u91cf\u5316LLM\u5728\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u7ec4\u7ec7\u7279\u5f81\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u76f8\u6bd4\u91cd\u590d\u3001\u566a\u58f0\u548c\u6270\u52a8\u6761\u4ef6\u8868\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u52a8\u529b\u5b66\u590d\u6742\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u9ad8\u7ef4\u5185\u90e8\u52a8\u529b\u5b66\u8fdb\u884c\u6587\u672c\u751f\u6210\uff0c\u4f46\u8fd9\u4e9b\u52a8\u529b\u5b66\u7684\u65f6\u95f4\u7ec4\u7ec7\u7279\u5f81\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u591a\u5173\u6ce8\u9759\u6001\u8868\u793a\u6216\u56e0\u679c\u5e72\u9884\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u7ed3\u6784\u7684\u63a2\u7d22\u3002\u7814\u7a76\u8005\u501f\u9274\u795e\u7ecf\u79d1\u5b66\u4e2d\u4f5c\u4e3a\u795e\u7ecf\u7ec4\u7ec7\u6838\u5fc3\u6807\u5fd7\u7684\u65f6\u95f4\u6574\u5408\u4e0e\u4e9a\u7a33\u6001\u6982\u5ff5\uff0c\u8bd5\u56fe\u5c06\u8fd9\u4e9b\u6982\u5ff5\u5e94\u7528\u4e8eTransformer\u6a21\u578b\uff0c\u4ee5\u63ed\u793a\u4e0d\u540c\u529f\u80fd\u72b6\u6001\u4e0b\u6a21\u578b\u5185\u90e8\u52a8\u529b\u5b66\u7684\u65f6\u95f4\u7ec4\u7ec7\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u5408\u52a8\u529b\u5b66\u6307\u6807\uff0c\u5728GPT-2-medium\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u8bbe\u8ba1\u4e86\u4e94\u79cd\u5b9e\u9a8c\u6761\u4ef6\uff1a\u7ed3\u6784\u5316\u63a8\u7406\u3001\u5f3a\u5236\u91cd\u590d\u3001\u9ad8\u6e29\u566a\u58f0\u91c7\u6837\u3001\u6ce8\u610f\u529b\u5934\u526a\u679d\u548c\u6743\u91cd\u566a\u58f0\u6ce8\u5165\u3002\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u65f6\u95f4\u5e8f\u5217\u8ba1\u7b97\u52a8\u529b\u5b66\u6307\u6807\uff0c\u4f7f\u7528\u5355\u56e0\u7d20\u65b9\u5dee\u5206\u6790(ANOVA)\u8fdb\u884c\u7edf\u8ba1\u68c0\u9a8c\uff0c\u5e76\u8bc4\u4f30\u4e86\u8be5\u6307\u6807\u5bf9\u5c42\u9009\u62e9\u3001\u901a\u9053\u5b50\u91c7\u6837\u548c\u968f\u673a\u79cd\u5b50\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7ed3\u6784\u5316\u63a8\u7406\u6761\u4ef6\u76f8\u6bd4\u91cd\u590d\u3001\u566a\u58f0\u548c\u6270\u52a8\u6761\u4ef6\u8868\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u52a8\u529b\u5b66\u6307\u6807\u3002\u7edf\u8ba1\u68c0\u9a8c\u663e\u793a\u7ec4\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5173\u952e\u6bd4\u8f83\u4e2d\u6548\u5e94\u91cf\u8f83\u5927\u3002\u8fd9\u4e9b\u7ed3\u679c\u5bf9\u5c42\u9009\u62e9\u3001\u901a\u9053\u5b50\u91c7\u6837\u548c\u968f\u673a\u79cd\u5b50\u5177\u6709\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u6a21\u578b\u7684\u5185\u90e8\u52a8\u529b\u5b66\u8868\u73b0\u51fa\u66f4\u590d\u6742\u7684\u65f6\u95f4\u7ec4\u7ec7\u7279\u5f81\u3002", "conclusion": "\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u52a8\u529b\u5b66\u6307\u6807\u80fd\u591f\u53ef\u9760\u5730\u8868\u5f81\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u529f\u80fd\u72b6\u6001\u4e0b\u7684\u8ba1\u7b97\u7ec4\u7ec7\u5dee\u5f02\u3002\u8be5\u6307\u6807\u6355\u6349\u7684\u662f\u5f62\u5f0f\u5316\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u800c\u975e\u4e3b\u89c2\u4f53\u9a8c\u3002\u7814\u7a76\u4e3a\u7406\u89e3LLM\u5185\u90e8\u52a8\u529b\u5b66\u7684\u65f6\u95f4\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u6982\u5ff5\u8fc1\u79fb\u5728AI\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12636", "abs": "https://arxiv.org/abs/2601.12636", "authors": ["Satyaki Roy Chowdhury", "Aswathnarayan Radhakrishnan", "Hsiao Jou Hsu", "Hari Subramoni", "Joachim Moortgat"], "title": "From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2", "comment": "Accepted by WACV 2026", "summary": "Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8eSwin-Transformer\u7684U-Net\u6a21\u578b(Swin-BathyUNet)\u5728Sentinel-2\u536b\u661f\u6d4b\u6df1(SDB)\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u3001\u6ce8\u610f\u529b\u673a\u5236\u7814\u7a76\u548c\u8de8\u533a\u57df\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u63a8\u65ad\u6df1\u5ea6\u7684\u673a\u5236\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u90e8\u7f72\u6307\u5357\u3002", "motivation": "Sentinel-2\u536b\u661f\u6d4b\u6df1(SDB)\u5728\u4e0d\u540c\u7ad9\u70b9\u95f4\u7684\u7a33\u5065\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u63a8\u65ad\u6c34\u6df1\u4ee5\u53ca\u4f55\u65f6\u5176\u9884\u6d4b\u662f\u53ef\u4fe1\u7684\uff0c\u4ee5\u63d0\u5347\u8de8\u533a\u57df\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528Swin-Transformer U-Net\u67b6\u6784\uff1b\u8fdb\u884c\u6ce2\u6bb5\u91cd\u8981\u6027\u5206\u6790\uff08leave-one-band out\uff09\uff1b\u5f00\u53d1\u56de\u5f52\u4efb\u52a1\u7684\u57fa\u4e8e\u6d88\u878d\u7684\u7c7b\u6fc0\u6d3b\u6620\u5c04(A-CAM-R)\u5e76\u8fdb\u884c\u53ef\u9760\u6027\u9a8c\u8bc1\uff1b\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\u6d88\u878d\u7814\u7a76\uff1b\u5b9e\u65bd\u8de8\u533a\u57df\u63a8\u7406\u5b9e\u9a8c\uff08\u5728\u4e00\u4e2a\u7ad9\u70b9\u8bad\u7ec3\uff0c\u5728\u53e6\u4e00\u4e2a\u7ad9\u70b9\u6d4b\u8bd5\uff09\u3002", "result": "\u6ce2\u6bb5\u91cd\u8981\u6027\u6392\u5e8f\u4e0e\u6d45\u6c34\u5149\u5b66\u7406\u8bba\u4e00\u81f4\uff1bA-CAM-R\u9a8c\u8bc1\u663e\u793a\u4ec5\u4fdd\u7559\u524dp%\u663e\u8457\u50cf\u7d20\u4f1a\u5bfc\u81f4RMSE\u5355\u8c03\u5927\u5e45\u589e\u52a0\uff0c\u8868\u660e\u89e3\u91ca\u5b9a\u4f4d\u5728\u6a21\u578b\u4f9d\u8d56\u7684\u8bc1\u636e\u4e0a\uff1b\u89e3\u7801\u5668\u6761\u4ef6\u5316\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u7729\u5149/\u6ce1\u6cab\u7684\u9c81\u68d2\u6027\uff1b\u8de8\u533a\u57df\u63a8\u7406\u663e\u793a\u8bef\u5dee\u968f\u6df1\u5ea6\u8fd1\u7ebf\u6027\u589e\u52a0\uff0c\u53cc\u5cf0\u6df1\u5ea6\u5206\u5e03\u52a0\u5267\u4e2d/\u6df1\u6c34\u533a\u8bef\u5dee\u3002", "conclusion": "\u4e3a\u7a33\u5065\u7684SDB\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff1a\u4fdd\u6301\u5bbd\u611f\u53d7\u91ce\uff0c\u4fdd\u62a4\u7eff/\u84dd\u901a\u9053\u7684\u8f90\u5c04\u6d4b\u91cf\u4fdd\u771f\u5ea6\uff0c\u9884\u8fc7\u6ee4\u8fd1\u5cb8\u9ad8\u4eae\u5ea6\u9ad8\u65b9\u5dee\u533a\u57df\uff0c\u7ed3\u5408\u8f7b\u91cf\u76ee\u6807\u7ad9\u70b9\u5fae\u8c03\u548c\u6df1\u5ea6\u611f\u77e5\u6821\u51c6\u4ee5\u5b9e\u73b0\u8de8\u533a\u57df\u8fc1\u79fb\u3002"}}
{"id": "2601.11625", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11625", "abs": "https://arxiv.org/abs/2601.11625", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance", "comment": "8 pages, Submitted to ACL Rolling Review and is under review", "summary": "Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65f6\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u5fae\u8c03\u8fc7\u7a0b\u4e2dtoken\u7ea7\u5f52\u56e0\u7684\u53d8\u5316\u6765\u76d1\u63a7\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u7684\u6f14\u53d8\uff0c\u5b9a\u4e49\u4e86\"\u89e3\u91ca\u6f02\u79fb\"\u548c\"\u63a8\u7406\u7a33\u5b9a\u70b9\"\u7684\u6982\u5ff5\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u867d\u7136\u80fd\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u4f1a\u5fae\u5999\u5730\u6539\u53d8\u6a21\u578b\u4f9d\u8d56\u7684\u8bc1\u636e\u4f9d\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6a21\u578b\u63a8\u7406\u4f9d\u636e\u5982\u4f55\u6f14\u53d8\u7684\u76d1\u63a7\u624b\u6bb5\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65f6\u89e3\u91ca\u6027\u89c6\u89d2\uff0c\u8ddf\u8e2a\u5fae\u8c03\u5404epoch\u4e2dtoken\u7ea7\u5f52\u56e0\u7684\u53d8\u5316\u3002\u5b9a\u4e49\"\u89e3\u91ca\u6f02\u79fb\"\u4e3a\u56fa\u5b9a\u63a2\u6d4b\u96c6\u4e0a\u5f52\u4e00\u5316token\u5f52\u56e0\u7684epoch\u95f4\u53d8\u5316\uff0c\u5f15\u5165\"\u63a8\u7406\u7a33\u5b9a\u70b9\"\u4f5c\u4e3a\u6f02\u79fb\u9996\u6b21\u8fdb\u5165\u6301\u7eed\u4f4e\u6c34\u5e73\u72b6\u6001\u7684\u6700\u65e9epoch\u3002", "result": "\u5728\u591a\u4e2a\u8f7b\u91cf\u7ea7transformer\u5206\u7c7b\u5668\u548c\u57fa\u51c6\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6f02\u79fb\u901a\u5e38\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u8fdb\u5165\u4f4e\u7a33\u5b9a\u72b6\u6001\uff0c\u800c\u9a8c\u8bc1\u51c6\u786e\u7387\u4ec5\u53d1\u751f\u8fb9\u9645\u53d8\u5316\u3002\u5728\u53d7\u63a7\u7684\u6377\u5f84\u8bbe\u7f6e\u4e2d\uff0c\u5f52\u56e0\u52a8\u6001\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6807\u7b7e\u76f8\u5173\u89e6\u53d1token\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u5373\u4f7f\u9a8c\u8bc1\u51c6\u786e\u7387\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u89e3\u91ca\u6f02\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u76d1\u63a7\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51b3\u7b56\u4f9d\u636e\u7684\u6f14\u53d8\uff0c\u5e76\u53ef\u5728\u7a33\u5b9a\u8bc1\u636e\u673a\u5236\u4e2d\u9009\u62e9\u68c0\u67e5\u70b9\u3002"}}
{"id": "2601.12638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12638", "abs": "https://arxiv.org/abs/2601.12638", "authors": ["Ninnart Fuengfusin", "Keisuke Yoneda", "Naoki Suganuma"], "title": "Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT", "comment": "6 pages, 3 figures", "summary": "LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8ePointPillars\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u654f\u611f\u5c42\u641c\u7d22\u548c\u5f02\u5e38\u503c\u5904\u7406\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u52a0\u901f\u3002", "motivation": "LIDAR 3D\u76ee\u6807\u68c0\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5b9e\u65f6\u8fd0\u884c\u3002\u6a21\u578b\u91cf\u5316\u53ef\u4ee5\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u91cf\u5316\u4f1a\u56e0LIDAR\u6570\u636e\u7684\u5bbd\u6570\u503c\u5206\u5e03\u548c\u6781\u7aef\u5f02\u5e38\u503c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "1. \u63d0\u51fa\u6df7\u5408\u7cbe\u5ea6\u6846\u67b6\uff1a\u901a\u8fc7\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u9010\u5c42\u91cf\u5316\u5230INT8\u5e76\u8bc4\u4f30AP\uff0c\u8bc6\u522b\u654f\u611f\u5c42\uff1b2. \u5c06top-k\u654f\u611f\u5c42\u4fdd\u6301\u4e3a\u6d6e\u70b9\uff08FP\uff09\uff1b3. \u8d2a\u5a6a\u641c\u7d22\u8fd9\u4e9b\u5c42\u7684\u7ec4\u5408\u751f\u6210\u5019\u9009\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\uff1b4. \u4f7f\u7528PTQ\u6216\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u5b8c\u6210\u6a21\u578b\uff1b5. \u4f7f\u7528\u5c11\u91cf\u6821\u51c6\u6570\u636e\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "1. PTQ\u6d41\u6c34\u7ebf\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\uff1b2. QAT\u6d41\u6c34\u7ebf\u8fbe\u5230\u4e0eFP\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff1b3. TensorRT\u90e8\u7f72\u4e0b\uff0c\u5ef6\u8fdf\u964d\u4f4e\u6700\u591a2.35\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u6700\u591a2.26\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LIDAR\u6570\u636e\u91cf\u5316\u4e2d\u7684\u6570\u503c\u5206\u5e03\u548c\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5b9e\u65f63D\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2601.12664", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12664", "abs": "https://arxiv.org/abs/2601.12664", "authors": ["Elisa Gon\u00e7alves Ribeiro", "Rodrigo Moreira", "Larissa Ferreira Rodrigues Moreira", "Andr\u00e9 Ricardo Backes"], "title": "Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images", "comment": "21st International Conference on Computer Vision Theory and Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain", "summary": "Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u9488\u5bf9\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\u7684\u8d85\u53c2\u6570\u4f18\u5316\u7b56\u7565\u53ca\u5176\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bad\u7ec3\u4e2d\u5b58\u5728\u9690\u79c1\u7ea6\u675f\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\uff0c\u4f46\u5176\u6027\u80fd\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u6570\u636e\u96c6\u4e0b\u9ad8\u5ea6\u4f9d\u8d56\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u9700\u8981\u7814\u7a76\u8d85\u53c2\u6570\u4f18\u5316\u7b56\u7565\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "method": "\u91c7\u7528\u96c6\u4e2d\u5f0f\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5c06\u6570\u636e\u96c6\u7279\u5b9a\u6700\u4f18\u914d\u7f6e\u8fc1\u79fb\u5230\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5e76\u5f15\u5165\u7b80\u5355\u7684\u8de8\u6570\u636e\u96c6\u805a\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u5747\u5b66\u4e60\u7387\u3001\u8003\u8651\u6a21\u6001\u4f18\u5316\u5668\u548c\u6279\u91cf\u5927\u5c0f\u6765\u7ec4\u5408\u914d\u7f6e", "result": "\u7ec4\u5408\u914d\u7f6e\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8868\u660e\u8be5\u7b56\u7565\u5728\u8de8\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u6709\u6548", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5355\u8de8\u6570\u636e\u96c6\u805a\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u4e3a\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u6709\u6548\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u9700\u6c42"}}
{"id": "2601.12666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12666", "abs": "https://arxiv.org/abs/2601.12666", "authors": ["Zonglin Li", "Jieji Ren", "Shuangfan Zhou", "Heng Guo", "Jinnuo Zhang", "Jiang Zhou", "Boxin Shi", "Zhanyu Ma", "Guoying Gu"], "title": "Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface", "comment": "5 pages 7figures", "summary": "Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u7684\u5355\u6b21\u5f69\u8272\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u6846\u67b6\uff0c\u7528\u4e8e\u8fd1\u5149\u548c\u590d\u6742\u53cd\u5c04\u6761\u4ef6\u4e0b\u7684\u8868\u9762\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u7d27\u51d1\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5668\u9a8c\u8bc1", "motivation": "\u4f20\u7edf\u5f69\u8272\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u5047\u8bbe\u7406\u60f3\u8fdc\u8ddd\u79bb\u7167\u660e\u548c\u6717\u4f2f\u53cd\u5c04\uff0c\u9650\u5236\u4e86\u5728\u66f4\u5b9e\u9645\u7684\u8fd1\u5149\u6761\u4ef6\u548c\u975e\u6717\u4f2f\u8868\u9762\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u5b9e\u9645\u6761\u4ef6\u63a2\u7d22\u4e0d\u8db3\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u66f4\u5b9e\u7528\u7684\u5355\u6b21\u8868\u9762\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u5bf9\u6df1\u5ea6\u548cBRDF\u8fdb\u884c\u5efa\u6a21\uff0c\u57fa\u4e8e\u5355\u8272\u6027\u5047\u8bbe\uff08\u5747\u5300\u8272\u5ea6\u548c\u540c\u8d28\u6750\u6599\uff09\uff0c\u7f13\u89e3\u5f69\u8272\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u56fa\u6709\u7684\u4e0d\u9002\u5b9a\u6027\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u7d27\u51d1\u7684\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5668\u6765\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u8868\u9762\u91cd\u5efa\uff0c\u7279\u522b\u662f\u5728\u8fd1\u5149\u6761\u4ef6\u548c\u590d\u6742\u53cd\u5c04\u8868\u9762\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u6269\u5c55\u4e86\u5f69\u8272\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u5b9e\u9645\u7684\u8fd1\u5149\u6761\u4ef6\u548c\u590d\u6742\u53cd\u5c04\u8868\u9762\uff0c\u901a\u8fc7\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u548c\u5355\u8272\u6027\u5047\u8bbe\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u8be6\u7ec6\u8868\u9762\u91cd\u5efa\u3002"}}
{"id": "2601.12682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12682", "abs": "https://arxiv.org/abs/2601.12682", "authors": ["Banglei Guan", "Dongcai Tan", "Jing Tao", "Ang Su", "Yang Shang", "Qifeng Yu"], "title": "Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement", "comment": null, "summary": "In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in \u03b5_xx is reduced by 85.3%, while the errors in \u03b5_yy and \u03b3_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u6e29\u7ed3\u6784\u53d8\u5f62\u6d4b\u91cf\u7684\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u66dd\u5149\u56fe\u50cf\u878d\u5408\u548c\u7070\u5ea6\u5e73\u5747\u7b97\u6cd5\u6765\u6291\u5236\u70ed\u8f90\u5c04\u548c\u70ed\u973e\u5e72\u6270\uff0c\u63d0\u9ad8\u6570\u5b57\u56fe\u50cf\u76f8\u5173\u6cd5\uff08DIC\uff09\u7684\u6d4b\u91cf\u7cbe\u5ea6\u3002", "motivation": "\u5728\u9ad8\u6e29\u7ed3\u6784\u53d8\u5f62\u6d4b\u91cf\u4e2d\uff0c\u70ed\u8f90\u5c04\u5f15\u8d77\u7684\u56fe\u50cf\u9000\u5316\u548c\u70ed\u973e\u5f15\u5165\u7684\u968f\u673a\u8bef\u5dee\u9650\u5236\u4e86\u53d8\u5f62\u6d4b\u91cf\u7684\u7cbe\u5ea6\u548c\u6709\u6548\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u5e72\u6270\u56e0\u7d20\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u56fe\u50cf\u5904\u7406\u6280\u672f\u6765\u6539\u5584\u9ad8\u6e29\u73af\u5883\u4e0b\u7684DIC\u6d4b\u91cf\u6027\u80fd\u3002", "method": "1. \u9488\u5bf9\u70ed\u8f90\u5c04\u5f15\u8d77\u7684\u56fe\u50cf\u9000\u5316\uff1a\u57fa\u4e8e\u56fe\u50cf\u5206\u5c42\u8868\u793a\uff0c\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u6b63\u8d1f\u901a\u9053\u8fdb\u884c\u5e76\u884c\u5904\u7406\uff0c\u901a\u8fc7\u591a\u66dd\u5149\u56fe\u50cf\u878d\u5408\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u3002\n2. \u9488\u5bf9\u70ed\u973e\u5f15\u5165\u7684\u9ad8\u9891\u968f\u673a\u8bef\u5dee\uff1a\u91c7\u7528FSIM\uff08\u7279\u5f81\u76f8\u4f3c\u5ea6\uff09\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u6307\u5bfc\u6a21\u578b\u53c2\u6570\u8fed\u4ee3\u4f18\u5316\uff0c\u5e94\u7528\u7070\u5ea6\u5e73\u5747\u7b97\u6cd5\u5747\u8861\u5f02\u5e38\u7070\u5ea6\u503c\u4ee5\u51cf\u5c11\u6d4b\u91cf\u8bef\u5dee\u3002", "result": "1. \u591a\u66dd\u5149\u56fe\u50cf\u878d\u5408\u7b97\u6cd5\u6709\u6548\u6291\u5236\u4e86\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u9000\u5316\uff0c\u5c06\u6b20\u66dd\u5149\u56fe\u50cf\u7684\u6709\u6548\u8ba1\u7b97\u533a\u57df\u4ece26%\u63d0\u5347\u81f350%\uff0c\u8fc7\u66dd\u5149\u56fe\u50cf\u4ece32%\u63d0\u5347\u81f340%\uff0c\u4e14\u4e0d\u964d\u4f4e\u6d4b\u91cf\u7cbe\u5ea6\u3002\n2. \u7ed3\u5408\u7070\u5ea6\u5e73\u5747\u7b97\u6cd5\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9759\u6001\u70ed\u53d8\u5f62\u6d4b\u91cf\u8bef\u5dee\uff1a\u03b5_xx\u8bef\u5dee\u51cf\u5c1185.3%\uff0c\u03b5_yy\u8bef\u5dee\u51cf\u5c1136.0%\uff0c\u03b3_xy\u8bef\u5dee\u51cf\u5c1136.4%\u3002", "conclusion": "\u63d0\u51fa\u7684\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u9ad8\u6e29\u53d8\u5f62\u6d4b\u91cf\u4e2d\u70ed\u8f90\u5c04\u548c\u70ed\u973e\u7684\u5e72\u6270\uff0c\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u5e76\u964d\u4f4e\u53d8\u5f62\u6d4b\u91cf\u8bef\u5dee\uff0c\u5728\u70ed\u53d8\u5f62\u6d4b\u91cf\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6539\u5584DIC\u6d4b\u91cf\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.12683", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.12683", "abs": "https://arxiv.org/abs/2601.12683", "authors": ["Liwei Liao", "Ronggang Wang"], "title": "GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation", "comment": null, "summary": "With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.", "AI": {"tldr": "\u63d0\u51faGaussianTrimmer\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u548c2D\u5206\u5272\u7ed3\u679c\u5728\u57fa\u5143\u7ea7\u522b\u4fee\u526a3D\u9ad8\u65af\u5206\u5272\u7684\u7c97\u7cd9\u8fb9\u754c", "motivation": "\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u57fa\u4e8e\u9ad8\u65af\u57fa\u5143\u8fdb\u884c\u5206\u5272\uff0c\u4f46\u7531\u4e8e3D\u9ad8\u65af\u5c3a\u5ea6\u53d8\u5316\u8303\u56f4\u5927\uff0c\u5927\u5c3a\u5bf8\u9ad8\u65af\u5e38\u8de8\u8d8a\u524d\u666f\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u5206\u5272\u7269\u4f53\u8fb9\u754c\u5448\u952f\u9f7f\u72b6", "method": "\u63d0\u51fa\u5728\u7ebf\u8fb9\u754c\u4fee\u526a\u65b9\u6cd5GaussianTrimmer\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a1.\u751f\u6210\u5747\u5300\u4e14\u8986\u76d6\u826f\u597d\u7684\u865a\u62df\u76f8\u673a\uff1b2.\u57fa\u4e8e\u865a\u62df\u76f8\u673a\u4e0a\u76842D\u5206\u5272\u7ed3\u679c\u5728\u57fa\u5143\u7ea7\u522b\u4fee\u526a\u9ad8\u65af", "result": "\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u7684\u5206\u5272\u8d28\u91cf", "conclusion": "GaussianTrimmer\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u4fee\u526a\u7c97\u7cd9\u8fb9\u754c"}}
{"id": "2601.12697", "categories": ["cs.CV", "cs.CG"], "pdf": "https://arxiv.org/pdf/2601.12697", "abs": "https://arxiv.org/abs/2601.12697", "authors": ["Chao Yang", "Deshui Miao", "Chao Tian", "Guoqing Zhu", "Yameng Gu", "Zhenyu He"], "title": "Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation", "comment": null, "summary": "Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51faIVGF\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u91cd\u5efa\u89e3\u51b3\u4f20\u7edf2D\u7ea2\u5916-\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u89c6\u89d2\u56fa\u5b9a\u3001\u573a\u666f\u7406\u89e3\u4e0d\u5168\u9762\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\u56fe\u50cf\u7684\u76f4\u63a5\u6e32\u67d3\u3002", "motivation": "\u73b0\u67092D\u878d\u5408\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u76f8\u673a\u89c6\u89d2\uff0c\u65e0\u6cd5\u5168\u9762\u7406\u89e3\u590d\u6742\u573a\u666f\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u5e76\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7ea2\u5916-\u53ef\u89c1\u5149\u9ad8\u65af\u878d\u5408(IVGF)\u6846\u67b6\uff1a1) \u4ece\u591a\u6a21\u60012D\u8f93\u5165\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff1b2) \u8bbe\u8ba1\u8de8\u6a21\u6001\u8c03\u6574(CMA)\u6a21\u5757\uff0c\u901a\u8fc7\u8c03\u5236\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\u89e3\u51b3\u8de8\u6a21\u6001\u51b2\u7a81\uff1b3) \u5f15\u5165\u878d\u5408\u635f\u5931\u6307\u5bfcCMA\u4f18\u5316\uff0c\u4fdd\u7559\u5404\u6a21\u6001\u5173\u952e\u7279\u5f81\u3002", "result": "\u5168\u9762\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u7559\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u5173\u952e\u7279\u5f81\u7684\u878d\u5408\u56fe\u50cf\u3002", "conclusion": "IVGF\u6846\u67b6\u901a\u8fc73D\u9ad8\u65af\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u8c03\u6574\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf2D\u878d\u5408\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u878d\u5408\u56fe\u50cf\u6e32\u67d3\u3002"}}
{"id": "2601.12714", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12714", "abs": "https://arxiv.org/abs/2601.12714", "authors": ["Songlin Dong", "Jiangyang Li", "Chenhao Ding", "Zhiheng Ma", "Haoyu Luo", "Yuhang He", "Yihong Gong"], "title": "P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning", "comment": "12 pages, 5 figures", "summary": "Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.", "AI": {"tldr": "P2L-CA\uff1a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5230\u6807\u7b7e\u6a21\u5757\u548c\u8fde\u7eed\u9002\u914d\u5668\u6a21\u5757\u89e3\u51b3\u7279\u5f81\u6df7\u6dc6\u548c\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a\u4e14\u53c2\u6570\u91cf\u5c11\u3002", "motivation": "\u73b0\u6709\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\uff08\u5168\u53c2\u6570\u5fae\u8c03\uff09\u3001\u5b58\u50a8\u5f00\u9500\u5927\uff08\u5185\u5b58\u7f13\u51b2\u533a\uff09\u4ee5\u53ca\u96be\u4ee5\u5145\u5206\u89e3\u51b3\u7279\u5f81\u6df7\u6dc6\u548c\u9886\u57df\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faP2L-CA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09P2L\u6a21\u5757\u4f7f\u7528\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u89e3\u8026\u591a\u6807\u7b7e\u8868\u793a\uff0c\u5e76\u5229\u7528\u8bed\u8a00\u5148\u9a8c\u7a33\u5b9a\u8bed\u4e49-\u89c6\u89c9\u5bf9\u9f50\uff1b2\uff09CA\u6a21\u5757\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u7f13\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u589e\u5f3a\u6a21\u578b\u53ef\u5851\u6027\u3002", "result": "\u5728MS-COCO\u548cPASCAL VOC\u7684\u6807\u51c6\u548c\u6311\u6218\u6027MLCIL\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cP2L-CA\u4e0d\u4ec5\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fd8\u5728CIL\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4ec5\u9700\u6781\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u4e14\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a\u3002", "conclusion": "P2L-CA\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u3001\u5b58\u50a8\u5f00\u9500\u548c\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12715", "abs": "https://arxiv.org/abs/2601.12715", "authors": ["Chengzhou Li", "Ping Guo", "Guanchen Meng", "Qi Jia", "Jinyuan Liu", "Zhu Liu", "Xiaokang Liu", "Yu Liu", "Zhongxuan Luo", "Xin Fan"], "title": "RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels", "comment": "Accepted by AAAI 2026,9 pages,10 figures", "summary": "Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.", "AI": {"tldr": "\u63d0\u51faRSOD\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u8bc4\u5206\u548c\u5bf9\u8c61\u6df7\u5408\u4f2a\u6807\u7b7e\u7b56\u7565\u89e3\u51b3\u58f0\u7eb3\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5728\u4ec55%\u6807\u6ce8\u6570\u636e\u4e0b\u8fbe\u5230\u4e0e100%\u6807\u6ce8\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u58f0\u7eb3\u56fe\u50cf\u76f8\u6bd4\u81ea\u7136\u56fe\u50cf\u7eb9\u7406\u7ec6\u8282\u5c11\u3001\u566a\u58f0\u591a\uff0c\u975e\u4e13\u5bb6\u96be\u4ee5\u533a\u5206\u7c7b\u522b\u5dee\u5f02\uff0c\u5bfc\u81f4\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u6807\u6ce8\u6570\u636e\u3002\u56e0\u6b64\uff0c\u5728\u6807\u6ce8\u6570\u636e\u6781\u5176\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8bbe\u8ba1\u6709\u6548\u7684\u58f0\u7eb3\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51faRSOD\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff1a1) \u901a\u8fc7\u8bc4\u4f30\u6559\u5e08\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u56fe\u4e0b\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u8ba1\u7b97\u53ef\u9760\u6027\u8bc4\u5206\uff1b2) \u5f15\u5165\u5bf9\u8c61\u6df7\u5408\u4f2a\u6807\u7b7e\u65b9\u6cd5\u89e3\u51b3\u58f0\u7eb3\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff1b3) \u5b9e\u65bd\u53ef\u9760\u6027\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u7ea6\u675f\u4f18\u5316\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728UATD\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u75285%\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u7ed3\u679c\u53ef\u4e0e\u4f7f\u7528100%\u6807\u6ce8\u6570\u636e\u7684\u57fa\u7ebf\u7b97\u6cd5\u76f8\u7ade\u4e89\u3002\u540c\u65f6\u6536\u96c6\u4e86\u65b0\u6570\u636e\u96c6\u4e3a\u58f0\u7eb3\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u66f4\u591a\u6709\u4ef7\u503c\u6570\u636e\u3002", "conclusion": "RSOD\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5728\u6807\u6ce8\u6570\u636e\u6781\u5176\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u58f0\u7eb3\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.12719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12719", "abs": "https://arxiv.org/abs/2601.12719", "authors": ["Lin Zhao", "Yushu Wu", "Aleksei Lebedev", "Dishani Lahiri", "Meng Dong", "Arpit Sahni", "Michael Vasilkovsky", "Hao Chen", "Ju Hu", "Aliaksandr Siarohin", "Sergey Tulyakov", "Yanzhi Wang", "Anil Kag", "Yanyu Li"], "title": "S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation", "comment": null, "summary": "Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.", "AI": {"tldr": "S2DiT\u662f\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u9ad8\u6548\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u89c4\u5212\u641c\u7d22\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u751f\u6210", "motivation": "\u5f53\u524dDiffusion Transformers\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8d28\u91cf\u867d\u597d\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6216\u8bbe\u5907\u7aef\u751f\u6210", "method": "\u63d0\u51faS2DiT\u6a21\u578b\uff0c\u91c7\u7528LinConv Hybrid Attention\u548cStride Self-Attention\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u9884\u7b97\u611f\u77e5\u52a8\u6001\u89c4\u5212\u641c\u7d22\u8bbe\u8ba1sandwich\u7ed3\u6784\uff0c\u5e76\u4f7f\u75282-in-1\u84b8\u998f\u6846\u67b6\u5c06\u5927\u6a21\u578b\u80fd\u529b\u8fc1\u79fb\u5230\u7d27\u51d1\u6a21\u578b", "result": "S2DiT\u5728\u8d28\u91cf\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u670d\u52a1\u5668\u89c6\u9891\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5728iPhone\u4e0a\u5b9e\u73b0\u8d85\u8fc710 FPS\u7684\u6d41\u5f0f\u751f\u6210", "conclusion": "S2DiT\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u8bbe\u5907\u7aef\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.11979", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11979", "abs": "https://arxiv.org/abs/2601.11979", "authors": ["Ang Gao", "Changshuo Zhang", "Xiao Zhang", "Deyang Li", "Minjun Zhao", "Fangchao Liu", "Xinyu Zhang"], "title": "Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion", "comment": null, "summary": "In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.", "AI": {"tldr": "\u63d0\u51faProcess In-Context Learning (PICL)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63d2\u5165\u76f8\u5173\u6f14\u793a\u6765\u89e3\u51b3\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5b9e\u65f6\u56f0\u60d1\u70b9\uff0c\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709ICL\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u7b49\u9700\u8981\u9010\u6b65\u903b\u8f91\u63a8\u5bfc\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u4f7f\u7528\u9759\u6001\u7684\u6f14\u793a\u793a\u4f8b\uff0c\u65e0\u6cd5\u9002\u5e94\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u52a8\u6001\u56f0\u60d1\u70b9\uff08\u5982\u6a21\u7cca\u8ba1\u7b97\u6216\u903b\u8f91\u6f0f\u6d1e\uff09\uff0c\u8fd9\u4e9b\u672a\u89e3\u51b3\u7684\u56f0\u60d1\u70b9\u4f1a\u5bfc\u81f4\u7ea7\u8054\u9519\u8bef\uff0c\u964d\u4f4e\u6700\u7ec8\u51c6\u786e\u6027\u3002", "method": "PICL\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u901a\u8fc7\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u548c\u71b5\u6765\u8bc6\u522b\u6f5c\u5728\u56f0\u60d1\u70b9\uff0c\u5e76\u603b\u7ed3\u5176\u6838\u5fc3\u7279\u5f81\uff1b2) \u5f53\u9047\u5230\u8fd9\u4e9b\u56f0\u60d1\u70b9\u65f6\uff0c\u4ece\u6f14\u793a\u6c60\u4e2d\u68c0\u7d22\u4e0e\u56f0\u60d1\u4e0a\u4e0b\u6587\u5339\u914d\u7684\u76f8\u5173\u6f14\u793a\uff0c\u5e76\u5c06\u5176\u76f4\u63a5\u63d2\u5165\u5230\u6b63\u5728\u8fdb\u884c\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u6307\u5bfc\u540e\u7eed\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePICL\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f13\u89e3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u56f0\u60d1\u70b9\uff0c\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u6f14\u793a\u63d2\u5165\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "PICL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u96c6\u6210\u6f14\u793a\u6765\u54cd\u5e94\u5b9e\u65f6\u63a8\u7406\u9700\u6c42\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u52a8\u6001\u56f0\u60d1\u70b9\u95ee\u9898\uff0c\u4e3a\u9700\u8981\u9010\u6b65\u903b\u8f91\u63a8\u5bfc\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684ICL\u65b9\u6cd5\u3002"}}
{"id": "2601.12729", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12729", "abs": "https://arxiv.org/abs/2601.12729", "authors": ["Hanyu Zhu", "Zhihao Zhan", "Yuhang Ming", "Liang Li", "Dibo Hou", "Javier Civera", "Wanzeng Kong"], "title": "DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition", "comment": "10 pages, 4 figures, 5 tables", "summary": "One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.", "AI": {"tldr": "DC-VLAQ\uff1a\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7684\u8868\u793a\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5f15\u5bfc\u7684\u4e92\u8865\u878d\u5408\u548c\u5411\u91cf\u5316\u5c40\u90e8\u805a\u5408\u67e5\u8be2\uff0c\u6574\u5408\u4e0d\u540c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u63d0\u5347\u5728\u89c6\u89d2\u53d8\u5316\u3001\u5149\u7167\u53d8\u5316\u548c\u57df\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u7684\u4e92\u8865\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5229\u7528\u8fd9\u4e9b\u4e92\u8865\u4fe1\u606f\u4f1a\u6539\u53d8token\u5206\u5e03\uff0c\u6311\u6218\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u7684\u5168\u5c40\u805a\u5408\u65b9\u6848\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faDC-VLAQ\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u5f15\u5bfc\u4e92\u8865\u878d\u5408\uff0c\u4ee5DINOv2\u7279\u5f81\u7a7a\u95f4\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6b8b\u5dee\u6821\u6b63\u6ce8\u5165CLIP\u7684\u4e92\u8865\u8bed\u4e49\uff1b2\uff09\u5411\u91cf\u5316\u5c40\u90e8\u805a\u5408\u67e5\u8be2\uff0c\u901a\u8fc7\u7f16\u7801\u5c40\u90e8token\u5bf9\u53ef\u5b66\u4e60\u67e5\u8be2\u7684\u6b8b\u5dee\u54cd\u5e94\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5168\u5c40\u805a\u5408\u5e76\u4fdd\u7559\u7ec6\u7c92\u5ea6\u5224\u522b\u7ebf\u7d22\u3002", "result": "\u5728Pitts30k\u3001Tokyo24/7\u3001MSLS\u3001Nordland\u3001SPED\u548cAmsterTime\u7b49\u6807\u51c6VPR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDC-VLAQ\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u57df\u504f\u79fb\u548c\u957f\u671f\u5916\u89c2\u53d8\u5316\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DC-VLAQ\u901a\u8fc7\u6709\u6548\u878d\u5408\u4e0d\u540c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e92\u8865\u4fe1\u606f\u5e76\u91c7\u7528\u7a33\u5b9a\u7684\u5168\u5c40\u805a\u5408\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u5728\u590d\u6742\u73af\u5883\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5224\u522b\u80fd\u529b\u3002"}}
{"id": "2601.12736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12736", "abs": "https://arxiv.org/abs/2601.12736", "authors": ["Qingtian Zhu", "Xu Cao", "Zhixiang Wang", "Yinqiang Zheng", "Takafumi Taketomi"], "title": "KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction", "comment": null, "summary": "We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.", "code_url": "https://github.com/CyberAgentAILab/KaoLRM", "AI": {"tldr": "KaoLRM\uff1a\u5229\u7528\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7FLAME\u53c2\u6570\u5316\u6a21\u578b\u548c2D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b9e\u73b0\u5355\u89c6\u89d2\u56fe\u50cf\u7684\u4eba\u81383D\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u7cbe\u5ea6", "motivation": "\u73b0\u67093D\u5f62\u53d8\u6a21\u578b\uff083DMM\uff09\u56de\u5f52\u5668\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5bf9\u89c6\u89d2\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u63d0\u5347\u5355\u89c6\u89d2\u4eba\u8138\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3\u7684LRM\u4e09\u7ef4\u5148\u9a8c\u77e5\u8bc6\uff1b2. \u5c06LRM\u7684\u4e09\u5e73\u9762\u7279\u5f81\u6295\u5f71\u5230FLAME\u53c2\u6570\u7a7a\u95f4\u6062\u590d\u51e0\u4f55\uff1b3. \u4f7f\u7528\u4e0eFLAME\u7f51\u683c\u7d27\u5bc6\u8026\u5408\u76842D\u9ad8\u65af\u57fa\u5143\u5efa\u6a21\u5916\u89c2\uff1b4. \u5728LRM\u6e32\u67d3\u7ba1\u7ebf\u4e2d\u96c6\u6210FLAME-based 2D\u9ad8\u65af\u6cfc\u6e85", "result": "\u5728\u53d7\u63a7\u548c\u91ce\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKaoLRM\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u81ea\u906e\u6321\u548c\u591a\u6837\u89c6\u89d2\u5177\u6709\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4ecd\u5bf9\u89c6\u89d2\u53d8\u5316\u654f\u611f", "conclusion": "KaoLRM\u901a\u8fc7\u5229\u7528LRM\u7684\u4e30\u5bcc\u5148\u9a8c\u77e5\u8bc6\u5e76\u7ed3\u5408\u53c2\u6570\u53163DMM\u4e0e\u795e\u7ecf\u6e32\u67d3\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u89c6\u89d2\u4e00\u81f4\u7684\u5355\u89c6\u89d2\u4eba\u8138\u91cd\u5efa\uff0c\u4e3a3D\u4eba\u8138\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.12747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12747", "abs": "https://arxiv.org/abs/2601.12747", "authors": ["Jingkai Li", "Xiaoze Tian", "Yuhang Shen", "Jia Wang", "Dianjie Lu", "Guijuan Zhang", "Zhuoran Zheng"], "title": "SSPFormer: Self-Supervised Pretrained Transformer for MRI Images", "comment": "Undergraduate student as first author submitted to IJCAI", "summary": "The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.", "AI": {"tldr": "SSPFormer\uff1a\u4e00\u79cd\u7528\u4e8eMRI\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3Transformer\uff0c\u901a\u8fc7\u9006\u9891\u7387\u6295\u5f71\u63a9\u7801\u548c\u9891\u7387\u52a0\u6743FFT\u566a\u58f0\u589e\u5f3a\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u9886\u57df\u9002\u5e94\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u5206\u5272\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u566a\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3Transformer\u5728\u81ea\u7136\u56fe\u50cf\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8eMRI\u56fe\u50cf\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u65e0\u6cd5\u9002\u5e94\u533b\u5b66\u89e3\u5256\u7ed3\u6784\u7684\u7279\u5f02\u6027\uff1b2\uff09\u533b\u5b66\u6570\u636e\u7684\u9690\u79c1\u6027\u548c\u7a00\u7f3a\u6027\u5e26\u6765\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faSSPFormer\u81ea\u76d1\u7763\u9884\u8bad\u7ec3Transformer\uff0c\u91c7\u7528\u4e24\u79cd\u6838\u5fc3\u7b56\u7565\uff1a1\uff09\u9006\u9891\u7387\u6295\u5f71\u63a9\u7801\uff0c\u4f18\u5148\u91cd\u5efa\u9ad8\u9891\u89e3\u5256\u533a\u57df\u4ee5\u5f3a\u5236\u7ed3\u6784\u611f\u77e5\u8868\u793a\u5b66\u4e60\uff1b2\uff09\u9891\u7387\u52a0\u6743FFT\u566a\u58f0\u589e\u5f3a\uff0c\u5728\u5085\u91cc\u53f6\u57df\u6ce8\u5165\u751f\u7406\u771f\u5b9e\u7684\u566a\u58f0\u4ee5\u589e\u5f3a\u5bf9MRI\u4f2a\u5f71\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5206\u5272\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u566a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSSPFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u5176\u6355\u83b7\u7ec6\u7c92\u5ea6MRI\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u9002\u5e94\u4e34\u5e8a\u5e94\u7528\u9700\u6c42\u7684\u80fd\u529b\u3002", "conclusion": "SSPFormer\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u9886\u57df\u9002\u5e94\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u80fd\u591f\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u548c\u4f2a\u5f71\u9c81\u68d2\u7684\u7279\u5f81\uff0c\u4e3a\u4e34\u5e8aMRI\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12770", "abs": "https://arxiv.org/abs/2601.12770", "authors": ["Shuling Zhao", "Dan Xu"], "title": "Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image", "comment": "Project page: https://shaelynz.github.io/fhavatar/", "summary": "Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u53ef\u52a8\u753b\u5316\u5934\u90e8\u865a\u62df\u5f62\u8c61\u7684\u5355\u6b21\u524d\u9988\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u548c360\u5ea6\u6e32\u67d3\uff0c\u901a\u8fc7\u9ad8\u65af\u57fa\u5143\u5efa\u6a21\u30013D GAN\u5148\u9a8c\u548cUV\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u53d8\u5316\u4e0b\u5bb9\u6613\u5931\u6548\uff0c\u5f71\u54cd3D\u865a\u62df\u5f62\u8c61\u7684\u771f\u5b9e\u611f\u3002\u9700\u8981\u89e3\u51b3\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u53ef\u52a8\u753b\u5316\u5934\u90e8\u865a\u62df\u5f62\u8c61\u7684\u6311\u6218\uff0c\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u548c\u5168\u65b9\u4f4d\u6e32\u67d3\u3002", "method": "1) \u5728\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578b\u7684UV\u7a7a\u95f4\u8868\u9762\u5d4c\u5165\u9ad8\u65af\u57fa\u5143\u8fdb\u884c3D\u5934\u90e8\u5efa\u6a21\uff1b2) \u5229\u7528\u9884\u8bad\u7ec33D GAN\u63d0\u53d6\u5168\u5c40\u5168\u5934\u90e8\u7279\u5f81\u5e76\u63d0\u4f9b\u591a\u89c6\u89d2\u76d1\u7763\uff1b3) \u5229\u7528UV\u7a7a\u95f4\u548c\u4eba\u8138\u7684\u5bf9\u79f0\u6027\uff0c\u5c06\u5c40\u90e8\u7ec6\u7c92\u5ea6\u8f93\u5165\u56fe\u50cf\u7279\u5f81\u4e0e\u5168\u5c40\u5168\u5934\u90e8\u7eb9\u7406\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u5168\u5934\u90e8\u5efa\u6a21\u548c\u5b9e\u65f6\u52a8\u753b\uff0c\u63d0\u5347\u4e863D\u8bf4\u8bdd\u865a\u62df\u5f62\u8c61\u7684\u771f\u5b9e\u611f\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c6\u89d2\u53d8\u5316\u5e76\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u53ef\u52a8\u753b\u5316\u5934\u90e8\u865a\u62df\u5f62\u8c61\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u57fa\u5143\u5efa\u6a21\u30013D GAN\u5148\u9a8c\u548c\u7279\u5f81\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u5b9e\u65f6\u52a8\u753b\u7684\u5168\u5934\u90e8\u865a\u62df\u5f62\u8c61\u91cd\u5efa\u3002"}}
{"id": "2601.12259", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12259", "abs": "https://arxiv.org/abs/2601.12259", "authors": ["Jiashuo Liu", "Siyuan Chen", "Zaiyuan Wang", "Zhiyuan Zeng", "Jiacheng Guo", "Liang Hu", "Lingyue Yin", "Suozhi Huang", "Wenxin Hao", "Yang Yang", "Zerui Cheng", "Zixin Yao", "Lingyue Yin", "Haoxin Liu", "Jiayi Cheng", "Yuzhen Li", "Zezhong Ma", "Bingjie Wang", "Bingsen Qiu", "Xiao Liu", "Zeyang Zhang", "Zijian Liu", "Jinpeng Wang", "Mingren Yin", "Tianci He", "Yali Liao", "Yixiao Tian", "Zhenwei Zhu", "Anqi Dai", "Ge Zhang", "Jingkai Liu", "Kaiyuan Zhang", "Wenlong Wu", "Xiang Gao", "Xinjie Chen", "Zhixin Yao", "Zhoufutu Wen", "B. Aditya Prakash", "Jose Blanchet", "Mengdi Wang", "Nian Si", "Wenhao Huang"], "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains", "comment": "21 pages", "summary": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.", "AI": {"tldr": "FutureX-Pro\u6269\u5c55\u4e86FutureX\u57fa\u51c6\uff0c\u4e3a\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u548c\u81ea\u7136\u707e\u5bb3\u56db\u4e2a\u9ad8\u4ef7\u503c\u5782\u76f4\u9886\u57df\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u672a\u6765\u9884\u6d4b\u6846\u67b6\uff0c\u8bc4\u4f30\u5f53\u524dSOTA\u667a\u80fd\u4f53LLM\u5728\u8fd9\u4e9b\u5173\u952e\u9886\u57df\u7684\u5de5\u4e1a\u90e8\u7f72\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u901a\u7528\u667a\u80fd\u4f53\u5728\u5f00\u653e\u9886\u57df\u641c\u7d22\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d44\u672c\u5bc6\u96c6\u578b\u548c\u5b89\u5168\u5173\u952e\u884c\u4e1a\u7684\u53ef\u9760\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53LLM\u662f\u5426\u5177\u5907\u5de5\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u9886\u57df\u57fa\u7840\u3002", "method": "\u57fa\u4e8eFutureX\u7684\u65e0\u6c61\u67d3\u5b9e\u65f6\u8bc4\u4f30\u7ba1\u9053\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4e94\u4e2a\u5b50\u9886\u57df\u7684FutureX-Pro\u6846\u67b6\uff1aFutureX-Finance\u3001FutureX-Retail\u3001FutureX-PublicHealth\u3001FutureX-NaturalDisaster\u548cFutureX-Search\u3002\u5728\u56db\u4e2a\u5173\u952e\u5782\u76f4\u9886\u57df\uff08\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u3001\u81ea\u7136\u707e\u5bb3\uff09\u4e0a\u5bf9\u667a\u80fd\u4f53LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u5e02\u573a\u6307\u6807\u9884\u6d4b\u3001\u4f9b\u5e94\u94fe\u9700\u6c42\u9884\u6d4b\u3001\u6d41\u884c\u75c5\u8d8b\u52bf\u8ddf\u8e2a\u548c\u81ea\u7136\u707e\u5bb3\u76d1\u6d4b\u7b49\u57fa\u7840\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53LLM\u5728\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u9ad8\u4ef7\u503c\u5782\u76f4\u5e94\u7528\u6240\u9700\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "FutureX-Pro\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53LLM\u5728\u5173\u952e\u5782\u76f4\u9886\u57df\u7684\u9884\u6d4b\u80fd\u529b\u63d0\u4f9b\u4e86\u4e13\u95e8\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u57fa\u7840\u5bf9\u4e8e\u9ad8\u4ef7\u503c\u5e94\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.12779", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12779", "abs": "https://arxiv.org/abs/2601.12779", "authors": ["Nafis Sadeq", "Qingfeng Liu", "Mostafa El-Khamy"], "title": "Open Vocabulary Panoptic Segmentation With Retrieval Augmentation", "comment": null, "summary": "Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.", "AI": {"tldr": "RetCLIP\uff1a\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u5168\u666f\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u6570\u636e\u5e93\uff0c\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u4f3c\u7279\u5f81\u548c\u7c7b\u522b\u6807\u7b7e\uff0c\u7ed3\u5408CLIP\u5206\u6570\u63d0\u5347\u672a\u89c1\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u666f\u5206\u5272\u65b9\u6cd5\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u5f00\u653e\u8bcd\u6c47\u5168\u666f\u5206\u5272\u9700\u8981\u6839\u636e\u7528\u6237\u8f93\u5165\u5206\u5272\u4efb\u610f\u7c7b\u522b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRetCLIP\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u56fe\u6587\u5bf9\u6570\u636e\u6784\u5efa\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u6570\u636e\u5e93\uff1b2\uff09\u63a8\u7406\u65f6\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u7684\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u4f5c\u4e3a\u67e5\u8be2\u952e\uff0c\u4ece\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76f8\u4f3c\u7279\u5f81\u53ca\u76f8\u5173\u7c7b\u522b\u6807\u7b7e\uff1b3\uff09\u57fa\u4e8e\u67e5\u8be2\u7279\u5f81\u4e0e\u68c0\u7d22\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u5206\u914d\u5206\u7c7b\u5206\u6570\uff1b4\uff09\u5c06\u68c0\u7d22\u5206\u7c7b\u5206\u6570\u4e0eCLIP\u5206\u6570\u7ed3\u5408\u5f97\u5230\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728COCO\u4e0a\u8bad\u7ec3\uff0c\u5728ADE20k\u6570\u636e\u96c6\u4e0a\u8fbe\u523030.9 PQ\u300119.3 mAP\u300144.0 mIoU\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08FC-CLIP\uff09\u5206\u522b\u63d0\u5347+4.5 PQ\u3001+2.5 mAP\u3001+10.0 mIoU\u7684\u7edd\u5bf9\u6539\u8fdb\u3002", "conclusion": "RetCLIP\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u5168\u666f\u5206\u5272\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u68c0\u7d22\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.12791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12791", "abs": "https://arxiv.org/abs/2601.12791", "authors": ["Zhihan Zeng", "Yang Zhao", "Kaihe Wang", "Dusit Niyato", "Hongyuan Shu", "Junchu Zhao", "Yanjun Huang", "Yue Xiu", "Zhongpei Zhang", "Ning Wei"], "title": "SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification", "comment": null, "summary": "As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.", "AI": {"tldr": "SKANet\uff1a\u4e00\u79cd\u57fa\u4e8e\u53cc\u6d41\u67b6\u6784\u7684\u8ba4\u77e5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6838\u4e0e\u4e0d\u5bf9\u79f0\u5377\u79ef\u5757\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\uff0c\u7ed3\u5408SE\u673a\u5236\u81ea\u9002\u5e94\u6821\u51c6\u591a\u6a21\u6001\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0bGNSS\u590d\u5408\u5e72\u6270\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7535\u78c1\u73af\u5883\u65e5\u76ca\u590d\u6742\uff0cGNSS\u9762\u4e34\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u5e72\u6270\u5a01\u80c1\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u57fa\u672c\u5e72\u6270\uff0c\u4f46\u5bf9\u590d\u5408\u5e72\u6270\u5206\u7c7b\u56f0\u96be\uff0c\u56e0\u4e3a\u4e0d\u540c\u5e72\u6270\u6e90\u53e0\u52a0\u4e14\u7279\u5f81\u5c3a\u5ea6\u51b2\u7a81\uff08\u77ac\u6001\u7a81\u53d1\u4fe1\u53f7\u9700\u8981\u5fae\u5c3a\u5ea6\u7279\u5f81\uff0c\u8fde\u7eed\u5168\u5c40\u4fe1\u53f7\u9700\u8981\u5b8f\u5c3a\u5ea6\u7279\u5f81\uff09\u3002\u5355\u57df\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u65b0\u7684\u591a\u6a21\u6001\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u65b9\u6848\u3002", "method": "\u63d0\u51faSKANet\u6846\u67b6\uff1a1\uff09\u53cc\u6d41\u67b6\u6784\u540c\u65f6\u5904\u7406\u65f6\u9891\u56fe\u50cf\uff08TFI\uff09\u548c\u529f\u7387\u8c31\u5bc6\u5ea6\uff08PSD\uff09\uff1b2\uff09\u591a\u5206\u652f\u9009\u62e9\u6027\u6838\uff08SK\uff09\u6a21\u5757\u7ed3\u5408\u4e0d\u5bf9\u79f0\u5377\u79ef\u5757\uff08ACB\uff09\uff0c\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\uff0c\u81ea\u9002\u5e94\u6355\u83b7\u5fae\u5c3a\u5ea6\u77ac\u6001\u7279\u5f81\u548c\u5b8f\u5c3a\u5ea6\u9891\u8c31\u8d8b\u52bf\uff1b3\uff09\u878d\u5408\u9636\u6bb5\u96c6\u6210SE\u673a\u5236\uff0c\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u5404\u6a21\u6001\u5f02\u8d28\u7279\u5f81\u7684\u8d21\u732e\u5ea6\u3002", "result": "\u5728405,000\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cSKANet\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523096.99%\uff0c\u5728\u590d\u5408\u5e72\u6270\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5e72\u566a\u6bd4\uff08JNR\uff09\u6761\u4ef6\u4e0b\u6027\u80fd\u7a81\u51fa\u3002", "conclusion": "SKANet\u901a\u8fc7\u52a8\u6001\u611f\u53d7\u91ce\u8c03\u6574\u548c\u591a\u6a21\u6001\u7279\u5f81\u81ea\u9002\u5e94\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u5408\u5e72\u6270\u5206\u7c7b\u4e2d\u7279\u5f81\u5c3a\u5ea6\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684GNSS\u5e72\u6270\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12798", "abs": "https://arxiv.org/abs/2601.12798", "authors": ["Zhihan Zeng", "Yang Zhao", "Kaihe Wang", "Dusit Niyato", "Yue Xiu", "Lu Chen", "Zhongpei Zhang", "Ning Wei"], "title": "PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition", "comment": null, "summary": "Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \\textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \\textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.", "AI": {"tldr": "PhyG-MoE\uff1a\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bb9\u91cf\u6765\u5339\u914d\u4fe1\u53f7\u590d\u6742\u5ea6\uff0c\u89e3\u51b3GNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u9759\u6001\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dGNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u91c7\u7528\u56fa\u5b9a\u7684\u8ba1\u7b97\u62d3\u6251\u7ed3\u6784\uff0c\u65e0\u8bba\u8f93\u5165\u4fe1\u53f7\u7684\u7269\u7406\u71b5\u5982\u4f55\u53d8\u5316\u3002\u8fd9\u79cd\u521a\u6027\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u7b80\u5355\u4fe1\u53f7\u548c\u590d\u6742\u9971\u548c\u4fe1\u53f7\u6d88\u8017\u76f8\u540c\u7684\u5904\u7406\u6210\u672c\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8ba4\u77e5\u63a5\u6536\u5668\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faPhyG-MoE\uff08\u7269\u7406\u5f15\u5bfc\u7684\u6df7\u5408\u4e13\u5bb6\uff09\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u9891\u8c31\u7684\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u4fe1\u53f7\u9891\u8c31\u7279\u5f81\u7ea0\u7f20\u7a0b\u5ea6\u8fdb\u884c\u8def\u7531\u3002\u7cfb\u7edf\u5305\u542b\u9ad8\u5bb9\u91cf\u7684TransNeXt\u4e13\u5bb6\uff08\u7528\u4e8e\u5904\u7406\u590d\u6742\u9971\u548c\u573a\u666f\uff09\u548c\u8f7b\u91cf\u7ea7\u4e13\u5bb6\uff08\u5904\u7406\u57fa\u7840\u4fe1\u53f7\uff09\uff0c\u5b9e\u73b0\u6a21\u578b\u5bb9\u91cf\u4e0e\u4fe1\u53f7\u590d\u6742\u5ea6\u7684\u52a8\u6001\u5bf9\u9f50\u3002", "result": "\u572821\u79cd\u5e72\u6270\u7c7b\u522b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPhyG-MoE\u8fbe\u523097.58%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u4e0b\u964d\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u8ba1\u7b97\u4e0e\u52a8\u6001\u7535\u78c1\u73af\u5883\u4e4b\u95f4\u7684\u5185\u5728\u51b2\u7a81\u3002", "conclusion": "PhyG-MoE\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bb9\u91cf\u6765\u5339\u914d\u4fe1\u53f7\u590d\u6742\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8ba4\u77e5\u63a5\u6536\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u9759\u6001\u6a21\u578b\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2601.12499", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12499", "abs": "https://arxiv.org/abs/2601.12499", "authors": ["Meiru Zhang", "Zaiqiao Meng", "Nigel Collier"], "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck", "comment": "preprint", "summary": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMFAI\u8bed\u4e49\u63a2\u9488\uff0c\u63ed\u793a\u4e86LLMs\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\"\u6700\u5f31\u73af\u8282\u5b9a\u5f8b\"\uff1a\u6027\u80fd\u53d6\u51b3\u4e8e\u6700\u4e0d\u53ef\u89c1\u8bc1\u636e\u7684\u4f4d\u7f6e\uff0c\u800c\u975e\u4e8b\u5b9e\u95f4\u7684\u7ebf\u6027\u8ddd\u79bb\uff0c\u5e76\u53d1\u73b0\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53cc\u91cd\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u62e5\u6709\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\uff0c\u5bfc\u81f4\u5ffd\u7565\u67d0\u4e9b\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\u9700\u8981\u533a\u5206\u8fd9\u79cd\u5931\u8d25\u662f\u7531\u4e8e\u65e0\u6cd5\u5b9a\u4f4d\u8bc1\u636e\uff08\u8bc6\u522b\u5931\u8d25\uff09\u8fd8\u662f\u65e0\u6cd5\u6574\u5408\u8bc1\u636e\uff08\u7efc\u5408\u5931\u8d25\uff09\u3002", "method": "\u5f15\u5165\u591a\u7126\u70b9\u6ce8\u610f\u529b\u6307\u4ee4\uff08MFAI\uff09\u4f5c\u4e3a\u8bed\u4e49\u63a2\u9488\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u9009\u5b9a\u4f4d\u7f6e\u6765\u89e3\u8026\u8bc6\u522b\u548c\u7efc\u5408\u673a\u5236\u3002\u57285\u4e2aLLMs\u4e0a\u5bf9\u4e24\u4e2a\u591a\u8df3QA\u4efb\u52a1\uff08MuSiQue\u548cNeoQA\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\"\u6700\u5f31\u73af\u8282\u5b9a\u5f8b\"\uff1a\u591a\u8df3\u63a8\u7406\u6027\u80fd\u5d29\u6e83\u5230\u6700\u4e0d\u53ef\u89c1\u8bc1\u636e\u7684\u6027\u80fd\u6c34\u5e73\u3002\u5931\u8d25\u7531\u7edd\u5bf9\u4f4d\u7f6e\u800c\u975e\u4e8b\u5b9e\u95f4\u7ebf\u6027\u8ddd\u79bb\u51b3\u5b9a\uff08\u6027\u80fd\u65b9\u5dee<3%\uff09\u3002\u5339\u914d\u7684MFAI\u53ef\u89e3\u51b3\u8bc6\u522b\u74f6\u9888\uff0c\u5728\u4f4e\u53ef\u89c1\u6027\u4f4d\u7f6e\u63d0\u9ad8\u51c6\u786e\u7387\u8fbe11.5%\uff1b\u8bef\u5bfc\u6027MFAI\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u5f15\u53d1\u6df7\u6dc6\u4f46\u5728\u5408\u6210\u4efb\u52a1\u4e2d\u88ab\u8fc7\u6ee4\u3002\u4f7f\u7528System-2\u63a8\u7406\u7684\"\u601d\u8003\"\u6a21\u578b\u80fd\u6709\u6548\u5b9a\u4f4d\u548c\u6574\u5408\u4fe1\u606f\u3002", "conclusion": "LLMs\u7684\u591a\u8df3\u63a8\u7406\u5931\u8d25\u4e3b\u8981\u7531\u4f4d\u7f6e\u504f\u89c1\u5bfc\u81f4\u7684\u8bc6\u522b\u5931\u8d25\u9a71\u52a8\uff0c\u800c\u975e\u7efc\u5408\u5931\u8d25\u3002\u6ce8\u610f\u529b\u5f15\u5bfc\u5177\u6709\u53cc\u91cd\u6027\uff0c\u800cSystem-2\u63a8\u7406\u6a21\u578b\u80fd\u6709\u6548\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5728\u5608\u6742\u957f\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\u5339\u914d\u9ec4\u91d1\u57fa\u51c6\u3002"}}
{"id": "2601.12809", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12809", "abs": "https://arxiv.org/abs/2601.12809", "authors": ["Takaki Yamamoto", "Chihiro Noguchi", "Toshihiro Tanizawa"], "title": "Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data", "comment": null, "summary": "Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u53ef\u63a7\u76841D\u56fe\u50cf-\u6587\u672c\u6d4b\u8bd5\u5e73\u53f0\u63a2\u7a76CLIP\u98ce\u683c\u6a21\u578b\u4e2d\u5de6\u53f3\u5173\u7cfb\u7406\u89e3\u7684\u6d8c\u73b0\u673a\u5236\uff0c\u53d1\u73b0\u6807\u7b7e\u591a\u6837\u6027\u662f\u6cdb\u5316\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u89e3\u63ed\u793a\u4e86\u4f4d\u7f6e\u4e0e\u8bcd\u5143\u5d4c\u5165\u7684\u4ea4\u4e92\u5982\u4f55\u6253\u7834\u5de6\u53f3\u5bf9\u79f0\u6027\u3002", "motivation": "\u7a7a\u95f4\u7406\u89e3\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u7406\u89e3\u662f\u5426\u771f\u6b63\u83b7\u5f97\u4ee5\u53ca\u901a\u8fc7\u4f55\u79cd\u673a\u5236\u83b7\u5f97\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76Transformer-based\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u5728CLIP\u98ce\u683c\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u4e0b\uff0c\u5de6\u53f3\u5173\u7cfb\u7406\u89e3\u5982\u4f55\u6d8c\u73b0\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7Transformer-based\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5904\u7406\u5355\u7269\u4f53\u548c\u53cc\u7269\u4f53\u573a\u666f\u7684\u914d\u5bf9\u63cf\u8ff0\u3002\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u6807\u7b7e\u548c\u5e03\u5c40\u591a\u6837\u6027\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u89e3\u5206\u6790\u4f4d\u7f6e\u5d4c\u5165\u548c\u8bcd\u5143\u5d4c\u5165\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5bf9\u6bd4\u8bad\u7ec3\u80fd\u591f\u5b66\u4e60\u5de6\u53f3\u5173\u7cfb\uff0c\u6807\u7b7e\u591a\u6837\u6027\uff08\u800c\u975e\u5e03\u5c40\u591a\u6837\u6027\uff09\u662f\u6cdb\u5316\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002\u6ce8\u610f\u529b\u5206\u89e3\u663e\u793a\u4f4d\u7f6e\u5d4c\u5165\u548c\u8bcd\u5143\u5d4c\u5165\u7684\u4ea4\u4e92\u8bf1\u5bfc\u4e86\u6c34\u5e73\u6ce8\u610f\u529b\u68af\u5ea6\uff0c\u6253\u7834\u4e86\u7f16\u7801\u5668\u7684\u5de6\u53f3\u5bf9\u79f0\u6027\uff1b\u6d88\u9664\u8fd9\u4e00\u8d21\u732e\u4f1a\u663e\u8457\u964d\u4f4e\u5de6\u53f3\u8fa8\u522b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86CLIP\u98ce\u683c\u6a21\u578b\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u83b7\u5f97\u5173\u7cfb\u80fd\u529b\u7684\u673a\u5236\u6027\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u6807\u7b7e\u591a\u6837\u6027\u5728\u7a7a\u95f4\u5173\u7cfb\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u548c\u4f4d\u7f6e-\u8bcd\u5143\u4ea4\u4e92\u5728\u6253\u7834\u5bf9\u79f0\u6027\u4e2d\u7684\u673a\u5236\u4f5c\u7528\u3002"}}
{"id": "2601.12814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12814", "abs": "https://arxiv.org/abs/2601.12814", "authors": ["Yu-Jen Tseng", "Chia-Hao Kao", "Jing-Zhong Chen", "Alessandro Gnutti", "Shao-Yuan Lo", "Yen-Yu Lin", "Wen-Hsiao Peng"], "title": "CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting", "comment": "Accepted at WACV 2026", "summary": "We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.", "AI": {"tldr": "\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u7387\u5931\u771f\u4f18\u5316\u538b\u7f29\u4e0e\u5206\u5272\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8d85\u5148\u9a8c\u548c\u538b\u7f29\u5f15\u5bfc\u7684\u5206\u5272\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4f20\u8f93\u6210\u672c\u3002", "motivation": "\u867d\u71363D\u9ad8\u65af\u6cfc\u6e85\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u8bed\u4e49\u573a\u666f\u7406\u89e3\u65b9\u9762\u90fd\u6709\u6548\uff0c\u4f46\u5148\u524d\u5de5\u4f5c\u5927\u591a\u72ec\u7acb\u5904\u7406\u8fd9\u4e9b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8054\u5408\u8003\u8651\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u8bed\u4e49\u5b66\u4e60\u6574\u5408\u5230\u538b\u7f29\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u89e3\u7801\u5668\u7aef\u7684\u573a\u666f\u7f16\u8f91\u548c\u64cd\u4f5c\u7b49\u5e94\u7528\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u573a\u666f\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u3002", "method": "1) \u91c7\u7528\u8f7b\u91cf\u7ea7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8d85\u5148\u9a8c\uff0c\u9ad8\u6548\u7f16\u7801\u989c\u8272\u548c\u8bed\u4e49\u5c5e\u6027\uff0c\u907f\u514d\u6602\u8d35\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u8d85\u5148\u9a8c\uff1b2) \u5f00\u53d1\u538b\u7f29\u5f15\u5bfc\u7684\u5206\u5272\u5b66\u4e60\uff0c\u5305\u62ec\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4ee5\u589e\u5f3a\u7279\u5f81\u53ef\u5206\u6027\uff0c\u4ee5\u53ca\u8d28\u91cf\u611f\u77e5\u52a0\u6743\u673a\u5236\u4ee5\u6291\u5236\u4e0d\u53ef\u9760\u7684\u9ad8\u65af\u57fa\u5143\u3002", "result": "\u5728LERF\u548c3D-OVS\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e32\u67d3\u8d28\u91cf\u548c\u5f3a\u5927\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u7387\u5931\u771f\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u4e0e\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u8bed\u4e49\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u8d85\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7f16\u7801\u548c\u9ad8\u8d28\u91cf\u7684\u89e3\u7801\u5668\u7aef\u5e94\u7528\uff0c\u4e3a3D\u573a\u666f\u7684\u538b\u7f29\u548c\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.12823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12823", "abs": "https://arxiv.org/abs/2601.12823", "authors": ["Belal Shaheen", "Minh-Hieu Nguyen", "Bach-Thuan Bui", "Shubham", "Tim Wu", "Michael Fairley", "Matthew David Zane", "Michael Wu", "James Tompkin"], "title": "TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement", "comment": null, "summary": "Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.", "AI": {"tldr": "TreeDGS\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u822a\u62cd\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u822a\u62cd\u56fe\u50cf\u4e2d\u7cbe\u786e\u6d4b\u91cf\u6811\u6728\u80f8\u5f84\uff08DBH\uff09\uff0c\u572810\u4e2a\u6837\u5730\u7684\u8bc4\u4f30\u4e2d\u8fbe\u52304.79cm RMSE\uff0c\u4f18\u4e8e\u6fc0\u5149\u96f7\u8fbe\u57fa\u7ebf\uff087.91cm RMSE\uff09\u3002", "motivation": "\u822a\u62cd\u9065\u611f\u867d\u80fd\u9ad8\u6548\u8fdb\u884c\u5927\u8303\u56f4\u52d8\u6d4b\uff0c\u4f46\u5728\u590d\u6742\u81ea\u7136\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u76f4\u63a5\u7269\u4f53\u7ea7\u6d4b\u91cf\u3002\u5c3d\u7ba1NeRF\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7b49\u5b66\u4e60\u8f90\u5c04\u573a\u8868\u793a\u63d0\u5347\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u5bc6\u5ea6\uff0c\u4f46\u822a\u62cd\u6d4b\u91cf\u6811\u6728\u80f8\u5f84\uff08DBH\uff09\u4ecd\u5177\u6311\u6218\u6027\uff1a\u6811\u5e72\u5728\u822a\u62cd\u626b\u63cf\u4e2d\u8ddd\u79bb\u8fdc\u3001\u56fe\u50cf\u89c2\u6d4b\u7a00\u758f\uff0c\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u7ea6\u675f\u80f8\u9ad8\u5904\u7684\u6811\u5e72\u51e0\u4f55\u3002", "method": "\u63d0\u51faTreeDGS\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528SfM-MVS\u8fdb\u884c\u521d\u59cb\u5316\u5e76\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u8fde\u7eed\u3001\u53ef\u5bc6\u96c6\u5316\u7684\u573a\u666f\u8868\u793a\uff1b2\uff09\u901a\u8fc7RaDe-GS\u7684\u6df1\u5ea6\u611f\u77e5\u7d2f\u79ef\u4e0d\u900f\u660e\u5ea6\u79ef\u5206\u4ece\u9ad8\u65af\u573a\u63d0\u53d6\u5bc6\u96c6\u70b9\u96c6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u591a\u89c6\u89d2\u4e0d\u900f\u660e\u5ea6\u53ef\u9760\u6027\u5206\u6570\uff1b3\uff09\u4ece\u9694\u79bb\u7684\u6811\u5e72\u70b9\u4f7f\u7528\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u5b9e\u5fc3\u5706\u62df\u5408\u4f30\u8ba1DBH\u3002", "result": "\u572810\u4e2a\u5177\u6709\u5b9e\u5730\u6d4b\u91cfDBH\u7684\u6837\u5730\u4e0a\u8bc4\u4f30\uff0cTreeDGS\u8fbe\u52304.79cm RMSE\uff08\u7ea6\u76f8\u5f53\u4e8e\u8be5\u5730\u9762\u91c7\u6837\u8ddd\u79bb\u4e0b\u76842.6\u50cf\u7d20\uff09\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6fc0\u5149\u96f7\u8fbe\u57fa\u7ebf\uff087.91cm RMSE\uff09\uff0c\u8868\u660e\u57fa\u4e8e\u6cfc\u6e85\u7684\u5bc6\u96c6\u5316\u51e0\u4f55\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u4f4e\u6210\u672c\u7684\u822a\u62cdDBH\u6d4b\u91cf\u3002", "conclusion": "TreeDGS\u8bc1\u660e\u4e863D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u8fde\u7eed\u3001\u53ef\u5bc6\u96c6\u5316\u573a\u666f\u8868\u793a\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u822a\u62cd\u56fe\u50cf\u4e2d\u91cd\u5efa\u8db3\u591f\u7cbe\u786e\u7684\u6811\u5e72\u51e0\u4f55\u4ee5\u8fdb\u884c\u80f8\u5f84\u6d4b\u91cf\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u5927\u8303\u56f4\u7684\u68ee\u6797\u8d44\u6e90\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.12711", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2601.12711", "abs": "https://arxiv.org/abs/2601.12711", "authors": ["Kevin Wang", "Neel P. Bhatt", "Cong Liu", "Junbo Li", "Runjin Chen", "Yihan Xi", "Timothy Barclay", "Alvaro Velasquez", "Ufuk Topcu", "Zhangyang Wang"], "title": "Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts", "comment": null, "summary": "Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff0c\u52a8\u6001\u7ed3\u5408\u53c2\u6570\u5fae\u8c03\u4e0e\u7b26\u53f7\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347LLM\u9002\u5e94\u80fd\u529b", "motivation": "\u4f20\u7edfLLM\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6570\u503c\u5fae\u8c03\u64c5\u957f\u6ce8\u5165\u65b0\u4e8b\u5b9e\u77e5\u8bc6\u4f46\u9700\u8981\u91cd\u8bad\u7ec3\uff0c\u7b26\u53f7\u64cd\u4f5c\u80fd\u7075\u6d3b\u63a7\u5236\u98ce\u683c\u4f46\u5bf9\u4e8b\u5b9e\u91cd\u6784\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u6a21\u578b\u9002\u5e94", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff0c\u5305\u542b\u7edf\u4e00\u76d1\u63a7\u4fe1\u53f7\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u5206\u7c7b\u5668\uff0c\u52a8\u6001\u51b3\u7b56\u4f55\u65f6\u4f7f\u7528LoRA\u8fdb\u884c\u6df1\u5ea6\u4e8b\u5b9e\u91cd\u6784\uff0c\u4f55\u65f6\u4f7f\u7528TextGrad\u8fdb\u884ctoken\u7ea7\u7f16\u8f91\u3002\u901a\u8fc7\u5916\u90e8LLM\u6309\u9700\u5904\u7406\u7b26\u53f7\u8f6c\u6362\u4fdd\u6301\u5185\u5b58\u6548\u7387", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u6570\u503c\u6216\u7eaf\u7b26\u53f7\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002\u7b26\u53f7\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u7cbe\u70bc\u63d0\u793a\u53ef\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u53ef\u91cd\u7528\u8bad\u7ec3\u6570\u636e", "conclusion": "\u6570\u503c\u66f4\u65b0\u4e0e\u7b26\u53f7\u66f4\u65b0\u7684\u4ea4\u7ec7\u7ed3\u5408\u4e3a\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u89e3\u9501\u4e86\u65b0\u7684\u591a\u529f\u80fd\u6027\u6c34\u5e73\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\u5c55\u793a\u4e86\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u7684\u663e\u8457\u4ef7\u503c"}}
{"id": "2601.12876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12876", "abs": "https://arxiv.org/abs/2601.12876", "authors": ["Zhenxuan Lu", "Zhihua Xu", "Zhijing Yang", "Feng Gao", "Yongyi Lu", "Keze Wang", "Tianshui Chen"], "title": "Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation", "comment": "Accepted by ACM Transactions on Multimedia Computing, Communications, and Applications", "summary": "Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTHFEM\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u4e0e\u8bed\u97f3\u4fdd\u7559\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u751f\u6210\u5e27\u7684\u8fde\u7eed\u6027\u548c\u8d28\u91cf\uff0c\u4ece\u800c\u5728\u6539\u53d8\u9762\u90e8\u8868\u60c5\u65f6\u66f4\u597d\u5730\u4fdd\u6301\u53e3\u578b\u540c\u6b65\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u4fdd\u7559\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u6280\u672f\u5728\u5904\u7406\u53e3\u578b\u540c\u6b65\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u9762\u90e8\u8868\u60c5\u548c\u5634\u90e8\u5f62\u72b6\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u7cbe\u786e\u53e3\u578b\u8fd0\u52a8\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u5e27\u751f\u6210\u4f1a\u635f\u5bb3\u56fe\u50cf\u771f\u5b9e\u6027\u548c\u8868\u60c5\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faTHFEM\u6846\u67b6\uff0c\u6574\u5408AD-THG\u6a21\u578b\u548cSPFEM\u6280\u672f\u3002\u5f00\u53d1\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\uff0c\u5fae\u8c03AD-THG\u6a21\u578b\u4ee5\u9884\u6d4b\u8fde\u7eed\u5e27\u5e8f\u5217\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u76f8\u90bb\u5e27\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8868\u60c5\u64cd\u7eb5\u8fc7\u7a0b\u4e2d\u6709\u6548\u4fdd\u6301\u4e86\u5634\u90e8\u5f62\u72b6\uff0c\u8bc1\u660e\u4e86\u5c06AD-THG\u4e0eSPFEM\u96c6\u6210\u7684\u663e\u8457\u4f18\u52bf\uff0c\u6539\u5584\u4e86\u53e3\u578b\u540c\u6b65\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u548c\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\uff0cTHFEM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u4fdd\u7559\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u4e2d\u7684\u53e3\u578b\u540c\u6b65\u95ee\u9898\uff0c\u4e3a\u8868\u60c5\u64cd\u7eb5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12804", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12804", "abs": "https://arxiv.org/abs/2601.12804", "authors": ["Hanwei Zhang", "Luo Cheng", "Rui Wen", "Yang Zhang", "Lijun Zhang", "Holger Hermanns"], "title": "SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability", "comment": null, "summary": "Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.", "AI": {"tldr": "SL-CBM\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u5c40\u90e8\u6027\u589e\u5f3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u751f\u6210\u7a7a\u95f4\u4e00\u81f4\u7684\u6982\u5ff5\u548c\u7c7b\u522b\u663e\u8457\u56fe\uff0c\u63d0\u9ad8\u5c40\u90e8\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7f3a\u4e4f\u5c40\u90e8\u5fe0\u5b9e\u6027\uff0c\u65e0\u6cd5\u5c06\u6982\u5ff5\u4e0e\u6709\u610f\u4e49\u7684\u56fe\u50cf\u533a\u57df\u7a7a\u95f4\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faSL-CBM\uff0c\u96c6\u62101x1\u5377\u79ef\u5c42\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u6982\u5ff5\u548c\u7c7b\u522b\u5c42\u9762\u751f\u6210\u7a7a\u95f4\u4e00\u81f4\u7684\u663e\u8457\u56fe\uff0c\u4f7f\u7528\u5bf9\u6bd4\u548c\u57fa\u4e8e\u71b5\u7684\u6b63\u5219\u5316\u5e73\u8861\u51c6\u786e\u6027\u3001\u7a00\u758f\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5c40\u90e8\u5fe0\u5b9e\u6027\u3001\u89e3\u91ca\u8d28\u91cf\u548c\u5e72\u9884\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "SL-CBM\u5f25\u5408\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u63a8\u7406\u548c\u7a7a\u95f4\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684\u6982\u5ff5\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2601.12889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12889", "abs": "https://arxiv.org/abs/2601.12889", "authors": ["Nazibul Basar Ayon", "Abdul Hasib", "Md. Faishal Ahmed", "Md. Sadiqur Rahman", "Kamrul Islam", "T. M. Mehrab Hasan", "A. S. M. Ahsanul Sarkar Akib"], "title": "Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning", "comment": null, "summary": "Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\\%, with macro-averaged precision of 98.2\\%, recall of 98.1\\%, F1-score of 98.1\\%, and an AUC-ROC of 99.5\\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.", "AI": {"tldr": "\u63d0\u51fa\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408VGG16\u3001ResNet50\u548cInceptionV3\uff0c\u901a\u8fc7\u4f18\u5316\u52a0\u6743\u5e73\u5747\u5b9e\u73b0\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u548c\u53e3\u8e44\u75ab\u7684\u540c\u65f6\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe98.2%\uff0c\u89e3\u51b3\u75c7\u72b6\u91cd\u53e0\u7684\u8bca\u65ad\u96be\u9898\u3002", "motivation": "\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u548c\u53e3\u8e44\u75ab\u662f\u9ad8\u5ea6\u4f20\u67d3\u6027\u75c5\u6bd2\u75be\u75c5\uff0c\u9020\u6210\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u89c6\u89c9\u8bca\u65ad\u56e0\u75c7\u72b6\u91cd\u53e0\uff08\u5305\u62ec\u76f8\u4e92\u4e4b\u95f4\u4ee5\u53ca\u4e0e\u826f\u6027\u72b6\u51b5\u5982\u6606\u866b\u53ee\u54ac\u6216\u5316\u5b66\u70e7\u4f24\uff09\u800c\u590d\u6742\u5316\uff0c\u963b\u788d\u53ca\u65f6\u63a7\u5236\u63aa\u65bd\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u5370\u5ea6\u3001\u5df4\u897f\u548c\u7f8e\u56fd18\u4e2a\u519c\u573a\u768410,516\u5f20\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408VGG16\u3001ResNet50\u548cInceptionV3\u4e09\u79cd\u67b6\u6784\uff0c\u91c7\u7528\u4f18\u5316\u52a0\u6743\u5e73\u5747\u7b56\u7565\u8fdb\u884c\u591a\u75be\u75c5\u68c0\u6d4b\u3002", "result": "\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u768498.2%\u51c6\u786e\u7387\uff0c\u5b8f\u5e73\u5747\u7cbe\u786e\u738798.2%\u3001\u53ec\u56de\u738798.1%\u3001F1\u5206\u657098.1%\u3001AUC-ROC 99.5%\uff0c\u6709\u6548\u89e3\u51b3\u75c7\u72b6\u91cd\u53e0\u7684\u591a\u75be\u75c5\u68c0\u6d4b\u6311\u6218\u3002", "conclusion": "\u8be5\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u65e9\u671f\u3001\u7cbe\u786e\u3001\u81ea\u52a8\u5316\u7684\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u548c\u53e3\u8e44\u75ab\u8bca\u65ad\uff0c\u6709\u6f5c\u529b\u6539\u5584\u75be\u75c5\u7ba1\u7406\uff0c\u652f\u6301\u5168\u7403\u519c\u4e1a\u53ef\u6301\u7eed\u6027\uff0c\u5e76\u8bbe\u8ba1\u7528\u4e8e\u672a\u6765\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2601.12929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12929", "abs": "https://arxiv.org/abs/2601.12929", "authors": ["Gonzalo Mancera", "Daniel DeAlcala", "Aythami Morales", "Ruben Tolosana", "Julian Fierrez"], "title": "Membership Inference Test: Auditing Training Data in Object Classification Models", "comment": "Deployable AI (DAI 2025) workshop co-located with AAAI-25", "summary": "In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u6210\u5458\u63a8\u7406\u6d4b\u8bd5\uff08MINT\uff09\u5728\u7269\u4f53\u8bc6\u522b\u9886\u57df\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9MINT\u6a21\u578b\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u5e93\u4e0a\u9a8c\u8bc1\u4e86\u67b6\u6784\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e8670%-80%\u7684\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5206\u6790\u6210\u5458\u63a8\u7406\u6d4b\u8bd5\u5728\u7269\u4f53\u8bc6\u522b\u9886\u57df\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u5982\u4f55\u786e\u5b9a\u7ed9\u5b9a\u6570\u636e\u662f\u5426\u5728\u8bad\u7ec3\u9636\u6bb5\u88ab\u4f7f\u7528\u7684\u95ee\u9898\u3002\u9488\u5bf9\u7269\u4f53\u8bc6\u522b\u9886\u57df\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u4f18\u5316\u7684MINT\u67b6\u6784\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9\u7269\u4f53\u8bc6\u522b\u9886\u57df\u7684MINT\u6a21\u578b\u67b6\u6784\uff1b2\uff09\u5229\u7528\u5377\u79ef\u5c42\u6355\u6349\u548c\u5efa\u6a21\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff1b3\uff09\u6784\u5efa\u5305\u542b\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u3001\u5d4c\u5165\u63d0\u53d6\u5668\u548cMINT\u6a21\u5757\u7684\u5b9e\u9a8c\u7cfb\u7edf\uff1b4\uff09\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u5e93\uff08\u603b\u8ba1\u8d85\u8fc7174K\u56fe\u50cf\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1b5\uff09\u5206\u6790\u4e0d\u540c\u68c0\u6d4b\u6a21\u5757\u5c42\u6df1\u5ea6\u5bf9MINT\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\uff1a1\uff09\u80fd\u591f\u8bc6\u522b\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bad\u7ec3\u7684\u6570\u636e\uff1b2\uff09\u7cbe\u5ea6\u8fbe\u523070%-80%\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u8f93\u5165MINT\u6a21\u5757\u7684\u68c0\u6d4b\u6a21\u5757\u5c42\u6df1\u5ea6\uff1b3\uff09\u5206\u6790\u4e86\u5f71\u54cdMINT\u6a21\u5757\u6027\u80fd\u7684\u56e0\u7d20\uff1b4\uff09\u6df1\u5165\u63a2\u8ba8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u900f\u660e\u5316\u7684\u8d21\u732e\u56e0\u7d20\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u63d0\u51fa\u7684\u4e13\u95e8\u9488\u5bf9\u7269\u4f53\u8bc6\u522b\u9886\u57df\u7684MINT\u67b6\u6784\u80fd\u591f\u6709\u6548\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u7cbe\u5ea6\u53ef\u8fbe70%-80%\u3002\u7814\u7a76\u4e3a\u7406\u89e3MINT\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\u548c\u5b9e\u73b0\u66f4\u900f\u660e\u7684\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.12936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12936", "abs": "https://arxiv.org/abs/2601.12936", "authors": ["Tianran Ouyang", "Xingping Dong", "Jing Zhang", "Mang Ye", "Jun Chen", "Bo Du"], "title": "QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning", "comment": null, "summary": "Slot Attention, an approach that binds different objects in a scene to a set of \"slots\", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.", "AI": {"tldr": "QASA\u662f\u4e00\u79cd\u8d28\u91cf\u5f15\u5bfc\u7684K\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u69fd\u9009\u62e9\u4e0e\u91cd\u5efa\u3001\u5f15\u5165\u65e0\u76d1\u7763\u69fd\u8d28\u91cf\u5ea6\u91cf\u3001\u8bbe\u8ba1\u8d28\u91cf\u5f15\u5bfc\u69fd\u9009\u62e9\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709K\u81ea\u9002\u5e94\u65b9\u6cd5\u4e2d\u69fd\u7ed1\u5b9a\u8d28\u91cf\u5dee\u548c\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709K\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u6ca1\u6709\u663e\u5f0f\u7ea6\u675f\u69fd\u7ed1\u5b9a\u8d28\u91cf\uff0c\u5bfc\u81f4\u4f4e\u8d28\u91cf\u69fd\u5f15\u8d77\u7279\u5f81\u5f52\u5c5e\u6a21\u7cca\uff1b2) \u5728\u91cd\u5efa\u76ee\u6807\u4e2d\u6dfb\u52a0\u69fd\u6570\u91cf\u60e9\u7f5a\u4f1a\u521b\u5efa\u76f8\u4e92\u51b2\u7a81\u7684\u4f18\u5316\u76ee\u6807\uff08\u51cf\u5c11\u6d3b\u8dc3\u69fd\u6570\u91cf vs \u4fdd\u6301\u91cd\u5efa\u4fdd\u771f\u5ea6\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u843d\u540e\u4e8eK\u56fa\u5b9a\u57fa\u7ebf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u5f15\u5bfc\u7684K\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b(QASA)\uff1a1) \u5c06\u69fd\u9009\u62e9\u4e0e\u91cd\u5efa\u89e3\u8026\uff0c\u6d88\u9664\u4e24\u4e2a\u76ee\u6807\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\uff1b2) \u63d0\u51fa\u65e0\u76d1\u7763\u7684\u69fd\u8d28\u91cf\u5ea6\u91cf\u6765\u8bc4\u4f30\u6bcf\u4e2a\u69fd\u7684\u8d28\u91cf\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u69fd-\u5bf9\u8c61\u7ed1\u5b9a\u63d0\u4f9b\u539f\u5219\u6027\u4fe1\u53f7\uff1b3) \u57fa\u4e8e\u6b64\u5ea6\u91cf\u8bbe\u8ba1\u8d28\u91cf\u5f15\u5bfc\u69fd\u9009\u62e9\u65b9\u6848\uff0c\u52a8\u6001\u9009\u62e9\u9ad8\u8d28\u91cf\u69fd\u5b50\u96c6\uff0c\u8f93\u5165\u5230\u65b0\u8bbe\u8ba1\u7684\u95e8\u63a7\u89e3\u7801\u5668\u8fdb\u884c\u8bad\u7ec3\u91cd\u5efa\uff1b4) \u63a8\u7406\u65f6\u901a\u8fc7\u69fd\u6ce8\u610f\u529b\u4e0a\u7684\u4ee4\u724c\u7ea7\u7ade\u4e89\u83b7\u5f97K\u81ea\u9002\u5e94\u7ed3\u679c\u3002", "result": "QASA\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709K\u81ea\u9002\u5e94\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cQASA\u751a\u81f3\u8d85\u8d8a\u4e86K\u56fa\u5b9a\u65b9\u6cd5\u3002", "conclusion": "QASA\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u69fd\u9009\u62e9\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86K\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u52a8\u6001\u69fd\u6570\u91cf\u8c03\u6574\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edfK\u56fa\u5b9a\u65b9\u6cd5\uff0c\u4e3a\u65e0\u76d1\u7763\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12913", "categories": ["cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.12913", "abs": "https://arxiv.org/abs/2601.12913", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Francesco Giannini", "Alberto Termine", "Filippo Bonchi", "Mateja Jamnik", "Giuseppe Marra"], "title": "Actionable Interpretability Must Be Defined in Terms of Symmetries", "comment": null, "summary": "This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u5f53\u524dAI\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u73b0\u6709\u5b9a\u4e49\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\uff0c\u65e0\u6cd5\u63a8\u5bfc\u51fa\u5177\u4f53\u7684\u5efa\u6a21\u548c\u63a8\u7406\u89c4\u5219\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u64cd\u4f5c\u6027\u5b9a\u4e49\uff0c\u5e76\u5047\u8bbe\u56db\u79cd\u5bf9\u79f0\u6027\u8db3\u4ee5\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u7684\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\u2014\u2014\u5b83\u4eec\u672a\u80fd\u63d0\u4f9b\u53ef\u4ee5\u63a8\u5bfc\u51fa\u5177\u4f53\u5efa\u6a21\u548c\u63a8\u7406\u89c4\u5219\u7684\u5f62\u5f0f\u5316\u539f\u5219\u3002\u8fd9\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7f3a\u4e4f\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u7edf\u4e00\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u64cd\u4f5c\u6027\u5b9a\u4e49\u6846\u67b6\u3002\u5047\u8bbe\u56db\u79cd\u5bf9\u79f0\u6027\u8db3\u4ee5\uff1a(1) \u6fc0\u53d1\u6838\u5fc3\u53ef\u89e3\u91ca\u6027\u5c5e\u6027\uff1b(2) \u523b\u753b\u53ef\u89e3\u91ca\u6a21\u578b\u7c7b\u522b\uff1b(3) \u63a8\u5bfc\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u63a8\u7406\u5f62\u5f0f\u5316\u8868\u8ff0\uff08\u5982\u5bf9\u9f50\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff09\uff0c\u5c06\u5176\u89c6\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u89e3\u91ca\u6027\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5b9a\u4e49\uff0c\u7edf\u4e00\u89e3\u91ca\u5bf9\u9f50\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7b49\u6982\u5ff5\uff0c\u5e76\u4e3a\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u5206\u7c7b\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9700\u8981\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u64cd\u4f5c\u6027\u5b9a\u4e49\u624d\u80fd\u6210\u4e3a\u4e25\u8c28\u7684\u79d1\u5b66\u9886\u57df\u3002\u56db\u79cd\u5bf9\u79f0\u6027\u5047\u8bbe\u4e3a\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u7684\u6838\u5fc3\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.12948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12948", "abs": "https://arxiv.org/abs/2601.12948", "authors": ["Riccardo Catalini", "Davide Di Nucci", "Guido Borghi", "Davide Davoli", "Lorenzo Garattoni", "Giampiero Francesca", "Yuki Kawana", "Roberto Vezzani"], "title": "GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation", "comment": null, "summary": "We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.", "AI": {"tldr": "GazeD\u662f\u4e00\u79cd\u4ece\u5355\u5f20RGB\u56fe\u50cf\u8054\u5408\u4f30\u8ba13D\u6ce8\u89c6\u65b9\u5411\u548c\u4eba\u4f53\u59ff\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u751f\u6210\u591a\u4e2a\u5408\u7406\u76843D\u6ce8\u89c6\u548c\u59ff\u6001\u5047\u8bbe\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u74063D\u6ce8\u89c6\u4f30\u8ba1\u548c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5ffd\u7565\u4e86\u4e8c\u8005\u4e4b\u95f4\u7684\u5bc6\u5207\u5173\u7cfb\u3002\u540c\u65f6\uff0c\u4ece\u5355\u5f20RGB\u56fe\u50cf\u8fdb\u884c3D\u6ce8\u89c6\u4f30\u8ba1\u5b58\u5728\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u591a\u5047\u8bbe\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGazeD\u65b9\u6cd5\uff1a1) \u5229\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u63d0\u53d6\u76842D\u4e0a\u4e0b\u6587\u4fe1\u606f\u751f\u6210\u591a\u4e2a\u5408\u7406\u76843D\u6ce8\u89c6\u548c\u59ff\u6001\u5047\u8bbe\uff1b2) \u5c06\u53bb\u566a\u8fc7\u7a0b\u6761\u4ef6\u5316\u4e8e2D\u59ff\u6001\u3001\u4e3b\u4f53\u5468\u56f4\u73af\u5883\u548c\u573a\u666f\u4e0a\u4e0b\u6587\uff1b3) \u5f15\u5165\u65b0\u9896\u76843D\u6ce8\u89c6\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5176\u4f5c\u4e3a\u8ddd\u79bb\u773c\u775b\u56fa\u5b9a\u8ddd\u79bb\u7684\u989d\u5916\u8eab\u4f53\u5173\u8282\u70b9\uff1b4) \u8054\u5408\u53bb\u566a\u6ce8\u89c6\u548c\u59ff\u6001\uff0c\u5229\u7528\u4e8c\u8005\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGazeD\u57283D\u6ce8\u89c6\u4f30\u8ba1\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u4f9d\u8d56\u65f6\u95f4\u4fe1\u606f\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u591a\u4e2a\u5408\u7406\u76843D\u6ce8\u89c6\u548c\u59ff\u6001\u5047\u8bbe\u3002", "conclusion": "GazeD\u6210\u529f\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u8054\u54083D\u6ce8\u89c6\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5c06\u6ce8\u89c6\u8868\u793a\u4e3a\u989d\u5916\u8eab\u4f53\u5173\u8282\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20RGB\u56fe\u50cf\u7684\u9ad8\u6027\u80fd\u591a\u5047\u8bbe\u751f\u6210\u3002"}}
{"id": "2601.12954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12954", "abs": "https://arxiv.org/abs/2601.12954", "authors": ["Zhou Hong", "Rongsheng Hu", "Yicheng Di", "Xiaolong Xu", "Ning Dong", "Yihua Shao", "Run Ling", "Yun Wang", "Juqin Wang", "Zhanjie Zhang", "Ao Ma"], "title": "StyMam: A Mamba-Based Generator for Artistic Style Transfer", "comment": "Accepted by ICASSP 2026", "summary": "Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u751f\u6210\u5668StyMam\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e2d\u5b58\u5728\u7684\u4f2a\u5f71\u3001\u4e0d\u548c\u8c10\u6a21\u5f0f\u3001\u5185\u5bb9\u7ed3\u6784\u4fdd\u6301\u4e0d\u8db3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eGAN\u6216\u7a33\u5b9a\u6269\u6563(SD)\u3002GAN\u65b9\u6cd5\u4f7f\u7528CNN\u6216Transformer\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u4e0d\u548c\u8c10\u6a21\u5f0f\uff1bSD\u65b9\u6cd5\u51cf\u5c11\u4e86\u8fd9\u4e9b\u95ee\u9898\u4f46\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u4e14\u63a8\u7406\u901f\u5ea6\u6162\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684\u751f\u6210\u5668StyMam\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u6b8b\u5dee\u53cc\u8def\u5f84\u6761\u5e26\u626b\u63cf\u673a\u5236\uff0c\u7528\u4e8e\u9ad8\u6548\u6355\u6349\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff1b2) \u901a\u9053\u91cd\u52a0\u6743\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7528\u4e8e\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u5316\u56fe\u50cf\u800c\u4e0d\u5f15\u5165\u4f2a\u5f71\u548c\u4e0d\u548c\u8c10\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6GAN\u67b6\u6784\u5e76\u5f15\u5165Mamba-based\u751f\u6210\u5668\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u98ce\u683c\u8fc1\u79fb\u3002"}}
{"id": "2601.12964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12964", "abs": "https://arxiv.org/abs/2601.12964", "authors": ["John Waithaka", "Gustave Bwirayesu", "Moise Busogi"], "title": "Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation", "comment": null, "summary": "Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u4eb2\u548c\u6027\u7ec4\u4ef6\uff0c\u53ef\u5c06\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u878d\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u4ee5\u589e\u5f3a\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u793a\u5b66\u4e60\u548c\u4e0b\u6e38\u5206\u5272\u6027\u80fd", "motivation": "\u5f53\u524d\u9065\u611f\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e3b\u8981\u4f7f\u7528\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4f46\u968f\u7740\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u6765\u589e\u5f3a\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u793a\u5b66\u4e60\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7a7a\u95f4\u4eb2\u548c\u6027\u7ec4\u4ef6\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6765\u5b66\u4e60\u66f4\u597d\u7684\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u8868\u793a", "result": "\u5728\u4e24\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u6216\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u5355\u72ec\u9884\u8bad\u7ec3\u7684\u6a21\u578b", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u4eb2\u548c\u6027\u7ec4\u4ef6\u5c06\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u878d\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u793a\u8d28\u91cf\u548c\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2601.13166", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13166", "abs": "https://arxiv.org/abs/2601.13166", "authors": ["Pedro M. Gordaliza", "Jaume Banus", "Beno\u00eet G\u00e9rin", "Maxence Wynen", "Nataliia Molchanova", "Jonas Richiardi", "Meritxell Bach Cuadra"], "title": "From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models", "comment": "Work presented at the SSL3D Challenge (1st place, ResEnc-L track) and FOMO Challenge (1st place, Methods track) on Brain MRI Foundation Models at MICCAI 2025", "summary": "Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.", "code_url": "https://github.com/jbanusco/BrainFM4Challenges", "code_stars": 0, "code_last_update": "2025-12-17", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728MICCAI 2025\u7684SSL3D\u548cFOMO25\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4f7f\u7528U-Net CNN\u67b6\u6784\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u77e5\u8bc6\uff0c\u76f8\u6bd4\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\u8bad\u7ec3\u901f\u5ea6\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\u4e14\u6a21\u578b\u5c0f10\u500d\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u57fa\u7840\u6a21\u578b\u5bf9\u4e8e\u514b\u670d\u653e\u5c04\u5b66\u4efb\u52a1\u7684\u72ec\u7279\u6311\u6218\u81f3\u5173\u91cd\u8981\u3002SSL3D\u548cFOMO25\u662f\u9996\u4e2a\u9488\u5bf93D\u8111MRI\u7684\u6b64\u7c7b\u6311\u6218\u8d5b\u3002", "method": "\u91c7\u7528U-Net CNN\u67b6\u6784\uff0c\u7ed3\u5408\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u77e5\u8bc6\u7684\u7b56\u7565\u3002\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u6bd4\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u6a21\u578b\u5927\u5c0f\u5c0f10\u500d\u3002", "result": "\u5728MICCAI 2025\u7684SSL3D\u548cFOMO25\u6311\u6218\u8d5b\u7684\u4e24\u4e2a\u8d5b\u9053\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\u3002\u6a21\u578b\u5df2\u5f00\u6e90\u5728GitHub\u4e0a\u3002", "conclusion": "U-Net CNN\u67b6\u6784\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u65b9\u6cd5\u57283D\u8111MRI\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8e\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.13029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13029", "abs": "https://arxiv.org/abs/2601.13029", "authors": ["Zaibin Zhang", "Yuhan Wu", "Lianjie Jia", "Yifan Wang", "Zhongbo Zhang", "Yijiang Li", "Binghao Ran", "Fuxi Zhang", "Zhuohan Sun", "Zhenfei Yin", "Lijun Wang", "Huchuan Lu"], "title": "Think3D: Thinking with Space for Spatial Reasoning", "comment": null, "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "code_url": "https://github.com/zhangzaibin/spagen", "AI": {"tldr": "Think3D\u6846\u67b6\u901a\u8fc73D\u91cd\u5efa\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u7a7a\u95f4\u64cd\u4f5c\uff0c\u4f7f\u89c6\u89c9\u5927\u6a21\u578b\u5177\u59073D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u5927\u6a21\u578b\u672c\u8d28\u4e0a\u662f2D\u611f\u77e5\u5668\uff0c\u7f3a\u4e4f\u771f\u6b63\u76843D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u800c\u7406\u89e3\u7269\u7406\u4e16\u754c\u9700\u8981\u7a7a\u95f4\u667a\u80fd\uff08\u51e0\u4f55\u3001\u900f\u89c6\u548c\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\uff09\u3002", "method": "\u5229\u75283D\u91cd\u5efa\u6a21\u578b\u4ece\u56fe\u50cf/\u89c6\u9891\u6062\u590d\u70b9\u4e91\u548c\u76f8\u673a\u4f4d\u59ff\uff0c\u901a\u8fc7\u76f8\u673a\u64cd\u4f5c\u548c\u81ea\u6211/\u5168\u5c40\u89c6\u89d2\u5207\u6362\u5b9e\u73b0\u4ea4\u4e92\u5f0f3D\u601d\u7ef4\u94fe\uff0c\u5c0f\u6a21\u578b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u70b9\u548c\u64cd\u4f5c\u3002", "result": "Think3D\u663e\u8457\u63d0\u5347\u4e86GPT-4.1\u548cGemini 2.5 Pro\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\uff1aBLINK Multi-view\u548cMindCube\u5e73\u5747\u63d0\u5347+7.8%\uff0cVSI-Bench\u63d0\u5347+4.7%\u3002\u5c0f\u6a21\u578b\u901a\u8fc7RL\u7b56\u7565\u5c06\u5de5\u5177\u4f7f\u7528\u6536\u76ca\u4ece+0.7%\u63d0\u5347\u5230+6.8%\u3002", "conclusion": "\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u5de5\u5177\u589e\u5f3a\u7684\u7a7a\u95f4\u63a2\u7d22\u662f\u5b9e\u73b0\u591a\u6a21\u6001\u667a\u80fd\u4f53\u66f4\u7075\u6d3b\u3001\u7c7b\u4eba3D\u63a8\u7406\u7684\u53ef\u884c\u8def\u5f84\uff0c\u5efa\u7acb\u4e86\u591a\u6a21\u6001\u667a\u80fd\u7684\u65b0\u7ef4\u5ea6\u3002"}}
{"id": "2601.13052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13052", "abs": "https://arxiv.org/abs/2601.13052", "authors": ["Antoine Carreaud", "Shanci Li", "Malo De Lacour", "Digre Frinde", "Jan Skaloud", "Adrien Gressin"], "title": "GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure", "comment": null, "summary": "This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.", "AI": {"tldr": "GridNet-HD\u662f\u4e00\u4e2a\u7528\u4e8e\u7535\u529b\u57fa\u7840\u8bbe\u65bd3D\u8bed\u4e49\u5206\u5272\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u9ad8\u5bc6\u5ea6LiDAR\u548c\u9ad8\u5206\u8fa8\u7387\u503e\u659c\u5f71\u50cf\uff0c\u5305\u542b7,694\u5f20\u56fe\u50cf\u548c25\u4ebf\u4e2a\u70b9\uff0c\u6807\u6ce8\u4e3a11\u4e2a\u7c7b\u522b\uff0c\u63d0\u4f9b\u57fa\u51c6\u6a21\u578b\u548c\u4ee3\u7801\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u540c\u65f6\u63d0\u4f9b\u9ad8\u5bc6\u5ea6LiDAR\u3001\u9ad8\u5206\u8fa8\u7387\u503e\u659c\u5f71\u50cf\u548c3D\u8bed\u4e49\u6807\u6ce8\u7684\u516c\u5f00\u7535\u529b\u7ebf\u8def\u8d44\u4ea7\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b7,694\u5f20\u56fe\u50cf\u548c2.5\u4ebf\u4e2a\u70b9\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6807\u6ce811\u4e2a\u8bed\u4e49\u7c7b\u522b\uff0c\u63d0\u4f9b\u9884\u5b9a\u4e49\u7684\u6570\u636e\u5212\u5206\u548cmIoU\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u5355\u6a21\u6001\uff08\u4ec5LiDAR\u3001\u4ec5\u56fe\u50cf\uff09\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u57fa\u51c6\u6a21\u578b\u3002", "result": "\u5728GridNet-HD\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u6bd4\u6700\u4f73\u5355\u6a21\u6001\u57fa\u51c6\u6a21\u578b\u63d0\u5347\u4e86+5.55 mIoU\uff0c\u8bc1\u660e\u4e86\u51e0\u4f55\u4fe1\u606f\u548c\u5916\u89c2\u4fe1\u606f\u7684\u4e92\u8865\u6027\u3002", "conclusion": "GridNet-HD\u586b\u8865\u4e86\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u591a\u6a21\u60013D\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u7535\u529b\u7ebf\u8def\u8d44\u4ea7\u8bc6\u522b\u4e2d\u7684\u4f18\u52bf\uff0c\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.13331", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13331", "abs": "https://arxiv.org/abs/2601.13331", "authors": ["Wei Wang", "Quoc-Toan Ly", "Chong Yu", "Jun Bai"], "title": "MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic", "comment": null, "summary": "Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.", "code_url": "https://github.com/LabJunBMI/MultiST", "AI": {"tldr": "MultiST\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u7a7a\u95f4\u62d3\u6251\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u7ec4\u7ec7\u5f62\u6001\u5b66\u7279\u5f81\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u5408\u7ec4\u7ec7\u5f62\u6001\u4e0e\u5206\u5b50\u8c31\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u83b7\u5f97\u66f4\u6e05\u6670\u7684\u7a7a\u95f4\u57df\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u7684\u7ec4\u7ec7\u5f62\u6001\u5b66\u4e0e\u5206\u5b50\u8c31\u6574\u5408\uff0c\u901a\u5e38\u91c7\u7528\u6d45\u5c42\u878d\u5408\u7b56\u7565\u6216\u5b8c\u5168\u5ffd\u7565\u7ec4\u7ec7\u56fe\u50cf\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u89e3\u6790\u6a21\u7cca\u7a7a\u95f4\u57df\u8fb9\u754c\u7684\u80fd\u529b\u3002", "method": "MultiST\u91c7\u7528\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u878d\u5408\u7b56\u7565\uff0c\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u62d3\u6251\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u7ec4\u7ec7\u5f62\u6001\u5b66\u3002\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u57fa\u56e0\u7f16\u7801\u5668\u7ed3\u5408\u5bf9\u6297\u5bf9\u9f50\u5b66\u4e60\u9c81\u68d2\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u540c\u65f6\u6574\u5408\u989c\u8272\u5f52\u4e00\u5316\u7684\u7ec4\u7ec7\u5b66\u7279\u5f81\u6765\u6355\u83b7\u5206\u5b50-\u5f62\u6001\u5b66\u4f9d\u8d56\u5173\u7cfb\u5e76\u7ec6\u5316\u57df\u8fb9\u754c\u3002", "result": "\u5728\u6db5\u76d6\u4e24\u4e2a\u5668\u5b98\uff08\u4eba\u8111\u76ae\u5c42\u548c\u4e73\u817a\u764c\u7ec4\u7ec7\uff09\u768413\u4e2a\u4e0d\u540cST\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMultiST\u4ea7\u751f\u7684\u7a7a\u95f4\u57df\u8fb9\u754c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6e05\u6670\u3001\u66f4\u8fde\u8d2f\uff0c\u5bfc\u81f4\u66f4\u7a33\u5b9a\u7684\u4f2a\u65f6\u95f4\u8f68\u8ff9\u548c\u66f4\u5177\u751f\u7269\u5b66\u53ef\u89e3\u91ca\u6027\u7684\u7ec6\u80de-\u7ec6\u80de\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u3002", "conclusion": "MultiST\u901a\u8fc7\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u6210\u529f\u89e3\u51b3\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u4e2d\u5f62\u6001\u5b66\u4e0e\u5206\u5b50\u8c31\u6574\u5408\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u7a7a\u95f4\u57df\u8bc6\u522b\u548c\u751f\u7269\u5b66\u6d1e\u89c1\uff0c\u6846\u67b6\u548c\u6e90\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.13358", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13358", "abs": "https://arxiv.org/abs/2601.13358", "authors": ["Samuel Cyrenius Anderson"], "title": "The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models", "comment": "34 pages, 10 figures", "summary": "Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u4e0d\u4f1a\u5747\u5300\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u91cd\u6784\u63a8\u7406\u8fc7\u7a0b\u3002\u901a\u8fc7\u5206\u679025,000+\u601d\u7ef4\u94fe\u8f68\u8ff9\u53d1\u73b0\uff0c\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u89e6\u53d1\u9886\u57df\u7279\u5b9a\u7684\u76f8\u53d8\u800c\u975e\u5747\u5300\u80fd\u529b\u63d0\u5347\uff0c\u63a8\u7406\u51e0\u4f55\u7ed3\u6784\u51b3\u5b9a\u53ef\u5b66\u4e60\u6027\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u6a21\u578b\u89c4\u6a21\u4f1a\u5747\u5300\u63d0\u5347\u6240\u6709\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u672c\u6587\u65e8\u5728\u63a2\u7a76\u89c4\u6a21\u5982\u4f55\u5177\u4f53\u5f71\u54cd\u4e0d\u540c\u9886\u57df\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63ed\u793a\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u7684\u5b9e\u9645\u4f5c\u7528\u673a\u5236\u3002", "method": "\u5206\u679025,000+\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u6db5\u76d6\u6cd5\u5f8b\u3001\u79d1\u5b66\u3001\u4ee3\u7801\u3001\u6570\u5b66\u56db\u4e2a\u9886\u57df\u548c8B\u300170B\u4e24\u79cd\u89c4\u6a21\u3002\u4f7f\u7528\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\u6d4b\u91cf\u8868\u793a\u7ef4\u5ea6\u3001\u8f68\u8ff9\u5bf9\u9f50\u3001\u6d41\u5f62\u89e3\u7f20\u7b49\u6307\u6807\u3002\u5f15\u5165\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\u4f5c\u4e3a\u4ece\u521d\u59cb\u5230\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7684\u6620\u5c04\u3002", "result": "\u6cd5\u5f8b\u63a8\u7406\u7ecf\u5386\"\u7ed3\u6676\u5316\"\uff1a\u8868\u793a\u7ef4\u5ea6\u4e0b\u964d45%\uff0c\u8f68\u8ff9\u5bf9\u9f50\u589e\u52a031%\uff0c\u6d41\u5f62\u89e3\u7f20\u63d0\u534710\u500d\u3002\u79d1\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4fdd\u6301\"\u6db2\u6001\"\u51e0\u4f55\u4e0d\u53d8\u3002\u4ee3\u7801\u63a8\u7406\u5f62\u6210\u79bb\u6563\"\u6676\u683c\"\u7ed3\u6784\u3002\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\u5728\u6cd5\u5f8b\u63a8\u7406\u4e0a\u901a\u8fc7\u63a2\u9488\u89e3\u7801\u8fbe\u523063.6%\u51c6\u786e\u7387\u3002\u53d1\u73b0\u8de8\u9886\u57df\u548c\u89c4\u6a21\u7684\u901a\u7528\u632f\u8361\u7279\u5f81\uff08\u76f8\u5e72\u6027~-0.4\uff09\u3002", "conclusion": "\u601d\u7ef4\u6210\u672c\u7531\u6d41\u5f62\u51e0\u4f55\u800c\u975e\u4efb\u52a1\u96be\u5ea6\u51b3\u5b9a\uff0c\u4e3a\u62d3\u6251\u7ed3\u6784\u5141\u8bb8\u7684\u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e0d\u662f\u5747\u5300\u7684\uff0c\u800c\u662f\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u76f8\u53d8\u5b9e\u73b0\uff0c\u51e0\u4f55\u7ed3\u6784\u53ef\u9884\u6d4b\u53ef\u5b66\u4e60\u6027\u3002"}}
{"id": "2601.13128", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13128", "abs": "https://arxiv.org/abs/2601.13128", "authors": ["Sung Ju Lee", "Nam Ik Cho"], "title": "PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain", "comment": "Accepted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.", "AI": {"tldr": "PhaseMark\uff1a\u4e00\u79cd\u57fa\u4e8eVAE\u6f5c\u5728\u9891\u7387\u57df\u76f8\u4f4d\u8c03\u5236\u7684\u5355\u6b21\u4f18\u5316\u514d\u8d39\u6c34\u5370\u6846\u67b6\uff0c\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5feb\u6570\u5343\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u6297\u653b\u51fb\u80fd\u529b", "motivation": "\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u751f\u6210\u8d85\u903c\u771f\u56fe\u50cf\u7684\u666e\u53ca\u9700\u8981\u9c81\u68d2\u7684\u6c34\u5370\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7684\u540e\u5904\u7406\u65b9\u6cd5\u7531\u4e8e\u8fed\u4ee3\u4f18\u5316\u6216\u53cd\u6f14\u8fc7\u7a0b\u800c\u901f\u5ea6\u6781\u6162", "method": "PhaseMark\u662f\u4e00\u79cd\u5355\u6b21\u3001\u65e0\u9700\u4f18\u5316\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u5728VAE\u6f5c\u5728\u9891\u7387\u57df\u8c03\u5236\u76f8\u4f4d\uff0c\u5206\u6790\u4e86\u56db\u79cd\u8c03\u5236\u53d8\u4f53\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861", "result": "PhaseMark\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u5feb\u6570\u5343\u500d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6297\u653b\u51fb\u80fd\u529b\uff08\u5305\u62ec\u518d\u751f\u653b\u51fb\uff09\uff0c\u4e14\u4e0d\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf", "conclusion": "PhaseMark\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u5185\u5728\u6f5c\u5728\u5c5e\u6027\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6c34\u5370\u6280\u672f"}}
{"id": "2601.13132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13132", "abs": "https://arxiv.org/abs/2601.13132", "authors": ["Kim Yu-Ji", "Dahye Lee", "Kim Jun-Seong", "GeonU Kim", "Nam Hyeon-Woo", "Yongjin Kwon", "Yu-Chiang Frank Wang", "Jaesung Choe", "Tae-Hyun Oh"], "title": "GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning", "comment": "Project page: https://gaussexplorer.github.io/", "summary": "We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.", "AI": {"tldr": "GaussExplorer\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u5177\u8eab\u63a2\u7d22\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u590d\u6742\u8bed\u8a00\u67e5\u8be2\u9a71\u52a8\u76843D\u573a\u666f\u63a2\u7d22", "motivation": "\u73b0\u6709\u8bed\u8a00\u5d4c\u51653D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u7b80\u5355\u6587\u672c\u67e5\u8be2\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7ec4\u5408\u8bed\u8a00\u67e5\u8be2\uff1b\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3RGB-D\u7ed3\u6784\u5316\u8bb0\u5fc6\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u89c6\u89d2\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u590d\u6742\u67e5\u8be2\u5e76\u652f\u6301\u81ea\u7531\u89c6\u89d2\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "method": "\u57283D\u9ad8\u65af\u6e85\u5c04\u57fa\u7840\u4e0a\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u9996\u5148\u8bc6\u522b\u4e0e\u67e5\u8be2\u95ee\u9898\u6700\u76f8\u5173\u7684\u9884\u6355\u83b7\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u5176\u8c03\u6574\u5230\u65b0\u89c6\u89d2\u4ee5\u66f4\u597d\u5730\u6355\u6349\u89c6\u89c9\u4fe1\u606f\u4f9bVLM\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86VLM\u63a8\u7406\u4e0e3DGS\u96c6\u6210\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "GaussExplorer\u6210\u529f\u6574\u5408\u4e86VLM\u63a8\u7406\u4e0e3DGS\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u8bed\u8a00\u67e5\u8be2\u9a71\u52a8\u76843D\u573a\u666f\u63a2\u7d22\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13133", "abs": "https://arxiv.org/abs/2601.13133", "authors": ["Mingshuang Luo", "Ruibing Hou", "Bo Chao", "Hong Chang", "Zimo Liu", "Yaowei Wang", "Shiguang Shan"], "title": "CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks", "comment": "Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures", "summary": "Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.", "AI": {"tldr": "CLASP\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u4efb\u52a1\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528CLIP\u751f\u6210\u591a\u7ea7\u8bed\u4e49\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u63d0\u793a\u63a7\u5236\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u4eba\u4f53\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6d8c\u73b0\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u652f\u6301\u591a\u6837\u5316\u7684\u4eba\u4f53\u4e2d\u5fc3\u4e0b\u6e38\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u591a\u7ea7\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u5bf9\u8bed\u4e49\u7c92\u5ea6\u7684\u4e0d\u540c\u9700\u6c42\u3002", "method": "1. \u5229\u7528CLIP\u751f\u6210\u4f4e\u5c42\uff08\u8eab\u4f53\u90e8\u4f4d\uff09\u548c\u9ad8\u5c42\uff08\u5c5e\u6027\uff09\u8bed\u4e49\u4f2a\u6807\u7b7e\uff1b2. \u5c06\u8fd9\u4e9b\u591a\u7ea7\u8bed\u4e49\u7ebf\u7d22\u6574\u5408\u5230\u5b66\u4e60\u7684\u89c6\u89c9\u8868\u793a\u4e2d\uff1b3. \u5f15\u5165\u63d0\u793a\u63a7\u5236\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u6839\u636e\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u52a8\u6001\u8c03\u6574\u7279\u5f81\u63d0\u53d6\uff1b4. \u91c7\u7528\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7531CLIP\u884d\u751f\u7684\u90e8\u4f4d\u548c\u5c5e\u6027\u7ea7\u4f2a\u6807\u7b7e\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCLASP\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "CLASP\u901a\u8fc7\u6574\u5408CLIP\u751f\u6210\u7684\u591a\u7ea7\u8bed\u4e49\u4f2a\u6807\u7b7e\u548c\u52a8\u6001\u9002\u5e94\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u4e3a\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u901a\u7528\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u6837\u5316\u7684\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2601.13142", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13142", "abs": "https://arxiv.org/abs/2601.13142", "authors": ["Zhantao Ma", "Quanfeng Lu", "Shuai Zhong", "Dahai Yu", "Ping Luo", "Michael K. Ng"], "title": "TVWorld: Foundations for Remote-Control TV Agents", "comment": null, "summary": "Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \\textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \\textbf{TVWorld-N} for topology-aware navigation and \\textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \\emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \\textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TVWorld\u57fa\u51c6\u548cTVTheseus\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7535\u89c6\u9065\u63a7\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u62d3\u6251\u611f\u77e5\u548c\u957f\u65f6\u7a0b\u5bfc\u822a\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u70b9\u9009\u4ea4\u4e92\uff0c\u800c\u65e5\u5e38\u7535\u89c6\u4f7f\u7528\u4e2d\u5e38\u89c1\u7684\u9065\u63a7\u4ea4\u4e92\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u514d\u90e8\u7f72\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u7535\u89c6\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "1) \u63d0\u51faTVWorld\u79bb\u7ebf\u56fe\u7ed3\u6784\u62bd\u8c61\uff0c\u6a21\u62df\u771f\u5b9e\u7535\u89c6\u5bfc\u822a\u73af\u5883\uff1b2) \u6784\u5efa\u4e24\u4e2a\u4e92\u8865\u57fa\u51c6\uff1aTVWorld-N\u7528\u4e8e\u62d3\u6251\u611f\u77e5\u5bfc\u822a\uff0cTVWorld-G\u7528\u4e8e\u7126\u70b9\u611f\u77e5\u5b9a\u4f4d\uff1b3) \u63d0\u51fa\u62d3\u6251\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u62d3\u6251\u610f\u8bc6\u6ce8\u5165\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b4) \u5f00\u53d1TVTheseus\u4e13\u95e8\u7528\u4e8e\u7535\u89c6\u5bfc\u822a\u7684\u57fa\u7840\u6a21\u578b\u3002", "result": "TVTheseus\u5728TVWorld-N\u57fa\u51c6\u4e0a\u8fbe\u523068.3%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86Gemini 3 Flash\u7b49\u5f3a\u95ed\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u667a\u80fd\u4f53\u5728\u57fa\u4e8e\u7126\u70b9\u7684\u957f\u65f6\u7a0b\u7535\u89c6\u5bfc\u822a\u4e2d\u62d3\u6251\u610f\u8bc6\u4e0d\u8db3\u7684\u5173\u952e\u9650\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7535\u89c6\u9065\u63a7\u4ea4\u4e92\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u62d3\u6251\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u7535\u89c6\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u6709\u6548\u7684\u7535\u89c6\u4f7f\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.13148", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13148", "abs": "https://arxiv.org/abs/2601.13148", "authors": ["Richard Shaw", "Youngkyoon Jang", "Athanasios Papaioannou", "Arthur Moreau", "Helisa Dhamo", "Zhensong Zhang", "Eduardo P\u00e9rez-Pellitero"], "title": "ICo3D: An Interactive Conversational 3D Virtual Human", "comment": "Accepted by International Journal on Computer Vision (IJCV). Project page: https://ico3d.github.io/. This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in International Journal of Computer Vision and is available online at https://doi.org/10.1007/s11263-025-02725-8", "summary": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/", "code_url": "https://ico3d.github.io", "AI": {"tldr": "ICo3D\u662f\u4e00\u79cd\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u5bf9\u8bdd\u5f0f\u3001\u903c\u771f3D\u865a\u62df\u4eba\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u91c7\u96c6\u521b\u5efa\u53ef\u52a8\u753b\u76843D\u9762\u90e8\u548c\u52a8\u60013D\u8eab\u4f53\u6a21\u578b\uff0c\u4f7f\u7528\u9ad8\u65af\u56fe\u5143\u6e32\u67d3\uff0c\u7ed3\u5408LLM\u5b9e\u73b0\u5bf9\u8bdd\u80fd\u529b\uff0c\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u5b9e\u73b0\u7cbe\u786e\u540c\u6b65\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b8c\u5168\u96c6\u6210\u7684\u865a\u62df\u5316\u8eab\u4f53\u9a8c\uff0c\u652f\u6301\u5728\u6c89\u6d78\u5f0f\u73af\u5883\u4e2d\u8fdb\u884c\u53e3\u5934\u548c\u4e66\u9762\u5f62\u5f0f\u7684\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u3001\u865a\u62df\u52a9\u624b\u548c\u4e2a\u6027\u5316\u6559\u80b2\u7b49\u591a\u4e2a\u9886\u57df\u3002", "method": "\u57fa\u4e8e\u591a\u89c6\u89d2\u91c7\u96c6\u521b\u5efa\u53ef\u52a8\u753b\u76843D\u9762\u90e8\u6a21\u578b\u548c\u52a8\u60013D\u8eab\u4f53\u6a21\u578b\uff0c\u4e24\u8005\u5747\u4f7f\u7528\u9ad8\u65af\u56fe\u5143\u6e32\u67d3\u3002\u5408\u5e76\u4e24\u4e2a\u6a21\u578b\u5f62\u6210\u903c\u771f\u7684\u865a\u62df\u4eba\u5316\u8eab\u3002\u91c7\u7528LLM\u8d4b\u4e88\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f7f\u7528\u97f3\u9891\u8bed\u97f3\u4f5c\u4e3a\u9a71\u52a8\u4fe1\u53f7\u6765\u52a8\u753b\u9762\u90e8\u6a21\u578b\u3002\u6539\u8fdb\u4e86\u52a8\u6001\u9ad8\u65af\u6a21\u578b\uff1aSWinGS++\u7528\u4e8e\u8eab\u4f53\u91cd\u5efa\uff0cHeadGaS++\u7528\u4e8e\u9762\u90e8\u91cd\u5efa\uff0c\u5e76\u63d0\u4f9b\u65e0\u4f2a\u5f71\u7684\u9762\u90e8\u548c\u8eab\u4f53\u6a21\u578b\u5408\u5e76\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u4e86ICo3D\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u5bf9\u8bdd\u5f0f\u3001\u903c\u771f\u76843D\u865a\u62df\u4eba\u5316\u8eab\uff0c\u652f\u6301\u5b9e\u65f6\u7528\u6237\u4ea4\u4e92\u3002\u5c55\u793a\u4e86\u5b8c\u6574\u7cfb\u7edf\u7684\u6f14\u793a\uff0c\u5305\u62ec\u591a\u4e2a\u5b9e\u65f6\u4e0e3D\u5316\u8eab\u5bf9\u8bdd\u7684\u4f7f\u7528\u6848\u4f8b\u3002", "conclusion": "ICo3D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u96c6\u6210\u7684\u865a\u62df\u5316\u8eab\u4f53\u9a8c\uff0c\u652f\u6301\u6c89\u6d78\u5f0f\u73af\u5883\u4e2d\u7684\u53e3\u5934\u548c\u4e66\u9762\u4ea4\u4e92\uff0c\u5728\u6e38\u620f\u3001\u865a\u62df\u52a9\u624b\u3001\u4e2a\u6027\u5316\u6559\u80b2\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.13562", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13562", "abs": "https://arxiv.org/abs/2601.13562", "authors": ["Zhiguang Liu", "Yi Shang"], "title": "Reasoning is a Modality", "comment": "Code access: https://github.com/lz7fd/Reasoning_is_a_Modality", "summary": "The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u63a8\u7406\u89c6\u4e3a\u72ec\u7acb\u6a21\u6001\uff0c\u8bbe\u8ba1\u89d2\u8272\u5206\u79bb\u7684Transformer\u5757\u6765\u533a\u5206\u5168\u5c40\u63a7\u5236\u5668token\u548c\u7f51\u683c\u5de5\u4f5c\u7a7a\u95f4token\uff0c\u5728ARC\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\uff08\u5982LLMs\u548cViTs\uff09\u4e3b\u8981\u4f5c\u4e3a\u884c\u4e3a\u5e8f\u5217\u9884\u6d4b\u673a\u5668\u8fd0\u884c\uff0c\u901a\u8fc7\u5efa\u6a21token\u7edf\u8ba1\u6765\u5339\u914d\u53ef\u89c2\u5bdf\u884c\u4e3a\uff0c\u4f46\u6ca1\u6709\u6301\u4e45\u3001\u53ef\u8bfb\u7684\u5185\u5fc3\u72b6\u6001\u3002\u8fd9\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5b58\u5728\u5dee\u8ddd\uff1a\u4eba\u7c7b\u53ef\u4ee5\u901a\u8fc7\u89e3\u7801\u5185\u90e8\u72b6\u6001\u6765\u89e3\u91ca\u884c\u52a8\uff0c\u800cAI\u7cfb\u7edf\u53ef\u4ee5\u4ea7\u751f\u6d41\u7545\u7684\u4e8b\u540e\u5408\u7406\u5316\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u5e76\u4e0d\u57fa\u4e8e\u8fd9\u6837\u7684\u5185\u90e8\u72b6\u6001\u3002\u4f5c\u8005\u5047\u8bbe\u63a8\u7406\u662f\u4e00\u79cd\u6a21\u6001\uff1a\u63a8\u7406\u5e94\u8be5\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u4e8e\u5e94\u7528\u89c4\u5219\u7684\u4f4e\u7ea7\u5de5\u4f5c\u7a7a\u95f4\u7684\u901a\u9053\u5b58\u5728\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89d2\u8272\u5206\u79bbTransformer\u5757\uff0c\u5c06\u5168\u5c40\u63a7\u5236\u5668token\u4e0e\u7f51\u683c\u5de5\u4f5c\u7a7a\u95f4token\u5206\u5f00\uff0c\u5b9e\u73b0\u8fed\u4ee3\u89c4\u5219\u6267\u884c\u3002\u8be5\u65b9\u6cd5\u5728VARC\u89c6\u89c9\u4e2d\u5fc3\u534f\u8bae\u4e0b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3ARC\u4efb\u52a1\u4f5c\u4e3a\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002", "result": "\u5728ARC-1\u4e0a\u8fbe\u523062.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0860.2%\uff09\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u4e0e\u5bc6\u96c6\u7684ViT\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u8fde\u8d2f\u7684\u89c4\u5219\u5e94\u7528\u7ed3\u6784\uff0c\u4ece\"\u5408\u7406\u7684\u6982\u7387\u6591\u5757\"\u8f6c\u5411\u63a7\u5236\u5668\u9a71\u52a8\u7684\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4e86\u63a8\u7406\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u7684Transformer\u67b6\u6784\u5b9e\u73b0\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u5728ARC\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0c\u4e3aAI\u7cfb\u7edf\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u3001\u57fa\u4e8e\u72b6\u6001\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.13207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13207", "abs": "https://arxiv.org/abs/2601.13207", "authors": ["Jinnao Li", "Zijian Chen", "Tingzhu Chen", "Changbo Wang"], "title": "GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction", "comment": null, "summary": "Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86GTPred\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u542b370\u5f20\u5168\u7403\u5206\u5e03\u3001\u8de8\u8d8a120\u5e74\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u5e74\u4efd\u548c\u5206\u5c42\u4f4d\u7f6e\u5e8f\u5217\u5339\u914d\u6765\u6d4b\u8bd5\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u5b9a\u4f4d\u57fa\u51c6\u5927\u591a\u5ffd\u7565\u4e86\u56fe\u50cf\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u65f6\u95f4\u4fe1\u606f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ea6\u675f\u4f4d\u7f6e\u63a8\u65ad\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u540c\u65f6\u8003\u8651\u5730\u7406\u548c\u65f6\u95f4\u4fe1\u606f\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86GTPred\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b370\u5f20\u5168\u7403\u5206\u5e03\u3001\u8de8\u8d8a120\u5e74\u7684\u56fe\u50cf\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\uff1a1) \u8054\u5408\u8003\u8651\u5e74\u4efd\u548c\u5206\u5c42\u4f4d\u7f6e\u5e8f\u5217\u5339\u914d\uff1b2) \u4f7f\u7528\u7cbe\u5fc3\u6807\u6ce8\u7684\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u94fe\u3002\u57288\u4e2a\u4e13\u6709\u548c7\u4e2a\u5f00\u6e90MLLMs\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u5f53\u524d\u6a21\u578b\u5177\u6709\u8f83\u5f3a\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5728\u4e16\u754c\u77e5\u8bc6\u548c\u5730\u7406\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u3002\u7ed3\u679c\u540c\u65f6\u663e\u793a\uff0c\u7eb3\u5165\u65f6\u95f4\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u4f4d\u7f6e\u63a8\u65ad\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5730\u7406\u65f6\u95f4\u9884\u6d4b\u57fa\u51c6GTPred\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u65f6\u95f4\u4fe1\u606f\u5bf9\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u65b9\u5411\u3002"}}
{"id": "2601.13707", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13707", "abs": "https://arxiv.org/abs/2601.13707", "authors": ["Yujin Jo", "Sangyoon Bae", "Taesup Kim"], "title": "Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs", "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.", "AI": {"tldr": "ACG\u662f\u4e00\u79cd\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u7684\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00\u548c\u7eaf\u8bed\u8a00\u6ce8\u610f\u529b\u8def\u5f84\u6765\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u4ea4\u5316\u6821\u6b63\u6d88\u9664\u8fd1\u4f3c\u504f\u5dee\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4e3b\u8981\u6e90\u4e8e\u8bed\u8a00\u5148\u9a8c\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u652f\u914d\uff0c\u5bfc\u81f4\u7269\u4f53\u8bef\u8bc6\u522b\u548c\u89c6\u89c9\u4e0d\u4e00\u81f4\u7684\u63cf\u8ff0\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u6b21\u524d\u5411\u8ba1\u7b97\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u53c8\u80fd\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u5f15\u5bfc\uff08ACG\uff09\uff0c\u5728\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u4e2d\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00\u548c\u7eaf\u8bed\u8a00\u6ce8\u610f\u529b\u8def\u5f84\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8fd9\u4e24\u79cd\u8def\u5f84\u6765\u5f15\u5bfc\u751f\u6210\u3002\u8fdb\u4e00\u6b65\u5e94\u7528\u6b63\u4ea4\u5316\u6821\u6b63\uff0c\u6d88\u9664\u4e0e\u7eaf\u8bed\u8a00\u8def\u5f84\u5bf9\u9f50\u7684\u5206\u91cf\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u89c6\u89c9\u8d21\u732e\u3002", "result": "\u5728CHAIR\u548cPOPE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5fe0\u5b9e\u5ea6\u548c\u5b57\u5e55\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u4e0e\u9700\u8981\u591a\u6b21\u524d\u5411\u8ba1\u7b97\u7684\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe2\u500d\u3002", "conclusion": "ACG\u4e3a\u5e7b\u89c9\u7f13\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u7406\u6027\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u7684\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u5f15\u5bfc\uff0c\u6709\u6548\u51cf\u5c11\u8bed\u8a00\u5148\u9a8c\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.13751", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13751", "abs": "https://arxiv.org/abs/2601.13751", "authors": ["Daniel Kyselica", "Jon\u00e1\u0161 Herec", "Oliver Kutis", "Rado Pito\u0148\u00e1k"], "title": "HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection", "comment": "19 pages, 9 figures, submitted to conference", "summary": "Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection", "code_url": "https://github.com/zaitra/HiT-change-detection", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c0f\u536b\u661f\u7684\u673a\u8f7d\u6d2a\u6c34\u68c0\u6d4b\u7cfb\u7edfHiT\uff08History Injection Transformer\uff09\uff0c\u901a\u8fc7\u5386\u53f2\u6ce8\u5165\u673a\u5236\u5728\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u6570\u636e\u5b58\u50a8\u9700\u6c42\u51cf\u5c1199%\u4ee5\u4e0a\uff0c\u5b9e\u73b0\u4e8643 FPS\u7684\u5b9e\u65f6\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u76d1\u6d4b\u9700\u8981\u5904\u7406\u591a\u65f6\u76f8\u536b\u661f\u6570\u636e\uff0c\u4f46\u5c0f\u536b\u661f\u5b58\u5728\u4e25\u683c\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u6d2a\u6c34\u68c0\u6d4b\u4f5c\u4e3a\u707e\u5bb3\u7ba1\u7406\u7684\u5173\u952e\u5e94\u7528\uff0c\u9700\u8981\u80fd\u591f\u5728\u673a\u8f7d\u73af\u5883\u4e0b\u5b9e\u65f6\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u5bf9\u5730\u9762\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86HiT\uff08History Injection Transformer\uff09\u673a\u5236\uff0c\u5728Transformer\u6a21\u578b\u4e2d\u7ef4\u62a4\u5386\u53f2\u89c2\u6d4b\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u5c06\u6570\u636e\u5b58\u50a8\u51cf\u5c11\u5230\u539f\u59cb\u56fe\u50cf\u5927\u5c0f\u7684\u4e0d\u52301%\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8ePrithvi-tiny\u57fa\u7840\u6a21\u578b\u6784\u5efa\uff0c\u5728STTORM-CD\u6d2a\u6c34\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "HiT\u673a\u5236\u5728\u4fdd\u6301\u4e0e\u53cc\u65f6\u76f8\u57fa\u7ebf\u76f8\u5f53\u7684\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u6570\u636e\u5b58\u50a8\u9700\u6c42\u51cf\u5c11\u4e8699%\u4ee5\u4e0a\u3002HiT-Prithvi\u6a21\u578b\u5728Jetson Orin Nano\uff08\u7eb3\u7c73\u536b\u661f\u4ee3\u8868\u6027\u786c\u4ef6\uff09\u4e0a\u5b9e\u73b0\u4e8643 FPS\u7684\u5904\u7406\u901f\u5ea6\uff0c\u6ee1\u8db3\u5b9e\u65f6\u76d1\u6d4b\u9700\u6c42\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u536b\u661f\u7684\u81ea\u7136\u707e\u5bb3\u8fde\u7eed\u76d1\u6d4b\u5efa\u7acb\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u65f6\u707e\u5bb3\u8bc4\u4f30\uff0c\u51cf\u5c11\u5bf9\u5730\u9762\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u7684\u4f9d\u8d56\u3002\u6a21\u578b\u67b6\u6784\u548c\u68c0\u67e5\u70b9\u5df2\u5f00\u6e90\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2601.13218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13218", "abs": "https://arxiv.org/abs/2601.13218", "authors": ["Igor Vozniak", "Philipp Mueller", "Nils Lipp", "Janis Sprenger", "Konstantin Poddubnyy", "Davit Hovhannisyan", "Christian Mueller", "Andreas Bulling", "Philipp Slusallek"], "title": "ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments", "comment": "Accepted for publication at the IEEE Intelligent Vehicles Symposium (IV), 2026", "summary": "The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \\dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \\dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.", "AI": {"tldr": "\u63d0\u51faStreetCrossingVR\u6570\u636e\u96c6\u548coSIM\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5bf9\u8c61\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86SUMGraph\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u5177\u6709\u57fa\u4e8e\u5bf9\u8c61\u7684\u7279\u6027\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002\u7279\u522b\u662f\u5728\u8857\u9053\u8fc7\u8857\u5bfc\u822a\u7b49\u590d\u6742\u573a\u666f\u4e2d\uff0c\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5b58\u5728\u4f26\u7406\u548c\u5b89\u5168\u6311\u6218\u3002", "method": "1) \u521b\u5efaStreetCrossingVR\u6570\u636e\u96c6\uff1a\u5305\u542b120\u540d\u53c2\u4e0e\u8005\u5728VR\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u8857\u9053\u8fc7\u8857\u5bfc\u822a\u6570\u636e\uff0c\u63d0\u4f9b\u51c6\u786e\u6ce8\u89c6\u6570\u636e\u3001\u5b8c\u6574\u7269\u4f53\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u3001\u5168\u666f\u5206\u5272\u3001\u6df1\u5ea6\u4fe1\u606f\u548c\u8f66\u8f86\u5173\u952e\u70b9\u7b49\u4e30\u5bcc\u6807\u6ce8\uff1b2) \u63d0\u51faoSIM\uff08\u57fa\u4e8e\u5bf9\u8c61\u76f8\u4f3c\u5ea6\uff09\u4f5c\u4e3a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1b3) \u5f00\u53d1SUMGraph\u6a21\u578b\uff1a\u57fa\u4e8eMamba U-Net\u67b6\u6784\uff0c\u5c06\u5173\u952e\u573a\u666f\u5bf9\u8c61\uff08\u8f66\u8f86\uff09\u7f16\u7801\u4e3a\u56fe\u8868\u793a\u3002", "result": "1) \u6570\u636e\u96c6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6536\u96c6\u53ef\u6bd4\u6570\u636e\u7684\u4f26\u7406\u548c\u5b89\u5168\u6311\u6218\uff1b2) \u5b9e\u9a8c\u8868\u660e\uff0c\u660e\u786e\u4f18\u5316\u57fa\u4e8e\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u4e0d\u4ec5\u80fd\u63d0\u9ad8oSIM\u6027\u80fd\uff0c\u8fd8\u80fd\u6539\u5584\u6a21\u578b\u5728\u5e38\u89c1\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff1b3) SUMGraph\u6a21\u578b\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u65b9\u6cd5\u4e2d\u53d6\u5f97\u4e86\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u57fa\u4e8e\u5bf9\u8c61\u89c6\u89c9\u6ce8\u610f\u529b\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u548c\u6307\u6807\u7a7a\u767d\uff0c\u63d0\u51fa\u7684StreetCrossingVR\u6570\u636e\u96c6\u3001oSIM\u6307\u6807\u548cSUMGraph\u6a21\u578b\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6240\u6709\u8d44\u6e90\u5c06\u516c\u5f00\u91ca\u653e\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2601.13770", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-fin.CP", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2601.13770", "abs": "https://arxiv.org/abs/2601.13770", "authors": ["Mostapha Benhenda"], "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance", "comment": null, "summary": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench", "code_url": "https://github.com/benstaf/lookaheadbench", "code_stars": 0, "code_last_update": "2026-01-16", "AI": {"tldr": "Look-Ahead-Bench\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u91d1\u878d\u5de5\u4f5c\u6d41\u4e2dPiT\u5927\u8bed\u8a00\u6a21\u578b\u7684look-ahead\u504f\u5dee\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5e02\u573a\u673a\u5236\u4e0b\u7684\u6027\u80fd\u8870\u51cf\u6765\u533a\u5206\u771f\u5b9e\u9884\u6d4b\u80fd\u529b\u4e0e\u8bb0\u5fc6\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u95ee\u7b54\u6d4b\u8bd5\u5185\u90e8\u524d\u77bb\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u6a21\u578b\u884c\u4e3a\u7684\u8bc4\u4f30\u3002\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8861\u91cf\u91d1\u878dLLM\u4e2d\u7684\u65f6\u95f4\u504f\u5dee\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u521b\u5efaLook-Ahead-Bench\u57fa\u51c6\uff0c\u5728\u73b0\u5b9e\u91d1\u878d\u5de5\u4f5c\u6d41\u4e2d\u8bc4\u4f30PiT LLM\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u65f6\u95f4\u5e02\u573a\u673a\u5236\u4e0b\u7684\u6027\u80fd\u8870\u51cf\u6765\u533a\u5206\u9884\u6d4b\u80fd\u529b\u4e0e\u8bb0\u5fc6\uff0c\u4f7f\u7528\u591a\u4e2a\u91cf\u5316\u57fa\u7ebf\u5efa\u7acb\u6027\u80fd\u9608\u503c\u3002\u8bc4\u4f30\u5f00\u6e90LLM\uff08Llama 3.1 8B/70B\u3001DeepSeek 3.2\uff09\u4e0ePiT-Inference\u6a21\u578b\u5bb6\u65cf\uff08Pitinf-Small\u3001Pitinf-Medium\u3001Pitinf-Large\uff09\u3002", "result": "\u6807\u51c6LLM\u663e\u793a\u51fa\u663e\u8457\u7684look-ahead\u504f\u5dee\uff08\u901a\u8fc7alpha\u8870\u51cf\u6d4b\u91cf\uff09\uff0c\u800cPitinf\u6a21\u578b\u968f\u7740\u89c4\u6a21\u6269\u5927\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002Pitinf\u6a21\u578b\u5728\u65f6\u95f4\u504f\u5dee\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6807\u51c6LLM\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u91d1\u878dLLM\u65f6\u95f4\u504f\u5dee\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u8bc6\u522b\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u6a21\u578b\u7684\u5b9e\u7528\u6846\u67b6\u3002Pitinf\u6a21\u578b\u5728\u51cf\u5c11look-ahead\u504f\u5dee\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u91d1\u878d\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13225", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13225", "abs": "https://arxiv.org/abs/2601.13225", "authors": ["Tim Lachmann", "Alexandra Israelsson", "Christina Tornberg", "Teimuraz Saghinadze", "Michal Balazia", "Philipp M\u00fcller", "Petri Laukka"], "title": "Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations", "comment": "Accepted for publication at IEEE Face & Gesture 2026", "summary": "Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.", "AI": {"tldr": "\u63d0\u51faBLEMORE\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u6df7\u5408\u60c5\u611f\u8bc6\u522b\uff0c\u5305\u542b\u60c5\u611f\u76f8\u5bf9\u663e\u8457\u5ea6\u4fe1\u606f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d", "motivation": "\u4eba\u7c7b\u901a\u5e38\u540c\u65f6\u4f53\u9a8c\u591a\u79cd\u60c5\u611f\u6df7\u5408\uff0c\u800c\u975e\u5355\u4e00\u60c5\u611f\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5927\u591a\u53ea\u80fd\u8bc6\u522b\u5355\u4e00\u60c5\u611f\uff0c\u7f3a\u4e4f\u5305\u542b\u60c5\u611f\u76f8\u5bf9\u663e\u8457\u5ea6\u6807\u6ce8\u7684\u6df7\u5408\u60c5\u611f\u6570\u636e\u96c6", "method": "\u6784\u5efa\u5305\u542b3000\u591a\u4e2a\u89c6\u9891\u7247\u6bb5\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6765\u81ea58\u540d\u6f14\u5458\u8868\u6f146\u79cd\u57fa\u672c\u60c5\u611f\u548c10\u79cd\u4e0d\u540c\u6df7\u5408\u60c5\u611f\uff0c\u6bcf\u79cd\u6df7\u5408\u67093\u79cd\u663e\u8457\u5ea6\u914d\u7f6e(50/50\u300170/30\u300130/70)\uff1b\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u60c5\u611f\u5b58\u5728\u9884\u6d4b\u548c\u60c5\u611f\u663e\u8457\u5ea6\u9884\u6d4b", "result": "\u5355\u6a21\u6001\u5206\u7c7b\u5668\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523029%\u7684\u5b58\u5728\u51c6\u786e\u7387\u548c13%\u7684\u663e\u8457\u5ea6\u51c6\u786e\u7387\uff1b\u591a\u6a21\u6001\u65b9\u6cd5\u6709\u660e\u663e\u6539\u8fdb\uff0cImageBind + WavLM\u8fbe\u523035%\u5b58\u5728\u51c6\u786e\u7387\uff0cHiCMAE\u8fbe\u523018%\u663e\u8457\u5ea6\u51c6\u786e\u7387\uff1b\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\u8fbe\u523033%\u5b58\u5728\u51c6\u786e\u7387(VideoMAEv2 + HuBERT)\u548c18%\u663e\u8457\u5ea6\u51c6\u786e\u7387(HiCMAE)", "conclusion": "BLEMORE\u6570\u636e\u96c6\u4e3a\u63a8\u8fdb\u8003\u8651\u6df7\u5408\u60c5\u611f\u8868\u8fbe\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\u7684\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90"}}
{"id": "2601.13798", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13798", "abs": "https://arxiv.org/abs/2601.13798", "authors": ["Kai Wittenmayer", "Sukrut Rao", "Amin Parchami-Araghi", "Bernt Schiele", "Jonas Fischer"], "title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders", "comment": "32 pages, 24 figures, 3 tables", "summary": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.", "code_url": "https://github.com/kawi19/Insigh", "AI": {"tldr": "Insight\u662f\u4e00\u4e2a\u8bed\u8a00\u5bf9\u9f50\u7684\u6982\u5ff5\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u591a\u7c92\u5ea6\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u63d0\u4f9b\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6982\u5ff5\u89e3\u91ca\uff0c\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1\u4e0d\u900f\u660e\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u8bed\u8a00\u5bf9\u9f50\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u8868\u5f81\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u89e3\u91ca\u51b3\u7b56\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u5c06\u8868\u5f81\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u4e14\u4ec5\u9650\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u5177\u6709\u5f3a\u8bed\u4e49\u8868\u5f81\u7684\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u591a\u7c92\u5ea6\u6982\u5ff5\uff1b\u901a\u8fc7\u5206\u6790\u6982\u5ff5\u7684\u5c40\u90e8\u5171\u73b0\u4f9d\u8d56\u5173\u7cfb\u5b9a\u4e49\u6982\u5ff5\u5173\u7cfb\uff1b\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u6539\u8fdb\u6982\u5ff5\u547d\u540d\u5e76\u83b7\u5f97\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u4e0a\uff0cInsight\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u4e0e\u4e0d\u900f\u660e\u57fa\u7840\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u9ad8\u8d28\u91cf\u7684\u6982\u5ff5\u89e3\u91ca\u3002", "conclusion": "Insight\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u8bed\u8a00\u5bf9\u9f50\u7684\u6982\u5ff5\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u7a7a\u95f4\u5b9a\u4f4d\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.13816", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13816", "abs": "https://arxiv.org/abs/2601.13816", "authors": ["Ra\u00fcl P\u00e9rez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "title": "Discriminant Learning-based Colorspace for Blade Segmentation", "comment": "Accepted to ICASSP 2026", "summary": "Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.", "AI": {"tldr": "\u63d0\u51faCSDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u975e\u7ebf\u6027\u5224\u522b\u5206\u6790\u4f18\u5316\u8272\u5f69\u8868\u793a\uff0c\u63d0\u5347\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u5e38\u5ffd\u89c6\u8272\u5f69\u8868\u793a\u8fd9\u4e00\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u6b21\u4f18\u7684\u8272\u5f69\u8868\u793a\u4f1a\u963b\u788d\u51c6\u786e\u5206\u5272", "method": "\u63d0\u51faColorspace Discriminant Analysis (CSDA)\u7b97\u6cd5\uff0c\u5c06\u7ebf\u6027\u5224\u522b\u5206\u6790\u6269\u5c55\u5230\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6700\u5927\u5316\u591a\u7ef4\u6709\u7b26\u53f7\u7c7b\u95f4\u53ef\u5206\u6027\u5e76\u6700\u5c0f\u5316\u7c7b\u5185\u53d8\u5f02\u6027\u6765\u5b9a\u5236\u8272\u5f69\u8868\u793a\uff1b\u5f15\u5165\u4e09\u79cd\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316", "result": "\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u7cbe\u5ea6\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5b9a\u5236\u5316\u9884\u5904\u7406\u5728\u9886\u57df\u7279\u5b9a\u5206\u5272\u4e2d\u7684\u91cd\u8981\u6027", "conclusion": "CSDA\u901a\u8fc7\u4f18\u5316\u8272\u5f69\u8868\u793a\u663e\u8457\u6539\u5584\u5206\u5272\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2601.13238", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13238", "abs": "https://arxiv.org/abs/2601.13238", "authors": ["Chengyin Hu", "Xiang Chen", "Zhe Jia", "Weiwen Shi", "Fengyu Zhang", "Jiujiang Guo", "Yiwei Wei"], "title": "A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528\u771f\u5b9e\u5929\u6c14\u6761\u4ef6\u653b\u51fb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53c2\u6570\u5316\u6270\u52a8\u6a21\u578b\u5206\u6790\u96e8\u5929\u6c14\u5019\u5bf9VLM\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u7269\u7406\u5408\u7406\u7684\u5929\u6c14\u6270\u52a8\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u8bed\u4e49\u9519\u4f4d\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u6807\u51c6\u89c6\u89c9\u6761\u4ef6\u4e0b\u8bad\u7ec3\uff0c\u5bf9\u771f\u5b9e\u5929\u6c14\u6761\u4ef6\uff08\u7279\u522b\u662f\u96e8\u5929\uff09\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u7814\u7a76\u4e0d\u8db3\u3002\u9700\u8981\u8bc4\u4f30VLM\u5728\u73b0\u5b9e\u5929\u6c14\u6270\u52a8\u4e0b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u53c2\u6570\u5316\u6270\u52a8\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4f4e\u7ef4\u5168\u5c40\u8c03\u5236\u5f31\u5316\u539f\u59cb\u8bed\u4e49\u51b3\u7b56\u8fb9\u754c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u663e\u5f0f\u5efa\u6a21\u591a\u5c3a\u5ea6\u96e8\u6ef4\u5916\u89c2\u548c\u964d\u96e8\u5f15\u8d77\u7684\u7167\u660e\u53d8\u5316\uff0c\u4f18\u5316\u4e0d\u53ef\u5fae\u7684\u5929\u6c14\u7a7a\u95f4\u4ee5\u8bf1\u5bfc\u7a33\u5b9a\u7684\u8bed\u4e49\u504f\u79fb\u3002\u8be5\u6846\u67b6\u5728\u975e\u50cf\u7d20\u53c2\u6570\u7a7a\u95f4\u4e2d\u751f\u6210\u7269\u7406\u57fa\u7840\u4e14\u53ef\u89e3\u91ca\u7684\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u7269\u7406\u5408\u7406\u4e14\u9ad8\u5ea6\u7ea6\u675f\u7684\u5929\u6c14\u6270\u52a8\u4e5f\u80fd\u5728\u4e3b\u6d41VLM\u4e2d\u5f15\u8d77\u663e\u8457\u7684\u8bed\u4e49\u9519\u4f4d\uff0c\u8bc1\u5b9e\u4e86\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u786e\u8ba4\u7167\u660e\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u96e8\u6ef4\u7ed3\u6784\u662f\u8bed\u4e49\u504f\u79fb\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u96e8\u5929\u6761\u4ef6\u5bf9VLM\u8bed\u4e49\u5bf9\u9f50\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u5bf9\u6297\u6846\u67b6\u63ed\u793a\u4e86\u73b0\u5b9e\u5929\u6c14\u6270\u52a8\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u5b89\u5168\u6027\u7684\u5a01\u80c1\uff0c\u5f3a\u8c03\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8003\u8651\u73af\u5883\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.13846", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13846", "abs": "https://arxiv.org/abs/2601.13846", "authors": ["Glinskaya Maria"], "title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments", "comment": null, "summary": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVirtual Urbanism (VU)\u6846\u67b6\uff0c\u901a\u8fc7AI\u751f\u6210\u5408\u6210\u57ce\u5e02\u590d\u5236\u54c1\u6765\u91cf\u5316\u57ce\u5e02\u8eab\u4efd\uff0c\u4ee5\u4e1c\u4eac\u4e5d\u4e2a\u533a\u57df\u4e3a\u6848\u4f8b\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u83b7\u5f97\u7ea681%\u7684\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u5f00\u53d1\u8ba1\u7b97\u53ef\u5904\u7406\u7684\u91cf\u5316\u57ce\u5e02\u8eab\u4efd\u6307\u6807\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u57ce\u5e02\u590d\u5236\u54c1\u6765\u5206\u6790\u548c\u6d4b\u91cf\u57ce\u5e02\u8eab\u4efd\u7684\u6838\u5fc3\u5f62\u6210\u8981\u7d20", "method": "\u6574\u5408Stable Diffusion\u548cLoRA\u6a21\u578b\u751f\u6210\u4e1c\u4eac\u4e5d\u4e2a\u533a\u57df\u7684\u52a8\u6001\u5408\u6210\u57ce\u5e02\u5e8f\u5217\uff0c\u6392\u9664\u73b0\u6709\u5bfc\u5411\u6807\u8bb0\u4ee5\u63d0\u53d6\u6838\u5fc3\u8eab\u4efd\u8981\u7d20\uff1b\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u9a8c\uff1a(I)\u8bc4\u4f30\u590d\u5236\u54c1\u611f\u77e5\u5408\u6cd5\u6027\uff0c(II)\u91cf\u5316\u533a\u57df\u7ea7\u8eab\u4efd\uff0c(III)\u63a8\u5bfc\u6838\u5fc3\u8eab\u4efd\u5f62\u6210\u8981\u7d20", "result": "\u590d\u5236\u54c1\u8bc6\u522b\u51c6\u786e\u7387\u5e73\u5747\u7ea681%\uff0c\u9a8c\u8bc1\u4e86\u590d\u5236\u54c1\u7684\u6709\u6548\u6027\uff1bUrban Identity Level (UIL)\u6307\u6807\u80fd\u8bc4\u4f30\u5404\u533a\u57df\u8eab\u4efd\u6c34\u5e73\uff1b\u8bed\u4e49\u5206\u6790\u63ed\u793a\u6587\u5316\u5d4c\u5165\u7684\u7c7b\u578b\u5b66\u4f5c\u4e3a\u6838\u5fc3\u8eab\u4efd\u5f62\u6210\u8981\u7d20", "conclusion": "VU\u662fAI\u589e\u5f3a\u57ce\u5e02\u5206\u6790\u7684\u53ef\u884c\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u3001\u591a\u53c2\u6570\u8eab\u4efd\u6307\u6807\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2601.13263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13263", "abs": "https://arxiv.org/abs/2601.13263", "authors": ["Chenyu Liu", "Marco Cecotti", "Harikrishnan Vijayakumar", "Patrick Robinson", "James Barson", "Mihai Caleap"], "title": "Deep Learning for Semantic Segmentation of 3D Ultrasound Data", "comment": "14 pages, 10 figures, 8 tables, presented at 2025 13th International Conference on Robot Intelligence Technology and Applications (RITA)", "summary": "Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCalyo Pulse\u56fa\u60013D\u8d85\u58f0\u4f20\u611f\u5668\u7684\u5b66\u4e60\u578b3D\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u9760\u7684\u611f\u77e5\u7cfb\u7edf\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6838\u5fc3\u6311\u6218\u3002\u5f53\u524d\u4e3b\u6d41\u7684LiDAR\u548c\u6444\u50cf\u5934\u7cfb\u7edf\u5728\u6210\u672c\u3001\u9c81\u68d2\u6027\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u5bfb\u627e\u8865\u5145\u611f\u77e5\u6a21\u6001\u3002", "method": "\u5f15\u5165\u57fa\u4e8eCalyo Pulse\u6a21\u5757\u5316\u56fa\u60013D\u8d85\u58f0\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u75283D U-Net\u67b6\u6784\u5728\u7a7a\u95f4\u8d85\u58f0\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4f53\u7d20\u7ea7\u8bed\u4e49\u5206\u5272\u3002", "result": "Calyo Pulse\u4f20\u611f\u5668\u5c55\u793a\u4e86\u7a33\u5065\u7684\u5206\u5272\u6027\u80fd\uff0c\u901a\u8fc7\u66f4\u5927\u6570\u636e\u96c6\u3001\u7cbe\u7ec6\u5316\u5730\u9762\u771f\u503c\u548c\u52a0\u6743\u635f\u5931\u51fd\u6570\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u6f5c\u529b\u3002", "conclusion": "3D\u8d85\u58f0\u4f20\u611f\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u8865\u5145\u6a21\u6001\uff0c\u80fd\u591f\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u6076\u52a3\u548c\u6742\u4e71\u73af\u5883\u4e2d\u3002"}}
{"id": "2601.13299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13299", "abs": "https://arxiv.org/abs/2601.13299", "authors": ["Ethan Seefried", "Prahitha Movva", "Naga Harshita Marupaka", "Tilak Kasturi", "Tirthankar Ghosal"], "title": "Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams", "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Ai4 Science", "summary": "We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.", "AI": {"tldr": "Enginuity\uff1a\u9996\u4e2a\u5f00\u653e\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u9762\u7684\u7ed3\u6784\u6807\u6ce8\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u56fe\u8868\u89e3\u6790", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u79d1\u5b66\u53d1\u73b0\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b58\u5728\u6839\u672c\u6027\u969c\u788d\uff0c\u65e0\u6cd5\u5145\u5206\u7406\u89e3\u548c\u64cd\u4f5c\u5de5\u7a0b\u56fe\u4e2d\u7684\u89c6\u89c9\u7ed3\u6784\u77e5\u8bc6\u3002\u5de5\u7a0b\u56fe\u89e3\u91ca\u3001\u6280\u672f\u56fe\u7eb8\u5206\u6790\u548c\u89c6\u89c9\u63a8\u7406\u5bf9\u4e8e\u5047\u8bbe\u751f\u6210\u3001\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u6807\u6ce8\u6570\u636e\u96c6\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u521b\u5efa\u5f00\u653e\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u9762\u7684\u7ed3\u6784\u6807\u6ce8\uff0c\u6355\u6349\u5c42\u6b21\u5316\u7ec4\u4ef6\u5173\u7cfb\u3001\u8fde\u63a5\u5173\u7cfb\u548c\u8bed\u4e49\u5143\u7d20\u3002\u8be5\u6570\u636e\u96c6\u8bbe\u8ba1\u7528\u4e8e\u652f\u6301\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5de5\u7a0b\u56fe\u89e3\u6790\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u7684Enginuity\u6570\u636e\u96c6\u5c06\u80fd\u591f\u652f\u6301\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u56fe\u8868\u89e3\u6790\u3001\u8de8\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548cAI\u8f85\u52a9\u5de5\u7a0b\u4eff\u771f\u3002\u8be5\u6570\u636e\u96c6\u5c06\u9996\u6b21\u4e3a\u591a\u9886\u57df\u5de5\u7a0b\u56fe\u63d0\u4f9b\u5927\u89c4\u6a21\u7684\u7ed3\u6784\u5316\u6807\u6ce8\u3002", "conclusion": "Enginuity\u6570\u636e\u96c6\u5c06\u53d8\u9769\u79d1\u5b66\u53d1\u73b0\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u548c\u64cd\u4f5c\u5de5\u7a0b\u56fe\u4e2d\u5d4c\u5165\u7684\u89c6\u89c9\u7ed3\u6784\u77e5\u8bc6\uff0c\u6253\u7834\u5f53\u524d\u963b\u788dAI\u5b8c\u5168\u53c2\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u6839\u672c\u969c\u788d\uff0c\u5728\u56fe\u8868\u89e3\u91ca\u3001\u6280\u672f\u56fe\u7eb8\u5206\u6790\u548c\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u5b9e\u73b0\u7a81\u7834\u3002"}}
{"id": "2601.13304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13304", "abs": "https://arxiv.org/abs/2601.13304", "authors": ["Wenxin Ma", "Chenlong Wang", "Ruisheng Yuan", "Hao Chen", "Nanru Dai", "S. Kevin Zhou", "Yijun Yang", "Alan Yuille", "Jieneng Chen"], "title": "CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning", "comment": "Code is available: https://github.com/CausalSpatial/CausalSpatial", "summary": "Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer \"what-if\" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial", "code_url": "https://github.com/CausalSpatial/CausalSpatial", "code_stars": 3, "code_last_update": "2026-01-15", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CausalSpatial\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5e76\u63d0\u51fa\u4e86Causal Object World\u6a21\u578b\uff08COW\uff09\u6846\u67b6\u6765\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u89c2\u5bdf\u9759\u6001\u573a\u666f\u9884\u6d4b\u540e\u7eed\u52a8\u6001\u7ed3\u679c\uff08\u5982\u7269\u4f53\u79fb\u52a8\u662f\u5426\u4f1a\u5bfc\u81f4\u78b0\u649e\uff09\uff0c\u8fd9\u79cd\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u662f\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6240\u7f3a\u4e4f\u7684\u3002\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u7a7a\u95f4\u611f\u77e5\uff0c\u96be\u4ee5\u56de\u7b543D\u573a\u666f\u4e2d\u7684\"\u5982\u679c...\u4f1a\u600e\u6837\"\u95ee\u9898\u3002", "method": "1. \u521b\u5efaCausalSpatial\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b\u78b0\u649e\u3001\u517c\u5bb9\u6027\u3001\u906e\u6321\u548c\u8f68\u8ff9\u56db\u4e2a\u4efb\u52a1\uff1b2. \u5206\u6790\u6a21\u578b\u5931\u8d25\u539f\u56e0\uff0c\u53d1\u73b0\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u94fe\u5f0f\u63a8\u7406\u800c\u504f\u79bb\u89c6\u89c9\u8bc1\u636e\uff1b3. \u63d0\u51faCausal Object World\u6a21\u578b\uff08COW\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5047\u8bbe\u52a8\u6001\u7684\u89c6\u9891\u6765\u5916\u90e8\u5316\u6a21\u62df\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u660e\u786e\u7684\u56e0\u679c\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u4eba\u7c7b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u520684%\uff0c\u800cGPT-5\u4ec5\u5f9754%\uff0c\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u5206\u6790\u8868\u660e\u6a21\u578b\u4f1a\u4ea7\u751f\u6d41\u7545\u4f46\u7a7a\u95f4\u65e0\u6839\u636e\u7684\u5e7b\u89c9\u3002COW\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u6a21\u62df\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u63a8\u7406\u5efa\u7acb\u5728\u7269\u7406\u73b0\u5b9e\u800c\u975e\u8bed\u8a00\u5148\u9a8c\u4e4b\u4e0a\u3002", "conclusion": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u63a8\u7406\u5bfc\u81f4\u7a7a\u95f4\u65e0\u6839\u636e\u7684\u5e7b\u89c9\u3002COW\u6846\u67b6\u901a\u8fc7\u5916\u90e8\u5316\u89c6\u89c9\u6a21\u62df\u8fc7\u7a0b\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u660e\u786e\u7684\u56e0\u679c\u89c6\u89c9\u7ebf\u7d22\uff0c\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.13935", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13935", "abs": "https://arxiv.org/abs/2601.13935", "authors": ["Anoushkrit Goel", "Simroop Singh", "Ankita Joshi", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Aditya Nigam", "Arnav Bhavsar"], "title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026", "summary": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.", "AI": {"tldr": "TrackletGPT\uff1a\u4e00\u79cd\u57fa\u4e8eGPT\u6846\u67b6\u7684\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u7247\u6bb5\uff08tracklets\uff09\u91cd\u65b0\u5f15\u5165\u5e8f\u5217\u4fe1\u606f\uff0c\u5728\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u5bf9\u4e8e\u7814\u7a76\u5927\u8111\u7ed3\u6784\u8fde\u63a5\u6027\u3001\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u548c\u795e\u7ecf\u5916\u79d1\u624b\u672f\u81f3\u5173\u91cd\u8981\u3002\u8be5\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7ea4\u7ef4\u675f\u5728\u4e0d\u540c\u4e2a\u4f53\u548c\u6761\u4ef6\u4e0b\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u5728\u534a\u7403\u548c\u4e2a\u4f53\u95f4\u5177\u6709\u76f8\u4f3c\u7684\u4e09\u7ef4\u7ed3\u6784\u3002", "method": "\u63d0\u51faTrackletGPT\uff0c\u4e00\u79cd\u7c7b\u4f3c\u8bed\u8a00\u7684GPT\u6846\u67b6\uff0c\u4f7f\u7528\u8f68\u8ff9\u7247\u6bb5\uff08tracklets\uff09\u5728\u6807\u8bb0\u4e2d\u91cd\u65b0\u5f15\u5165\u5e8f\u5217\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u7f1d\u6cdb\u5316\u5230\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5e76\u7f16\u7801\u7ec6\u7c92\u5ea6\u7684\u5b50\u6d41\u7ebf\u7247\u6bb5\uff08tracklets\uff09\uff0c\u5728\u7ea4\u7ef4\u675f\u6210\u50cf\u5206\u5272\u4e2d\u6269\u5c55\u548c\u4f18\u5316GPT\u6a21\u578b\u3002", "result": "TrackletGPT\u5728TractoInferno\u548cHCP\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747DICE\u3001Overlap\u548cOverreach\u5206\u6570\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "TrackletGPT\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u7247\u6bb5\u548cGPT\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6027\u548c\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.13364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13364", "abs": "https://arxiv.org/abs/2601.13364", "authors": ["Zhenan Liu", "Yaodong Cui", "Amir Khajepour", "George Shaker"], "title": "Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments", "comment": null, "summary": "This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7c89\u5c18\u73af\u5883\u4e2d\u751f\u6210\u53ef\u63a7\u591a\u7ea7\u7c89\u5c18\u6d53\u5ea6\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e864D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u9608\u503c\u7684\u566a\u58f0\u8fc7\u6ee4\u6846\u67b6\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u7ba1\u9053\uff0c\u7528\u4e8e\u7c89\u5c18\u73af\u5883\u4e0b\u7684\u53ef\u9760\u884c\u4eba\u68c0\u6d4b\u3002", "motivation": "\u5730\u4e0b\u77ff\u4e95\u3001\u9053\u8def\u96a7\u9053\u6216\u5012\u584c\u5efa\u7b51\u7269\u7b49\u6076\u52a3\u5c01\u95ed\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u7c89\u5c18\u6c61\u67d3\u548c\u7535\u78c1\u7ea6\u675f\uff0c\u8fd9\u4e9b\u6761\u4ef6\u4f1a\u663e\u8457\u5f71\u54cd\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u4f20\u64ad\u7279\u6027\u548c\u611f\u77e5\u529f\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u7c89\u5c18\u73af\u5883\u4e2d\u53ef\u9760\u5de5\u4f5c\u7684\u611f\u77e5\u7cfb\u7edf\u3002", "method": "1) \u5f00\u53d1\u4e86\u5728\u9ad8\u5ea6\u6742\u4e71\u73af\u5883\u4e2d\u751f\u6210\u53ef\u63a7\u591a\u7ea7\u7c89\u5c18\u6d53\u5ea6\u7684\u65b9\u6cd5\uff1b2) \u521b\u5efa\u4e86\u5305\u542b\u76f8\u673a\u548cLiDAR\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\uff1b3) \u5efa\u7acb\u4e86\u57fa\u4e8e\u5173\u952e\u96f7\u8fbe\u53c2\u6570\uff08RCS\u3001\u901f\u5ea6\u3001\u65b9\u4f4d\u89d2\u3001\u4ef0\u89d2\uff09\u7684\u9608\u503c\u566a\u58f0\u8fc7\u6ee4\u6846\u67b6\uff1b4) \u8bbe\u8ba1\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u805a\u7c7b\u7ea7\u5206\u7c7b\u7ba1\u9053\uff0c\u5229\u7528\u96f7\u8fbe\u8bed\u4e49\u7279\u5f81\uff08\u901f\u5ea6\u3001RCS\u3001\u4f53\u79ef\u5206\u5e03\uff09\u8fdb\u884c\u884c\u4eba\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u96c6\u6210\u65b9\u6cd5\u5728\u7c89\u5c18\u73af\u5883\u4e2d\u663e\u8457\u589e\u5f3a\u4e86\u6742\u6ce2\u6291\u5236\u3001\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u6574\u4f53\u5f39\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u3001\u5b9e\u65f6\u7684\u884c\u4eba\u68c0\u6d4b\uff0c\u65e0\u9700\u5927\u91cf\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6076\u52a3\u7c89\u5c18\u73af\u5883\u4e2d\u7684\u6beb\u7c73\u6ce2\u4f20\u64ad\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u901a\u8fc7\u6570\u636e\u7ea7\u8fc7\u6ee4\u548c\u8bed\u4e49\u7ea7\u5206\u7c7b\u7684\u96c6\u6210\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7c89\u5c18\u9897\u7c92\u548c\u53cd\u5c04\u8868\u9762\u5bf9\u611f\u77e5\u529f\u80fd\u7684\u8054\u5408\u5f71\u54cd\uff0c\u63d0\u5347\u4e86\u5728\u4e25\u91cd\u7535\u78c1\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.13975", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13975", "abs": "https://arxiv.org/abs/2601.13975", "authors": ["Marco Piccolo", "Qiwei Han", "Astrid van Toor", "Joachim Vanneste"], "title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains", "comment": "9 pages, 4 figures 8 tables", "summary": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u5317\u6781\u548c\u5927\u897f\u6d0b\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u5165\u4fb5\u7269\u79cd\u76d1\u6d4b\u5efa\u7acb\u4e86\u57fa\u7840\u68c0\u6d4b\u5c42\uff0c\u901a\u8fc7\u7edf\u4e00\u4fe1\u606f\u7ba1\u9053\u6807\u51c6\u5316\u5f02\u6784\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u573a\u666f\u7ed3\u6784\u56e0\u7d20\u6bd4\u89c6\u89c9\u9000\u5316\u5bf9\u8de8\u57df\u6027\u80fd\u5f71\u54cd\u66f4\u5927\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u9700\u8981\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\uff0c\u4ee5\u652f\u6301\u4fdd\u62a4\u548c\u5165\u4fb5\u7269\u79cd\u7ba1\u7406\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6848\u5b58\u5728\u660e\u663e\u7684\u90e8\u7f72\u5dee\u8ddd\uff0c\u5f53\u8f6c\u79fb\u5230\u65b0\u5730\u70b9\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u591a\u5e74\u5165\u4fb5\u7269\u79cd\u76d1\u6d4b\u8ba1\u5212\u5efa\u7acb\u57fa\u7840\u68c0\u6d4b\u5c42\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u4fe1\u606f\u7ba1\u9053\uff0c\u5c06\u5f02\u6784\u6570\u636e\u96c6\u6807\u51c6\u5316\u4e3a\u53ef\u6bd4\u4fe1\u606f\u6d41\uff1b\u5728\u53d7\u63a7\u8de8\u57df\u534f\u8bae\u4e0b\u8bc4\u4f30\u56fa\u5b9a\u7684\u90e8\u7f72\u76f8\u5173\u68c0\u6d4b\u5668\uff1b\u5206\u6790\u7ed3\u6784\u56e0\u7d20\uff08\u573a\u666f\u7ec4\u6210\u3001\u7269\u4f53\u5bc6\u5ea6\u3001\u4e0a\u4e0b\u6587\u5197\u4f59\uff09\u548c\u89c6\u89c9\u9000\u5316\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u63a8\u7406\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u7ed3\u6784\u56e0\u7d20\uff08\u573a\u666f\u7ec4\u6210\u3001\u7269\u4f53\u5bc6\u5ea6\u3001\u4e0a\u4e0b\u6587\u5197\u4f59\uff09\u6bd4\u89c6\u89c9\u9000\u5316\uff08\u5982\u6d51\u6d4a\u5ea6\uff09\u66f4\u80fd\u89e3\u91ca\u8de8\u57df\u6027\u80fd\u635f\u5931\uff1b\u7a00\u758f\u573a\u666f\u5f15\u53d1\u7279\u5f81\u6027\u7684\"\u4e0a\u4e0b\u6587\u5d29\u6e83\"\u6545\u969c\u6a21\u5f0f\uff1b\u8fd0\u884c\u65f6\u4f18\u5316\u4f7f\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u80fd\u591f\u5b9e\u73b0\u5b9e\u9645\u91c7\u6837\u7387\uff0c\u6ee1\u8db3\u8fdc\u7a0b\u76d1\u6d4b\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06\u91cd\u70b9\u4ece\u56fe\u50cf\u589e\u5f3a\u8f6c\u5411\u7ed3\u6784\u611f\u77e5\u53ef\u9760\u6027\uff0c\u4e3a\u4e00\u81f4\u7684\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6c11\u4e3b\u5316\u5de5\u5177\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u57fa\u7840\u68c0\u6d4b\u5c42\u3002"}}
{"id": "2601.13371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13371", "abs": "https://arxiv.org/abs/2601.13371", "authors": ["Junyi Zhang", "Yiming Wang", "Yunhong Lu", "Qichao Wang", "Wenzhe Qian", "Xiaoyin Xu", "David Gu", "Min Zhang"], "title": "Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations", "comment": "Association for the Advancement of Artificial Intelligence", "summary": "A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u5f62\u51e0\u4f55\u8868\u793a\u7684\u6587\u672c\u52303D\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u4fe1\u53f7\u951a\u5b9a\u5230\u5747\u5300\u7403\u9762\u5750\u6807\u6765\u7b80\u5316\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\uff0c\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u51e0\u4f55\u548c\u7eb9\u7406\u751f\u6210\u3002", "motivation": "\u6587\u672c\u52303D\u4eba\u8138\u751f\u6210\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u74063D\u7a7a\u95f4\u4e2d\u9876\u70b9\u5206\u5e03\u7684\u4efb\u610f\u6027\u548c\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u8fde\u63a5\u6027\u4e0d\u4f73\u548c\u51e0\u4f55\u8d28\u91cf\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7b80\u5316\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7403\u5f62\u51e0\u4f55\u8868\u793a\uff0c\u5c06\u51e0\u4f55\u4fe1\u53f7\u951a\u5b9a\u5230\u5747\u5300\u7403\u9762\u5750\u6807\uff0c\u786e\u4fdd\u89c4\u5219\u7684\u70b9\u5206\u5e03\uff0c\u4ece\u800c\u7a33\u5065\u5730\u91cd\u5efa\u7f51\u683c\u8fde\u63a5\u6027\u3002\u8be5\u89c4\u8303\u7403\u9762\u53ef\u65e0\u7f1d\u5c55\u5f00\u4e3a2D\u6620\u5c04\uff0c\u4e0e\u5f3a\u5927\u76842D\u751f\u6210\u6a21\u578b\u5f62\u6210\u5b8c\u7f8e\u534f\u540c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u7403\u5f62\u51e0\u4f55\u6269\u6563\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e2D\u6620\u5c04\u7684\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u51e0\u4f55\u548c\u7eb9\u7406\u5b9e\u73b0\u591a\u6837\u5316\u53ef\u63a7\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u52303D\u751f\u6210\u3001\u4eba\u8138\u91cd\u5efa\u548c\u57fa\u4e8e\u6587\u672c\u76843D\u7f16\u8f91\u7b49\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u8d28\u91cf\u3001\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u51e0\u4f55\u7ea6\u675f\u5230\u7b80\u5355\u7684\u62d3\u6251\u7403\u9762\u6d41\u5f62\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672c\u52303D\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u3002\u7403\u5f62\u51e0\u4f55\u8868\u793a\u7b80\u5316\u4e86\u5e95\u5c42\u7ed3\u6784\uff0c\u800c\u7403\u5f62\u51e0\u4f55\u6269\u6563\u6846\u67b6\u5219\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u548c\u53ef\u63a7\u7684\u751f\u6210\uff0c\u4e3a3D\u4eba\u8138\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13373", "abs": "https://arxiv.org/abs/2601.13373", "authors": ["Zhenan Liu", "Amir Khajepour", "George Shaker"], "title": "A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions", "comment": null, "summary": "Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b8c\u5168\u6a21\u578b\u9a71\u52a8\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u5b9e\u65f6\u5d4c\u5165\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u7684\u4eba\u4f53\u68c0\u6d4b\uff0c\u5728\u7c89\u5c18/\u70df\u96fe\u7b49\u80fd\u89c1\u5ea6\u9000\u5316\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u7c89\u5c18\u3001\u70df\u96fe\u3001\u53d7\u9650\u51e0\u4f55\u7ed3\u6784\u548c\u91d1\u5c5e\u7ed3\u6784\u4e25\u91cd\u5236\u7ea6\u5149\u5b66\u548cLiDAR\u611f\u77e5\uff0c\u800c4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5bf9\u6b64\u7c7b\u6761\u4ef6\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u5176\u7a00\u758f\u5404\u5411\u5f02\u6027\u70b9\u4e91\u7684\u5904\u7406\u65b9\u6cd5\u7406\u89e3\u6709\u9650\uff0c\u9700\u8981\u53ef\u9760\u7684\u4eba\u4f53\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u6a21\u578b\u9a71\u52a8\u76844D\u96f7\u8fbe\u611f\u77e5\u6846\u67b6\uff0c\u96c6\u6210\u9886\u57df\u611f\u77e5\u591a\u9608\u503c\u6ee4\u6ce2\u3001\u81ea\u8fd0\u52a8\u8865\u507f\u65f6\u95f4\u7d2f\u79ef\u3001KD\u6811\u6b27\u51e0\u91cc\u5f97\u805a\u7c7b\u4e0e\u591a\u666e\u52d2\u611f\u77e5\u7ec6\u5316\u3001\u57fa\u4e8e\u89c4\u5219\u76843D\u5206\u7c7b\u5668\uff0c\u4ec5\u4f7f\u7528\u96f7\u8fbe\u4f5c\u4e3a\u611f\u77e5\u6a21\u6001\u3002", "result": "\u5728\u7c89\u5c18\u586b\u5145\u7684\u5c01\u95ed\u62d6\u8f66\u548c\u771f\u5b9e\u5730\u4e0b\u91c7\u77ff\u96a7\u9053\u4e2d\u8bc4\u4f30\uff0c\u5728\u76f8\u673a\u548cLiDAR\u56e0\u4e25\u91cd\u80fd\u89c1\u5ea6\u9000\u5316\u800c\u5931\u6548\u7684\u573a\u666f\u4e0b\uff0c\u96f7\u8fbe\u68c0\u6d4b\u5668\u4fdd\u6301\u7a33\u5b9a\u7684\u884c\u4eba\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e3a\u6076\u52a3\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14042", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14042", "abs": "https://arxiv.org/abs/2601.14042", "authors": ["Jiaze Li", "Haoran Xu", "Wanyi Wu", "Changwei Wang", "Shuaiguang Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Youyang Qu", "Longxiang Gao", "Xudong Yang", "Lumin Xing"], "title": "Federated Balanced Learning", "comment": null, "summary": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.", "AI": {"tldr": "\u63d0\u51faFederated Balanced Learning (FBL)\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u6837\u672c\u5e73\u8861\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u5229\u7528\u8fb9\u7f18\u751f\u6210\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u586b\u5145\u548c\u91c7\u6837\uff0c\u5e76\u8bbe\u8ba1\u4e86\u77e5\u8bc6\u5bf9\u9f50\u548c\u4e22\u5f03\u7b56\u7565\u3002", "motivation": "\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u5168\u5c40\u6a21\u578b\u4f1a\u51fa\u73b0\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u6216\u68af\u5ea6\u4fee\u6b63\u5df2\u504f\u79bb\u7684\u5168\u5c40\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u6837\u672c\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faFederated Balanced Learning (FBL)\u6846\u67b6\uff1a1) \u5728\u5ba2\u6237\u7aef\u4fa7\u901a\u8fc7\u8fb9\u7f18\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u77e5\u8bc6\u586b\u5145\u548c\u77e5\u8bc6\u91c7\u6837\uff0c\u5728\u56fa\u5b9a\u6570\u636e\u6837\u672c\u9650\u5236\u4e0b\u5b9e\u73b0\u6837\u672c\u5e73\u8861\uff1b2) \u8bbe\u8ba1\u77e5\u8bc6\u5bf9\u9f50\u7b56\u7565\u5f25\u5408\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u5dee\u8ddd\uff1b3) \u8bbe\u8ba1\u77e5\u8bc6\u4e22\u5f03\u7b56\u7565\u8fdb\u884c\u6b63\u5219\u5316\uff1b4) \u6269\u5c55\u5230\u771f\u5b9e\u590d\u6742\u573a\u666f\uff0c\u5141\u8bb8\u4e0d\u540c\u5ba2\u6237\u7aef\u91c7\u7528\u4e0d\u540c\u65b9\u6cd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u5ba2\u6237\u7aef\u4fa7\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4ece\u6e90\u5934\u9884\u9632\u5ba2\u6237\u7aef\u6f02\u79fb\u7684FBL\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u6837\u672c\u5e73\u8861\u3001\u77e5\u8bc6\u5bf9\u9f50\u548c\u4e22\u5f03\u7b56\u7565\uff0c\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2601.13380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13380", "abs": "https://arxiv.org/abs/2601.13380", "authors": ["Chaoxin Wang", "Bharaneeshwar Balasubramaniyam", "Anurag Sangem", "Nicolais Guevara", "Doina Caragea"], "title": "Practical Insights into Semi-Supervised Object Detection Approaches", "comment": null, "summary": "Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.", "AI": {"tldr": "\u5bf9\u4e09\u79cd\u5148\u8fdb\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08MixPL\u3001Semi-DETR\u3001Consistent-Teacher\uff09\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u7efc\u5408\u6027\u80fd\u6bd4\u8f83\u7814\u7a76", "motivation": "\u7814\u7a76\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u7279\u522b\u662f\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\uff08SSOD\uff09\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u56fe\u50cf\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7406\u89e3\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u6807\u6ce8\u6570\u636e\u91cf\u4e0b\u7684\u6027\u80fd\u53d8\u5316", "method": "\u4f7f\u7528MS-COCO\u548cPascal VOC\u4e24\u4e2a\u6807\u51c6\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u81ea\u5b9a\u4e49\u7684Beetle\u6570\u636e\u96c6\uff0c\u5bf9\u4e09\u79cd\u5148\u8fdbSSOD\u65b9\u6cd5\uff08MixPL\u3001Semi-DETR\u3001Consistent-Teacher\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u5206\u6790\u6807\u6ce8\u56fe\u50cf\u6570\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u89c1\u89e3", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6bd4\u8f83\u4e09\u79cd\u5148\u8fdbSSOD\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6807\u6ce8\u6570\u636e\u91cf\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u6307\u5bfc"}}
{"id": "2601.13385", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13385", "abs": "https://arxiv.org/abs/2601.13385", "authors": ["Lavsen Dahal", "Yubraj Bhandari", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Organ-Aware Attention Improves CT Triage and Classification", "comment": null, "summary": "There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.", "code_url": "https://github.com/lavsendahal/oracle-c", "AI": {"tldr": "ORACLE-CT\uff1a\u4e00\u79cd\u7528\u4e8eCT\u5f71\u50cf\u5206\u7c7b\u7684\u5668\u5b98\u611f\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u5668\u5b98\u63a9\u7801\u6ce8\u610f\u529b\u548c\u5668\u5b98\u6807\u91cf\u878d\u5408\u6280\u672f\uff0c\u5728\u80f8\u90e8\u548c\u8179\u90e8CT\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u5f71\u50cf\uff08\u7279\u522b\u662fCT\uff09\u6570\u91cf\u6fc0\u589e\uff0c\u6025\u9700\u6709\u6548\u7684\u5206\u7c7b\u548c\u5206\u8bca\u7cfb\u7edf\u4ee5\u6539\u5584\u60a3\u8005\u62a4\u7406\u5e76\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u8d1f\u62c5\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u74063D\u89e3\u5256\u7ed3\u6784\u3001\u534f\u8bae\u53d8\u5316\u548c\u566a\u58f0\u62a5\u544a\u76d1\u7763\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faORACLE-CT\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5668\u5b98\u63a9\u7801\u6ce8\u610f\u529b\uff08Organ-Masked Attention\uff09- \u57fa\u4e8e\u5668\u5b98\u63a9\u7801\u7684\u53d7\u9650\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u4f9b\u7a7a\u95f4\u8bc1\u636e\uff1b2) \u5668\u5b98\u6807\u91cf\u878d\u5408\uff08Organ-Scalar Fusion\uff09- \u8f7b\u91cf\u7ea7\u878d\u5408\u5f52\u4e00\u5316\u4f53\u79ef\u548c\u5e73\u5747HU\u503c\u7ebf\u7d22\u3002\u6a21\u578b\u5728CT-RATE\u548cRADCHEST-CT\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u80f8\u90e8CT\u8bbe\u7f6e\u4e2d\uff0cORACLE-CT\u5728CT-RATE\u4e0a\u8fbe\u5230AUROC 0.86\uff1b\u5728\u8179\u90e8CT\u8bbe\u7f6e\u4e2d\uff08MERLIN\u6570\u636e\u96c6\uff0c30\u4e2a\u53d1\u73b0\uff09\uff0c\u76d1\u7763\u57fa\u7ebf\u8d85\u8fc7\u590d\u73b0\u7684\u96f6\u6837\u672cVLM\u57fa\u7ebf\uff0c\u6dfb\u52a0\u63a9\u7801\u6ce8\u610f\u529b\u548c\u6807\u91cf\u878d\u5408\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u81f3AUROC 0.85\u3002", "conclusion": "ORACLE-CT\u5728\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0c\u5728\u80f8\u90e8\u548c\u8179\u90e8CT\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76d1\u7763\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u8bca\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13386", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13386", "abs": "https://arxiv.org/abs/2601.13386", "authors": ["Changxu Zhang", "Zhaoze Wang", "Tai Fei", "Christopher Grimm", "Yi Jin", "Claas Tebruegge", "Ernst Warsitz", "Markus Gardill"], "title": "Leveraging Transformer Decoder for Automotive Radar Object Detection", "comment": null, "summary": "In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u76843D\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff0c\u4f7f\u7528\u65b0\u578bTransformer\u89e3\u7801\u5668\u4f5c\u4e3a\u9884\u6d4b\u5934\uff0c\u76f4\u63a5\u4ece\u96f7\u8fbe\u7279\u5f81\u56de\u5f523D\u8fb9\u754c\u6846\u548c\u7c7b\u522b\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548c\u542f\u53d1\u5f0f\u540e\u5904\u7406\uff08\u5982NMS\u8c03\u4f18\uff09\uff0c\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u68c0\u6d4b\u6846\u67b6\u6765\u5efa\u6a21\u96f7\u8fbe\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u7a7a\u5173\u8054\u3002", "method": "1) \u4f7f\u7528Transformer\u89e3\u7801\u5668\u4f5c\u4e3a\u9884\u6d4b\u5934\uff0c\u5c06\u68c0\u6d4b\u5efa\u6a21\u4e3a\u96c6\u5408\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5bf9\u8c61\u67e5\u8be2\u548c\u4f4d\u7f6e\u7f16\u7801\u76f4\u63a5\u56de\u5f523D\u8fb9\u754c\u6846\u548c\u7c7b\u522b\u5206\u6570\uff1b2) \u63d0\u51fa\u91d1\u5b57\u5854\u4ee4\u724c\u878d\u5408(PTF)\u6a21\u5757\uff0c\u5c06\u591a\u5c3a\u5ea6\u96f7\u8fbe\u7279\u5f81\u91d1\u5b57\u5854\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u5c3a\u5ea6\u611f\u77e5\u4ee4\u724c\u5e8f\u5217\uff1b3) \u6d88\u9664\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548cNMS\u540e\u5904\u7406\u3002", "result": "\u5728RADDet\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u7eaf\u96f7\u8fbe\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684Transformer\u67b6\u6784\u901a\u8fc7\u7aef\u5230\u7aef\u96c6\u5408\u9884\u6d4b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u7b80\u5316\u4e86\u68c0\u6d4b\u6d41\u7a0b\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2601.13400", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13400", "abs": "https://arxiv.org/abs/2601.13400", "authors": ["Nhat Thanh Tran", "Kevin Bui", "Jack Xin"], "title": "Deep Image Prior with L0 Gradient Regularizer for Image Smoothing", "comment": "To be published in the Proceedings of IEEE ICASSP 2026", "summary": "Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\\ell_0$, a deep image prior framework that incorporates the $\\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\\ell_0$ ``norm\", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.", "AI": {"tldr": "DIP-\u2113\u2080\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u56fe\u50cf\u5e73\u6ed1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u4e0e\u2113\u2080\u68af\u5ea6\u6b63\u5219\u5316\uff0c\u5728\u8fb9\u7f18\u4fdd\u6301\u548cJPEG\u4f2a\u5f71\u53bb\u9664\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5e73\u6ed1\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u7a97\u53e3\u7edf\u8ba1\u6216\u4f18\u5316\u95ee\u9898\u6c42\u89e3\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u65b9\u6cd5\u9700\u8981\u7cbe\u5fc3\u6784\u5efa\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u7531\u4e8e\u4e3a\u56fe\u50cf\u5e73\u6ed1\u6784\u5efa\u5408\u9002\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u6846\u67b6\u3002", "method": "\u63d0\u51faDIP-\u2113\u2080\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u4e0e\u2113\u2080\u68af\u5ea6\u6b63\u5219\u5316\u76f8\u7ed3\u5408\u3002\u4e3a\u6700\u5c0f\u5316\u5305\u542b\u975e\u51f8\u3001\u975e\u5149\u6ed1\u2113\u2080\"\u8303\u6570\"\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f00\u53d1\u4e86\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u73b0\u6210\u7684\u2113\u2080\u68af\u5ea6\u6700\u5c0f\u5316\u6c42\u89e3\u5668\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cDIP-\u2113\u2080\u5728\u8fb9\u7f18\u4fdd\u6301\u56fe\u50cf\u5e73\u6ed1\u548cJPEG\u4f2a\u5f71\u53bb\u9664\u65b9\u9762\u4f18\u4e8e\u8bb8\u591a\u73b0\u6709\u56fe\u50cf\u5e73\u6ed1\u7b97\u6cd5\u3002", "conclusion": "DIP-\u2113\u2080\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5e73\u6ed1\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u548c\u2113\u2080\u68af\u5ea6\u6b63\u5219\u5316\uff0c\u5728\u4fdd\u6301\u91cd\u8981\u7ed3\u6784\u7279\u5f81\u7684\u540c\u65f6\u6709\u6548\u53bb\u9664\u7ec6\u8282\u548c\u7eb9\u7406\u3002"}}
{"id": "2601.14208", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14208", "abs": "https://arxiv.org/abs/2601.14208", "authors": ["Nitin Kulkarni", "Akhil Devarashetti", "Charlie Cluss", "Livio Forte", "Dan Buckmaster", "Philip Schneider", "Chunming Qiao", "Alina Vereshchaka"], "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting", "comment": "8 pages, 9 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u4f7f\u7528\u4e09\u76f8\u673a\u7cfb\u7edf\u6355\u83b7\u8f66\u8f86\u5e95\u76d8\u89c6\u9891\u5e76\u751f\u6210\u4ea4\u4e92\u5f0f3D\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684SfM\u65b9\u6cd5\u89e3\u51b3\u5e7f\u89d2\u955c\u5934\u7578\u53d8\u548c\u4f4e\u89c6\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u7684\u903c\u771f\u5e95\u76d8\u68c0\u6d4b", "motivation": "\u4f20\u7edf\u8f66\u8f86\u5e95\u76d8\u68c0\u67e5\u9700\u8981\u68c0\u67e5\u5458\u8e72\u4e0b\u6216\u722c\u884c\u5230\u8f66\u4e0b\uff0c\u5de5\u4f5c\u5f3a\u5ea6\u5927\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff1b\u5728\u7ebf\u4e70\u5bb6\u5f88\u5c11\u80fd\u770b\u5230\u5e95\u76d8\u7167\u7247\uff0c\u5f71\u54cd\u8d2d\u4e70\u4fe1\u5fc3", "method": "\u4f7f\u7528\u4e09\u76f8\u673a\u7cfb\u7edf\u540c\u6b65\u6355\u83b7\u5e95\u76d8\u89c6\u9891\uff0c\u63d0\u51fa\u9488\u5bf9\u76f8\u673a\u7cfb\u7edf\u7684SfM\u7ba1\u9053\uff0c\u96c6\u6210\u7cbe\u786e\u76f8\u673a\u6807\u5b9a\u3001\u540c\u6b65\u89c6\u9891\u6d41\u548c\u51e0\u4f55\u5148\u9a8c\uff0c\u91c7\u7528\u7ea6\u675f\u5339\u914d\u7b56\u7565\u7ed3\u5408DISK\u7279\u5f81\u63d0\u53d6\u5668\u548cLightGlue\u5339\u914d\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u7a00\u758f\u70b9\u4e91\uff0c\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u751f\u6210\u5b9e\u65f6\u6e32\u67d3\u7684\u903c\u771f3D\u6a21\u578b", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u4ea4\u4e92\u5f0f3D\u5e95\u76d8\u6a21\u578b\uff0c\u652f\u6301\u65cb\u8f6c\u3001\u7f29\u653e\u548c\u5207\u7247\u64cd\u4f5c\uff0c\u53ef\u5728\u6570\u79d2\u5185\u68c0\u6d4b\u9508\u8680\u3001\u6cc4\u6f0f\u6216\u649e\u51fb\u635f\u4f24\uff0c\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\u8bbe\u8ba1\u9009\u62e9\u5bf9\u5b9e\u73b0\u6700\u5148\u8fdb\u8d28\u91cf\u81f3\u5173\u91cd\u8981", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u7ba1\u9053\u901a\u8fc7\u521b\u65b0\u7684SfM\u65b9\u6cd5\u89e3\u51b3\u4e86\u5e7f\u89d2\u955c\u5934\u7578\u53d8\u548c\u4f4e\u89c6\u5dee\u573a\u666f\u7684\u6311\u6218\uff0c\u751f\u6210\u7684\u4ea4\u4e92\u5f0f3D\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4f5c\u573a\u6240\u5b89\u5168\u548c\u4e70\u5bb6\u4fe1\u5fc3\uff0c\u4e3a\u8f66\u8f86\u5e95\u76d8\u68c0\u67e5\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13401", "abs": "https://arxiv.org/abs/2601.13401", "authors": ["Peter A. Massih", "Eric Cosatto"], "title": "Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics", "comment": "Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565", "summary": "Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u5931\u8d25\uff0c\u63d0\u51fa\u4e86SQuID\u57fa\u51c6\u6570\u636e\u96c6\u548cQVLM\u67b6\u6784\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u65b9\u5f0f\u4fdd\u6301\u50cf\u7d20\u7ea7\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u56fe\u50cf\u5b9a\u91cf\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5176\u67b6\u6784\u7834\u574f\u4e86\u50cf\u7d20\u7ea7\u4fe1\u606f\u3002\u89c6\u89c9\u7f16\u7801\u5668\u901a\u8fc7\u8865\u4e01\u5d4c\u5165\u538b\u7f29\u56fe\u50cf\uff0c\u51cf\u5c11\u4e86\u7a7a\u95f4\u7d22\u5f15\u80fd\u529b\uff0c\u4e22\u5931\u4e86\u7cbe\u786e\u8ba1\u6570\u548c\u6d4b\u91cf\u6240\u9700\u7684\u50cf\u7d20\u7ea7\u8ddf\u8e2a\u80fd\u529b\u3002\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u91cf\u5316\u7684\u5e94\u7528\uff08\u5982\u536b\u661f\u56fe\u50cf\u5206\u6790\uff09\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a1) SQuID\u6570\u636e\u96c6\uff1a\u5305\u542b2000\u4e2a\u536b\u661f\u56fe\u50cf\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u6570\u503c\u8303\u56f4\u548c\u5206\u7c7b\u7b54\u6848\uff0c\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u5c42\u7ea7\uff0c\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u548c\u5b66\u4e60\u7684\u53d8\u5f02\u6027\u81ea\u52a8\u751f\u6210\u6ce8\u91ca\uff1b2) QVLM\u67b6\u6784\uff1a\u91c7\u7528\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u8bed\u8a00\u7406\u89e3\u4e0e\u89c6\u89c9\u5206\u6790\u89e3\u8026\u3002QVLM\u4e0d\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u5d4c\u5165\uff0c\u800c\u662f\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u9996\u5148\u8c03\u7528\u5206\u5272\u6a21\u578b\u83b7\u53d6\u50cf\u7d20\u7ea7\u63a9\u7801\uff0c\u7136\u540e\u76f4\u63a5\u5728\u8fd9\u4e9b\u63a9\u7801\u4e0a\u64cd\u4f5c\uff0c\u5728\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a7a\u95f4\u7d22\u5f15\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528GPT-5\u4f5c\u4e3a\u7f16\u7801\u5668\u7684QVLM\u5728SQuID\u6570\u636e\u96c6\u4e0a\u8fbe\u523042.0%\u7684\u51c6\u786e\u7387\uff0c\u800c\u4f20\u7edfVLM\uff08\u4ec5\u4f7f\u7528\u56fe\u50cf-\u95ee\u9898\u5bf9\u63d0\u793a\uff09\u7684\u51c6\u786e\u7387\u4e3a28.1%\u3002\u8fd9\u8868\u660e\u5728\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u67b6\u6784\u89e3\u8026\u80fd\u591f\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u67b6\u6784\u89e3\u8026\uff08\u5c06\u8bed\u8a00\u7406\u89e3\u4e0e\u89c6\u89c9\u5206\u6790\u5206\u79bb\uff09\u5e76\u4fdd\u6301\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u6bd4\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002QVLM\u7684\u6210\u529f\u8868\u660e\uff0c\u5728\u5904\u7406\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u91cf\u5316\u7684\u4efb\u52a1\u65f6\uff0c\u907f\u514d\u50cf\u7d20\u7ea7\u4fe1\u606f\u4e22\u5931\u662f\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.13412", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13412", "abs": "https://arxiv.org/abs/2601.13412", "authors": ["Puneet Sharma", "Kristian Dalsb\u00f8 Hindberg", "Benedicte Schelde-Olesen", "Ulrik Deding", "Esmaeil S. Nadimi", "Jan-Matthias Braun"], "title": "Using deep learning for predicting cleansing quality of colon capsule endoscopy images", "comment": "24 pages", "summary": "In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u7ed3\u80a0\u80f6\u56ca\u5185\u955c\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\uff0c\u4f7f\u7528ResNet-18\u6a21\u578b\u548c\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\uff0c\u5728\u4fdd\u630188%\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b079%\u7a00\u758f\u5ea6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u80a0\u80f6\u56ca\u5185\u955c\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\u8bc4\u4f30\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u9ad8\u6548\u51c6\u786e\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u5e94\u7528\u9700\u6c42\u3002", "method": "\u4f7f\u7528500\u5f20\u753114\u540d\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684CCE\u56fe\u50cf\uff08Leighton-Rex\u91cf\u8868\uff1a\u5dee\u3001\u4e00\u822c\u3001\u597d\u3001\u4f18\u79c0\uff09\uff0c\u91c7\u7528ResNet-18\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002\u5e94\u7528\u5206\u5c42K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u786e\u4fdd\u7a33\u5065\u6027\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\u4f18\u5316\u6a21\u578b\u3002\u4f7f\u7528Grad-CAM\u3001Grad-CAM++\u3001Eigen-CAM\u3001Ablation-CAM\u548cRandom-CAM\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5e76\u7528ROAD\u65b9\u6cd5\u8fdb\u884c\u4e00\u81f4\u6027\u8bc4\u4f30\u3002\u6700\u540e\u91c7\u7528\u81ea\u9002\u5e94\u6e29\u5ea6\u7f29\u653e\u53d8\u4f53\u5bf9\u526a\u679d\u6a21\u578b\u8fdb\u884c\u5916\u90e8\u6570\u636e\u96c6\u6821\u51c6\u3002", "result": "\u526a\u679d\u6a21\u578b\u5728\u4fdd\u630188%\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e8679%\u7684\u7a00\u758f\u5ea6\uff0c\u76f8\u6bd4\u672a\u526a\u679d\u6a21\u578b\u768484%\u51c6\u786e\u7387\u6709\u6240\u63d0\u5347\u3002\u7814\u7a76\u5c55\u793a\u4e86\u526a\u679d\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4e0d\u635f\u5bb3\u6027\u80fd\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86CCE\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\u8bc4\u4f30\u7684\u6311\u6218\u4ee5\u53caROAD\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728CCE\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\u9884\u6d4b\u4e2d\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5bf9\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u4f18\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002\u81ea\u9002\u5e94\u6e29\u5ea6\u7f29\u653e\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6821\u51c6\u5e94\u7528\u3002"}}
{"id": "2601.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13416", "abs": "https://arxiv.org/abs/2601.13416", "authors": ["A. Nieto Juscafresa", "\u00c1. Mazcu\u00f1\u00e1n Herreros", "J. Sullivan"], "title": "Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study", "comment": "21 pages, 6 figures, CVPR format", "summary": "Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\u7684\u6f5c\u529b\u7814\u7a76\uff1a\u5728\u6d6e\u6e38\u751f\u7269\u76d1\u6d4b\u4efb\u52a1\u4e2d\uff0c\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u7279\u5f81\u4e0e\u76d1\u7763\u57fa\u7ebf\u7ade\u4e89\uff0c\u5e76\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u56fe\u50cf\u5408\u6210\u7684\u6700\u5148\u8fdb\u751f\u6210\u65b9\u6cd5\uff0c\u4f46\u5176\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u6761\u4ef6\u4e0b\u8bad\u7ec3\u7528\u4e8e\u53bb\u566a\u548c\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3a\u80fd\u591f\u6355\u83b7\u4f4e\u5c42\u548c\u9ad8\u5c42\u7ed3\u6784\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5668\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u4e3b\u5e72\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u63a2\u6d4b\u4e0d\u540c\u5c42\u548c\u53bb\u566a\u65f6\u95f4\u6b65\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u4e3a\u6bcf\u4e2a\u5c42-\u65f6\u95f4\u6b65\u5bf9\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6d6e\u6e38\u751f\u7269\u76d1\u6d4b\u573a\u666f\u4e2d\uff0c\u4f7f\u7528\u53d7\u63a7\u4e14\u53ef\u6bd4\u8f83\u7684\u8bad\u7ec3\u8bbe\u7f6e\uff0c\u4e0e\u5df2\u5efa\u7acb\u7684\u76d1\u7763\u548c\u81ea\u76d1\u7763\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u51bb\u7ed3\u6269\u6563\u7279\u5f81\u4e0e\u76d1\u7763\u57fa\u7ebf\u7ade\u4e89\uff0c\u5e76\u5728\u5e73\u8861\u548c\u81ea\u7136\u957f\u5c3e\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5176\u4ed6\u81ea\u76d1\u7763\u65b9\u6cd5\u3002\u5728\u65f6\u95f4\u548c\u5730\u7406\u5206\u5e03\u504f\u79fb\u7684\u6d6e\u6e38\u751f\u7269\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5206\u5e03\u5916\u8bc4\u4f30\u663e\u793a\uff0c\u51bb\u7ed3\u6269\u6563\u7279\u5f81\u5728\u663e\u8457\u5206\u5e03\u504f\u79fb\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u51c6\u786e\u6027\u548cMacro F1\u5206\u6570\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\uff0c\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13417", "abs": "https://arxiv.org/abs/2601.13417", "authors": ["Yujian Xiong", "Xuanzhao Dong", "Wenhui Zhu", "Xin Li", "Oana Dumitrascu", "Yalin Wang"], "title": "SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement", "comment": null, "summary": "Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.", "AI": {"tldr": "SGW-GAN\uff1a\u9996\u4e2a\u5c06\u5207\u7247Gromov Wasserstein\uff08SGW\uff09\u878d\u5165\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6295\u5f71\u8fd1\u4f3cGW\u8ddd\u79bb\uff0c\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u7684\u7c7b\u5185\u51e0\u4f55\u7ed3\u6784\uff0c\u6539\u5584\u4e0b\u6e38\u75be\u75c5\u5206\u7ea7\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u4f1a\u626d\u66f2\u7c7b\u5185\u51e0\u4f55\u7ed3\u6784\uff1a\u4e34\u5e8a\u76f8\u5173\u6837\u672c\u53d8\u5f97\u5206\u6563\uff0c\u75be\u75c5\u7c7b\u522b\u8fb9\u754c\u6a21\u7cca\uff0c\u4ece\u800c\u635f\u5bb3\u4e0b\u6e38\u5206\u7ea7\u6216\u75c5\u53d8\u68c0\u6d4b\u4efb\u52a1\u3002Gromov Wasserstein\uff08GW\uff09\u8ddd\u79bb\u901a\u8fc7\u5185\u90e8\u6210\u5bf9\u8ddd\u79bb\u5bf9\u9f50\u5206\u5e03\uff0c\u80fd\u81ea\u7136\u4fdd\u6301\u7c7b\u5185\u7ed3\u6784\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faSGW-GAN\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5207\u7247Gromov Wasserstein\uff08SGW\uff09\u878d\u5165\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\u3002SGW\u901a\u8fc7\u968f\u673a\u6295\u5f71\u8fd1\u4f3cGW\u8ddd\u79bb\uff0c\u4fdd\u7559\u5173\u7cfb\u4fdd\u771f\u5ea6\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u8be5\u65b9\u6cd5\u7528\u4e8e\u65e0\u914d\u5bf9\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\uff0c\u5728\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aSGW-GAN\u80fd\u751f\u6210\u89c6\u89c9\u4e0a\u4ee4\u4eba\u4fe1\u670d\u7684\u589e\u5f3a\u56fe\u50cf\uff1b\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff1b\u5728\u75be\u75c5\u6807\u7b7e\u4e0a\u62a5\u544a\u4e86\u6700\u4f4e\u7684GW\u5dee\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u65e0\u914d\u5bf9\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u6548\u7387\u548c\u4e34\u5e8a\u4fdd\u771f\u5ea6\u3002", "conclusion": "SGW-GAN\u901a\u8fc7\u5c06\u5207\u7247Gromov Wasserstein\u8ddd\u79bb\u878d\u5165\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u626d\u66f2\u7c7b\u5185\u51e0\u4f55\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u7ed3\u6784\u4fdd\u771f\uff0c\u4e3a\u65e0\u914d\u5bf9\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4e34\u5e8a\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13440", "abs": "https://arxiv.org/abs/2601.13440", "authors": ["Mohit Kakda", "Mirudula Shri Muthukumaran", "Uttapreksha Patel", "Lawrence Swaminathan Xavier Prince"], "title": "Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation", "comment": "10 pages,4 images", "summary": "Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u91cd\u70b9\u7814\u7a76\u4e86CLIP\u7b49\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5f02\u5e38\u5206\u7c7b\u4e0e\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u67b6\u6784\u8303\u5f0f\u3001\u7279\u5f81\u63d0\u53d6\u673a\u5236\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662fCLIP\uff09\u901a\u8fc7\u56fe\u50cf\u548c\u6587\u672c\u7684\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u7f3a\u9677\u6837\u672c\u7684\u65b0\u8303\u5f0f\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u548c\u7f3a\u9677\u793a\u4f8b\uff0c\u800cVLMs\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6b63\u5e38\u548c\u5f02\u5e38\u72b6\u6001\u6765\u5b9e\u73b0\u96f6\u6837\u672c/\u5c11\u6837\u672c\u68c0\u6d4b\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5bf9\u5176\u5728\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u5e94\u7528\u6f5c\u529b\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u4e09\u79cd\u5173\u952e\u67b6\u6784\u8303\u5f0f\uff1a1) \u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u7684\u5bc6\u96c6\u7279\u5f81\u63d0\u53d6\uff08WinCLIP\uff09\uff1b2) \u5177\u6709\u53ef\u5b66\u4e60\u6295\u5f71\u7684\u591a\u9636\u6bb5\u7279\u5f81\u5bf9\u9f50\uff08AprilLab\u6846\u67b6\uff09\uff1b3) \u7ec4\u5408\u63d0\u793a\u96c6\u6210\u7b56\u7565\u3002\u5206\u6790\u7ef4\u5ea6\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u673a\u5236\u3001\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u7b56\u7565\u3001\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3001\u96f6\u6837\u672c\u4e0e\u5c11\u6837\u672c\u6743\u8861\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u5728MVTec AD\u548cVisA\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e25\u683c\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3001\u5206\u5272\u7cbe\u5ea6\u548c\u63a8\u7406\u6548\u7387\u3002\u7814\u7a76\u63ed\u793a\u4e86VLMs\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6210\u529f\u7684\u539f\u56e0\u548c\u673a\u5236\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u9650\u5236\u3002\u7ed3\u679c\u8868\u660eVLMs\u5728\u96f6\u6837\u672c/\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u5bf9VLM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5e94\u7528\u7684\u57fa\u7840\u7406\u89e3\uff0c\u4e3a\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002VLMs\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u7684\u80fd\u529b\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u548c\u7f3a\u9677\u793a\u4f8b\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13498", "abs": "https://arxiv.org/abs/2601.13498", "authors": ["Nimrod Kruger", "Nicholas Owen Ralph", "Gregory Cohen", "Paul Hurley"], "title": "Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging", "comment": null, "summary": "Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6620\u5c04\u5230\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\uff0c\u5e76\u5d4c\u5165\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u4e8b\u4ef6\u6570\u636e\u7684\u9006\u6ee4\u6ce2\u548c\u7ef4\u7eb3\u89e3\u5377\u79ef", "motivation": "\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\uff08\u795e\u7ecf\u5f62\u6001\u76f8\u673a\uff09\u8f93\u51fa\u7a00\u758f\u3001\u5f02\u6b65\u7684ON/OFF\u4e8b\u4ef6\uff0c\u5177\u6709\u5fae\u79d2\u7ea7\u4f20\u611f\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u6570\u636e\u5e26\u5bbd\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u975e\u7ebf\u6027\u4e8b\u4ef6\u8868\u793a\u96be\u4ee5\u4e0e\u5927\u591a\u6570\u8ba1\u7b97\u6210\u50cf\u548c\u5149\u5b66\u7cfb\u7edf\u8bbe\u8ba1\u6240\u4f9d\u8d56\u7684\u7ebf\u6027\u524d\u5411\u6a21\u578b\u96c6\u6210", "method": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684\u5904\u7406\u6d41\u7a0b\uff1a1\uff09\u5c06\u4e8b\u4ef6\u6d41\u6620\u5c04\u5230\u6bcf\u50cf\u7d20\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\uff1b2\uff09\u5c06\u8fd9\u4e9b\u6d4b\u91cf\u5d4c\u5165\u5177\u6709\u65f6\u53d8\u70b9\u6269\u6563\u51fd\u6570\u7684\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff1b3\uff09\u4f7f\u7528\u5df2\u77e5\uff08\u6216\u53c2\u6570\u5316\uff09\u52a8\u6001\u4f20\u9012\u51fd\u6570\uff0c\u901a\u8fc7\u9891\u57df\u7ef4\u7eb3\u89e3\u5377\u79ef\u76f4\u63a5\u4ece\u4e8b\u4ef6\u6570\u636e\u8fdb\u884c\u9006\u6ee4\u6ce2", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u8c03\u5236\u79bb\u7126\u4e0b\u5355\u70b9\u548c\u91cd\u53e0\u70b9\u6e90\u7684\u5904\u7406\u6548\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\uff08\u53ef\u8c03\u7126\u671b\u8fdc\u955c\u89c2\u6d4b\u661f\u573a\uff09\u4e0a\u5c55\u793a\u4e86\u6e90\u5b9a\u4f4d\u548c\u53ef\u5206\u79bb\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u5149\u5b66\u7cfb\u7edf\u4e2d\u7684\u4e8b\u4ef6\u4f20\u611f\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u50cf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6865\u6881\uff0c\u4f7f\u4e8b\u4ef6\u6570\u636e\u80fd\u591f\u76f4\u63a5\u7528\u4e8e\u9006\u6ee4\u6ce2\u548c\u56fe\u50cf\u91cd\u5efa"}}
{"id": "2601.13502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13502", "abs": "https://arxiv.org/abs/2601.13502", "authors": ["Nhi Kieu", "Kien Nguyen", "Arnold Wiliem", "Clinton Fookes", "Sridha Sridharan"], "title": "DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities", "comment": "Accepted to WACV 2026 - Computer Vision for Earth Observation Workshop", "summary": "The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.", "AI": {"tldr": "DIS2\uff1a\u9488\u5bf9\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7DLKD\u534f\u540c\u89e3\u8026\u5b66\u4e60\u4e0e\u77e5\u8bc6\u84b8\u998f\uff0c\u7ed3\u5408\u7c7b\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u548c\u591a\u5206\u8fa8\u7387\u6df7\u5408\u878d\u5408\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5f15\u5bfc\u7684\u7f3a\u5931\u7279\u5f81\u8865\u507f\u3002", "motivation": "\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u9762\u4e34\u6a21\u6001\u7f3a\u5931\u7684\u4e25\u91cd\u6311\u6218\uff0c\u4e14\u9065\u611f\u6570\u636e\u5177\u6709\u9ad8\u5ea6\u5f02\u6784\u6027\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\u7684\u7279\u70b9\u3002\u4f20\u7edf\u8de8\u57df\u6709\u6548\u7684\u65b9\u6cd5\uff08\u5982\u89e3\u8026\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff09\u5728\u9065\u611f\u9886\u57df\u5931\u6548\uff1a\u89e3\u8026\u5b66\u4e60\u4f9d\u8d56\u6a21\u6001\u95f4\u663e\u8457\u7279\u5f81\u91cd\u53e0\uff0c\u800c\u9065\u611f\u6570\u636e\u5f02\u6784\u6027\u5f3a\uff1b\u77e5\u8bc6\u84b8\u998f\u6210\u4e3a\u4e0d\u9002\u5b9a\u7684\u6a21\u4eff\u4efb\u52a1\uff0c\u5b66\u751f\u6a21\u578b\u65e0\u6cd5\u805a\u7126\u5fc5\u8981\u7684\u8865\u507f\u77e5\u8bc6\uff0c\u8bed\u4e49\u9e3f\u6c9f\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faDIS2\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff1a1\uff09\u539f\u5219\u6027\u7f3a\u5931\u4fe1\u606f\u8865\u507f\uff1b2\uff09\u7c7b\u7279\u5b9a\u6a21\u6001\u8d21\u732e\uff1b3\uff09\u591a\u5206\u8fa8\u7387\u7279\u5f81\u91cd\u8981\u6027\u3002\u6838\u5fc3\u521b\u65b0\u662fDLKD\uff08\u89e3\u8026\u5b66\u4e60\u4e0e\u77e5\u8bc6\u84b8\u998f\u534f\u540c\uff09\uff0c\u660e\u786e\u6355\u83b7\u8865\u507f\u7279\u5f81\uff0c\u4e0e\u53ef\u7528\u6a21\u6001\u7279\u5f81\u878d\u5408\u4ee5\u8fd1\u4f3c\u5168\u6a21\u6001\u7684\u7406\u60f3\u878d\u5408\u8868\u793a\u3002CFLM\uff08\u7c7b\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u6a21\u5757\uff09\u81ea\u9002\u5e94\u5b66\u4e60\u6bcf\u4e2a\u76ee\u6807\u7684\u5224\u522b\u6027\u8bc1\u636e\u3002DLKD\u548cCFLM\u7531\u5206\u5c42\u6df7\u5408\u878d\u5408\u7ed3\u6784\u652f\u6301\uff0c\u5229\u7528\u8de8\u5206\u8fa8\u7387\u7279\u5f81\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DIS2\u4ee3\u8868\u4e86\u4ece\u6a21\u6001\u5171\u4eab\u7279\u5f81\u4f9d\u8d56\u548c\u65e0\u76ee\u6807\u6a21\u4eff\u5411\u4e3b\u52a8\u5f15\u5bfc\u7f3a\u5931\u7279\u5f81\u8865\u507f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u6311\u6218\uff0c\u901a\u8fc7DLKD\u534f\u540c\u3001\u7c7b\u7279\u5b9a\u5b66\u4e60\u548c\u591a\u5206\u8fa8\u7387\u878d\u5408\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.13524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13524", "abs": "https://arxiv.org/abs/2601.13524", "authors": ["Yang Yu", "Yunze Deng", "Yige Zhang", "Yanjie Xiao", "Youkun Ou", "Wenhao Hu", "Mingchao Li", "Bin Feng", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models", "comment": "5pages, 3 figures", "summary": "Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.", "code_url": "https://upyuyang.github.io/go-mlvton/", "AI": {"tldr": "GO-MLVTON\u662f\u9996\u4e2a\u591a\u5c42\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u670d\u88c5\u906e\u6321\u5b66\u4e60\u548c\u57fa\u4e8eStableDiffusion\u7684\u670d\u88c5\u53d8\u5f62\u62df\u5408\u6a21\u5757\uff0c\u89e3\u51b3\u591a\u5c42\u670d\u88c5\u8bd5\u7a7f\u4e2d\u7684\u906e\u6321\u5173\u7cfb\u548c\u53d8\u5f62\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86MLG\u6570\u636e\u96c6\u548cLACD\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u5c42\u6216\u591a\u4ef6\u670d\u88c5\u8bd5\u7a7f\uff0c\u5ffd\u7565\u4e86\u591a\u5c42\u865a\u62df\u8bd5\u7a7f\uff08ML-VTON\uff09\u8fd9\u4e00\u91cd\u8981\u4efb\u52a1\u3002\u591a\u5c42\u8bd5\u7a7f\u9700\u8981\u5c06\u591a\u5c42\u670d\u88c5\u7a7f\u5230\u4eba\u4f53\u4e0a\uff0c\u5e76\u5b9e\u73b0\u771f\u5b9e\u7684\u53d8\u5f62\u548c\u5c42\u6b21\u5173\u7cfb\uff0c\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u7ed3\u679c\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u51c6\u786e\u5efa\u6a21\u5185\u5916\u5c42\u670d\u88c5\u4e4b\u95f4\u7684\u906e\u6321\u5173\u7cfb\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u5185\u5c42\u670d\u88c5\u7279\u5f81\u7684\u5e72\u6270\u3002", "method": "\u63d0\u51faGO-MLVTON\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u670d\u88c5\u906e\u6321\u5b66\u4e60\u6a21\u5757\uff0c\u7528\u4e8e\u5b66\u4e60\u5185\u5916\u5c42\u670d\u88c5\u4e4b\u95f4\u7684\u906e\u6321\u5173\u7cfb\uff1b2\uff09\u57fa\u4e8eStableDiffusion\u7684\u670d\u88c5\u53d8\u5f62\u4e0e\u62df\u5408\u6a21\u5757\uff0c\u7528\u4e8e\u5c06\u670d\u88c5\u53d8\u5f62\u5e76\u8d34\u5408\u5230\u4eba\u4f53\u4e0a\u3002\u6b64\u5916\u8fd8\u6784\u5efa\u4e86MLG\u6570\u636e\u96c6\u7528\u4e8e\u8be5\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807LACD\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eGO-MLVTON\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u5c42\u8bd5\u7a7f\u7ed3\u679c\uff0c\u6709\u6548\u5904\u7406\u591a\u5c42\u670d\u88c5\u4e4b\u95f4\u7684\u906e\u6321\u5173\u7cfb\u548c\u53d8\u5f62\u95ee\u9898\u3002", "conclusion": "GO-MLVTON\u662f\u9996\u4e2a\u89e3\u51b3\u591a\u5c42\u865a\u62df\u8bd5\u7a7f\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u906e\u6321\u5b66\u4e60\u548c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u670d\u88c5\u53d8\u5f62\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u5c42\u670d\u88c5\u8bd5\u7a7f\u6548\u679c\uff0c\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2601.13551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13551", "abs": "https://arxiv.org/abs/2601.13551", "authors": ["Feng Ding", "Wenhui Yi", "Xinan He", "Mengyao Xiao", "Jianfeng Xu", "Jianqiang Du"], "title": "DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis", "comment": null, "summary": "Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.", "code_url": "https://github.com/ywh1093/DiffFace-Ed", "AI": {"tldr": "DiffFace-Edit\u662f\u4e00\u4e2a\u5305\u542b200\u591a\u4e07\u5f20AI\u751f\u6210\u4f2a\u9020\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u9762\u90e8\u7ec6\u7c92\u5ea6\u533a\u57df\u7f16\u8f91\uff0c\u5e76\u9996\u6b21\u7814\u7a76\u4e86\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u771f\u5b9e\u5f71\u54cd", "motivation": "\u73b0\u6709AI\u751f\u6210\u4eba\u8138\u6570\u636e\u96c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u533a\u57df\u7f16\u8f91\u6837\u672c\uff0c\u4e14\u65e0\u4eba\u7814\u7a76\u771f\u5b9e\u4e0e\u4f2a\u9020\u6837\u672c\u95f4\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\uff08\u68c0\u6d4b\u5668\u89c4\u907f\u6837\u672c\uff09", "method": "\u521b\u5efaDiffFace-Edit\u6570\u636e\u96c6\uff0c\u5305\u542b8\u4e2a\u9762\u90e8\u533a\u57df\uff08\u5982\u773c\u775b\u3001\u9f3b\u5b50\uff09\u7684\u7f16\u8f91\uff0c\u652f\u6301\u5355\u533a\u57df\u548c\u591a\u533a\u57df\u7f16\u8f91\u7ec4\u5408\uff0c\u5e76\u8fdb\u884c\u8de8\u57df\u8bc4\u4f30\u7ed3\u5408IMDL\u65b9\u6cd5", "result": "\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7200\u4e07\u5f20AI\u751f\u6210\u4f2a\u9020\u56fe\u50cf\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u7f16\u8f91\u7ec4\u5408\uff0c\u5e76\u9996\u6b21\u5206\u6790\u4e86\u68c0\u6d4b\u5668\u89c4\u907f\u6837\u672c\u5bf9\u68c0\u6d4b\u6a21\u578b\u7684\u5f71\u54cd", "conclusion": "DiffFace-Edit\u586b\u8865\u4e86\u7ec6\u7c92\u5ea6\u533a\u57df\u7f16\u8f91\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u53d1\u5e03"}}
{"id": "2601.13565", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13565", "abs": "https://arxiv.org/abs/2601.13565", "authors": ["Yu Qin", "Shimeng Fan", "Fan Yang", "Zixuan Xue", "Zijie Mai", "Wenrui Chen", "Kailun Yang", "Zhiyong Li"], "title": "Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation", "comment": "The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP", "summary": "Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.", "code_url": "https://github.com/zjjqinyu/FiCoP", "code_stars": 1, "code_last_update": "2026-01-20", "AI": {"tldr": "FiCoP\u63d0\u51fa\u4ece\u566a\u58f0\u6613\u6270\u7684\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684\u5757\u7ea7\u5bf9\u5e94\uff0c\u901a\u8fc7\u8865\u4e01-\u8865\u4e01\u76f8\u5173\u77e9\u9635\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u6765\u7f29\u5c0f\u5339\u914d\u8303\u56f4\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u65e0\u7ea6\u675f\u7684\u5168\u5c40\u5339\u914d\u7b56\u7565\uff0c\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u5c06\u951a\u70b9\u7279\u5f81\u4e0e\u6574\u4e2a\u67e5\u8be2\u56fe\u50cf\u7a7a\u95f4\u5339\u914d\u4f1a\u5f15\u5165\u8fc7\u5ea6\u6b67\u4e49\uff0c\u76ee\u6807\u7279\u5f81\u5bb9\u6613\u4e0e\u80cc\u666f\u5e72\u6270\u7269\u6df7\u6dc6\uff0c\u5bfc\u81f4\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u4e0b\u964d\u3002", "method": "1) \u7269\u4f53\u4e2d\u5fc3\u89e3\u8026\u9884\u5904\u7406\uff1a\u9694\u79bb\u8bed\u4e49\u76ee\u6807\u4e0e\u73af\u5883\u566a\u58f0\uff1b2) \u8de8\u89c6\u89d2\u5168\u5c40\u611f\u77e5\u6a21\u5757\uff1a\u878d\u5408\u53cc\u89c6\u56fe\u7279\u5f81\uff0c\u901a\u8fc7\u663e\u5f0f\u4e0a\u4e0b\u6587\u63a8\u7406\u5efa\u7acb\u7ed3\u6784\u5171\u8bc6\uff1b3) \u8865\u4e01\u76f8\u5173\u9884\u6d4b\u5668\uff1a\u751f\u6210\u7cbe\u786e\u7684\u5757\u7ea7\u5173\u8054\u56fe\uff0c\u4f5c\u4e3a\u7a7a\u95f4\u6ee4\u6ce2\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u6297\u566a\u58f0\u5339\u914d\u3002", "result": "\u5728REAL275\u548cToyota-Light\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFiCoP\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u5e73\u5747\u53ec\u56de\u73878.0%\u548c6.1%\uff0c\u5728\u590d\u6742\u65e0\u7ea6\u675f\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9c81\u68d2\u4e14\u6cdb\u5316\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "FiCoP\u901a\u8fc7\u4ece\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684\u5757\u7ea7\u5bf9\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c476D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u611f\u77e5\u57fa\u7840\u3002"}}
{"id": "2601.13606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13606", "abs": "https://arxiv.org/abs/2601.13606", "authors": ["Zheng Liu", "Honglin Lin", "Chonghan Qin", "Xiaoyang Wang", "Xin Gao", "Yu Li", "Mengzhang Cai", "Yun Zhu", "Zhanping Zhong", "Qizhi Pei", "Zhuoshi Pan", "Xiaoran Shang", "Bin Cui", "Conghui He", "Wentao Zhang", "Lijun Wu"], "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch", "comment": "29 pages", "summary": "Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.", "AI": {"tldr": "ChartVerse\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u56fe\u8868\u590d\u6742\u5ea6\u548c\u7b54\u6848\u4f18\u5148\u7684QA\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u4e0a\u53d7\u5230\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u4e25\u91cd\u5236\u7ea6\u3002\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u53cc\u91cd\u6311\u6218\uff1a\u5408\u6210\u56fe\u8868\u8fc7\u4e8e\u7b80\u5355\u91cd\u590d\uff0c\u800c\u76f8\u5173QA\u5bf9\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4e14\u7f3a\u4e4f\u590d\u6742\u4efb\u52a1\u6240\u9700\u7684\u63a8\u7406\u6df1\u5ea6\u3002", "method": "\u63d0\u51faChartVerse\u6846\u67b6\uff1a1) \u5f15\u5165Rollout Posterior Entropy(RPE)\u91cf\u5316\u56fe\u8868\u590d\u6742\u5ea6\uff0c\u57fa\u4e8eRPE\u5f00\u53d1\u590d\u6742\u5ea6\u611f\u77e5\u7684\u56fe\u8868\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u53ef\u6267\u884c\u7a0b\u5e8f\u81ea\u4e3b\u5408\u6210\u591a\u6837\u5316\u7684\u9ad8\u590d\u6742\u5ea6\u56fe\u8868\uff1b2) \u91c7\u7528\u771f\u5b9e\u951a\u5b9a\u7684\u9006\u5411QA\u5408\u6210\u65b9\u6cd5\uff0c\u91c7\u7528\u7b54\u6848\u4f18\u5148\u8303\u5f0f\uff1a\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u63d0\u53d6\u786e\u5b9a\u6027\u7b54\u6848\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u951a\u70b9\u751f\u6210\u95ee\u9898\uff0c\u5e76\u6267\u884c\u4e25\u683c\u7684\u4e00\u81f4\u6027\u9a8c\u8bc1\uff1b3) \u57fa\u4e8e\u6a21\u578b\u5931\u8d25\u7387\u7b5b\u9009\u6837\u672c\u5e76\u63d0\u70bc\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u4f7f\u7528Qwen3-VL-30B-A3B-Thinking\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u6784\u5efaChartVerse-SFT-600K\u548cChartVerse-RL-40K\u6570\u636e\u96c6\u3002", "result": "ChartVerse-8B\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u5176\u6559\u5e08\u6a21\u578bQwen3-VL-30B-A3B-Thinking\uff0c\u5e76\u4e0e\u66f4\u5f3a\u7684Qwen3-VL-32B-Thinking\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "ChartVerse\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u8868\u590d\u6742\u5ea6\u91cf\u5316\u548c\u7b54\u6848\u4f18\u5148\u7684QA\u5408\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u4e3a\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u8868\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13622", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13622", "abs": "https://arxiv.org/abs/2601.13622", "authors": ["Donghee Lee", "Rui Cai", "Zhe Zhao"], "title": "CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models", "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.", "AI": {"tldr": "CARPE\uff1a\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u96c6\u6210\u5c42\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u8ba9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u8868\u793a\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e4b\u95f4\u81ea\u9002\u5e94\u9009\u62e9\uff0c\u63d0\u5347\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u56fe\u50cf\u5206\u7c7b\u7b49\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u5176\u57fa\u7840\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982CLIP\uff09\u3002\u9700\u8981\u89e3\u51b3LVLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faCARPE\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u89c6\u89c9\u96c6\u6210\u5c42\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u56fe\u50cf\u8868\u793a\u7684\u4e0d\u540c\u65b9\u9762\uff1b2\uff09\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u8ba9\u6a21\u578b\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u4f18\u5148\u4f7f\u7528\u56fe\u50cf\u8868\u793a\uff0c\u4f55\u65f6\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "CARPE\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53d6\u5f97\u6539\u8fdb\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u96c6\u6210\u5230\u5927\u591a\u6570\u5f00\u6e90LVLMs\u4e2d\uff0c\u5177\u6709\u826f\u597d\u7684\u67b6\u6784\u9002\u5e94\u6027\u3002", "conclusion": "CARPE\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u6743\u8861\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u89e3\u51b3\u4e86LVLMs\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\uff0c\u4e3a\u63d0\u5347LVLMs\u7684\u89c6\u89c9\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13633", "abs": "https://arxiv.org/abs/2601.13633", "authors": ["Guanqi Zhan", "Changye Li", "Zhijian Liu", "Yao Lu", "Yi Wu", "Song Han", "Ligeng Zhu"], "title": "Scaling Test-time Inference for Visual Grounding", "comment": null, "summary": "Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.", "AI": {"tldr": "EGM\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u751f\u6210\u66f4\u591atoken\uff09\u6765\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u4fdd\u6301\u90e8\u7f72\u53cb\u597d\u6027\u7684\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u901a\u5e38\u89c4\u6a21\u5e9e\u5927\uff0c\u90e8\u7f72\u56f0\u96be\u4e14\u63a8\u7406\u7f13\u6162\u3002\u7814\u7a76\u53d1\u73b0\u5c0f\u578b\u548c\u5927\u578bVLM\u7684\u4e3b\u8981\u5dee\u5f02\u5728\u4e8e\u8bed\u8a00\u6a21\u578b\u5927\u5c0f\u800c\u975e\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5c0f\u578b\u6a21\u578b\u56e0\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\u800c\u5728\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "method": "\u63d0\u51faEGM\uff08\u9ad8\u6548\u89c6\u89c9\u5b9a\u4f4d\u8bed\u8a00\u6a21\u578b\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u589e\u52a0\u751f\u6210token\u6570\u91cf\uff09\u6765\u5f25\u8865\u5c0f\u578b\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u5dee\u8ddd\u3002\u8be5\u65b9\u6cd5\u90e8\u7f72\u53cb\u597d\uff0c\u56e0\u4e3a\u6bcf\u4e2atoken\u7684\u751f\u6210\u6210\u672c\u8fdc\u4f4e\u4e8e\u76f4\u63a5\u8fd0\u884c\u5927\u578b\u6a21\u578b\u3002", "result": "\u5728RefCOCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEGM-Qwen3-VL-8B\u8fbe\u523091.4 IoU\uff0c\u5e73\u5747\u5ef6\u8fdf737ms\uff08\u6bd4Qwen3-VL-235B\u5feb5.9\u500d\uff09\uff0c\u800c\u540e\u8005\u9700\u89814320ms\u8fbe\u523090.5 IoU\u3002\u5728\u65b0\u5efa\u7acb\u7684\u975e\u6a21\u6001\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u6027\u80fd\uff0c\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u6c34\u5e73\u3002", "conclusion": "EGM\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u5b9a\u4f4d\u7684\u6548\u7387\u3002"}}
{"id": "2601.13651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13651", "abs": "https://arxiv.org/abs/2601.13651", "authors": ["Marta Moscati", "Oleksandr Kats", "Mubashir Noman", "Muhammad Zaigham Zaheer", "Yufang Hou", "Markus Schedl", "Shah Nawaz"], "title": "Face-Voice Association with Inductive Bias for Maximum Class Separation", "comment": "Accepted at ICASSP 2026", "summary": "Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4eba\u8138-\u8bed\u97f3\u5173\u8054\u4efb\u52a1\u4e2d\u5e94\u7528\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u8fd9\u4e00\u5206\u7c7b\u6280\u672f\u5f15\u5165\u591a\u6a21\u6001\u5b66\u4e60\u9886\u57df\uff0c\u5e76\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u8138-\u8bed\u97f3\u5173\u8054\u7814\u7a76\u901a\u5e38\u901a\u8fc7\u635f\u5931\u51fd\u6570\u4f7f\u540c\u4e00\u4eba\u7684\u9762\u90e8\u548c\u8bed\u97f3\u5d4c\u5165\u63a5\u8fd1\u800c\u4e0d\u540c\u4eba\u7684\u5206\u79bb\u3002\u5206\u7c7b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u901a\u8fc7\u65bd\u52a0\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u53ef\u4ee5\u589e\u5f3a\u5d4c\u5165\u7684\u5224\u522b\u80fd\u529b\uff0c\u4f46\u8fd9\u4e00\u6280\u672f\u4ece\u672a\u5728\u4eba\u8138-\u8bed\u97f3\u5173\u8054\u9886\u57df\u5e94\u7528\u8fc7\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4eba\u8138-\u8bed\u97f3\u5173\u8054\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u8bf4\u8bdd\u8005\u7684\u591a\u6a21\u6001\u8868\u793a\u4e4b\u95f4\u7684\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7c7b\u95f4\u6b63\u4ea4\u6027\u635f\u5931\uff0c\u901a\u8fc7\u91cf\u5316\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eba\u8138-\u8bed\u97f3\u5173\u8054\u7684\u4e24\u4e2a\u4efb\u52a1\u5236\u5b9a\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5f53\u5f52\u7eb3\u504f\u7f6e\u4e0e\u7c7b\u95f4\u6b63\u4ea4\u6027\u635f\u5931\u7ed3\u5408\u65f6\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5e94\u7528\u5e76\u8bc1\u660e\u4e86\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5efa\u7acb\u65b0\u8303\u5f0f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.13664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13664", "abs": "https://arxiv.org/abs/2601.13664", "authors": ["Tiancheng Fang", "Bowen Pan", "Lingxi Chen", "Jiangjing Lyu", "Chengfei Lyu", "Chaoyue Niu", "Fan Wu"], "title": "VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement", "comment": "Under review at CVPR 2026", "summary": "We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.", "AI": {"tldr": "VIAFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u89c6\u56fe\u6761\u4ef6\u4f53\u7d20\u7ec6\u5316\u7684\u4f53\u7d20-\u56fe\u50cf\u5bf9\u9f50Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u6307\u5bfc\u4fee\u590d\u4e0d\u5b8c\u6574\u566a\u58f0\u4f53\u7d20", "motivation": "\u89e3\u51b3\u4f7f\u7528\u591a\u89c6\u56fe\u56fe\u50cf\u6307\u5bfc\u4fee\u590d\u4e0d\u5b8c\u6574\u566a\u58f0\u4f53\u7d20\u7684\u95ee\u9898\uff0c\u4e3a\u4f53\u7d20\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u5927\u6570\u636e\u6d6a\u6f6e\u4e2d\u63d0\u4f9b\u5b9e\u7528\u53ef\u9760\u7684\u6865\u6881", "method": "\u91c7\u7528\u534f\u540c\u8bbe\u8ba1\uff1a\u63d0\u4f9b2D\u56fe\u50cf\u6807\u8bb0\u663e\u5f0f3D\u7a7a\u95f4\u57fa\u7840\u7684\u56fe\u50cf\u7d22\u5f15\u3001\u5b66\u4e60\u76f4\u63a5\u4f53\u7d20\u7ec6\u5316\u8f68\u8ff9\u7684\u6821\u6b63\u6d41\u76ee\u6807\u3001\u5b9e\u73b0\u9c81\u68d2\u8de8\u6a21\u6001\u878d\u5408\u7684\u6df7\u5408\u6d41Transformer", "result": "\u5728\u7ea0\u6b63\u4ece\u5f3a\u5927\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u83b7\u5f97\u7684\u4f53\u7d20\u5f62\u72b6\u7684\u4e25\u91cd\u5408\u6210\u635f\u574f\u548c\u771f\u5b9e\u4f2a\u5f71\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "VIAFormer\u4e3a\u5b9e\u96453D\u521b\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u9760\u7684\u6865\u6881\uff0c\u4e3a\u4f53\u7d20\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u5927\u6570\u636e\u6d6a\u6f6e\u4e2d\u84ec\u52c3\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2601.13677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13677", "abs": "https://arxiv.org/abs/2601.13677", "authors": ["Carsten T. L\u00fcth", "Jeremias Traub", "Kim-Celine Kahl", "Till J. Bungert", "Lukas Klein", "Lars Kr\u00e4mer", "Paul F. J\u00e4ger", "Klaus Maier-Hein", "Fabian Isensee"], "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging", "comment": "Accepted at TMLR", "summary": "Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.", "code_url": "https://github.com/MIC-DKFZ/nnActive", "code_stars": 11, "code_last_update": "2026-01-20", "AI": {"tldr": "ClaSP PE\u662f\u4e00\u79cd\u7528\u4e8e3D\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u5206\u5c42\u67e5\u8be2\u548c\u529f\u7387\u566a\u58f0\u8c03\u5ea6\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u65e9\u671f\u9009\u62e9\u5197\u4f59\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6539\u8fdb\u7684\u968f\u673a\u57fa\u7ebf\u3002", "motivation": "3D\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u57283D\u6570\u636e\u4e0a\u7a33\u5b9a\u8d85\u8d8a\u6539\u8fdb\u7684\u968f\u673a\u91c7\u6837\u57fa\u7ebf\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faClass-stratified Scheduled Power Predictive Entropy (ClaSP PE)\u67e5\u8be2\u7b56\u7565\uff0c\u7ed3\u5408\u7c7b\u5206\u5c42\u67e5\u8be2\u786e\u4fdd\u5bf9\u5c11\u6570\u7c7b\u7ed3\u6784\u7684\u8986\u76d6\uff0c\u4ee5\u53ca\u91c7\u7528\u5bf9\u6570\u5c3a\u5ea6\u529f\u7387\u566a\u58f0\u548c\u8870\u51cf\u8c03\u5ea6\u6765\u589e\u5f3a\u65e9\u671fAL\u9636\u6bb5\u7684\u67e5\u8be2\u591a\u6837\u6027\u5e76\u4fc3\u8fdb\u540e\u671f\u5229\u7528\u3002", "result": "\u5728nnActive\u57fa\u51c6\u6d4b\u8bd5\u768424\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0cClaSP PE\u662f\u552f\u4e00\u5728\u5206\u5272\u8d28\u91cf\u548c\u6807\u6ce8\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u6539\u8fdb\u968f\u673a\u57fa\u7ebf\u7684\u65b9\u6cd5\uff0c\u4e14\u5728\u56db\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5373\u53ef\u7a33\u5065\u6cdb\u5316\u3002", "conclusion": "ClaSP PE\u8bc1\u660e\u4e86\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u57283D\u5206\u5272\u4efb\u52a1\u4e2d\u7a33\u5b9a\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u6e90\u5b9e\u73b0\u548c\u90e8\u7f72\u6307\u5357\u4f7f\u5176\u6613\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2601.13683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13683", "abs": "https://arxiv.org/abs/2601.13683", "authors": ["Boyuan Cao", "Xingbo Yao", "Chenhui Wang", "Jiaxin Ye", "Yujie Wei", "Hongming Shan"], "title": "Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation", "comment": null, "summary": "Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.", "AI": {"tldr": "DyDiLA\u662f\u4e00\u79cd\u65b0\u578b\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u6295\u5f71\u6a21\u5757\u3001\u52a8\u6001\u6d4b\u91cf\u6838\u548c\u4ee4\u724c\u5dee\u5206\u7b97\u5b50\u89e3\u51b3\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668\u4e2d\u6ce8\u610f\u529b\u6743\u91cd\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiTs)\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u6210\u4e3a\u4e3b\u8981\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u867d\u7136\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668(LiTs)\u901a\u5e38\u4ee5\u727a\u7272\u751f\u6210\u6027\u80fd\u4e3a\u4ee3\u4ef7\uff0c\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5dee\u5206\u7ebf\u6027\u6ce8\u610f\u529b(DyDiLA)\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1) \u52a8\u6001\u6295\u5f71\u6a21\u5757\uff0c\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u5206\u914d\u7684\u77e5\u8bc6\u4fc3\u8fdb\u4ee4\u724c\u8868\u793a\u7684\u89e3\u8026\uff1b2) \u52a8\u6001\u6d4b\u91cf\u6838\uff0c\u901a\u8fc7\u4e3a\u4ee4\u724c\u5904\u7406\u52a8\u6001\u5206\u914d\u6838\u51fd\u6570\u63d0\u4f9b\u66f4\u597d\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\uff1b3) \u4ee4\u724c\u5dee\u5206\u7b97\u5b50\uff0c\u901a\u8fc7\u8ba1\u7b97\u4ee4\u724c\u4e0e\u5176\u52a8\u6001\u6d4b\u91cf\u6838\u4ea7\u751f\u7684\u4fe1\u606f\u5197\u4f59\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u67e5\u8be2\u5230\u952e\u68c0\u7d22\u3002\u57fa\u4e8eDyDiLA\u6784\u5efa\u4e86\u6539\u8fdb\u7684\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668DyDi-LiT\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDyDi-LiT\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u5b9e\u9645\u6f5c\u529b\u3002", "conclusion": "DyDiLA\u901a\u8fc7\u89e3\u51b3\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.13705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13705", "abs": "https://arxiv.org/abs/2601.13705", "authors": ["Maria Lymperaiou", "Vasileios Karampinis", "Giorgos Filandrianos", "Angelos Vlachos", "Chrysoula Zerva", "Athanasios Voulodimos"], "title": "Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles", "comment": null, "summary": "Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u89c6\u89c9\u8c1c\u9898\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e2d\u7684\u7efc\u8ff0\u7814\u7a76\uff0c\u5c06\u89c6\u89c9\u8c1c\u9898\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30LVLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5206\u7c7b\u548c\u6a21\u578b\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u89c9\u8c1c\u9898\u957f\u671f\u4ee5\u6765\u4f5c\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u7684\u7d27\u51d1\u63a2\u6d4b\u5de5\u5177\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5148\u9a8c\u77e5\u8bc6\u4f9d\u8d56\u6765\u9694\u79bb\u62bd\u8c61\u3001\u89c4\u5219\u53d1\u73b0\u548c\u7cfb\u7edf\u63a8\u7406\u3002\u5229\u7528\u8fd9\u4e9b\u7279\u6027\uff0c\u89c6\u89c9\u8c1c\u9898\u6700\u8fd1\u6210\u4e3a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5927\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u5f00\u653e\u5f0f\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u53ef\u9a8c\u8bc1\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u7684\u62bd\u8c61\u6846\u67b6\u6765\u7406\u89e3\u89c6\u89c9\u8c1c\u9898\uff0c\u5e76\u6839\u636e\u76ee\u6807\u63a8\u7406\u673a\u5236\uff08\u5f52\u7eb3\u3001\u7c7b\u6bd4\u3001\u7b97\u6cd5\u3001\u6f14\u7ece\u548c\u51e0\u4f55/\u7a7a\u95f4\u63a8\u7406\uff09\u7ec4\u7ec7\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ece\u800c\u5c06\u8c1c\u9898\u8bbe\u8ba1\u4e0e\u89e3\u51b3\u6240\u9700\u7684\u8ba4\u77e5\u64cd\u4f5c\u8054\u7cfb\u8d77\u6765\u3002\u7efc\u5408\u8fd9\u4e9b\u7c7b\u522b\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5206\u6790\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "result": "\u8bc6\u522b\u51fa\u5f53\u524d\u6a21\u578b\u7684\u4e00\u81f4\u5c40\u9650\u6027\uff1a\u8106\u5f31\u7684\u6cdb\u5316\u80fd\u529b\u3001\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u7d27\u5bc6\u7ea0\u7f20\u3001\u6d41\u7545\u89e3\u91ca\u4e0e\u5fe0\u5b9e\u6267\u884c\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u8c1c\u9898\u89c6\u4e3a\u8bca\u65ad\u5de5\u5177\u800c\u975e\u4efb\u52a1\u683c\u5f0f\uff0c\u9610\u660e\u4e86LVLM\u63a8\u7406\u7684\u73b0\u72b6\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a8\u7406\u611f\u77e5\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5173\u952e\u65b9\u5411\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5f3a\u8c03\u89c6\u89c9\u8c1c\u9898\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.13797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13797", "abs": "https://arxiv.org/abs/2601.13797", "authors": ["Gabriele Serussi", "David Vainshtein", "Jonathan Kouchly", "Dotan Di Castro", "Chaim Baskin"], "title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval", "comment": null, "summary": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.", "AI": {"tldr": "PREGEN\uff1a\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6VLM\u9690\u85cf\u72b6\u6001\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7f16\u7801\u5668\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u7684\u7d27\u51d1\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8981\u4e48\u4f7f\u7528\u8fc7\u65f6\u67b6\u6784\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03\u548c\u7f13\u6162\u7684\u6807\u9898\u751f\u6210\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "PREGEN\u91c7\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u4e0e\u8f7b\u91cf\u7f16\u7801\u6a21\u578b\u914d\u5bf9\u3002\u5c06\u67e5\u8be2\u89c6\u9891\u548c\u4fee\u6539\u6587\u672c\u8f93\u5165VLM\uff0c\u63d0\u53d6\u6bcf\u5c42\u6700\u540e\u4e00\u4e2atoken\u7684\u9690\u85cf\u72b6\u6001\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u6c60\u5316\u8868\u793a\u4e0a\u8bad\u7ec3\u7b80\u5355\u7f16\u7801\u5668\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u4e14\u7d27\u51d1\u7684\u68c0\u7d22\u5d4c\u5165\u3002", "result": "PREGEN\u663e\u8457\u63a8\u8fdb\u4e86\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u6709\u5148\u524d\u65b9\u6cd5\uff0cRecall@1\u5206\u522b\u63d0\u5347+27.23\u548c+69.59\u3002\u65b9\u6cd5\u5728\u4e0d\u540cVLM\u9aa8\u5e72\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5bf9\u66f4\u590d\u6742\u7684\u6587\u672c\u4fee\u6539\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PREGEN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u65e0\u9700\u5fae\u8c03VLM\u5373\u53ef\u5145\u5206\u5229\u7528\u5176\u8bed\u4e49\u80fd\u529b\uff0c\u5728\u6027\u80fd\u548c\u6cdb\u5316\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13839", "abs": "https://arxiv.org/abs/2601.13839", "authors": ["Aisha Al-Mohannadi", "Ayisha Firoz", "Yin Yang", "Muhammad Imran", "Ferda Ofli"], "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes", "comment": null, "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.", "AI": {"tldr": "DisasterVQA\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u707e\u5bb3\u54cd\u5e94\u8bbe\u8ba1\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,395\u5f20\u771f\u5b9e\u707e\u5bb3\u56fe\u50cf\u548c4,405\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u707e\u5bb3\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u5728\u707e\u5bb3\u671f\u95f4\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7684\u6001\u52bf\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5728\u707e\u5bb3\u54cd\u5e94\u8fd9\u79cd\u590d\u6742\u3001\u5b89\u5168\u5173\u952e\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u707e\u5bb3\u573a\u666f\u7684\u57fa\u51c6\u6765\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u64cd\u4f5c\u6027\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u57fa\u4e8eFEMA ESF\u548cOCHA MIRA\u7b49\u4eba\u9053\u4e3b\u4e49\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5305\u542b\u6d2a\u6c34\u3001\u91ce\u706b\u3001\u5730\u9707\u7b49\u591a\u79cd\u707e\u5bb3\u7c7b\u578b\u7684\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u3002\u8bbe\u8ba1\u4e86\u4e8c\u5143\u3001\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u6db5\u76d6\u6001\u52bf\u611f\u77e5\u548c\u64cd\u4f5c\u51b3\u7b56\u4efb\u52a1\u3002\u5bf97\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u5728\u4e8c\u5143\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u5b9a\u91cf\u63a8\u7406\u3001\u7269\u4f53\u8ba1\u6570\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u707e\u5bb3\u573a\u666f\u4e2d\u3002\u6027\u80fd\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u3001\u707e\u5bb3\u7c7b\u522b\u3001\u533a\u57df\u548c\u4eba\u9053\u4e3b\u4e49\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "DisasterVQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u6307\u5bfc\u5f00\u53d1\u66f4\u9002\u5408\u707e\u5bb3\u54cd\u5e94\u7684\u9c81\u68d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u586b\u8865\u4e86\u707e\u5bb3\u54cd\u5e94\u9886\u57df\u4e13\u95e8\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.13871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13871", "abs": "https://arxiv.org/abs/2601.13871", "authors": ["Michail Spanakis", "Iason Oikonomidis", "Antonis Argyros"], "title": "OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting", "comment": null, "summary": "Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.", "code_url": "https://mikespanak.github.io/OCCAM_counter", "AI": {"tldr": "OCCAM\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\uff0c\u53ef\u5904\u7406\u56fe\u50cf\u4e2d\u4efb\u610f\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\uff0c\u57fa\u4e8eSAM2\u548c\u81ea\u5b9a\u4e49FINCH\u7b97\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6bcf\u5f20\u56fe\u50cf\u53ea\u6709\u4e00\u4e2a\u76ee\u6807\u7c7b\u522b\uff0c\u4f9d\u8d56\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u9700\u8981\u989d\u5916\u4fe1\u606f\uff08\u5982\u89c6\u89c9\u793a\u4f8b\u6216\u6587\u672c\u63d0\u793a\uff09\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u8865\u5145\u4fe1\u606f\u4e14\u80fd\u5904\u7406\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528Segment Anything Model 2 (SAM2)\u57fa\u7840\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u9608\u503c\u7248\u672c\u7684First Integer Neighbor Clustering Hierarchy (FINCH)\u7b97\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\u3002", "result": "\u5728FSC-147\u548cCARPK\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u5408\u6210\u591a\u7c7b\u6570\u636e\u96c6\u548c\u66f4\u5408\u9002\u7684F1\u8bc4\u5206\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "OCCAM\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13895", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13895", "abs": "https://arxiv.org/abs/2601.13895", "authors": ["Xu Zhang", "Danyang Li", "Yingjie Xia", "Xiaohang Dong", "Hualong Yu", "Jianye Wang", "Qicheng Li"], "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3", "comment": null, "summary": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.", "AI": {"tldr": "\u63d0\u51faOmniOVCD\u6846\u67b6\uff0c\u5229\u7528SAM 3\u7684\u89e3\u8026\u8f93\u51fa\u5934\uff0c\u901a\u8fc7SFID\u7b56\u7565\u878d\u5408\u8bed\u4e49\u3001\u5b9e\u4f8b\u548c\u5b58\u5728\u6027\u8f93\u51fa\u6784\u5efa\u5730\u7269\u63a9\u7801\uff0c\u518d\u5206\u89e3\u4e3a\u5b9e\u4f8b\u63a9\u7801\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u514d\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56CLIP\u8bc6\u522b\u7c7b\u522b\uff0c\u5e76\u9700\u8981\u989d\u5916\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5bfc\u81f4\u7279\u5f81\u5339\u914d\u95ee\u9898\u548c\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u3002SAM 3\u7684\u63a8\u51fa\u4e3aOVCD\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u5176\u96c6\u6210\u5206\u5272\u548c\u8bc6\u522b\u80fd\u529b\u4e8e\u5355\u4e00\u53ef\u63d0\u793a\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51faOmniOVCD\u6846\u67b6\uff0c\u5229\u7528SAM 3\u7684\u89e3\u8026\u8f93\u51fa\u5934\uff0c\u8bbe\u8ba1\u534f\u540c\u878d\u5408\u5230\u5b9e\u4f8b\u89e3\u8026\u7b56\u7565\u3002SFID\u9996\u5148\u878d\u5408SAM 3\u7684\u8bed\u4e49\u3001\u5b9e\u4f8b\u548c\u5b58\u5728\u6027\u8f93\u51fa\u6765\u6784\u5efa\u5730\u7269\u8986\u76d6\u63a9\u7801\uff0c\u7136\u540e\u5c06\u5176\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u5b9e\u4f8b\u63a9\u7801\u8fdb\u884c\u53d8\u5316\u6bd4\u8f83\u3002\u8be5\u8bbe\u8ba1\u4fdd\u6301\u4e86\u7c7b\u522b\u8bc6\u522b\u7684\u9ad8\u7cbe\u5ea6\u548c\u8de8\u56fe\u50cf\u7684\u5b9e\u4f8b\u7ea7\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\uff08LEVIR-CD\u3001WHU-CD\u3001S2Looking\u548cSECOND\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5206\u522b\u83b7\u5f97\u4e8667.2\u300166.5\u300124.5\u548c27.1\u7684IoU\u5206\u6570\uff08\u7c7b\u522b\u5e73\u5747\uff09\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "OmniOVCD\u901a\u8fc7\u5229\u7528SAM 3\u7684\u89e3\u8026\u80fd\u529b\uff0c\u63d0\u51faSFID\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u5339\u914d\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13899", "abs": "https://arxiv.org/abs/2601.13899", "authors": ["Masoumeh Javanbakhat", "Piotr Komorowski", "Dilyara Bareeva", "Wei-Chang Lai", "Wojciech Samek", "Christoph Lippert"], "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging", "comment": null, "summary": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u4e3a\u6df1\u5ea6\u53cc\u6837\u672c\u6d4b\u8bd5\u63d0\u4f9b\u6837\u672c\u7ea7\u548c\u7279\u5f81\u7ea7\u89e3\u91ca\uff0c\u63ed\u793a\u54ea\u4e9b\u6837\u672c\u548c\u7279\u5f81\u9a71\u52a8\u7ec4\u95f4\u5dee\u5f02", "motivation": "\u6df1\u5ea6\u53cc\u6837\u672c\u6d4b\u8bd5\u867d\u7136\u68c0\u6d4b\u80fd\u529b\u5f3a\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u4f9d\u8d56\u7c7b\u522b\u6807\u7b7e\uff0c\u4e0d\u9002\u7528\u4e8e\u65e0\u6807\u7b7e\u7684\u7edf\u8ba1\u6d4b\u8bd5\u573a\u666f\uff0c\u9650\u5236\u4e86\u5728\u751f\u7269\u533b\u5b66\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u5e94\u7528", "method": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u7ea7\u548c\u7279\u5f81\u7ea7\u89e3\u91ca\u589e\u5f3a\u6df1\u5ea6\u53cc\u6837\u672c\u6d4b\u8bd5\uff0c\u8bc6\u522b\u9a71\u52a8\u7edf\u8ba1\u663e\u8457\u5dee\u5f02\u7684\u4e2a\u4f53\u6837\u672c\u548c\u8f93\u5165\u7279\u5f81", "result": "\u65b9\u6cd5\u80fd\u7a81\u51fa\u663e\u793a\u5bf9\u68c0\u6d4b\u5230\u7684\u7ec4\u95f4\u5dee\u5f02\u8d21\u732e\u6700\u5927\u7684\u56fe\u50cf\u533a\u57df\u548c\u4e2a\u4f53\u6837\u672c\uff0c\u63d0\u4f9b\u7a7a\u95f4\u548c\u5b9e\u4f8b\u5c42\u9762\u7684\u6d1e\u5bdf\uff0c\u5e94\u7528\u4e8e\u751f\u7269\u533b\u5b66\u6210\u50cf\u6570\u636e\u65f6\u80fd\u8bc6\u522b\u6709\u5f71\u54cd\u529b\u7684\u6837\u672c\u548c\u4e0e\u75be\u75c5\u76f8\u5173\u53d8\u5f02\u76f8\u5173\u7684\u89e3\u5256\u5b66\u610f\u4e49\u533a\u57df", "conclusion": "\u8be5\u5de5\u4f5c\u6865\u63a5\u4e86\u7edf\u8ba1\u63a8\u65ad\u548c\u53ef\u89e3\u91caAI\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u6210\u50cf\u4e2d\u53ef\u89e3\u91ca\u3001\u65e0\u6807\u7b7e\u7684\u7fa4\u4f53\u5206\u6790"}}
{"id": "2601.13942", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13942", "abs": "https://arxiv.org/abs/2601.13942", "authors": ["Hongbo Bai", "Yujin Zhou", "Yile Wu", "Chi-Min Chan", "Pengcheng Wen", "Kunhao Pan", "Sirui Han", "Yike Guo"], "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning", "comment": null, "summary": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.", "AI": {"tldr": "GoG\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u89c6\u673a\u5236\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u67e5\u8be2\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u611f\u77e5\u5230\u4e3b\u52a8\u89c6\u89c9\u89c4\u5212\u7684\u8f6c\u53d8\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u957f\u5c3e\u5b9e\u4f53\u6216\u52a8\u6001\u4fe1\u606f\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u65f6\uff0c\u7531\u4e8e\u9759\u6001\u53c2\u6570\u5316\u77e5\u8bc6\u7684\u9650\u5236\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684\u641c\u7d22\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u5197\u4f59\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u8fed\u4ee3\u53cd\u601d\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u89c6\u89c9\u67e5\u8be2\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86Glance-or-Gaze\uff08GoG\uff09\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u6027\u6ce8\u89c6\u673a\u5236\uff08\u52a8\u6001\u9009\u62e9\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u9ad8\u4ef7\u503c\u533a\u57df\uff09\uff0c\u4ee5\u53ca\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fdb\u884c\u53cd\u5c04\u6027GoG\u884c\u4e3a\u5bf9\u9f50\uff0c\u4ee5\u53ca\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6765\u589e\u5f3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u7684\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u9009\u62e9\u6027\u6ce8\u89c6\u673a\u5236\u548c\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u6709\u6548\u7684\u89c6\u89c9\u641c\u7d22\u90fd\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002", "conclusion": "GoG\u6846\u67b6\u901a\u8fc7\u4ece\u88ab\u52a8\u611f\u77e5\u8f6c\u5411\u4e3b\u52a8\u89c6\u89c9\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u67e5\u8be2\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13951", "abs": "https://arxiv.org/abs/2601.13951", "authors": ["Shengyi Wu", "Yan Hong", "Shengyao Chen", "Zheng Wang", "Xianbing Sun", "Jiahui Zhan", "Jun Lan", "Jianfu Zhang"], "title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content", "comment": null, "summary": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.", "AI": {"tldr": "\u63d0\u51faVTONGuard\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7775,000\u5f20\u771f\u5b9e\u548c\u5408\u6210\u8bd5\u7a7f\u56fe\u50cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u865a\u62df\u8bd5\u7a7f\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u865a\u62df\u8bd5\u7a7f(VTON)\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u7684\u8bd5\u7a7f\u5185\u5bb9\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u771f\u5b9e\u6027\u548c\u8d1f\u8d23\u4efb\u4f7f\u7528\u7684\u8feb\u5207\u62c5\u5fe7\u3002\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "1) \u6784\u5efaVTONGuard\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7775,000\u5f20\u771f\u5b9e\u548c\u5408\u6210\u8bd5\u7a7f\u56fe\u50cf\uff0c\u6db5\u76d6\u59ff\u52bf\u3001\u80cc\u666f\u3001\u670d\u88c5\u98ce\u683c\u7b49\u591a\u79cd\u771f\u5b9e\u573a\u666f\uff1b2) \u5728\u7edf\u4e00\u8bad\u7ec3\u548c\u6d4b\u8bd5\u534f\u8bae\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u68c0\u6d4b\u8303\u5f0f\uff1b3) \u8bbe\u8ba1\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u96c6\u6210\u8f85\u52a9\u5206\u5272\u4ee5\u589e\u5f3a\u8fb9\u754c\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u3002", "result": "1) \u63ed\u793a\u4e86\u5404\u79cd\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff1b2) \u7a81\u51fa\u4e86\u8de8\u8303\u5f0f\u6cdb\u5316\u7684\u6301\u7eed\u6311\u6218\uff1b3) \u63d0\u51fa\u7684\u591a\u4efb\u52a1\u6846\u67b6\u5728VTONGuard\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "VTONGuard\u57fa\u51c6\u6570\u636e\u96c6\u80fd\u591f\u5b9e\u73b0\u516c\u5e73\u6bd4\u8f83\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u6a21\u578b\u53d1\u5c55\uff0c\u63a8\u52a8VTON\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002"}}
{"id": "2601.13954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13954", "abs": "https://arxiv.org/abs/2601.13954", "authors": ["Adrien Meyer", "Didier Mutter", "Nicolas Padoy"], "title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging", "comment": null, "summary": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.", "AI": {"tldr": "DExTeR\u662f\u57fa\u4e8eTransformer\u7684\u70b9\u5230\u6846\u56de\u5f52\u5668\uff0c\u4e13\u4e3a\u533b\u5b66\u5f71\u50cf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7c7b\u5f15\u5bfc\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u3001CLICK-MoE\u4e13\u5bb6\u6df7\u5408\u548c\u591a\u70b9\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u70b9\u6807\u6ce8\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u89e3\u5256\u6807\u5fd7\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u4ecb\u5165\u6307\u5bfc\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u7684\u8fb9\u754c\u6846\u6807\u6ce8\u3002\u5f31\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u4f7f\u7528\u70b9\u6807\u6ce8\u53ef\u51cf\u5c11\u6807\u6ce8\u6210\u672c\uff0c\u4f46\u533b\u5b66\u5f71\u50cf\u5b58\u5728\u89e3\u5256\u91cd\u53e0\u3001\u5c3a\u5bf8\u591a\u53d8\u548c\u7ed3\u6784\u6a21\u7cca\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u8fb9\u754c\u6846\u63a8\u65ad\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8ePoint-DETR\u6784\u5efaDExTeR\uff0c\u5c06\u5355\u70b9\u6807\u6ce8\u7f16\u7801\u4e3a\u5bf9\u8c61\u67e5\u8be2\uff1b\u5f15\u5165\u7c7b\u5f15\u5bfc\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\uff0c\u5229\u7528\u70b9\u5750\u6807\u548c\u7c7b\u522b\u6807\u7b7e\u6307\u5bfc\u6ce8\u610f\u529b\u91c7\u6837\uff1b\u63d0\u51faCLICK-MoE\uff08\u7c7b\u522b\u3001\u5b9e\u4f8b\u548c\u901a\u7528\u77e5\u8bc6\u6df7\u5408\u4e13\u5bb6\uff09\u89e3\u8026\u7c7b\u522b\u548c\u5b9e\u4f8b\u8868\u793a\uff1b\u91c7\u7528\u591a\u70b9\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u5bf9\u6807\u6ce8\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u533b\u5b66\u9886\u57df\u6570\u636e\u96c6\uff08\u5185\u7aa5\u955c\u3001\u80f8\u90e8X\u5149\u548c\u5185\u955c\u8d85\u58f0\uff09\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "DExTeR\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684Transformer\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u70b9\u6807\u6ce8\u5230\u8fb9\u754c\u6846\u63a8\u65ad\u7684\u6311\u6218\uff0c\u4e3a\u51cf\u5c11\u533b\u5b66\u5f71\u50cf\u6807\u6ce8\u6210\u672c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13974", "abs": "https://arxiv.org/abs/2601.13974", "authors": ["Shih-Yao Lin"], "title": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames", "comment": "This paper corresponds to the camera-ready version of a WACV 2026 Workshop paper", "summary": "Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.\n  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.\n  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.\n  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.", "AI": {"tldr": "STEC\u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5e27\u91c7\u6837\u8d28\u91cf\u7684\u65e0\u53c2\u8003\u6307\u6807\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4fe1\u606f\u5f3a\u5ea6\u3001\u65f6\u95f4\u5206\u6563\u6027\u548c\u975e\u5197\u4f59\u6027\u6765\u8861\u91cf\u91c7\u6837\u5e27\u7684\u4fe1\u606f\u8986\u76d6\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5e27\u91c7\u6837\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\u6216\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u91c7\u6837\u5e27\u662f\u5426\u5145\u5206\u6355\u6349\u89c6\u9891\u4fe1\u606f\u5185\u5bb9\u548c\u4ee3\u8868\u6027\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u8bca\u65ad\u5de5\u5177\u6765\u5206\u6790\u6709\u9650\u9884\u7b97\u4e0b\u7684\u5e27\u91c7\u6837\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u71b5\u8986\u76d6\u7387(STEC)\uff0c\u57fa\u4e8e\u65f6\u7a7a\u5e27\u71b5(STFE)\u6d4b\u91cf\u6bcf\u5e27\u7684\u7a7a\u95f4\u4fe1\u606f\uff08\u57fa\u4e8e\u71b5\u7684\u7ed3\u6784\u590d\u6742\u5ea6\uff09\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u8986\u76d6\u5ea6\u548c\u5197\u4f59\u5ea6\u8bc4\u4f30\u91c7\u6837\u5e27\uff0c\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4fe1\u606f\u5f3a\u5ea6\u3001\u65f6\u95f4\u5206\u6563\u6027\u548c\u975e\u5197\u4f59\u6027\u3002", "result": "\u5728MSR-VTT test-1k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTEC\u80fd\u6e05\u6670\u533a\u5206\u968f\u673a\u3001\u5747\u5300\u548c\u5185\u5bb9\u611f\u77e5\u7b49\u5e38\u89c1\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u63ed\u793a\u5355\u4e2a\u89c6\u9891\u7684\u9c81\u68d2\u6027\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u65e0\u6cd5\u901a\u8fc7\u5e73\u5747\u6027\u80fd\u5355\u72ec\u6355\u83b7\u3002", "conclusion": "STEC\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u539f\u5219\u6027\u7684\u91c7\u6837\u8d28\u91cf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u901a\u7528\u8bc4\u4f30\u5de5\u5177\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u4f46\u5f3a\u8c03\u5176\u8bbe\u8ba1\u76ee\u7684\u4e0d\u662f\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u800c\u662f\u63d0\u4f9b\u4efb\u52a1\u65e0\u5173\u7684\u8bca\u65ad\u4fe1\u53f7\u3002"}}
{"id": "2601.13976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.13976", "abs": "https://arxiv.org/abs/2601.13976", "authors": ["Jing Zuo", "Lingzhou Mu", "Fan Jiang", "Chengcheng Ma", "Mu Xu", "Yonggang Qi"], "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation", "comment": null, "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.", "AI": {"tldr": "FantasyVLN\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u60f3\u8c61\u7684\u89c6\u89c9\u6807\u8bb0\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u4fdd\u6301CoT\u63a8\u7406\u4f18\u52bf\u7684\u540c\u65f6\u907f\u514d\u4e86\u663e\u5f0f\u6807\u8bb0\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u611f\u77e5\u7684\u5b9e\u65f6\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709CoT\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u7eaf\u6587\u672cCoT\u7f3a\u4e4f\u7a7a\u95f4\u57fa\u7840\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u7a00\u758f\u6807\u6ce8\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u800c\u591a\u6a21\u6001CoT\u7531\u4e8e\u751f\u6210\u60f3\u8c61\u7684\u89c6\u89c9\u89c2\u5bdf\u5bfc\u81f4\u4e25\u91cd\u7684\u6807\u8bb0\u81a8\u80c0\uff0c\u4f7f\u5f97\u5b9e\u65f6\u5bfc\u822a\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51faFantasyVLN\u7edf\u4e00\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u81ea\u56de\u5f52\u5668\uff08VAR\uff09\u5c06\u60f3\u8c61\u7684\u89c6\u89c9\u6807\u8bb0\u7f16\u7801\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u5728CoT\u63a8\u7406\u8bad\u7ec3\u4e2d\u8054\u5408\u5b66\u4e60\u6587\u672c\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001CoT\u6a21\u5f0f\uff1b3\uff09\u91c7\u7528\u7edf\u4e00\u7684\u591aCoT\u7b56\u7565\uff1b4\uff09\u63a8\u7406\u65f6\u76f4\u63a5\u8fdb\u884c\u6307\u4ee4\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u611f\u77e5\u7684\u8868\u793a\u3002", "result": "\u5728LH-VLN\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a8\u7406\u611f\u77e5\u7684\u5b9e\u65f6\u5bfc\u822a\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u540c\u65f6\u76f8\u6bd4\u663e\u5f0fCoT\u65b9\u6cd5\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FantasyVLN\u6846\u67b6\u5728\u4fdd\u6301CoT\u63a8\u7406\u4f18\u52bf\u7684\u540c\u65f6\u907f\u514d\u4e86\u663e\u5f0f\u6807\u8bb0\u5f00\u9500\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u5177\u6709\u63a8\u7406\u80fd\u529b\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u63a8\u7406\u6df1\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.13986", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13986", "abs": "https://arxiv.org/abs/2601.13986", "authors": ["Zhang Wen", "Jiangwei Xie", "Dongdong Chen"], "title": "Equivariant Learning for Unsupervised Image Dehazing", "comment": "Technical report", "summary": "Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u53bb\u96fe\u6846\u67b6EID\uff0c\u5229\u7528\u56fe\u50cf\u4fe1\u53f7\u7684\u5bf9\u79f0\u6027\u4ece\u539f\u59cb\u6709\u96fe\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u56fe\u50cf\uff0c\u65e0\u9700\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5148\u9a8c\u6216\u5927\u91cf\u65e0\u96fe\u5730\u9762\u771f\u503c\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5148\u9a8c\u6216\u5927\u91cf\u65e0\u96fe\u5730\u9762\u771f\u503c\uff0c\u8fd9\u4e9b\u5728\u79d1\u5b66\u6210\u50cf\u4e2d\u83b7\u53d6\u6210\u672c\u9ad8\u6216\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEID\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u4fe1\u53f7\u7684\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u5f3a\u5236\u96fe\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u7cfb\u7edf\u6027\u7b49\u53d8\u6027\u6765\u6062\u590d\u6e05\u6670\u6a21\u5f0f\uff1b\u540c\u65f6\u63d0\u51fa\u5bf9\u6297\u5b66\u4e60\u7b56\u7565\u6765\u5efa\u6a21\u672a\u77e5\u7684\u96fe\u7269\u7406\u7279\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u79d1\u5b66\u56fe\u50cf\u53bb\u96fe\u57fa\u51c6\uff08\u7ec6\u80de\u663e\u5fae\u955c\u548c\u533b\u5b66\u5185\u7aa5\u955c\uff09\u4ee5\u53ca\u81ea\u7136\u56fe\u50cf\u53bb\u96fe\u5b9e\u9a8c\u4e2d\uff0cEID\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7b49\u53d8\u5b66\u4e60\u4e0e\u96fe\u7269\u7406\u5efa\u6a21\u76f8\u7ed3\u5408\uff0cEID\u6709\u671b\u5728\u79d1\u5b66\u6210\u50cf\u4e2d\u5b9e\u73b0\u66f4\u901a\u7528\u548c\u6709\u6548\u7684\u96fe\u53bb\u9664\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2601.14037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14037", "abs": "https://arxiv.org/abs/2601.14037", "authors": ["Kumar Ashutosh", "XuDong Wang", "Xi Yin", "Kristen Grauman", "Adam Polyak", "Ishan Misra", "Rohit Girdhar"], "title": "Human detectors are surprisingly powerful reward models", "comment": "Technical report", "summary": "Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.", "AI": {"tldr": "\u63d0\u51faHuDA\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u4f53\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u65f6\u5e8f\u63d0\u793a\u5bf9\u9f50\u5206\u6570\u6765\u91cf\u5316\u751f\u6210\u89c6\u9891\u4e2d\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u6548\u679c", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u65b9\u9762\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u975e\u521a\u6027\u8fd0\u52a8\uff08\u7279\u522b\u662f\u4eba\u4f53\u52a8\u6001\u52a8\u4f5c\u5982\u8fd0\u52a8\u3001\u821e\u8e48\u7b49\uff09\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5e38\u51fa\u73b0\u80a2\u4f53\u7f3a\u5931/\u591a\u4f59\u3001\u59ff\u6001\u626d\u66f2\u6216\u7269\u7406\u4e0a\u4e0d\u53ef\u4fe1\u7684\u52a8\u4f5c", "method": "\u63d0\u51faHuDA\u5956\u52b1\u6a21\u578b\uff0c\u6574\u5408\u4eba\u4f53\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff08\u8bc4\u4f30\u5916\u89c2\u8d28\u91cf\uff09\u548c\u65f6\u5e8f\u63d0\u793a\u5bf9\u9f50\u5206\u6570\uff08\u6355\u6349\u8fd0\u52a8\u771f\u5b9e\u6027\uff09\uff0c\u4f7f\u7528\u73b0\u6210\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff1b\u91c7\u7528Group Reward Policy Optimization (GRPO)\u540e\u8bad\u7ec3\u65b9\u6cd5\u4f18\u5316\u89c6\u9891\u6a21\u578b", "result": "HuDA\u5728\u8bc4\u4f30\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5fae\u8c03\u7684\u4e13\u7528\u6a21\u578b\uff1b\u901a\u8fc7GRPO\u540e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u4e0a\uff0c\u4ee573%\u80dc\u7387\u8d85\u8d8aWan 2.1\u7b49SOTA\u6a21\u578b\uff1bHuDA\u8fd8\u80fd\u6539\u5584\u52a8\u7269\u89c6\u9891\u548c\u4eba\u7269-\u7269\u4f53\u4ea4\u4e92\u7684\u751f\u6210\u8d28\u91cf", "conclusion": "HuDA\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u591f\u51c6\u786e\u91cf\u5316\u751f\u6210\u89c6\u9891\u4e2d\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\uff0c\u901a\u8fc7GRPO\u540e\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u4eba\u4f53\u52a8\u4f5c\uff0c\u8fd8\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u590d\u6742\u8fd0\u52a8\u573a\u666f"}}
{"id": "2601.14038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14038", "abs": "https://arxiv.org/abs/2601.14038", "authors": ["Alexandre Justo Miro", "Ludvig af Klinteberg", "Bogdan Timus", "Aron Asefaw", "Ajinkya Khoche", "Thomas Gustafsson", "Sina Sharif Mansouri", "Masoud Daneshtalab"], "title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving", "comment": "Accepted to The IEEE/CVF Winter Conference on Applications of Computer Vision 2026", "summary": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.", "code_url": "https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes", "code_stars": 0, "code_last_update": "2026-01-21", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u53d1\u73b0\u5e76\u7ea0\u6b63\u4e86\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u76843D\u6846\u6807\u6ce8\u8bef\u5dee\uff0c\u901a\u8fc7\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u4f7f\u6807\u6ce8\u7b26\u5408\u7269\u7406\u53ef\u884c\u8f68\u8ff9\u5e76\u4e0e\u4f20\u611f\u5668\u6570\u636e\u4fdd\u6301\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u51c6\u786e\u76843D\u6846\u6807\u6ce8\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u548c\u6027\u80fd\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u7269\u4f53\u5728\u4e0d\u540c\u65f6\u95f4\u6233\u88ab\u89c2\u6d4b\u5230\uff0c\u57fa\u4e8eLiDAR\u7b49\u4e3b\u52a8\u4f20\u611f\u5668\u76843D\u6846\u6807\u6ce8\u5bb9\u6613\u5f15\u5165\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u8fd9\u4e9b\u8bef\u5dee\u5728\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u5c1a\u672a\u88ab\u53d1\u73b0\u548c\u7ea0\u6b63\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea0\u6b63\u6807\u6ce8\u4f7f\u5176\u9075\u5faa\u7269\u7406\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u4e0e\u4f20\u611f\u5668\u6570\u636e\u4fdd\u6301\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9a\u4e49\u4e86\u8be5\u95ee\u9898\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5728Argoverse 2\u3001MAN TruckScenes\u548c\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u6846\u6807\u6ce8\u8d28\u91cf\u63d0\u5347\u4e86\u8d85\u8fc717%\u3002\u91cf\u5316\u5206\u6790\u663e\u793a\u539f\u59cb\u6807\u6ce8\u8bef\u5dee\u6700\u5927\u53ef\u8fbe2.5\u7c73\uff0c\u9ad8\u5ea6\u52a8\u6001\u7269\u4f53\u53d7\u5f71\u54cd\u6700\u4e25\u91cd\u3002\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8fd9\u4e9b\u8bef\u5dee\u7684\u5f71\u54cd\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6539\u8fdb\u5e45\u5ea6\u3002", "conclusion": "\u51c6\u786e\u76843D\u6846\u6807\u6ce8\u5bf9\u4e8e\u6b63\u786e\u89e3\u91ca\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u8be5\u5de5\u4f5c\u9996\u6b21\u53d1\u73b0\u5e76\u7ea0\u6b63\u4e86\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\u8bef\u5dee\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.14039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14039", "abs": "https://arxiv.org/abs/2601.14039", "authors": ["Wesam Moustafa", "Hossam Elsafty", "Helen Schneider", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation", "comment": null, "summary": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.", "code_url": "https://github.com/wemous/abstention-for-segmentation", "code_stars": 0, "code_last_update": "2025-09-21", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u5f03\u6743\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5ffd\u7565\u566a\u58f0\u6837\u672c\u6765\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u566a\u58f0\u9c81\u68d2\u6027", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u7814\u7a76\u4e0d\u8db3\u3002\u5f03\u6743\u673a\u5236\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5728\u5206\u5272\u9886\u57df\u6f5c\u529b\u672a\u7ecf\u9a8c\u8bc1", "method": "\u63d0\u51fa\u901a\u7528\u6a21\u5757\u5316\u5f03\u6743\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5f15\u5bfc\u5f03\u6743\u884c\u4e3a\u7684\u77e5\u60c5\u6b63\u5219\u5316\u9879\u548c\u57fa\u4e8e\u5e42\u5f8b\u7684\u81ea\u9002\u5e94\u8c03\u53c2\u7b97\u6cd5\u3002\u5c06\u8be5\u6846\u67b6\u4e0e\u4e09\u79cd\u4e0d\u540c\u635f\u5931\u51fd\u6570\u96c6\u6210\uff0c\u521b\u5efa\u4e86GAC\u3001SAC\u548cADS\u4e09\u79cd\u566a\u58f0\u9c81\u68d2\u53d8\u4f53", "result": "\u5728CaDIS\u548cDSAD\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u975e\u5f03\u6743\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u7a81\u51fa", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u5ffd\u7565\u635f\u574f\u6837\u672c\u7684\u5f03\u6743\u673a\u5236\u662f\u6784\u5efa\u66f4\u53ef\u9760\u5206\u5272\u6a21\u578b\u7684\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7b56\u7565\uff0c\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\u548c\u6a21\u5757\u5316\u7279\u70b9"}}
{"id": "2601.14044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14044", "abs": "https://arxiv.org/abs/2601.14044", "authors": ["Kaiyu Wu", "Pucheng Han", "Hualong Zhang", "Naigeng Wu", "Keze Wang"], "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology", "comment": null, "summary": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.", "code_url": "https://github.com/Marcowky/Weather-R1", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6c14\u8c61\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9886\u57df\u9e3f\u6c9f\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86WeatherQA\u57fa\u51c6\u548cLoCo-RFT\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u9996\u4e2a\u5177\u6709\u903b\u8f91\u5fe0\u5b9e\u5ea6\u7684\u6c14\u8c61\u63a8\u7406VLM Weather-R1", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u8c61\u5b66\u5e94\u7528\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9886\u57df\u9e3f\u6c9f\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\u9e3f\u6c9f\u3002\u4e3b\u6d41\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u81ea\u76f8\u77db\u76fe\u63a8\u7406\uff0c\u5373\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5176\u6700\u7ec8\u7b54\u6848\u76f8\u77db\u76fe\uff0c\u8fd9\u5728\u6c14\u8c61\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002", "method": "1. \u6784\u5efaWeatherQA\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6c14\u8c61\u5b66\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\uff1b2. \u63d0\u51fa\u903b\u8f91\u4e00\u81f4\u6027\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u903b\u8f91\u4e00\u81f4\u6027\u5956\u52b1\u6765\u89e3\u51b3\u81ea\u76f8\u77db\u76fe\u63a8\u7406\u95ee\u9898\uff1b3. \u5f00\u53d1Weather-R1\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u6c14\u8c61\u9886\u57df\u5177\u6709\u903b\u8f91\u5fe0\u5b9e\u5ea6\u7684\u63a8\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWeather-R1\u5728WeatherQA\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e869.8\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u4f20\u7edf\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u539f\u59cb\u7684Qwen2.5-VL-32B\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684LoCo-RFT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6c14\u8c61\u9886\u57df\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4e00\u81f4\u6027\u95ee\u9898\uff0cWeather-R1\u6a21\u578b\u5728\u6c14\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u53ef\u9760AI\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14052", "abs": "https://arxiv.org/abs/2601.14052", "authors": ["Haoran Xu", "Yanlin Liu", "Zizhao Tong", "Jiaze Li", "Kexue Fu", "Yuyang Zhang", "Longxiang Gao", "Shuaiguang Li", "Xingyu Li", "Yanran Xu", "Changwei Wang"], "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model", "comment": null, "summary": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.", "AI": {"tldr": "\u63d0\u51faMM-OOD\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672cOOD\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u9488\u5bf9\u8fd1OOD\u548c\u8fdcOOD\u4efb\u52a1\u91c7\u7528\u4e0d\u540c\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cOOD\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u7a7a\u95f4\u68c0\u6d4b\u7684\u56fa\u6709\u6311\u6218\u3002CLIP\u7684\u51fa\u73b0\u63a8\u52a8\u4e86\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7814\u7a76\uff0c\u4f46\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u5904\u7406\u56fe\u50cf\u7a7a\u95f4\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51faMM-OOD\u7ba1\u9053\uff1a1) \u5bf9\u4e8e\u8fd1OOD\u4efb\u52a1\uff0c\u76f4\u63a5\u5c06ID\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u8f93\u5165MLLMs\u8bc6\u522b\u6f5c\u5728\u5f02\u5e38\uff1b2) \u5bf9\u4e8e\u8fdcOOD\u4efb\u52a1\uff0c\u91c7\u7528sketch-generate-elaborate\u6846\u67b6\uff1a\u5148\u7528\u6587\u672c\u63d0\u793a\u52fe\u753b\u5f02\u5e38\u66b4\u9732\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u89c6\u89c9OOD\u6837\u672c\uff0c\u6700\u540e\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u8fdb\u884c\u8be6\u7ec6\u9610\u8ff0\u3002", "result": "\u5728Food-101\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u5728ImageNet-1K\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MM-OOD\u65b9\u6cd5\u901a\u8fc7\u5229\u7528MLLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u548c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6709\u6548\u63d0\u5347\u4e86\u96f6\u6837\u672cOOD\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u56fe\u50cf\u7a7a\u95f4\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2601.14056", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14056", "abs": "https://arxiv.org/abs/2601.14056", "authors": ["Andrea Rigo", "Luca Stornaiuolo", "Weijie Wang", "Mauro Martino", "Bruno Lepri", "Nicu Sebe"], "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion", "comment": null, "summary": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.", "AI": {"tldr": "POCI-Diff\uff1a\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc73D\u5e03\u5c40\u63a7\u5236\u548c\u8bed\u4e49\u7ed1\u5b9a\u5b9e\u73b0\u4e00\u81f4\u7684\u591a\u5bf9\u8c61\u573a\u666f\u5408\u6210\u4e0e\u7f16\u8f91", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u75282D\u7ebf\u7d22\u6216\u8fed\u4ee3\u590d\u5236-\u53d8\u5f62-\u7c98\u8d34\u7b56\u7565\u6765\u6539\u5584\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4f46\u7ecf\u5e38\u626d\u66f2\u5bf9\u8c61\u51e0\u4f55\u5f62\u72b6\u4e14\u65e0\u6cd5\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5f3a\u5236\u6267\u884c3D\u51e0\u4f55\u7ea6\u675f\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPOCI-Diff\u6846\u67b6\uff1a1\uff09\u901a\u8fc7Blended Latent Diffusion\u5c06\u5355\u4e2a\u6587\u672c\u63cf\u8ff0\u7ed1\u5b9a\u5230\u7279\u5b9a3D\u8fb9\u754c\u6846\uff0c\u5b9e\u73b0\u663e\u5f0f\u7684\u6bcf\u5bf9\u8c61\u8bed\u4e49\u63a7\u5236\uff1b2\uff09\u91c7\u7528\u65e0\u53d8\u5f62\u751f\u6210\u7f16\u8f91\u7ba1\u9053\uff0c\u901a\u8fc7\u91cd\u65b0\u751f\u6210\u800c\u975e\u50cf\u7d20\u53d8\u5f62\u652f\u6301\u5bf9\u8c61\u63d2\u5165\u3001\u79fb\u9664\u548c\u53d8\u6362\uff1b3\uff09\u4f7f\u7528IP-Adapter\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u6761\u4ef6\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u4ea4\u4e92\u5f0f3D\u7f16\u8f91\u4e2d\u4fdd\u6301\u5bf9\u8c61\u8eab\u4efd\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPOCI-Diff\u751f\u6210\u4e0e\u6307\u5b9a3D\u5e03\u5c40\u548c\u7f16\u8f91\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5e03\u5c40\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u53d8\u5f62\u5f15\u8d77\u7684\u51e0\u4f55\u4f2a\u5f71\u3002", "conclusion": "POCI-Diff\u901a\u8fc7\u7edf\u4e00\u7684\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u4e863D\u51e0\u4f55\u7ea6\u675f\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u5177\u6709\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u6027\u76843D\u5e03\u5c40\u63a7\u5236\u4e0e\u7f16\u8f91\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.14066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14066", "abs": "https://arxiv.org/abs/2601.14066", "authors": ["Hendrik M\u00f6ller", "Hanna Schoen", "Robert Graf", "Matan Atad", "Nathan Molinier", "Anjany Sekuboyina", "Bettina K. Budai", "Fabian Bamberg", "Steffen Ringhof", "Christopher Schlett", "Tobias Pischon", "Thoralf Niendorf", "Josua A. Decker", "Marc-Andr\u00e9 Weber", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences", "comment": null, "summary": "The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing \"Vertebra Identification with Anomaly Handling\" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 M\u00f6ller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.", "code_url": "https://github.com/Hendrik-code/spineps", "code_stars": 53, "code_last_update": "2025-12-01", "AI": {"tldr": "VERIDAH\u7b97\u6cd5\u901a\u8fc7\u591a\u5206\u7c7b\u5934\u7ed3\u5408\u52a0\u6743\u690e\u4f53\u5e8f\u5217\u9884\u6d4b\uff0c\u80fd\u81ea\u52a8\u8bc6\u522b\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff0c\u5728T2w\u548cCT\u5f71\u50cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff08\u598211\u621613\u4e2a\u80f8\u690e\u30014\u62166\u4e2a\u8170\u690e\uff09\u5177\u6709\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u4e34\u5e8a\u62a5\u544a\u4e2d\u5e38\u88ab\u5ffd\u89c6\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u8bc6\u522b\u5f02\u5e38\u7684\u80fd\u529b", "method": "\u63d0\u51faVERIDAH\u7b97\u6cd5\uff0c\u57fa\u4e8e\u591a\u5206\u7c7b\u5934\u7ed3\u5408\u52a0\u6743\u690e\u4f53\u5e8f\u5217\u9884\u6d4b\uff0c\u80fd\u5904\u7406\u4efb\u610f\u89c6\u91ce\u56fe\u50cf\u5e76\u81ea\u52a8\u8bc6\u522b\u8ba1\u6570\u5f02\u5e38", "result": "\u5728T2w TSE\u77e2\u72b6\u4f4d\u5f71\u50cf\u4e0a\u6b63\u786e\u6807\u8bb0\u6240\u6709\u690e\u4f53\u7684\u6bd4\u4f8b\u4ece94.24%\u63d0\u5347\u81f398.30%\uff0cCT\u5f71\u50cf\u4e0a\u4ece77.26%\u63d0\u5347\u81f399.18%\uff1b\u80f8\u690e\u8ba1\u6570\u5f02\u5e38\u8bc6\u522b\u7387T2w\u4e3a87.80%\u3001CT\u4e3a96.30%\uff0c\u8170\u690e\u5f02\u5e38\u8bc6\u522b\u7387T2w\u4e3a94.48%\u3001CT\u4e3a97.22%", "conclusion": "VERIDAH\u80fd\u6709\u6548\u81ea\u52a8\u8bc6\u522b\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2601.14079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14079", "abs": "https://arxiv.org/abs/2601.14079", "authors": ["Paul Walker", "James A. D. Gardner", "Andreea Ardelean", "William A. P. Smith", "Bernhard Egger"], "title": "VENI: Variational Encoder for Natural Illumination", "comment": "Project Repo - https://github.com/paul-pw/veni Project page - https://paul-pw.github.io/veni", "summary": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65cb\u8f6c\u7b49\u53d8\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5728\u7403\u9762\u4e0a\u5efa\u6a21\u81ea\u7136\u5149\u7167\uff0c\u907f\u514d\u4f7f\u75282D\u6295\u5f71\uff0c\u901a\u8fc7\u65b0\u578bVN-ViT\u7f16\u7801\u5668\u548c\u65cb\u8f6c\u7b49\u53d8\u6761\u4ef6\u795e\u7ecf\u573a\u89e3\u7801\u5668\u5b9e\u73b0SO(2)-\u7b49\u53d8\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u9006\u6e32\u67d3\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u4e86\u5149\u7167\u73af\u5883\u7684\u7403\u9762\u548c\u65cb\u8f6c\u7b49\u53d8\u7279\u6027\uff0c\u8981\u4e48\u672a\u80fd\u63d0\u4f9b\u826f\u597d\u884c\u4e3a\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301SO(2)-\u7b49\u53d8\u6027\u7684\u73af\u5883\u6620\u5c04\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u65b0\u578b\u5411\u91cf\u795e\u7ecf\u5143\u89c6\u89c9\u53d8\u6362\u5668(VN-ViT)\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u65cb\u8f6c\u7b49\u53d8\u6761\u4ef6\u795e\u7ecf\u573a\u4f5c\u4e3a\u89e3\u7801\u5668\u3002\u5728\u7f16\u7801\u5668\u4e2d\u901a\u8fc7\u65b0\u578bSO(2)-\u7b49\u53d8\u5168\u8fde\u63a5\u5c42\u5c06\u7b49\u53d8\u6027\u4eceSO(3)\u964d\u81f3SO(2)\uff0c\u8fd9\u662f\u5411\u91cf\u795e\u7ecf\u5143\u7684\u6269\u5c55\u3002", "result": "\u63d0\u51fa\u7684SO(2)-\u7b49\u53d8\u5168\u8fde\u63a5\u5c42\u5728SO(2)-\u7b49\u53d8\u6a21\u578b\u4e2d\u4f18\u4e8e\u6807\u51c6\u5411\u91cf\u795e\u7ecf\u5143\u3002\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u8be5\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u63d2\u503c\uff0c\u5e76\u63d0\u4f9b\u66f4\u826f\u597d\u884c\u4e3a\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5efa\u6a21\u4e86\u7403\u9762\u4e0a\u7684\u81ea\u7136\u5149\u7167\uff0c\u4fdd\u6301\u4e86SO(2)-\u7b49\u53d8\u6027\uff0c\u907f\u514d\u4e862D\u6295\u5f71\uff0c\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u4e3a\u9006\u6e32\u67d3\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5148\u9a8c\u3002"}}
{"id": "2601.14084", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14084", "abs": "https://arxiv.org/abs/2601.14084", "authors": ["Abdurrahim Yilmaz", "Ozan Erdem", "Ece Gokyayla", "Ayda Acar", "Burc Bugra Dagtas", "Dilara Ilhan Erdil", "Gulsum Gencoglan", "Burak Temelkuran"], "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning", "comment": null, "summary": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.", "AI": {"tldr": "DermaBench\uff1a\u57fa\u4e8eDDI\u6570\u636e\u96c6\u7684\u76ae\u80a4\u79d1\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b656\u5f20\u4e34\u5e8a\u56fe\u50cf\u300122\u4e2a\u4e3b\u8981\u95ee\u9898\u7c7b\u578b\uff0c\u7ea61.45\u4e07\u6761\u4e13\u5bb6\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u76ae\u80a4\u79d1\u7684\u89c6\u89c9\u7406\u89e3\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u79d1\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u56fe\u50cf\u7ea7\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u75c5\u53d8\u8bc6\u522b\uff09\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u3001\u8bed\u8a00\u57fa\u7840\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5982\u4f55\u89e3\u91ca\u76ae\u80a4\u79d1\u56fe\u50cf\u3001\u63a8\u7406\u7ec6\u7c92\u5ea6\u5f62\u6001\u7279\u5f81\u5e76\u751f\u6210\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u63cf\u8ff0\u3002", "method": "\u57fa\u4e8e\u591a\u6837\u5316\u76ae\u80a4\u79d1\u56fe\u50cf\uff08DDI\uff09\u6570\u636e\u96c6\u6784\u5efaDermaBench\u57fa\u51c6\uff0c\u5305\u542b656\u5f20\u6765\u81ea570\u540d\u4e0d\u540c\u60a3\u8005\u7684\u4e34\u5e8a\u56fe\u50cf\uff0c\u6db5\u76d6Fitzpatrick\u76ae\u80a4\u7c7b\u578bI-VI\u3002\u91c7\u7528\u5206\u5c42\u6807\u6ce8\u65b9\u6848\uff0c\u7531\u76ae\u80a4\u79d1\u4e13\u5bb6\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u8bca\u65ad\u3001\u89e3\u5256\u90e8\u4f4d\u3001\u75c5\u53d8\u5f62\u6001\u3001\u5206\u5e03\u3001\u8868\u9762\u7279\u5f81\u3001\u989c\u8272\u548c\u56fe\u50cf\u8d28\u91cf\u7b49\u65b9\u9762\u7684\u6807\u6ce8\uff0c\u5305\u542b22\u4e2a\u4e3b\u8981\u95ee\u9898\u7c7b\u578b\uff08\u5355\u9009\u3001\u591a\u9009\u548c\u5f00\u653e\u5f0f\uff09\uff0c\u751f\u6210\u7ea614,474\u6761VQA\u98ce\u683c\u6807\u6ce8\u3002\u4ee5\u4ec5\u5143\u6570\u636e\u5f62\u5f0f\u53d1\u5e03\u4ee5\u9075\u5b88\u4e0a\u6e38\u8bb8\u53ef\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86DermaBench\u57fa\u51c6\uff0c\u5305\u542b656\u5f20\u4e34\u5e8a\u56fe\u50cf\u3001570\u540d\u60a3\u8005\u3001\u8986\u76d6\u6240\u6709Fitzpatrick\u76ae\u80a4\u7c7b\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u4ea7\u751f\u4e86\u7ea614,474\u6761\u89c6\u89c9\u95ee\u7b54\u6807\u6ce8\uff0c\u6db5\u76d6\u8bca\u65ad\u3001\u89e3\u5256\u90e8\u4f4d\u3001\u75c5\u53d8\u5f62\u6001\u3001\u5206\u5e03\u3001\u8868\u9762\u7279\u5f81\u3001\u989c\u8272\u3001\u56fe\u50cf\u8d28\u91cf\u7b49\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5305\u542b\u5f00\u653e\u5f0f\u53d9\u8ff0\u63cf\u8ff0\u548c\u603b\u7ed3\u3002", "conclusion": "DermaBench\u586b\u8865\u4e86\u76ae\u80a4\u79d1\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u76ae\u80a4\u79d1\u7684\u89c6\u89c9\u7406\u89e3\u3001\u8bed\u8a00\u57fa\u7840\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u4ee5\u5143\u6570\u636e\u5f62\u5f0f\u5728\u54c8\u4f5bDataverse\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u76ae\u80a4\u79d1AI\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2601.14101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14101", "abs": "https://arxiv.org/abs/2601.14101", "authors": ["Emily Kim", "Allen Wu", "Jessica Hodgins"], "title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition", "comment": null, "summary": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.\n  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.\n  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bfe\u7a0b\u5b66\u4e60\u5728\u8de8\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u822a\u62cd\u6570\u636e\u548c\u771f\u5b9e\u5730\u9762\u6570\u636e\uff0c\u5728\u4e0d\u4f7f\u7528\u771f\u5b9e\u822a\u62cd\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u771f\u5b9e\u822a\u62cd\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5730\u9762\u89c6\u89d2\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u822a\u62cd\u89c6\u89d2\u7b49\u4e0d\u540c\u9886\u57df\u3002\u771f\u5b9e\u822a\u62cd\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u4f7f\u7528\u771f\u5b9e\u822a\u62cd\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u8de8\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a1\uff09\u4e24\u9636\u6bb5\u8bfe\u7a0b\uff08\u76f4\u63a5\u5fae\u8c03\uff09\uff1a\u5148\u5728\u5408\u6210\u822a\u62cd\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u771f\u5b9e\u5730\u9762\u6570\u636e\u4e0a\u5fae\u8c03\uff1b2\uff09\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\uff08\u591a\u9636\u6bb5\u6269\u5c55\uff09\uff1a\u901a\u8fc7\u591a\u4e2a\u9636\u6bb5\u9010\u6b65\u6269\u5c55\u6570\u636e\u96c6\u540e\u518d\u5fae\u8c03\u3002\u4f7f\u7528SlowFast\uff08CNN\uff09\u548cMViTv2\uff08Transformer\uff09\u67b6\u6784\u5728REMAG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u7ed3\u5408\u4e24\u79cd\u57df\u5916\u6570\u636e\u96c6\u660e\u663e\u4f18\u4e8e\u5355\u4e00\u57df\u8bad\u7ec3\u3002\u4e24\u79cd\u8bfe\u7a0b\u7b56\u7565\u5728\u4fdd\u6301top-1\u51c6\u786e\u7387\uff08\u57283%\u8303\u56f4\u5185\uff09\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff1a\u4e24\u9636\u6bb5\u5fae\u8c03\u4f7fSlowFast\u51cf\u5c1137%\u8fed\u4ee3\u6b21\u6570\uff0cMViTv2\u51cf\u5c1130%\uff1b\u6e10\u8fdb\u5f0f\u65b9\u6cd5\u8fdb\u4e00\u6b65\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff08SlowFast 9%\uff0cMViTv2 30%\uff09\u3002", "conclusion": "\u8bfe\u7a0b\u5b66\u4e60\u5728\u8de8\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u901a\u8fc7\u5408\u7406\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u771f\u5b9e\u822a\u62cd\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u8de8\u57df\u8bc6\u522b\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14103", "abs": "https://arxiv.org/abs/2601.14103", "authors": ["Xiaolu Liu", "Yicong Li", "Qiyuan He", "Jiayin Zhu", "Wei Ji", "Angela Yao", "Jianke Zhu"], "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing", "comment": "22 pages, 12 figures", "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.", "code_url": "https://github.com/xiaolul2/Interp3D", "code_stars": 2, "code_last_update": "2026-01-21", "AI": {"tldr": "Interp3D\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e09\u7ef4\u7eb9\u7406\u53d8\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5148\u9a8c\u548c\u6e10\u8fdb\u5bf9\u9f50\u7b56\u7565\u5b9e\u73b0\u51e0\u4f55\u4fdd\u771f\u548c\u7eb9\u7406\u8fde\u8d2f\u7684\u5e73\u6ed1\u4e09\u7ef4\u8d44\u4ea7\u8fc7\u6e21\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u5904\u7406\u51e0\u4f55\u5f62\u72b6\u5ffd\u7565\u7eb9\u7406\uff0c\u8981\u4e48\u5c062D\u63d2\u503c\u7b56\u7565\u6269\u5c55\u52303D\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u3001\u7ed3\u6784\u9519\u4f4d\u548c\u7eb9\u7406\u6a21\u7cca\u3002\u9700\u8981\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u7eb9\u7406\u5bf9\u9f50\u548c\u8fc7\u6e21\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faInterp3D\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u5148\u9a8c\u548c\u6e10\u8fdb\u5bf9\u9f50\u539f\u5219\uff1a1\uff09\u6761\u4ef6\u7a7a\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u63d2\u503c\uff1b2\uff09SLAT\u5f15\u5bfc\u7684\u7ed3\u6784\u63d2\u503c\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff1b3\uff09\u7ec6\u7c92\u5ea6\u7eb9\u7406\u878d\u5408\u4f20\u9012\u5916\u89c2\u7ec6\u8282\u3002", "result": "\u6784\u5efa\u4e86\u5206\u7ea7\u96be\u5ea6\u6570\u636e\u96c6Interp3DData\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u7814\u7a76\u5747\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u3001\u8fc7\u6e21\u5e73\u6ed1\u6027\u548c\u5408\u7406\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "Interp3D\u901a\u8fc7\u8054\u5408\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u7eb9\u7406\u53d8\u5f62\uff0c\u4e3a\u52a8\u753b\u3001\u7f16\u8f91\u548c\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14111", "abs": "https://arxiv.org/abs/2601.14111", "authors": ["Jiaying Wu", "Can Gao", "Jinglu Hu", "Hui Li", "Xiaofeng Cao", "Jingcai Guo"], "title": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning", "comment": null, "summary": "Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D", "AI": {"tldr": "PMCE\uff1a\u4e00\u4e2a\u6982\u7387\u6027\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8bed\u4e49\u548c\u5b57\u5e55\u5f15\u5bfc\u589e\u5f3a\u6765\u6539\u5584\u539f\u578b\u4f30\u8ba1\uff0c\u5229\u7528\u57fa\u7840\u7c7b\u77e5\u8bc6\u5e93\u548cBLIP\u5b57\u5e55\u5668\u63d0\u5347\u5c11\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u4ece\u5c11\u91cf\u6807\u8bb0\u6837\u672c\u4f30\u8ba1\u7684\u539f\u578b\u5f80\u5f80\u5b58\u5728\u504f\u5dee\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u73b0\u6709\u7684\u8bed\u4e49\u65b9\u6cd5\u4e3b\u8981\u5e94\u7528\u4e8e\u652f\u6301\u96c6\u4fa7\uff0c\u800c\u67e5\u8be2\u8868\u793a\u4fdd\u6301\u4e0d\u53d8\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u5229\u7528\u591a\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u6539\u5584\u539f\u578b\u4f30\u8ba1\u3002", "method": "1. \u6784\u5efa\u975e\u53c2\u6570\u77e5\u8bc6\u5e93\uff1a\u5b58\u50a8\u57fa\u7840\u7c7b\u522b\u7684\u89c6\u89c9\u7edf\u8ba1\u4fe1\u606f\u548cCLIP\u7f16\u7801\u7684\u7c7b\u522b\u540d\u79f0\u5d4c\u5165\u30022. \u5143\u6d4b\u8bd5\u65f6\u68c0\u7d22\uff1a\u57fa\u4e8e\u7c7b\u522b\u540d\u79f0\u5d4c\u5165\u76f8\u4f3c\u6027\u4e3a\u6bcf\u4e2a\u65b0\u7c7b\u522b\u68c0\u7d22\u6700\u76f8\u5173\u7684\u57fa\u7840\u7c7b\u30023. \u5148\u9a8c\u4fe1\u606f\u878d\u5408\uff1a\u5c06\u68c0\u7d22\u5230\u7684\u7edf\u8ba1\u4fe1\u606f\u805a\u5408\u4e3a\u7c7b\u522b\u7279\u5b9a\u5148\u9a8c\uff0c\u901a\u8fc7MAP\u66f4\u65b0\u4e0e\u652f\u6301\u96c6\u539f\u578b\u878d\u5408\u30024. \u5b57\u5e55\u5f15\u5bfc\u589e\u5f3a\uff1a\u4f7f\u7528\u51bb\u7ed3\u7684BLIP\u5b57\u5e55\u5668\u63d0\u4f9b\u65e0\u6807\u7b7e\u5b9e\u4f8b\u7ea7\u56fe\u50cf\u63cf\u8ff0\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u589e\u5f3a\u5668\u5728\u57fa\u7840\u7c7b\u4e0a\u8bad\u7ec3\uff0c\u4f18\u5316\u652f\u6301\u539f\u578b\u548c\u67e5\u8be2\u7279\u5f81\uff0c\u91c7\u7528\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7a33\u5b9a\u566a\u58f0\u5b57\u5e55\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cPMCE\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728MiniImageNet\u76841-shot\u8bbe\u7f6e\u4e2d\uff0c\u6bd4\u6700\u5f3a\u7684\u8bed\u4e49\u7ade\u4e89\u5bf9\u624b\u83b7\u5f97\u4e86\u9ad8\u8fbe7.71%\u7684\u7edd\u5bf9\u589e\u76ca\u63d0\u5347\u3002", "conclusion": "PMCE\u901a\u8fc7\u7ed3\u5408\u591a\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u548c\u5b57\u5e55\u5f15\u5bfc\u589e\u5f3a\uff0c\u6709\u6548\u6539\u5584\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u539f\u578b\u4f30\u8ba1\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u57fa\u7840\u7c7b\u77e5\u8bc6\u5e93\u63d0\u4f9b\u7c7b\u522b\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u5b57\u5e55\u63cf\u8ff0\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.14127", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14127", "abs": "https://arxiv.org/abs/2601.14127", "authors": ["Renmiao Chen", "Yida Lu", "Shiyao Cui", "Xuan Ouyang", "Victor Shea-Jay Huang", "Shumin Zhang", "Chengwei Pan", "Han Qiu", "Minlie Huang"], "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning", "comment": "*15 pages, 5 figures. Introduces MIR-SafetyBench (2,676 instances; 9 multi-image relations). Equal contribution; \u2020Corresponding author. Code/data: https://github.com/thu-coai/MIR-SafetyBench", "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.", "code_url": "https://github.com/thu-coai/MIR-SafetyBench", "code_stars": 0, "code_last_update": "2026-01-19", "AI": {"tldr": "MIR-SafetyBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u591a\u56fe\u50cf\u63a8\u7406\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,676\u4e2a\u5b9e\u4f8b\u548c9\u79cd\u591a\u56fe\u50cf\u5173\u7cfb\u5206\u7c7b\uff0c\u8bc4\u4f30\u53d1\u73b0\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u5728\u5b89\u5168\u6027\u65b9\u9762\u66f4\u8106\u5f31\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u590d\u6742\u6307\u4ee4\u5904\u7406\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u589e\u5f3a\uff0c\u8fd9\u79cd\u8fdb\u6b65\u53ef\u80fd\u5e26\u6765\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u591a\u56fe\u50cf\u63a8\u7406\u573a\u666f\u4e0b\u7684\u6a21\u578b\u5b89\u5168\u6027\u3002", "method": "\u6784\u5efaMIR-SafetyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,676\u4e2a\u5b9e\u4f8b\u548c9\u79cd\u591a\u56fe\u50cf\u5173\u7cfb\u5206\u7c7b\uff0c\u5bf919\u4e2aMLLM\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5206\u6790\u653b\u51fb\u6210\u529f\u7387\u3001\u5b89\u5168\u54cd\u5e94\u8d28\u91cf\u4ee5\u53ca\u6ce8\u610f\u529b\u71b5\u7279\u5f81\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff1a\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u5728MIR-SafetyBench\u4e0a\u66f4\u8106\u5f31\uff1b\u8bb8\u591a\u6807\u8bb0\u4e3a\u5b89\u5168\u7684\u54cd\u5e94\u662f\u80a4\u6d45\u7684\uff0c\u6e90\u4e8e\u8bef\u89e3\u6216\u56de\u907f\u6027\u56de\u7b54\uff1b\u4e0d\u5b89\u5168\u751f\u6210\u7684\u5e73\u5747\u6ce8\u610f\u529b\u71b5\u4f4e\u4e8e\u5b89\u5168\u751f\u6210\u3002", "conclusion": "\u6a21\u578b\u53ef\u80fd\u5728\u8fc7\u5ea6\u4e13\u6ce8\u4e8e\u4efb\u52a1\u89e3\u51b3\u65f6\u5ffd\u89c6\u5b89\u5168\u7ea6\u675f\uff0c\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u53ef\u80fd\u4f34\u968f\u5b89\u5168\u8106\u5f31\u6027\u589e\u52a0\uff0c\u9700\u8981\u5173\u6ce8\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u5b89\u5168\u7ea6\u675f\u7684\u5e73\u8861\u3002"}}
{"id": "2601.14154", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14154", "abs": "https://arxiv.org/abs/2601.14154", "authors": ["Shubham Pandey", "Bhavin Jawade", "Srirangaraj Setlur", "Venu Govindaraju", "Kenneth Seastedt"], "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery", "comment": "Accepted to P2P-CV @ WACV 2026", "summary": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.", "AI": {"tldr": "MIRACLE\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u672f\u524d\u4e34\u5e8a\u548c\u653e\u5c04\u5b66\u6570\u636e\u6765\u9884\u6d4b\u80ba\u764c\u624b\u672f\u672f\u540e\u5e76\u53d1\u75c7\u98ce\u9669\uff0c\u91c7\u7528\u8d85\u7403\u9762\u5d4c\u5165\u7a7a\u95f4\u878d\u5408\u5f02\u8d28\u8f93\u5165\uff0c\u5e76\u5305\u542b\u5e72\u9884\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\u4ee5\u63d0\u9ad8\u9884\u6d4b\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u672f\u540e\u5e76\u53d1\u75c7\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u9884\u540e\u5e76\u589e\u52a0\u533b\u7597\u6210\u672c\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u6539\u5584\u80ba\u764c\u624b\u672f\u60a3\u8005\u7684\u98ce\u9669\u7ba1\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51faMIRACLE\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u91c7\u7528\u8d85\u7403\u9762\u5d4c\u5165\u7a7a\u95f4\u878d\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u8bb0\u5f55\u548c\u9ad8\u7ef4\u653e\u5c04\u5b66\u56fe\u50cf\uff0c\u5e76\u5305\u542b\u5e72\u9884\u6027\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u5141\u8bb8\u4e34\u5e8a\u4e13\u5bb6\u57fa\u4e8e\u4e13\u4e1a\u77e5\u8bc6\u4ea4\u4e92\u8c03\u6574\u63a8\u8350\u3002", "result": "\u5728\u5305\u542b3,094\u540d\u80ba\u764c\u624b\u672f\u60a3\u8005\u7684POC-L\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMIRACLE\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\uff0c\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u548c\u53ef\u89e3\u91ca\u7684\u672f\u540e\u98ce\u9669\u7ba1\u7406\u3002", "conclusion": "MIRACLE\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u4e3a\u80ba\u764c\u624b\u672f\u672f\u540e\u5e76\u53d1\u75c7\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.14180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14180", "abs": "https://arxiv.org/abs/2601.14180", "authors": ["Yichao Liu", "Yueyang Teng", "Junwen Guo"], "title": "Progressive self-supervised blind-spot denoising method for LDCT denoising", "comment": null, "summary": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528\u4f4e\u5242\u91cfCT\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u76f2\u70b9\u53bb\u566a\u673a\u5236\u548c\u566a\u58f0\u6b63\u5219\u5316\uff0c\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u83b7\u53d6\u914d\u5bf9\u7684\u6b63\u5e38\u5242\u91cfCT\u6570\u636e\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u4f4e\u5242\u91cfCT\u56fe\u50cf\u53bb\u566a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4ec5\u4f7f\u7528\u4f4e\u5242\u91cfCT\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u542b\u6e10\u8fdb\u5f0f\u76f2\u70b9\u53bb\u566a\u673a\u5236\uff08\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u53bb\u566a\u5b66\u4e60\uff09\u548c\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u4f5c\u4e3a\u6b63\u5219\u5316\u4ee5\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728Mayo\u4f4e\u5242\u91cfCT\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u591a\u4e2a\u4ee3\u8868\u6027\u76d1\u7763\u53bb\u566a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u5242\u91cfCT\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u914d\u5bf9\u6b63\u5e38\u5242\u91cfCT\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.14188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14188", "abs": "https://arxiv.org/abs/2601.14188", "authors": ["Liang Shi", "Wei Li", "Kevin M Beussman", "Lin Chen", "Yun Fu"], "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models", "comment": null, "summary": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.", "AI": {"tldr": "IIR-VLM\uff1a\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4e13\u5bb6\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b9e\u4f8b\u7ea7\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e00\u6b21\u6027\u5b66\u4e60\u65b0\u5b9e\u4f8b\u5e76\u8fdb\u884c\u5b9e\u4f8b\u611f\u77e5\u7684\u89c6\u89c9\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fdc\u4f4e\u4e8e\u9886\u57df\u7279\u5b9a\u7684ILR\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86VLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bc6\u522b\u719f\u6089\u4eba\u7269\u548c\u7269\u4f53\u7684\u573a\u666f\u4e2d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u9010\u4e2a\u5b66\u4e60\u5b9e\u4f8b\uff0c\u6570\u636e\u6536\u96c6\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4e14\u96be\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u533a\u5206\u3002", "method": "\u63d0\u51faIIR-VLM\u6846\u67b6\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4e13\u5bb6\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u63d0\u4f9b\u4e13\u95e8\u7684\u7279\u5f81\u7528\u4e8e\u5b66\u4e60\u591a\u6837\u5b9e\u4f8b\u3002\u8be5\u65b9\u6cd5\u4f7fVLM\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ee5\u4e00\u6b21\u6027\u65b9\u5f0f\u5b66\u4e60\u65b0\u5b9e\u4f8b\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u8fdb\u884c\u5b9e\u4f8b\u611f\u77e5\u7684\u89c6\u89c9\u7406\u89e3\u3002", "result": "\u5728\u73b0\u6709\u7684\u5b9e\u4f8b\u4e2a\u6027\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86IIR-VLM\u7684\u6709\u6548\u6027\u3002\u5728\u4e00\u4e2a\u5305\u542b\u4e0d\u540c\u96be\u5ea6\u548c\u591a\u6837\u7c7b\u522b\uff08\u4eba\u7269\u3001\u9762\u90e8\u3001\u5ba0\u7269\u548c\u901a\u7528\u7269\u4f53\uff09\u7684\u65b0\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "IIR-VLM\u901a\u8fc7\u96c6\u6210ILR\u4e13\u5bb6\u6a21\u578b\u6210\u529f\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e00\u6b21\u6027\u5b9e\u4f8b\u5b66\u4e60\u548c\u5b9e\u4f8b\u611f\u77e5\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u4e3aVLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u4f8b\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14246", "abs": "https://arxiv.org/abs/2601.14246", "authors": ["Zeyuan Chen", "Kai Zhang", "Zhuowen Tu", "Yuanjun Xiong"], "title": "Soft Tail-dropping for Adaptive Visual Tokenization", "comment": null, "summary": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.", "AI": {"tldr": "STAT\u662f\u4e00\u79cd1D\u79bb\u6563\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u80fd\u6839\u636e\u56fe\u50cf\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u7ec6\u8282\u6c34\u5e73\u81ea\u9002\u5e94\u9009\u62e9\u8f93\u51fatoken\u6570\u91cf\uff0c\u4e3a\u56e0\u679c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u957f\u5ea6\u81ea\u9002\u5e94\u76841D\u89c6\u89c9token\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u7684token\u5e8f\u5217\uff0c\u65e0\u6cd5\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574token\u6570\u91cf\u3002\u8fd9\u9650\u5236\u4e86\u56e0\u679c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u5904\u7406\u957f\u5ea6\u53ef\u53d8\u7684\u5e8f\u5217\u3002", "method": "STAT\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u79bb\u6563\u4ee3\u7801\u5e8f\u5217\u53ca\u6bcf\u4e2atoken\u7684\u4fdd\u7559\u6982\u7387\u3002\u9664\u4e86\u6807\u51c6\u81ea\u7f16\u7801\u5668\u76ee\u6807\u5916\uff0c\u8fd8\u6b63\u5219\u5316\u4fdd\u7559\u6982\u7387\u4f7f\u5176\u6cbf\u5e8f\u5217\u5355\u8c03\u9012\u51cf\uff0c\u5e76\u663e\u5f0f\u5bf9\u9f50\u5176\u5206\u5e03\u4e0e\u56fe\u50cf\u7ea7\u590d\u6742\u5ea6\u5ea6\u91cf\u3002", "result": "\u5728ImageNet-1k\u4e0a\uff0c\u914d\u5907STAT\u7684\u56e0\u679c\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u6bd4\u5176\u4ed6\u6982\u7387\u6a21\u578b\u5bb6\u65cf\u5c55\u73b0\u51fa\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u89c6\u89c9\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u5148\u524dvanilla AR\u89c6\u89c9\u751f\u6210\u5c1d\u8bd5\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7684\u826f\u597d\u7f29\u653e\u884c\u4e3a\u3002", "conclusion": "STAT\u901a\u8fc7\u81ea\u9002\u5e94token\u5316\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u56e0\u679c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e8f\u5217\u957f\u5ea6\u95ee\u9898\uff0c\u4e3a\u8fd9\u7c7b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\uff0c\u4f7f\u5176\u5728\u751f\u6210\u8d28\u91cf\u548c\u7f29\u653e\u884c\u4e3a\u65b9\u9762\u8fbe\u5230\u65b0\u6c34\u5e73\u3002"}}
{"id": "2601.14250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14250", "abs": "https://arxiv.org/abs/2601.14250", "authors": ["Pengze Zhang", "Yanze Wu", "Mengtian Li", "Xu Bai", "Songtao Zhao", "Fulong Ye", "Chong Mou", "Xinghui Li", "Zhuowei Chen", "Qian He", "Mingyuan Gao"], "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "comment": "Github Page: https://pangzecheung.github.io/OmniTransfer/", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "AI": {"tldr": "OmniTransfer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u7a7a\u89c6\u9891\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u4fe1\u606f\u589e\u5f3a\u5916\u89c2\u4e00\u81f4\u6027\uff0c\u5229\u7528\u65f6\u5e8f\u7ebf\u7d22\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a7\u5236\uff0c\u5728\u5404\u79cd\u89c6\u9891\u8fc1\u79fb\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u6216\u4efb\u52a1\u7279\u5b9a\u7684\u65f6\u5e8f\u5148\u9a8c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u56fa\u6709\u7684\u4e30\u5bcc\u65f6\u7a7a\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faOmniTransfer\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u4efb\u52a1\u611f\u77e5\u4f4d\u7f6e\u504f\u7f6e\u81ea\u9002\u5e94\u5229\u7528\u53c2\u8003\u89c6\u9891\u4fe1\u606f\uff1b\u53c2\u8003\u89e3\u8026\u56e0\u679c\u5b66\u4e60\u5206\u79bb\u53c2\u8003\u548c\u76ee\u6807\u5206\u652f\uff1b\u4efb\u52a1\u81ea\u9002\u5e94\u591a\u6a21\u6001\u5bf9\u9f50\u4f7f\u7528\u591a\u6a21\u6001\u8bed\u4e49\u6307\u5bfc\u52a8\u6001\u533a\u5206\u4e0d\u540c\u4efb\u52a1\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0cOmniTransfer\u5728\u5916\u89c2\uff08ID\u548c\u98ce\u683c\uff09\u548c\u65f6\u5e8f\u8fc1\u79fb\uff08\u76f8\u673a\u8fd0\u52a8\u548c\u89c6\u9891\u6548\u679c\uff09\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u8fd0\u52a8\u8fc1\u79fb\u4e2d\u4e0d\u4f7f\u7528\u59ff\u6001\u4e5f\u80fd\u5339\u914d\u59ff\u6001\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "OmniTransfer\u4e3a\u7075\u6d3b\u3001\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5145\u5206\u5229\u7528\u89c6\u9891\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5916\u89c2\u4e00\u81f4\u6027\u548c\u65f6\u5e8f\u63a7\u5236\u3002"}}
{"id": "2601.14251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14251", "abs": "https://arxiv.org/abs/2601.14251", "authors": ["Said Taghadouini", "Adrien Cavaill\u00e8s", "Baptiste Aubertin"], "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR", "comment": null, "summary": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.", "AI": {"tldr": "LightOnOCR-2-1B\u662f\u4e00\u4e2a10\u4ebf\u53c2\u6570\u7684\u7aef\u5230\u7aef\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u6587\u6863\u56fe\u50cf\u76f4\u63a5\u8f6c\u6362\u4e3a\u5e72\u51c0\u3001\u81ea\u7136\u6392\u5e8f\u7684\u6587\u672c\uff0c\u65e0\u9700\u4f20\u7edfOCR\u6d41\u7a0b\u3002\u8be5\u6a21\u578b\u5728\u5305\u542b\u626b\u63cf\u4ef6\u3001\u6cd5\u8bed\u6587\u6863\u548c\u79d1\u5b66PDF\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u84b8\u998f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728OlmOCR-Bench\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u6bd4\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u5c0f9\u500d\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edfOCR\u6d41\u7a0b\u901a\u5e38\u8106\u5f31\u4e14\u590d\u6742\uff0c\u9700\u8981\u591a\u4e2a\u5904\u7406\u6b65\u9aa4\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u6587\u6863\u56fe\u50cf\u751f\u6210\u5e72\u51c0\u3001\u81ea\u7136\u6392\u5e8f\u7684\u6587\u672c\uff0c\u907f\u514d\u4f20\u7edfOCR\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u652f\u6301\u591a\u8bed\u8a00\u6587\u6863\u5904\u7406\u3002", "method": "1. \u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u84b8\u998f\u6570\u636e\u96c6\uff0c\u8986\u76d6\u626b\u63cf\u4ef6\u3001\u6cd5\u8bed\u6587\u6863\u548c\u79d1\u5b66PDF\uff1b2. \u91c7\u75281B\u53c2\u6570\u7684\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00\u67b6\u6784\uff1b3. \u6269\u5c55\u8f93\u51fa\u683c\u5f0f\u4ee5\u9884\u6d4b\u5d4c\u5165\u5f0f\u56fe\u50cf\u7684\u5f52\u4e00\u5316\u8fb9\u754c\u6846\uff1b4. \u901a\u8fc7resume\u7b56\u7565\u5728\u9884\u8bad\u7ec3\u4e2d\u5f15\u5165\u5b9a\u4f4d\u80fd\u529b\uff1b5. \u4f7f\u7528\u57fa\u4e8eIoU\u5956\u52b1\u7684RLVR\u8fdb\u884c\u7ec6\u5316\uff1b6. \u91c7\u7528\u68c0\u67e5\u70b9\u5e73\u5747\u548c\u4efb\u52a1\u7b97\u672f\u5408\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728OlmOCR-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u5c0f9\u500d\u4e14\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5d4c\u5165\u5f0f\u56fe\u50cf\u7684\u8fb9\u754c\u6846\u4f4d\u7f6e\u3002\u53d1\u5e03\u4e86Apache 2.0\u8bb8\u53ef\u7684\u6a21\u578b\u68c0\u67e5\u70b9\u3001\u6570\u636e\u96c6\u548cLightOnOCR-bbox-bench\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "LightOnOCR-2-1B\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863OCR\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7387\u7684\u591a\u8bed\u8a00\u6587\u6863\u5904\u7406\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5f00\u6e90\u6a21\u578b\u548c\u8bc4\u4f30\u57fa\u51c6\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2601.14253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14253", "abs": "https://arxiv.org/abs/2601.14253", "authors": ["Hongyuan Chen", "Xingyu Chen", "Youjia Zhang", "Zexiang Xu", "Anpei Chen"], "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis", "comment": "Project page: https://motion3-to-4.github.io/. Code: https://github.com/Inception3D/Motion324", "summary": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.", "code_url": "https://motion3-to-4.github.io/", "AI": {"tldr": "Motion 3-to-4\u662f\u4e00\u4e2a\u524d\u9988\u6846\u67b6\uff0c\u53ef\u4ece\u5355\u76ee\u89c6\u9891\u548c\u53ef\u9009\u76843D\u53c2\u8003\u7f51\u683c\u5408\u6210\u9ad8\u8d28\u91cf4D\u52a8\u6001\u5bf9\u8c61\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u9759\u60013D\u5f62\u72b6\u751f\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\u6765\u89e3\u51b34D\u5408\u6210\u7684\u6311\u6218\u3002", "motivation": "4D\u5408\u6210\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u4ece\u5355\u76ee\u89c6\u89d2\u6062\u590d\u51e0\u4f55\u4e0e\u8fd0\u52a8\u5b58\u5728\u56fa\u6709\u6a21\u7cca\u6027\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u57282D\u3001\u89c6\u9891\u548c3D\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u57284D\u5408\u6210\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u89c4\u8303\u53c2\u8003\u7f51\u683c\uff0c\u6a21\u578b\u5b66\u4e60\u7d27\u51d1\u7684\u8fd0\u52a8\u6f5c\u5728\u8868\u793a\u5e76\u9884\u6d4b\u6bcf\u5e27\u9876\u70b9\u8f68\u8ff9\u4ee5\u6062\u590d\u5b8c\u6574\u7684\u65f6\u95f4\u76f8\u5e72\u51e0\u4f55\uff1b\u91c7\u7528\u53ef\u6269\u5c55\u7684\u9010\u5e27transformer\u5b9e\u73b0\u5bf9\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5177\u6709\u7cbe\u786e\u5730\u9762\u771f\u5b9e\u51e0\u4f55\u7684\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cMotion 3-to-4\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u5177\u6709\u66f4\u4f18\u7684\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "Motion 3-to-4\u901a\u8fc7\u5c064D\u5408\u6210\u5206\u89e3\u4e3a\u9759\u6001\u5f62\u72b6\u751f\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6709\u9650\u548c\u5355\u76ee\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76844D\u52a8\u6001\u5bf9\u8c61\u5408\u6210\u3002"}}
{"id": "2601.14255", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14255", "abs": "https://arxiv.org/abs/2601.14255", "authors": ["Sangbeom Lim", "Seoung Wug Oh", "Jiahui Huang", "Heeji Yoon", "Seungryong Kim", "Joon-Young Lee"], "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior", "comment": "Project page: https://cvlab-kaist.github.io/VideoMaMa/", "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.", "AI": {"tldr": "VideoMaMa\u6a21\u578b\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u7c97\u7cd9\u5206\u5272\u63a9\u7801\u8f6c\u6362\u4e3a\u7cbe\u786ealpha\u906e\u7f69\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u771f\u5b9e\u89c6\u9891\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b5\u4e07+\u771f\u5b9e\u89c6\u9891\u7684MA-V\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684SAM2-Matte\u6a21\u578b\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89c6\u9891\u62a0\u56fe\u6a21\u578b\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u6cdb\u5316\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u590d\u6742\u573a\u666f\u548c\u8fd0\u52a8\u3002", "method": "\u63d0\u51faVideoMaMa\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u7c97\u7cd9\u5206\u5272\u63a9\u7801\u8f6c\u6362\u4e3a\u7cbe\u786ealpha\u906e\u7f69\uff1b\u57fa\u4e8e\u6b64\u6784\u5efa\u53ef\u6269\u5c55\u7684\u4f2a\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u521b\u5efa\u5927\u89c4\u6a21\u89c6\u9891\u62a0\u56fe\u6570\u636e\u96c6MA-V\uff1b\u4f7f\u7528MA-V\u5fae\u8c03SAM2\u6a21\u578b\u5f97\u5230SAM2-Matte\u3002", "result": "VideoMaMa\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u771f\u5b9e\u89c6\u9891\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff1b\u6784\u5efa\u7684MA-V\u6570\u636e\u96c6\u5305\u542b5\u4e07+\u771f\u5b9e\u89c6\u9891\u7684\u9ad8\u8d28\u91cf\u906e\u7f69\u6807\u6ce8\uff1bSAM2-Matte\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u9c81\u68d2\u6027\u4f18\u4e8e\u4f7f\u7528\u73b0\u6709\u62a0\u56fe\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u76f8\u540c\u6a21\u578b\u3002", "conclusion": "\u5927\u89c4\u6a21\u4f2a\u6807\u6ce8\u89c6\u9891\u62a0\u56fe\u6570\u636e\u5bf9\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u751f\u6210\u5148\u9a8c\u548c\u6613\u83b7\u53d6\u7684\u5206\u5272\u7ebf\u7d22\u80fd\u591f\u63a8\u52a8\u89c6\u9891\u62a0\u56fe\u7814\u7a76\u7684\u53ef\u6269\u5c55\u8fdb\u5c55\u3002"}}
{"id": "2601.12781", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12781", "abs": "https://arxiv.org/abs/2601.12781", "authors": ["Hyejin Park", "Junhyuk Kwon", "Suha Kwak", "Jungseul Ok"], "title": "VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension", "comment": null, "summary": "Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.", "AI": {"tldr": "VIRO\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u5230\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u6b65\u9aa4\u4e2d\uff0c\u89e3\u51b3REC\u4e2d\u56e0\u4e2d\u95f4\u6b65\u9aa4\u9519\u8bef\u5bfc\u81f4\u7684\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u5728\u76ee\u6807\u5b58\u5728\u548c\u4e0d\u5b58\u5728\u573a\u666f\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7REC\u65b9\u6cd5\u4f9d\u8d56LLM\u548cVLM\u8fdb\u884c\u7ec4\u5408\u63a8\u7406\uff0c\u4f46\u5047\u8bbe\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u51c6\u786e\uff0c\u5bfc\u81f4\u7ea7\u8054\u9519\u8bef\uff1a\u9519\u8bef\u68c0\u6d4b\u548c\u65e0\u6548\u5173\u7cfb\u5728\u63a8\u7406\u94fe\u4e2d\u4f20\u64ad\uff0c\u5373\u4f7f\u56fe\u50cf\u4e2d\u6ca1\u6709\u76ee\u6807\u4e5f\u4f1a\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5047\u9633\u6027\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u9a8c\u8bc1\u96c6\u6210\u63a8\u7406\u7b97\u5b50\uff08VIRO\uff09\u6846\u67b6\uff0c\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u5d4c\u5165\u8f7b\u91cf\u7ea7\u7684\u7b97\u5b50\u7ea7\u9a8c\u8bc1\u5668\u3002\u6bcf\u4e2a\u7b97\u5b50\u6267\u884c\u5e76\u9a8c\u8bc1\u5176\u8f93\u51fa\uff08\u5982\u5bf9\u8c61\u5b58\u5728\u6027\u6216\u7a7a\u95f4\u5173\u7cfb\uff09\uff0c\u5f53\u9a8c\u8bc1\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u80fd\u591f\u9c81\u68d2\u5730\u5904\u7406\u65e0\u76ee\u6807\u60c5\u51b5\u3002", "result": "\u5728\u76ee\u6807\u5b58\u5728\u548c\u65e0\u76ee\u6807\u8bbe\u7f6e\u4e0b\u8fbe\u523061.1%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u4e0a\u5c55\u793a\u6cdb\u5316\u80fd\u529b\uff1b\u8ba1\u7b97\u6548\u7387\u9ad8\uff08\u541e\u5410\u91cf\uff09\u3001\u53ef\u9760\u6027\u5f3a\uff08\u7a0b\u5e8f\u5931\u8d25\u7387\u4f4e\u4e8e0.3%\uff09\uff0c\u901a\u8fc7\u89e3\u8026\u7a0b\u5e8f\u751f\u6210\u4e0e\u6267\u884c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "VIRO\u901a\u8fc7\u96c6\u6210\u9a8c\u8bc1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7b26\u53f7REC\u4e2d\u7684\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u89e3\u91ca\u6027\u63a8\u7406\u548c\u96f6\u6837\u672c\u6cdb\u5316\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u3001\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
